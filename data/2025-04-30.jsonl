{"id": "2504.20049", "pdf": "https://arxiv.org/pdf/2504.20049", "abs": "https://arxiv.org/abs/2504.20049", "authors": ["Marina Mayor-Rocher", "Cristina Pozo", "Nina Melero", "Gonzalo Martínez", "María Grandury", "Pedro Reviriego"], "title": "It's the same but not the same: Do LLMs distinguish Spanish varieties?", "categories": ["cs.CL"], "comment": "in Spanish language", "summary": "In recent years, large language models (LLMs) have demonstrated a high\ncapacity for understanding and generating text in Spanish. However, with five\nhundred million native speakers, Spanish is not a homogeneous language but\nrather one rich in diatopic variations spanning both sides of the Atlantic. For\nthis reason, in this study, we evaluate the ability of nine language models to\nidentify and distinguish the morphosyntactic and lexical peculiarities of seven\nvarieties of Spanish (Andean, Antillean, Continental Caribbean, Chilean,\nPeninsular, Mexican and Central American and Rioplatense) through a\nmultiple-choice test. The results indicate that the Peninsular Spanish variety\nis the best identified by all models and that, among them, GPT-4o is the only\nmodel capable of recognizing the variability of the Spanish language.\n  --\n  En los \\'ultimos a\\~nos, los grandes modelos de lenguaje (LLMs, por sus\nsiglas en ingl\\'es) han demostrado una alta capacidad para comprender y generar\ntexto en espa\\~nol. Sin embargo, con quinientos millones de hablantes nativos,\nla espa\\~nola no es una lengua homog\\'enea, sino rica en variedades\ndiat\\'opicas que se extienden a ambos lados del Atl\\'antico. Por todo ello,\nevaluamos en este trabajo la capacidad de nueve modelos de lenguaje de\nidentificar y discernir las peculiaridades morfosint\\'acticas y l\\'exicas de\nsiete variedades de espa\\~nol (andino, antillano, caribe\\~no continental,\nchileno, espa\\~nol peninsular, mexicano y centroamericano y rioplatense)\nmediante un test de respuesta m\\'ultiple. Los resultados obtenidos indican que\nla variedad de espa\\~nol peninsular es la mejor identificada por todos los\nmodelos y que, de entre todos, GPT-4o es el \\'unico modelo capaz de identificar\nla variabilidad de la lengua espa\\~nola."}
{"id": "2504.20334", "pdf": "https://arxiv.org/pdf/2504.20334", "abs": "https://arxiv.org/abs/2504.20334", "authors": ["Yuzhe Liang", "Wenzhe Liu", "Chunyu Qiang", "Zhikang Niu", "Yushen Chen", "Ziyang Ma", "Wenxi Chen", "Nan Li", "Chen Zhang", "Xie Chen"], "title": "Towards Flow-Matching-based TTS without Classifier-Free Guidance", "categories": ["eess.AS"], "comment": null, "summary": "Flow matching has demonstrated strong generative capabilities and has become\na core component in modern Text-to-Speech (TTS) systems. To ensure high-quality\nspeech synthesis, Classifier-Free Guidance (CFG) is widely used during the\ninference of flow-matching-based TTS models. However, CFG incurs substantial\ncomputational cost as it requires two forward passes, which hinders its\napplicability in real-time scenarios. In this paper, we explore removing CFG\nfrom flow-matching-based TTS models to improve inference efficiency, while\nmaintaining performance. Specifically, we reformulated the flow matching\ntraining target to directly approximate the CFG optimization trajectory. This\ntraining method eliminates the need for unconditional model evaluation and\nguided tuning during inference, effectively cutting the computational overhead\nin half. Furthermore, It can be seamlessly integrated with existing optimized\nsampling strategies. We validate our approach using the F5-TTS model on the\nLibriTTS dataset. Experimental results show that our method achieves a\n9$\\times$ inference speed-up compared to the baseline F5-TTS, while preserving\ncomparable speech quality. We will release the code and models to support\nreproducibility and foster further research in this area."}
{"id": "2504.20051", "pdf": "https://arxiv.org/pdf/2504.20051", "abs": "https://arxiv.org/abs/2504.20051", "authors": ["Frances Laureano De Leon", "Harish Tayyar Madabushi", "Mark G. Lee"], "title": "Evaluating Large Language Models on Multiword Expressions in Multilingual and Code-Switched Contexts", "categories": ["cs.CL"], "comment": null, "summary": "Multiword expressions, characterised by non-compositional meanings and\nsyntactic irregularities, are an example of nuanced language. These expressions\ncan be used literally or idiomatically, leading to significant changes in\nmeaning. While large language models have demonstrated strong performance\nacross many tasks, their ability to handle such linguistic subtleties remains\nuncertain. Therefore, this study evaluates how state-of-the-art language models\nprocess the ambiguity of potentially idiomatic multiword expressions,\nparticularly in contexts that are less frequent, where models are less likely\nto rely on memorisation. By evaluating models across in Portuguese and\nGalician, in addition to English, and using a novel code-switched dataset and a\nnovel task, we find that large language models, despite their strengths,\nstruggle with nuanced language. In particular, we find that the latest models,\nincluding GPT-4, fail to outperform the xlm-roBERTa-base baselines in both\ndetection and semantic tasks, with especially poor performance on the novel\ntasks we introduce, despite its similarity to existing tasks. Overall, our\nresults demonstrate that multiword expressions, especially those which are\nambiguous, continue to be a challenge to models."}
{"id": "2504.20630", "pdf": "https://arxiv.org/pdf/2504.20630", "abs": "https://arxiv.org/abs/2504.20630", "authors": ["Yu Zhang", "Wenxiang Guo", "Changhao Pan", "Zhiyuan Zhu", "Tao Jin", "Zhou Zhao"], "title": "ISDrama: Immersive Spatial Drama Generation through Multimodal Prompting", "categories": ["eess.AS", "cs.MM", "cs.SD"], "comment": null, "summary": "Multimodal immersive spatial drama generation focuses on creating continuous\nmulti-speaker binaural speech with dramatic prosody based on multimodal\nprompts, with potential applications in AR, VR, and others. This task requires\nsimultaneous modeling of spatial information and dramatic prosody based on\nmultimodal inputs, with high data collection costs. To the best of our\nknowledge, our work is the first attempt to address these challenges. We\nconstruct MRSDrama, the first multimodal recorded spatial drama dataset,\ncontaining binaural drama audios, scripts, videos, geometric poses, and textual\nprompts. Then, we propose ISDrama, the first immersive spatial drama generation\nmodel through multimodal prompting. ISDrama comprises these primary components:\n1) Multimodal Pose Encoder, based on contrastive learning, considering the\nDoppler effect caused by moving speakers to extract unified pose information\nfrom multimodal prompts. 2) Immersive Drama Transformer, a flow-based\nmamba-transformer model that generates high-quality drama, incorporating\nDrama-MOE to select proper experts for enhanced prosody and pose control. We\nalso design a context-consistent classifier-free guidance strategy to\ncoherently generate complete drama. Experimental results show that ISDrama\noutperforms baseline models on objective and subjective metrics. The demos and\ndataset are available at https://aaronz345.github.io/ISDramaDemo."}
{"id": "2504.20086", "pdf": "https://arxiv.org/pdf/2504.20086", "abs": "https://arxiv.org/abs/2504.20086", "authors": ["Sebastian Gehrmann", "Claire Huang", "Xian Teng", "Sergei Yurovski", "Iyanuoluwa Shode", "Chirag S. Patel", "Arjun Bhorkar", "Naveen Thomas", "John Doucette", "David Rosenberg", "Mark Dredze", "David Rabinowitz"], "title": "Understanding and Mitigating Risks of Generative AI in Financial Services", "categories": ["cs.CL", "cs.AI", "cs.CY", "cs.LG"], "comment": "Accepted to FAccT 2025", "summary": "To responsibly develop Generative AI (GenAI) products, it is critical to\ndefine the scope of acceptable inputs and outputs. What constitutes a \"safe\"\nresponse is an actively debated question. Academic work puts an outsized focus\non evaluating models by themselves for general purpose aspects such as\ntoxicity, bias, and fairness, especially in conversational applications being\nused by a broad audience. In contrast, less focus is put on considering\nsociotechnical systems in specialized domains. Yet, those specialized systems\ncan be subject to extensive and well-understood legal and regulatory scrutiny.\nThese product-specific considerations need to be set in industry-specific laws,\nregulations, and corporate governance requirements. In this paper, we aim to\nhighlight AI content safety considerations specific to the financial services\ndomain and outline an associated AI content risk taxonomy. We compare this\ntaxonomy to existing work in this space and discuss implications of risk\ncategory violations on various stakeholders. We evaluate how existing\nopen-source technical guardrail solutions cover this taxonomy by assessing them\non data collected via red-teaming activities. Our results demonstrate that\nthese guardrails fail to detect most of the content risks we discuss."}
{"id": "2504.20447", "pdf": "https://arxiv.org/pdf/2504.20447", "abs": "https://arxiv.org/abs/2504.20447", "authors": ["Zhicheng Lian", "Lizhi Wang", "Hua Huang"], "title": "APG-MOS: Auditory Perception Guided-MOS Predictor for Synthetic Speech", "categories": ["cs.SD", "cs.AI", "eess.AS"], "comment": null, "summary": "Automatic speech quality assessment aims to quantify subjective human\nperception of speech through computational models to reduce the need for\nlabor-consuming manual evaluations. While models based on deep learning have\nachieved progress in predicting mean opinion scores (MOS) to assess synthetic\nspeech, the neglect of fundamental auditory perception mechanisms limits\nconsistency with human judgments. To address this issue, we propose an auditory\nperception guided-MOS prediction model (APG-MOS) that synergistically\nintegrates auditory modeling with semantic analysis to enhance consistency with\nhuman judgments. Specifically, we first design a perceptual module, grounded in\nbiological auditory mechanisms, to simulate cochlear functions, which encodes\nacoustic signals into biologically aligned electrochemical representations.\nSecondly, we propose a residual vector quantization (RVQ)-based semantic\ndistortion modeling method to quantify the degradation of speech quality at the\nsemantic level. Finally, we design a residual cross-attention architecture,\ncoupled with a progressive learning strategy, to enable multimodal fusion of\nencoded electrochemical signals and semantic representations. Experiments\ndemonstrate that APG-MOS achieves superior performance on two primary\nbenchmarks. Our code and checkpoint will be available on a public repository\nupon publication."}
{"id": "2504.20368", "pdf": "https://arxiv.org/pdf/2504.20368", "abs": "https://arxiv.org/abs/2504.20368", "authors": ["David Gordon", "Panayiotis Petousis", "Susanne B. Nicholas", "Alex A. T. Bui"], "title": "AKIBoards: A Structure-Following Multiagent System for Predicting Acute Kidney Injury", "categories": ["cs.MA", "cs.AI", "cs.LG"], "comment": "Accepted at International Conference on Autonomous Agents and\n  Multiagent Systems (AAMAS) Workshop, 2025", "summary": "Diagnostic reasoning entails a physician's local (mental) model based on an\nassumed or known shared perspective (global model) to explain patient\nobservations with evidence assigned towards a clinical assessment. But in\nseveral (complex) medical situations, multiple experts work together as a team\nto optimize health evaluation and decision-making by leveraging different\nperspectives. Such consensus-driven reasoning reflects individual knowledge\ncontributing toward a broader perspective on the patient. In this light, we\nintroduce STRUCture-following for Multiagent Systems (STRUC-MAS), a framework\nautomating the learning of these global models and their incorporation as prior\nbeliefs for agents in multiagent systems (MAS) to follow. We demonstrate proof\nof concept with a prosocial MAS application for predicting acute kidney\ninjuries (AKIs). In this case, we found that incorporating a global structure\nenabled multiple agents to achieve better performance (average precision, AP)\nin predicting AKI 48 hours before onset (structure-following-fine-tuned, SF-FT,\nAP=0.195; SF-FT-retrieval-augmented generation, SF-FT-RAG, AP=0.194) vs.\nbaseline (non-structure-following-FT, NSF-FT, AP=0.141; NSF-FT-RAG, AP=0.180)\nfor balanced precision-weighted-recall-weighted voting. Markedly, SF-FT agents\nwith higher recall scores reported lower confidence levels in the initial round\non true positive and false negative cases. But after explicit interactions,\ntheir confidence in their decisions increased (suggesting reinforced belief).\nIn contrast, the SF-FT agent with the lowest recall decreased its confidence in\ntrue positive and false negative cases (suggesting a new belief). This approach\nsuggests that learning and leveraging global structures in MAS is necessary\nprior to achieving competitive classification and diagnostic reasoning\nperformance."}
{"id": "2504.20157", "pdf": "https://arxiv.org/pdf/2504.20157", "abs": "https://arxiv.org/abs/2504.20157", "authors": ["Zae Myung Kim", "Chanwoo Park", "Vipul Raheja", "Dongyeop Kang"], "title": "Toward Evaluative Thinking: Meta Policy Optimization with Evolving Reward Models", "categories": ["cs.CL"], "comment": null, "summary": "Reward-based alignment methods for large language models (LLMs) face two key\nlimitations: vulnerability to reward hacking, where models exploit flaws in the\nreward signal; and reliance on brittle, labor-intensive prompt engineering when\nLLMs are used as reward models. We introduce Meta Policy Optimization (MPO), a\nframework that addresses these challenges by integrating a meta-reward model\nthat dynamically refines the reward model's prompt throughout training. In MPO,\nthe meta-reward model monitors the evolving training context and continuously\nadjusts the reward model's prompt to maintain high alignment, providing an\nadaptive reward signal that resists exploitation by the policy. This\nmeta-learning approach promotes a more stable policy optimization, and greatly\nreduces the need for manual reward prompt design. It yields performance on par\nwith or better than models guided by extensively hand-crafted reward prompts.\nFurthermore, we show that MPO maintains its effectiveness across diverse tasks,\nsuch as question answering and mathematical reasoning, without requiring\nspecialized reward designs. Beyond standard RLAIF, MPO's meta-learning\nformulation is readily extensible to higher-level alignment frameworks.\nOverall, this method addresses theoretical and practical challenges in\nreward-based RL alignment for LLMs, paving the way for more robust and\nadaptable alignment strategies. The code and models will be publicly shared."}
{"id": "2504.20532", "pdf": "https://arxiv.org/pdf/2504.20532", "abs": "https://arxiv.org/abs/2504.20532", "authors": ["Yue Li", "Weizhi Liu", "Dongdong Lin"], "title": "TriniMark: A Robust Generative Speech Watermarking Method for Trinity-Level Attribution", "categories": ["cs.MM", "cs.CR", "cs.SD", "eess.AS"], "comment": null, "summary": "The emergence of diffusion models has facilitated the generation of speech\nwith reinforced fidelity and naturalness. While deepfake detection technologies\nhave manifested the ability to identify AI-generated content, their efficacy\ndecreases as generative models become increasingly sophisticated. Furthermore,\ncurrent research in the field has not adequately addressed the necessity for\nrobust watermarking to safeguard the intellectual property rights associated\nwith synthetic speech and generative models. To remedy this deficiency, we\npropose a \\textbf{ro}bust generative \\textbf{s}peech wat\\textbf{e}rmarking\nmethod (TriniMark) for authenticating the generated content and safeguarding\nthe copyrights by enabling the traceability of the diffusion model. We first\ndesign a structure-lightweight watermark encoder that embeds watermarks into\nthe time-domain features of speech and reconstructs the waveform directly. A\ntemporal-aware gated convolutional network is meticulously designed in the\nwatermark decoder for bit-wise watermark recovery. Subsequently, the\nwaveform-guided fine-tuning strategy is proposed for fine-tuning the diffusion\nmodel, which leverages the transferability of watermarks and enables the\ndiffusion model to incorporate watermark knowledge effectively. When an\nattacker trains a surrogate model using the outputs of the target model, the\nembedded watermark can still be learned by the surrogate model and correctly\nextracted. Comparative experiments with state-of-the-art methods demonstrate\nthe superior robustness of our method, particularly in countering compound\nattacks."}
{"id": "2504.20903", "pdf": "https://arxiv.org/pdf/2504.20903", "abs": "https://arxiv.org/abs/2504.20903", "authors": ["Prothit Sen", "Sai Mihir Jakkaraju"], "title": "Modeling AI-Human Collaboration as a Multi-Agent Adaptation", "categories": ["cs.MA", "cs.AI", "cs.HC"], "comment": "Manuscript under review for the Special Issue: 'Can AI Do Strategy?'\n  at Strategy Science (May 1, 2025)", "summary": "We develop an agent-based simulation to formalize AI-human collaboration as a\nfunction of task structure, advancing a generalizable framework for strategic\ndecision-making in organizations. Distinguishing between heuristic-based human\nadaptation and rule-based AI search, we model interactions across modular\n(parallel) and sequenced (interdependent) tasks using an NK model. Our results\nreveal that in modular tasks, AI often substitutes for humans - delivering\nhigher payoffs unless human expertise is very high, and the AI search space is\neither narrowly focused or extremely broad. In sequenced tasks, interesting\ncomplementarities emerge. When an expert human initiates the search and AI\nsubsequently refines it, aggregate performance is maximized. Conversely, when\nAI leads, excessive heuristic refinement by the human can reduce payoffs. We\nalso show that even \"hallucinatory\" AI - lacking memory or structure - can\nimprove outcomes when augmenting low-capability humans by helping escape local\noptima. These results yield a robust implication: the effectiveness of AI-human\ncollaboration depends less on context or industry, and more on the underlying\ntask structure. By elevating task decomposition as the central unit of\nanalysis, our model provides a transferable lens for strategic decision-making\ninvolving humans and an agentic AI across diverse organizational settings."}
{"id": "2504.20124", "pdf": "https://arxiv.org/pdf/2504.20124", "abs": "https://arxiv.org/abs/2504.20124", "authors": ["Abul Ehtesham", "Saket Kumar", "Aditi Singh", "Tala Talaei Khoei"], "title": "Pediatric Asthma Detection with Googles HeAR Model: An AI-Driven Respiratory Sound Classifier", "categories": ["cs.SD", "cs.AI"], "comment": null, "summary": "Early detection of asthma in children is crucial to prevent long-term\nrespiratory complications and reduce emergency interventions. This work\npresents an AI-powered diagnostic pipeline that leverages Googles Health\nAcoustic Representations (HeAR) model to detect early signs of asthma from\npediatric respiratory sounds. The SPRSound dataset, the first open-access\ncollection of annotated respiratory sounds in children aged 1 month to 18\nyears, is used to extract 2-second audio segments labeled as wheeze, crackle,\nrhonchi, stridor, or normal. Each segment is embedded into a 512-dimensional\nrepresentation using HeAR, a foundation model pretrained on 300 million\nhealth-related audio clips, including 100 million cough sounds. Multiple\nclassifiers, including SVM, Random Forest, and MLP, are trained on these\nembeddings to distinguish between asthma-indicative and normal sounds. The\nsystem achieves over 91\\% accuracy, with strong performance on precision-recall\nmetrics for positive cases. In addition to classification, learned embeddings\nare visualized using PCA, misclassifications are analyzed through waveform\nplayback, and ROC and confusion matrix insights are provided. This method\ndemonstrates that short, low-resource pediatric recordings, when powered by\nfoundation audio models, can enable fast, noninvasive asthma screening. The\napproach is especially promising for digital diagnostics in remote or\nunderserved healthcare settings."}
{"id": "2504.20370", "pdf": "https://arxiv.org/pdf/2504.20370", "abs": "https://arxiv.org/abs/2504.20370", "authors": ["Yongxuan Han", "Shengzhong Liu", "Fan Wu", "Guihai Chen"], "title": "ABO: Abandon Bayer Filter for Adaptive Edge Offloading in Responsive Augmented Reality", "categories": ["cs.MM"], "comment": "WWW 2025 final version", "summary": "Bayer-patterned color filter array (CFA) has been the go-to solution for\ncolor image sensors. In augmented reality (AR), although color interpolation\n(i.e., demosaicing) of pre-demosaic RAW images facilitates a user-friendly\nrendering, it creates no benefits in offloaded DNN analytics but increases the\nimage channels by 3 times inducing higher transmission overheads. The potential\noptimization in frame preprocessing of DNN offloading is yet to be\ninvestigated. To that end, we propose ABO, an adaptive RAW frame offloading\nframework that parallelizes demosaicing with DNN computation. Its contributions\nare three-fold: First, we design a configurable tile-wise RAW image neural\ncodec to compress frame sizes while sustaining downstream DNN accuracy under\nbandwidth constraints. Second, based on content-aware tiles-in-frame selection\nand runtime bandwidth estimation, a dynamic transmission controller adaptively\ncalibrates codec configurations to maximize the DNN accuracy. Third, we further\noptimize the system pipelining to achieve lower end-to-end frame processing\nlatency and higher throughput. Through extensive evaluations on a prototype\nplatform, ABO consistently achieves 40% more frame processing throughput and\n30% less end-to-end latency while improving the DNN accuracy by up to 15% than\nSOTA baselines. It also exhibits improved robustness against dim lighting and\nmotion blur situations."}
{"id": "2504.20405", "pdf": "https://arxiv.org/pdf/2504.20405", "abs": "https://arxiv.org/abs/2504.20405", "authors": ["Sahil Sethi", "Sai Reddy", "Mansi Sakarvadia", "Jordan Serotte", "Darlington Nwaudo", "Nicholas Maassen", "Lewis Shi"], "title": "SCOPE-MRI: Bankart Lesion Detection as a Case Study in Data Curation and Deep Learning for Challenging Diagnoses", "categories": ["eess.IV", "cs.AI", "cs.CV", "cs.LG"], "comment": null, "summary": "While deep learning has shown strong performance in musculoskeletal imaging,\nexisting work has largely focused on pathologies where diagnosis is not a\nclinical challenge, leaving more difficult problems underexplored, such as\ndetecting Bankart lesions (anterior-inferior glenoid labral tears) on standard\nMRIs. Diagnosing these lesions is challenging due to their subtle imaging\nfeatures, often leading to reliance on invasive MRI arthrograms (MRAs). This\nstudy introduces ScopeMRI, the first publicly available, expert-annotated\ndataset for shoulder pathologies, and presents a deep learning (DL) framework\nfor detecting Bankart lesions on both standard MRIs and MRAs. ScopeMRI includes\n586 shoulder MRIs (335 standard, 251 MRAs) from 558 patients who underwent\narthroscopy. Ground truth labels were derived from intraoperative findings, the\ngold standard for diagnosis. Separate DL models for MRAs and standard MRIs were\ntrained using a combination of CNNs and transformers. Predictions from\nsagittal, axial, and coronal views were ensembled to optimize performance. The\nmodels were evaluated on a 20% hold-out test set (117 MRIs: 46 MRAs, 71\nstandard MRIs). The models achieved an AUC of 0.91 and 0.93, sensitivity of 83%\nand 94%, and specificity of 91% and 86% for standard MRIs and MRAs,\nrespectively. Notably, model performance on non-invasive standard MRIs matched\nor surpassed radiologists interpreting MRAs. External validation demonstrated\ninitial generalizability across imaging protocols. This study demonstrates that\nDL models can achieve radiologist-level diagnostic performance on standard\nMRIs, reducing the need for invasive MRAs. By releasing ScopeMRI and a modular\ncodebase for training and evaluating deep learning models on 3D medical imaging\ndata, we aim to accelerate research in musculoskeletal imaging and support the\ndevelopment of new datasets for clinically challenging diagnostic tasks."}
{"id": "2504.20052", "pdf": "https://arxiv.org/pdf/2504.20052", "abs": "https://arxiv.org/abs/2504.20052", "authors": ["Floriane Magera", "Thomas Hoyoux", "Martin Castin", "Olivier Barnich", "Anthony Cioppa", "Marc Van Droogenbroeck"], "title": "Can Geometry Save Central Views for Sports Field Registration?", "categories": ["cs.CV"], "comment": "10 pages, 10 figures, 1 table, 40 references", "summary": "Single-frame sports field registration often serves as the foundation for\nextracting 3D information from broadcast videos, enabling applications related\nto sports analytics, refereeing, or fan engagement. As sports fields have\nrigorous specifications in terms of shape and dimensions of their line, circle\nand point components, sports field markings are commonly used as calibration\ntargets for this task. However, because of the sparse and uneven distribution\nof field markings, close-up camera views around central areas of the field\noften depict only line and circle markings. On these views, sports field\nregistration is challenging for the vast majority of existing methods, as they\nfocus on leveraging line field markings and their intersections. It is indeed a\nchallenge to include circle correspondences in a set of linear equations. In\nthis work, we propose a novel method to derive a set of points and lines from\ncircle correspondences, enabling the exploitation of circle correspondences for\nboth sports field registration and image annotation. In our experiments, we\nillustrate the benefits of our bottom-up geometric method against\ntop-performing detectors and show that our method successfully complements\nthem, enabling sports field registration in difficult scenarios."}
{"id": "2504.20082", "pdf": "https://arxiv.org/pdf/2504.20082", "abs": "https://arxiv.org/abs/2504.20082", "authors": ["Firuz Kamalov", "David Santandreu Calonge", "Linda Smail", "Dilshod Azizov", "Dimple R. Thadani", "Theresa Kwong", "Amara Atif"], "title": "Evolution of AI in Education: Agentic Workflows", "categories": ["cs.AI", "cs.CY"], "comment": null, "summary": "Artificial intelligence (AI) has transformed various aspects of education,\nwith large language models (LLMs) driving advancements in automated tutoring,\nassessment, and content generation. However, conventional LLMs are constrained\nby their reliance on static training data, limited adaptability, and lack of\nreasoning. To address these limitations and foster more sustainable\ntechnological practices, AI agents have emerged as a promising new avenue for\neducational innovation. In this review, we examine agentic workflows in\neducation according to four major paradigms: reflection, planning, tool use,\nand multi-agent collaboration. We critically analyze the role of AI agents in\neducation through these key design paradigms, exploring their advantages,\napplications, and challenges. To illustrate the practical potential of agentic\nsystems, we present a proof-of-concept application: a multi-agent framework for\nautomated essay scoring. Preliminary results suggest this agentic approach may\noffer improved consistency compared to stand-alone LLMs. Our findings highlight\nthe transformative potential of AI agents in educational settings while\nunderscoring the need for further research into their interpretability,\ntrustworthiness, and sustainable impact on pedagogical impact."}
{"id": "2504.20055", "pdf": "https://arxiv.org/pdf/2504.20055", "abs": "https://arxiv.org/abs/2504.20055", "authors": ["Juan D. Pinto", "Luc Paquette"], "title": "A constraints-based approach to fully interpretable neural networks for detecting learner behaviors", "categories": ["cs.LG", "cs.AI"], "comment": "Accepted to International Conference on Educational Data Mining (EDM)\n  2025", "summary": "The increasing use of complex machine learning models in education has led to\nconcerns about their interpretability, which in turn has spurred interest in\ndeveloping explainability techniques that are both faithful to the model's\ninner workings and intelligible to human end-users. In this paper, we describe\na novel approach to creating a neural-network-based behavior detection model\nthat is interpretable by design. Our model is fully interpretable, meaning that\nthe parameters we extract for our explanations have a clear interpretation,\nfully capture the model's learned knowledge about the learner behavior of\ninterest, and can be used to create explanations that are both faithful and\nintelligible. We achieve this by implementing a series of constraints to the\nmodel that both simplify its inference process and bring it closer to a human\nconception of the task at hand. We train the model to detect gaming-the-system\nbehavior, evaluate its performance on this task, and compare its learned\npatterns to those identified by human experts. Our results show that the model\nis successfully able to learn patterns indicative of gaming-the-system behavior\nwhile providing evidence for fully interpretable explanations. We discuss the\nimplications of our approach and suggest ways to evaluate explainability using\na human-grounded approach."}
{"id": "2504.20168", "pdf": "https://arxiv.org/pdf/2504.20168", "abs": "https://arxiv.org/abs/2504.20168", "authors": ["Nishant Subramani", "Jason Eisner", "Justin Svegliato", "Benjamin Van Durme", "Yu Su", "Sam Thomson"], "title": "MICE for CATs: Model-Internal Confidence Estimation for Calibrating Agents with Tools", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "Accepted at NAACL 2025. Code:\n  https://github.com/microsoft/mice_for_cats", "summary": "Tool-using agents that act in the world need to be both useful and safe.\nWell-calibrated model confidences can be used to weigh the risk versus reward\nof potential actions, but prior work shows that many models are poorly\ncalibrated. Inspired by interpretability literature exploring the internals of\nmodels, we propose a novel class of model-internal confidence estimators (MICE)\nto better assess confidence when calling tools. MICE first decodes from each\nintermediate layer of the language model using logitLens and then computes\nsimilarity scores between each layer's generation and the final output. These\nfeatures are fed into a learned probabilistic classifier to assess confidence\nin the decoded output. On the simulated trial and error (STE) tool-calling\ndataset using Llama3 models, we find that MICE beats or matches the baselines\non smoothed expected calibration error. Using MICE confidences to determine\nwhether to call a tool significantly improves over strong baselines on a new\nmetric, expected tool-calling utility. Further experiments show that MICE is\nsample-efficient, can generalize zero-shot to unseen APIs, and results in\nhigher tool-calling utility in scenarios with varying risk levels. Our code is\nopen source, available at https://github.com/microsoft/mice_for_cats."}
{"id": "2504.20625", "pdf": "https://arxiv.org/pdf/2504.20625", "abs": "https://arxiv.org/abs/2504.20625", "authors": ["Sagi Della Torre", "Mirco Pezzoli", "Fabio Antonacci", "Sharon Gannot"], "title": "DiffusionRIR: Room Impulse Response Interpolation using Diffusion Models", "categories": ["cs.SD", "cs.AI", "eess.AS"], "comment": null, "summary": "Room Impulse Responses (RIRs) characterize acoustic environments and are\ncrucial in multiple audio signal processing tasks. High-quality RIR estimates\ndrive applications such as virtual microphones, sound source localization,\naugmented reality, and data augmentation. However, obtaining RIR measurements\nwith high spatial resolution is resource-intensive, making it impractical for\nlarge spaces or when dense sampling is required. This research addresses the\nchallenge of estimating RIRs at unmeasured locations within a room using\nDenoising Diffusion Probabilistic Models (DDPM). Our method leverages the\nanalogy between RIR matrices and image inpainting, transforming RIR data into a\nformat suitable for diffusion-based reconstruction.\n  Using simulated RIR data based on the image method, we demonstrate our\napproach's effectiveness on microphone arrays of different curvatures, from\nlinear to semi-circular. Our method successfully reconstructs missing RIRs,\neven in large gaps between microphones. Under these conditions, it achieves\naccurate reconstruction, significantly outperforming baseline Spline Cubic\nInterpolation in terms of Normalized Mean Square Error and Cosine Distance\nbetween actual and interpolated RIRs.\n  This research highlights the potential of using generative models for\neffective RIR interpolation, paving the way for generating additional data from\nlimited real-world measurements."}
{"id": "2406.11357", "pdf": "https://arxiv.org/pdf/2406.11357", "abs": "https://arxiv.org/abs/2406.11357", "authors": ["Zhonghao Li", "Xuming Hu", "Aiwei Liu", "Kening Zheng", "Sirui Huang", "Hui Xiong"], "title": "Refiner: Restructure Retrieval Content Efficiently to Advance Question-Answering Capabilities", "categories": ["cs.CL", "cs.AI", "cs.HC", "cs.IR", "cs.MA"], "comment": "8 pages", "summary": "Large Language Models (LLMs) are limited by their parametric knowledge,\nleading to hallucinations in knowledge-extensive tasks. To address this,\nRetrieval-Augmented Generation (RAG) incorporates external document chunks to\nexpand LLM knowledge. Furthermore, compressing information from document chunks\nthrough extraction or summarization can improve LLM performance. Nonetheless,\nLLMs still struggle to notice and utilize scattered key information, a problem\nknown as the \"lost-in-the-middle\" syndrome. Therefore, we typically need to\nrestructure the content for LLM to recognize the key information. We propose\n$\\textit{Refiner}$, an end-to-end extract-and-restructure paradigm that\noperates in the post-retrieval process of RAG. $\\textit{Refiner}$ leverages a\nsingle decoder-only LLM to adaptively extract query-relevant contents verbatim\nalong with the necessary context, and section them based on their\ninterconnectedness, thereby highlights information distinction, and aligns\ndownstream LLMs with the original context effectively. Experiments show that a\ntrained $\\textit{Refiner}$ (with 7B parameters) exhibits significant gain to\ndownstream LLM in improving answer accuracy, and outperforms other\nstate-of-the-art advanced RAG and concurrent compressing approaches in various\nsingle-hop and multi-hop QA tasks. Notably, $\\textit{Refiner}$ achieves a 80.5%\ntokens reduction and a 1.6-7.0% improvement margin in multi-hop tasks compared\nto the next best solution. $\\textit{Refiner}$ is a plug-and-play solution that\ncan be seamlessly integrated with RAG systems, facilitating its application\nacross diverse open-source frameworks."}
{"id": "2504.20776", "pdf": "https://arxiv.org/pdf/2504.20776", "abs": "https://arxiv.org/abs/2504.20776", "authors": ["David Funosas", "Elodie Massol", "Yves Bas", "Svenja Schmidt", "Dominik Arend", "Alexander Gebhard", "Luc Barbaro", "Sebastian König", "Rafael Carbonell Font", "David Sannier", "Fernand Deroussen", "Jérôme Sueur", "Christian Roesti", "Tomi Trilar", "Wolfgang Forstmeier", "Lucas Roger", "Eloïsa Matheu", "Piotr Guzik", "Julien Barataud", "Laurent Pelozuelo", "Stéphane Puissant", "Sandra Mueller", "Björn Schuller", "Jose M. Montoya", "Andreas Triantafyllopoulos", "Maxime Cauchoix"], "title": "ECOSoundSet: a finely annotated dataset for the automated acoustic identification of Orthoptera and Cicadidae in North, Central and temperate Western Europe", "categories": ["cs.SD", "cs.AI", "eess.AS"], "comment": "3 Figures + 2 Supplementary Figures, 2 Tables + 3 Supplementary\n  Tables", "summary": "Currently available tools for the automated acoustic recognition of European\ninsects in natural soundscapes are limited in scope. Large and ecologically\nheterogeneous acoustic datasets are currently needed for these algorithms to\ncross-contextually recognize the subtle and complex acoustic signatures\nproduced by each species, thus making the availability of such datasets a key\nrequisite for their development. Here we present ECOSoundSet (European\nCicadidae and Orthoptera Sound dataSet), a dataset containing 10,653 recordings\nof 200 orthopteran and 24 cicada species (217 and 26 respective taxa when\nincluding subspecies) present in North, Central, and temperate Western Europe\n(Andorra, Belgium, Denmark, mainland France and Corsica, Germany, Ireland,\nLuxembourg, Monaco, Netherlands, United Kingdom, Switzerland), collected partly\nthrough targeted fieldwork in South France and Catalonia and partly through\ncontributions from various European entomologists. The dataset is composed of a\ncombination of coarsely labeled recordings, for which we can only infer the\npresence, at some point, of their target species (weak labeling), and finely\nannotated recordings, for which we know the specific time and frequency range\nof each insect sound present in the recording (strong labeling). We also\nprovide a train/validation/test split of the strongly labeled recordings, with\nrespective approximate proportions of 0.8, 0.1 and 0.1, in order to facilitate\ntheir incorporation in the training and evaluation of deep learning algorithms.\nThis dataset could serve as a meaningful complement to recordings already\navailable online for the training of deep learning algorithms for the acoustic\nclassification of orthopterans and cicadas in North, Central, and temperate\nWestern Europe."}
{"id": "2504.20658", "pdf": "https://arxiv.org/pdf/2504.20658", "abs": "https://arxiv.org/abs/2504.20658", "authors": ["Stefano Dell'Anna", "Andrea Montibeller", "Giulia Boato"], "title": "TrueFake: A Real World Case Dataset of Last Generation Fake Images also Shared on Social Networks", "categories": ["cs.MM", "cs.AI", "cs.CV"], "comment": null, "summary": "AI-generated synthetic media are increasingly used in real-world scenarios,\noften with the purpose of spreading misinformation and propaganda through\nsocial media platforms, where compression and other processing can degrade fake\ndetection cues. Currently, many forensic tools fail to account for these\nin-the-wild challenges. In this work, we introduce TrueFake, a large-scale\nbenchmarking dataset of 600,000 images including top notch generative\ntechniques and sharing via three different social networks. This dataset allows\nfor rigorous evaluation of state-of-the-art fake image detectors under very\nrealistic and challenging conditions. Through extensive experimentation, we\nanalyze how social media sharing impacts detection performance, and identify\ncurrent most effective detection and training strategies. Our findings\nhighlight the need for evaluating forensic models in conditions that mirror\nreal-world use."}
{"id": "2504.20454", "pdf": "https://arxiv.org/pdf/2504.20454", "abs": "https://arxiv.org/abs/2504.20454", "authors": ["Jiajun Ding", "Beiyao Zhu", "Xiaosheng Liu", "Lishen Zhang", "Zhao Liu"], "title": "LymphAtlas- A Unified Multimodal Lymphoma Imaging Repository Delivering AI-Enhanced Diagnostic Insight", "categories": ["eess.IV", "cs.CV"], "comment": "17pages,4 figures", "summary": "This study integrates PET metabolic information with CT anatomical structures\nto establish a 3D multimodal segmentation dataset for lymphoma based on\nwhole-body FDG PET/CT examinations, which bridges the gap of the lack of\nstandardised multimodal segmentation datasets in the field of haematological\nmalignancies. We retrospectively collected 483 examination datasets acquired\nbetween March 2011 and May 2024, involving 220 patients (106 non-Hodgkin\nlymphoma, 42 Hodgkin lymphoma); all data underwent ethical review and were\nrigorously de-identified. Complete 3D structural information was preserved\nduring data acquisition, preprocessing and annotation, and a high-quality\ndataset was constructed based on the nnUNet format. By systematic technical\nvalidation and evaluation of the preprocessing process, annotation quality and\nautomatic segmentation algorithm, the deep learning model trained based on this\ndataset is verified to achieve accurate segmentation of lymphoma lesions in\nPET/CT images with high accuracy, good robustness and reproducibility, which\nproves the applicability and stability of this dataset in accurate segmentation\nand quantitative analysis. The deep fusion of PET/CT images achieved with this\ndataset not only significantly improves the accurate portrayal of the\nmorphology, location and metabolic features of tumour lesions, but also\nprovides solid data support for early diagnosis, clinical staging and\npersonalized treatment, and promotes the development of automated image\nsegmentation and precision medicine based on deep learning. The dataset and\nrelated resources are available at https://github.com/SuperD0122/LymphAtlas-."}
{"id": "2504.20054", "pdf": "https://arxiv.org/pdf/2504.20054", "abs": "https://arxiv.org/abs/2504.20054", "authors": ["Jiayang Sun", "Hongbo Wang", "Jie Cao", "Huaibo Huang", "Ran He"], "title": "Marmot: Multi-Agent Reasoning for Multi-Object Self-Correcting in Improving Image-Text Alignment", "categories": ["cs.CV"], "comment": null, "summary": "While diffusion models excel at generating high-quality images, they often\nstruggle with accurate counting, attributes, and spatial relationships in\ncomplex multi-object scenes. To address these challenges, we propose Marmot, a\nnovel and generalizable framework that employs Multi-Agent Reasoning for\nMulti-Object Self-Correcting, enhancing image-text alignment and facilitating\nmore coherent multi-object image editing. Our framework adopts a\ndivide-and-conquer strategy that decomposes the self-correction task into three\ncritical dimensions (counting, attributes, and spatial relationships), and\nfurther divided into object-level subtasks. We construct a multi-agent editing\nsystem featuring a decision-execution-verification mechanism, effectively\nmitigating inter-object interference and enhancing editing reliability. To\nresolve the problem of subtask integration, we propose a Pixel-Domain Stitching\nSmoother that employs mask-guided two-stage latent space optimization. This\ninnovation enables parallel processing of subtask results, thereby enhancing\nruntime efficiency while eliminating multi-stage distortion accumulation.\nExtensive experiments demonstrate that Marmot significantly improves accuracy\nin object counting, attribute assignment, and spatial relationships for image\ngeneration tasks."}
{"id": "2504.20084", "pdf": "https://arxiv.org/pdf/2504.20084", "abs": "https://arxiv.org/abs/2504.20084", "authors": ["Xiaojian Li", "Haoyuan Shi", "Rongwu Xu", "Wei Xu"], "title": "AI Awareness", "categories": ["cs.AI", "cs.CL", "cs.CY"], "comment": null, "summary": "Recent breakthroughs in artificial intelligence (AI) have brought about\nincreasingly capable systems that demonstrate remarkable abilities in\nreasoning, language understanding, and problem-solving. These advancements have\nprompted a renewed examination of AI awareness, not as a philosophical question\nof consciousness, but as a measurable, functional capacity. In this review, we\nexplore the emerging landscape of AI awareness, which includes meta-cognition\n(the ability to represent and reason about its own state), self-awareness\n(recognizing its own identity, knowledge, limitations, inter alia), social\nawareness (modeling the knowledge, intentions, and behaviors of other agents),\nand situational awareness (assessing and responding to the context in which it\noperates).\n  First, we draw on insights from cognitive science, psychology, and\ncomputational theory to trace the theoretical foundations of awareness and\nexamine how the four distinct forms of AI awareness manifest in\nstate-of-the-art AI. Next, we systematically analyze current evaluation methods\nand empirical findings to better understand these manifestations. Building on\nthis, we explore how AI awareness is closely linked to AI capabilities,\ndemonstrating that more aware AI agents tend to exhibit higher levels of\nintelligent behaviors. Finally, we discuss the risks associated with AI\nawareness, including key topics in AI safety, alignment, and broader ethical\nconcerns.\n  AI awareness is a double-edged sword: it improves general capabilities, i.e.,\nreasoning, safety, while also raises concerns around misalignment and societal\nrisks, demanding careful oversight as AI capabilities grow. On the whole, our\ninterdisciplinary review provides a roadmap for future research and aims to\nclarify the role of AI awareness in the ongoing development of intelligent\nmachines."}
{"id": "2504.20069", "pdf": "https://arxiv.org/pdf/2504.20069", "abs": "https://arxiv.org/abs/2504.20069", "authors": ["Junhong Lai", "Jiyu Wei", "Lin Yao", "Yueming Wang"], "title": "A Simple Review of EEG Foundation Models: Datasets, Advancements and Future Perspectives", "categories": ["cs.LG", "cs.AI", "eess.SP"], "comment": null, "summary": "Electroencephalogram (EEG) signals play a crucial role in understanding brain\nactivity and diagnosing neurological disorders. This review focuses on the\nrecent development of EEG foundation models(EEG-FMs), which have shown great\npotential in processing and analyzing EEG data. We discuss various EEG-FMs,\nincluding their architectures, pre-training strategies, their pre-training and\ndownstream datasets and other details. The review also highlights the\nchallenges and future directions in this field, aiming to provide a\ncomprehensive overview for researchers and practitioners interested in EEG\nanalysis and related EEG-FMs."}
{"id": "2504.20220", "pdf": "https://arxiv.org/pdf/2504.20220", "abs": "https://arxiv.org/abs/2504.20220", "authors": ["Henning Schäfer", "Cynthia S. Schmidt", "Johannes Wutzkowsky", "Kamil Lorek", "Lea Reinartz", "Johannes Rückert", "Christian Temme", "Britta Böckmann", "Peter A. Horn", "Christoph M. Friedrich"], "title": "A Multimodal Pipeline for Clinical Data Extraction: Applying Vision-Language Models to Scans of Transfusion Reaction Reports", "categories": ["cs.CL", "cs.CV", "68T07", "I.7.5; I.4.7; I.2.7; H.3.3; J.3"], "comment": null, "summary": "Despite the growing adoption of electronic health records, many processes\nstill rely on paper documents, reflecting the heterogeneous real-world\nconditions in which healthcare is delivered. The manual transcription process\nis time-consuming and prone to errors when transferring paper-based data to\ndigital formats. To streamline this workflow, this study presents an\nopen-source pipeline that extracts and categorizes checkbox data from scanned\ndocuments. Demonstrated on transfusion reaction reports, the design supports\nadaptation to other checkbox-rich document types. The proposed method\nintegrates checkbox detection, multilingual optical character recognition (OCR)\nand multilingual vision-language models (VLMs). The pipeline achieves high\nprecision and recall compared against annually compiled gold-standards from\n2017 to 2024. The result is a reduction in administrative workload and accurate\nregulatory reporting. The open-source availability of this pipeline encourages\nself-hosted parsing of checkbox forms."}
{"id": "2504.20678", "pdf": "https://arxiv.org/pdf/2504.20678", "abs": "https://arxiv.org/abs/2504.20678", "authors": ["Yaroslav Getman", "Tamás Grósz", "Mikko Kurimo", "Giampiero Salvi"], "title": "Non-native Children's Automatic Speech Assessment Challenge (NOCASA)", "categories": ["cs.CL", "eess.AS"], "comment": "First draft of the baseline paper for the NOCASA competition\n  (https://teflon.aalto.fi/nocasa-2025/), 5 pages", "summary": "This paper presents the \"Non-native Children's Automatic Speech Assessment\"\n(NOCASA) - a data competition part of the IEEE MLSP 2025 conference. NOCASA\nchallenges participants to develop new systems that can assess single-word\npronunciations of young second language (L2) learners as part of a gamified\npronunciation training app. To achieve this, several issues must be addressed,\nmost notably the limited nature of available training data and the highly\nunbalanced distribution among the pronunciation level categories. To expedite\nthe development, we provide a pseudo-anonymized training data (TeflonNorL2),\ncontaining 10,334 recordings from 44 speakers attempting to pronounce 205\ndistinct Norwegian words, human-rated on a 1 to 5 scale (number of stars that\nshould be given in the game). In addition to the data, two already trained\nsystems are released as official baselines: an SVM classifier trained on the\nComParE_16 acoustic feature set and a multi-task wav2vec 2.0 model. The latter\nachieves the best performance on the challenge test set, with an unweighted\naverage recall (UAR) of 36.37%."}
{"id": "2504.20071", "pdf": "https://arxiv.org/pdf/2504.20071", "abs": "https://arxiv.org/abs/2504.20071", "authors": ["Pranav Kedia", "Madhav Rao"], "title": "GenGrid: A Generalised Distributed Experimental Environmental Grid for Swarm Robotics", "categories": ["cs.RO", "cs.MA"], "comment": "2021 IEEE International Conference on Robotics and Automation (ICRA)", "summary": "GenGrid is a novel comprehensive open-source, distributed platform intended\nfor conducting extensive swarm robotic experiments. The modular platform is\ndesigned to run swarm robotics experiments that are compatible with different\ntypes of mobile robots ranging from Colias, Kilobot, and E puck. The platform\noffers programmable control over the experimental setup and its parameters and\nacts as a tool to collect swarm robot data, including localization, sensory\nfeedback, messaging, and interaction. GenGrid is designed as a modular grid of\nattachable computing nodes that offers bidirectional communication between the\nrobotic agent and grid nodes and within grids. The paper describes the hardware\nand software architecture design of the GenGrid system. Further, it discusses\nsome common experimental studies covering multi-robot and swarm robotics to\nshowcase the platform's use. GenGrid of 25 homogeneous cells with identical\nsensing and communication characteristics with a footprint of 37.5 cm X 37.5\ncm, exhibits multiple capabilities with minimal resources. The open-source\nhardware platform is handy for running swarm experiments, including robot\nhopping based on multiple gradients, collective transport, shepherding,\ncontinuous pheromone deposition, and subsequent evaporation. The low-cost,\nmodular, and open-source platform is significant in the swarm robotics research\ncommunity, which is currently driven by commercial platforms that allow minimal\nmodifications."}
{"id": "2504.20835", "pdf": "https://arxiv.org/pdf/2504.20835", "abs": "https://arxiv.org/abs/2504.20835", "authors": ["Hongfei Xue", "Yufeng Tang", "Hexin Liu", "Jun Zhang", "Xuelong Geng", "Lei Xie"], "title": "Enhancing Non-Core Language Instruction-Following in Speech LLMs via Semi-Implicit Cross-Lingual CoT Reasoning", "categories": ["cs.SD", "eess.AS"], "comment": "10 pages, 6 figures, Submitted to ACM MM 2025", "summary": "Large language models have been extended to the speech domain, leading to the\ndevelopment of speech large language models (SLLMs). While existing SLLMs\ndemonstrate strong performance in speech instruction-following for core\nlanguages (e.g., English), they often struggle with non-core languages due to\nthe scarcity of paired speech-text data and limited multilingual semantic\nreasoning capabilities. To address this, we propose the semi-implicit\nCross-lingual Speech Chain-of-Thought (XS-CoT) framework, which integrates\nspeech-to-text translation into the reasoning process of SLLMs. The XS-CoT\ngenerates four types of tokens: instruction and response tokens in both core\nand non-core languages, enabling cross-lingual transfer of reasoning\ncapabilities. To mitigate inference latency in generating target non-core\nresponse tokens, we incorporate a semi-implicit CoT scheme into XS-CoT, which\nprogressively compresses the first three types of intermediate reasoning tokens\nwhile retaining global reasoning logic during training. By leveraging the\nrobust reasoning capabilities of the core language, XS-CoT improves responses\nfor non-core languages by up to 45\\% in GPT-4 score when compared to direct\nsupervised fine-tuning on two representative SLLMs, Qwen2-Audio and SALMONN.\nMoreover, the semi-implicit XS-CoT reduces token delay by more than 50\\% with a\nslight drop in GPT-4 scores. Importantly, XS-CoT requires only a small amount\nof high-quality training data for non-core languages by leveraging the\nreasoning capabilities of core languages. To support training, we also develop\na data pipeline and open-source speech instruction-following datasets in\nJapanese, German, and French."}
{"id": "2504.20064", "pdf": "https://arxiv.org/pdf/2504.20064", "abs": "https://arxiv.org/abs/2504.20064", "authors": ["Qi Yang", "Marlo Ongpin", "Sergey Nikolenko", "Alfred Huang", "Aleksandr Farseev"], "title": "Against Opacity: Explainable AI and Large Language Models for Effective Digital Advertising", "categories": ["cs.IR", "cs.MM", "cs.NE"], "comment": null, "summary": "The opaqueness of modern digital advertising, exemplified by platforms such\nas Meta Ads, raises concerns regarding their autonomous control over audience\ntargeting, pricing structures, and ad relevancy assessments. Locked in their\nleading positions by network effects, ``Metas and Googles of the world''\nattract countless advertisers who rely on intuition, with billions of dollars\nlost on ineffective social media ads. The platforms' algorithms use huge\namounts of data unavailable to advertisers, and the algorithms themselves are\nopaque as well. This lack of transparency hinders the advertisers' ability to\nmake informed decisions and necessitates efforts to promote transparency,\nstandardize industry metrics, and strengthen regulatory frameworks. In this\nwork, we propose novel ways to assist marketers in optimizing their advertising\nstrategies via machine learning techniques designed to analyze and evaluate\ncontent, in particular, predict the click-through rates (CTR) of novel\nadvertising content. Another important problem is that large volumes of data\navailable in the competitive landscape, e.g., competitors' ads, impede the\nability of marketers to derive meaningful insights. This leads to a pressing\nneed for a novel approach that would allow us to summarize and comprehend\ncomplex data. Inspired by the success of ChatGPT in bridging the gap between\nlarge language models (LLMs) and a broader non-technical audience, we propose a\nnovel system that facilitates marketers in data interpretation, called SODA,\nthat merges LLMs with explainable AI, enabling better human-AI collaboration\nwith an emphasis on the domain of digital marketing and advertising. By\ncombining LLMs and explainability features, in particular modern text-image\nmodels, we aim to improve the synergy between human marketers and AI systems."}
{"id": "2504.20479", "pdf": "https://arxiv.org/pdf/2504.20479", "abs": "https://arxiv.org/abs/2504.20479", "authors": ["Elena Martinez", "Beatrice Moscoloni", "Matteo Salvador", "Fanwei Kong", "Mathias Peirlinck", "Alison Lesley Marsden"], "title": "Full-field surrogate modeling of cardiac function encoding geometric variability", "categories": ["eess.IV", "cs.LG"], "comment": "34 pages, 19 figures", "summary": "Combining physics-based modeling with data-driven methods is critical to\nenabling the translation of computational methods to clinical use in\ncardiology. The use of rigorous differential equations combined with machine\nlearning tools allows for model personalization with uncertainty quantification\nin time frames compatible with clinical practice. However, accurate and\nefficient surrogate models of cardiac function, built from physics-based\nnumerical simulation, are still mostly geometry-specific and require retraining\nfor different patients and pathological conditions. We propose a novel\ncomputational pipeline to embed cardiac anatomies into full-field surrogate\nmodels. We generate a dataset of electrophysiology simulations using a complex\nmulti-scale mathematical model coupling partial and ordinary differential\nequations. We adopt Branched Latent Neural Maps (BLNMs) as an effective\nscientific machine learning method to encode activation maps extracted from\nphysics-based numerical simulations into a neural network. Leveraging large\ndeformation diffeomorphic metric mappings, we build a biventricular anatomical\natlas and parametrize the anatomical variability of a small and challenging\ncohort of 13 pediatric patients affected by Tetralogy of Fallot. We propose a\nnovel statistical shape modeling based z-score sampling approach to generate a\nnew synthetic cohort of 52 biventricular geometries that are compatible with\nthe original geometrical variability. This synthetic cohort acts as the\ntraining set for BLNMs. Our surrogate model demonstrates robustness and great\ngeneralization across the complex original patient cohort, achieving an average\nadimensional mean squared error of 0.0034. The Python implementation of our\nBLNM model is publicly available under MIT License at\nhttps://github.com/StanfordCBCL/BLNM."}
{"id": "2504.20077", "pdf": "https://arxiv.org/pdf/2504.20077", "abs": "https://arxiv.org/abs/2504.20077", "authors": ["Manish Kansana", "Keyan Alexander Rahimi", "Elias Hossain", "Iman Dehzangi", "Noorbakhsh Amiri Golilarz"], "title": "Edge-Based Learning for Improved Classification Under Adversarial Noise", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Adversarial noise introduces small perturbations in images, misleading deep\nlearning models into misclassification and significantly impacting recognition\naccuracy. In this study, we analyzed the effects of Fast Gradient Sign Method\n(FGSM) adversarial noise on image classification and investigated whether\ntraining on specific image features can improve robustness. We hypothesize that\nwhile adversarial noise perturbs various regions of an image, edges may remain\nrelatively stable and provide essential structural information for\nclassification. To test this, we conducted a series of experiments using brain\ntumor and COVID datasets. Initially, we trained the models on clean images and\nthen introduced subtle adversarial perturbations, which caused deep learning\nmodels to significantly misclassify the images. Retraining on a combination of\nclean and noisy images led to improved performance. To evaluate the robustness\nof the edge features, we extracted edges from the original/clean images and\ntrained the models exclusively on edge-based representations. When noise was\nintroduced to the images, the edge-based models demonstrated greater resilience\nto adversarial attacks compared to those trained on the original or clean\nimages. These results suggest that while adversarial noise is able to exploit\ncomplex non-edge regions significantly more than edges, the improvement in the\naccuracy after retraining is marginally more in the original data as compared\nto the edges. Thus, leveraging edge-based learning can improve the resilience\nof deep learning models against adversarial perturbations."}
{"id": "2504.20090", "pdf": "https://arxiv.org/pdf/2504.20090", "abs": "https://arxiv.org/abs/2504.20090", "authors": ["Aishik Sanyal", "Samuel Schapiro", "Sumuk Shashidhar", "Royce Moon", "Lav R. Varshney", "Dilek Hakkani-Tur"], "title": "Spark: A System for Scientifically Creative Idea Generation", "categories": ["cs.AI", "cs.IR", "cs.LG"], "comment": null, "summary": "Recently, large language models (LLMs) have shown promising abilities to\ngenerate novel research ideas in science, a direction which coincides with many\nfoundational principles in computational creativity (CC). In light of these\ndevelopments, we present an idea generation system named Spark that couples\nretrieval-augmented idea generation using LLMs with a reviewer model named\nJudge trained on 600K scientific reviews from OpenReview. Our work is both a\nsystem demonstration and intended to inspire other CC researchers to explore\ngrounding the generation and evaluation of scientific ideas within foundational\nCC principles. To this end, we release the annotated dataset used to train\nJudge, inviting other researchers to explore the use of LLMs for idea\ngeneration and creative evaluations."}
{"id": "2504.20070", "pdf": "https://arxiv.org/pdf/2504.20070", "abs": "https://arxiv.org/abs/2504.20070", "authors": ["Altun Shukurlu"], "title": "Improving Deep Knowledge Tracing via Gated Architectures and Adaptive Optimization", "categories": ["cs.LG"], "comment": "11 pages, 2 figures", "summary": "Deep Knowledge Tracing (DKT) models student learning behavior by using\nRecurrent Neural Networks (RNNs) to predict future performance based on\nhistorical interaction data. However, the original implementation relied on\nstandard RNNs in the Lua-based Torch framework, which limited extensibility and\nreproducibility. In this work, we revisit the DKT model from two perspectives:\narchitectural improvements and optimization efficiency. First, we enhance the\nmodel using gated recurrent units, specifically Long Short-Term Memory (LSTM)\nnetworks and Gated Recurrent Units (GRU), which better capture long-term\ndependencies and help mitigate vanishing gradient issues. Second, we\nre-implement DKT using the PyTorch framework, enabling a modular and accessible\ninfrastructure compatible with modern deep learning workflows. We also\nbenchmark several optimization algorithms SGD, RMSProp, Adagrad, Adam, and\nAdamW to evaluate their impact on convergence speed and predictive accuracy in\neducational modeling tasks. Experiments on the Synthetic-5 and Khan Academy\ndatasets show that GRUs and LSTMs achieve higher accuracy and improved training\nstability compared to basic RNNs, while adaptive optimizers such as Adam and\nAdamW consistently outperform SGD in both early-stage learning and final model\nperformance. Our open-source PyTorch implementation provides a reproducible and\nextensible foundation for future research in neural knowledge tracing and\npersonalized learning systems."}
{"id": "2504.20251", "pdf": "https://arxiv.org/pdf/2504.20251", "abs": "https://arxiv.org/abs/2504.20251", "authors": ["Aiala Rosá", "Santiago Góngora", "Juan Pablo Filevich", "Ignacio Sastre", "Laura Musto", "Brian Carpenter", "Luis Chiruzzo"], "title": "A Platform for Generating Educational Activities to Teach English as a Second Language", "categories": ["cs.CL", "cs.AI", "cs.CY"], "comment": "Unpublished report written in 2023", "summary": "We present a platform for the generation of educational activities oriented\nto teaching English as a foreign language. The different activities -- games\nand language practice exercises -- are strongly based on Natural Language\nProcessing techniques. The platform offers the possibility of playing\nout-of-the-box games, generated from resources created semi-automatically and\nthen manually curated. It can also generate games or exercises of greater\ncomplexity from texts entered by teachers, providing a stage of review and\nedition of the generated content before use. As a way of expanding the variety\nof activities in the platform, we are currently experimenting with image and\ntext generation. In order to integrate them and improve the performance of\nother neural tools already integrated, we are working on migrating the platform\nto a more powerful server. In this paper we describe the development of our\nplatform and its deployment for end users, discussing the challenges faced and\nhow we overcame them, and also detail our future work plans."}
{"id": "2504.20923", "pdf": "https://arxiv.org/pdf/2504.20923", "abs": "https://arxiv.org/abs/2504.20923", "authors": ["Andrea Di Pierno", "Luca Guarnera", "Dario Allegra", "Sebastiano Battiato"], "title": "End-to-end Audio Deepfake Detection from RAW Waveforms: a RawNet-Based Approach with Cross-Dataset Evaluation", "categories": ["cs.SD", "cs.CV", "eess.AS"], "comment": null, "summary": "Audio deepfakes represent a growing threat to digital security and trust,\nleveraging advanced generative models to produce synthetic speech that closely\nmimics real human voices. Detecting such manipulations is especially\nchallenging under open-world conditions, where spoofing methods encountered\nduring testing may differ from those seen during training. In this work, we\npropose an end-to-end deep learning framework for audio deepfake detection that\noperates directly on raw waveforms. Our model, RawNetLite, is a lightweight\nconvolutional-recurrent architecture designed to capture both spectral and\ntemporal features without handcrafted preprocessing. To enhance robustness, we\nintroduce a training strategy that combines data from multiple domains and\nadopts Focal Loss to emphasize difficult or ambiguous samples. We further\ndemonstrate that incorporating codec-based manipulations and applying\nwaveform-level audio augmentations (e.g., pitch shifting, noise, and time\nstretching) leads to significant generalization improvements under realistic\nacoustic conditions. The proposed model achieves over 99.7% F1 and 0.25% EER on\nin-domain data (FakeOrReal), and up to 83.4% F1 with 16.4% EER on a challenging\nout-of-distribution test set (AVSpoof2021 + CodecFake). These findings\nhighlight the importance of diverse training data, tailored objective functions\nand audio augmentations in building resilient and generalizable audio forgery\ndetectors. Code and pretrained models are available at\nhttps://iplab.dmi.unict.it/mfs/Deepfakes/PaperRawNet2025/."}
{"id": "2504.20091", "pdf": "https://arxiv.org/pdf/2504.20091", "abs": "https://arxiv.org/abs/2504.20091", "authors": ["Noriyuki Kugo", "Xiang Li", "Zixin Li", "Ashish Gupta", "Arpandeep Khatua", "Nidhish Jain", "Chaitanya Patel", "Yuta Kyuragi", "Masamoto Tanabiki", "Kazuki Kozuka", "Ehsan Adeli"], "title": "VideoMultiAgents: A Multi-Agent Framework for Video Question Answering", "categories": ["cs.CV", "cs.MA"], "comment": null, "summary": "Video Question Answering (VQA) inherently relies on multimodal reasoning,\nintegrating visual, temporal, and linguistic cues to achieve a deeper\nunderstanding of video content. However, many existing methods rely on feeding\nframe-level captions into a single model, making it difficult to adequately\ncapture temporal and interactive contexts. To address this limitation, we\nintroduce VideoMultiAgents, a framework that integrates specialized agents for\nvision, scene graph analysis, and text processing. It enhances video\nunderstanding leveraging complementary multimodal reasoning from independently\noperating agents. Our approach is also supplemented with a question-guided\ncaption generation, which produces captions that highlight objects, actions,\nand temporal transitions directly relevant to a given query, thus improving the\nanswer accuracy. Experimental results demonstrate that our method achieves\nstate-of-the-art performance on Intent-QA (79.0%, +6.2% over previous SOTA),\nEgoSchema subset (75.4%, +3.4%), and NExT-QA (79.6%, +0.4%)."}
{"id": "2504.20844", "pdf": "https://arxiv.org/pdf/2504.20844", "abs": "https://arxiv.org/abs/2504.20844", "authors": ["Angelika Kothe", "Volker Hohmann", "Giso Grimm"], "title": "Effect of Avatar Head Movement on Communication Behaviour, Experience of Presence and Conversation Success in Triadic Conversations", "categories": ["cs.HC", "cs.SD"], "comment": null, "summary": "Interactive communication in virtual reality can be used in experimental\nparadigms to increase the ecological validity of hearing device evaluations.\nThis requires the virtual environment to elicit natural communication behaviour\nin listeners. This study evaluates the effect of virtual animated characters'\nhead movements on participants' communication behaviour and experience.\n  Triadic conversations were conducted between a test participant and two\nconfederates. To facilitate the manipulation of head movements, the\nconversation was conducted in telepresence using a system that transmitted\naudio, head movement data and video with low delay. The confederates were\nrepresented by virtual animated characters (avatars) with different levels of\nanimation: Static heads, automated head movement animations based on speech\nlevel onsets, and animated head movements based on the transmitted head\nmovements of the interlocutors. A condition was also included in which the\nvideos of the interlocutors' heads were embedded in the visual scene.\n  The results show significant effects of animation level on the participants'\nspeech and head movement behaviour as recorded by physical sensors, as well as\non the subjective sense of presence and the success of the conversation. The\nlargest effects were found for the range of head orientation during speech and\nthe perceived realism of avatars. Participants reported that they were spoken\nto in a more helpful way when the avatars showed head movements transmitted\nfrom the interlocutors than when the avatars' heads were static.\n  We therefore conclude that the representation of interlocutors must include\nsufficiently realistic head movements in order to elicit natural communication\nbehaviour."}
{"id": "2504.20629", "pdf": "https://arxiv.org/pdf/2504.20629", "abs": "https://arxiv.org/abs/2504.20629", "authors": ["Jeongsoo Choi", "Ji-Hoon Kim", "Kim Sung-Bin", "Tae-Hyun Oh", "Joon Son Chung"], "title": "AlignDiT: Multimodal Aligned Diffusion Transformer for Synchronized Speech Generation", "categories": ["cs.CV", "cs.AI", "cs.MM"], "comment": null, "summary": "In this paper, we address the task of multimodal-to-speech generation, which\naims to synthesize high-quality speech from multiple input modalities: text,\nvideo, and reference audio. This task has gained increasing attention due to\nits wide range of applications, such as film production, dubbing, and virtual\navatars. Despite recent progress, existing methods still suffer from\nlimitations in speech intelligibility, audio-video synchronization, speech\nnaturalness, and voice similarity to the reference speaker. To address these\nchallenges, we propose AlignDiT, a multimodal Aligned Diffusion Transformer\nthat generates accurate, synchronized, and natural-sounding speech from aligned\nmultimodal inputs. Built upon the in-context learning capability of the DiT\narchitecture, AlignDiT explores three effective strategies to align multimodal\nrepresentations. Furthermore, we introduce a novel multimodal classifier-free\nguidance mechanism that allows the model to adaptively balance information from\neach modality during speech synthesis. Extensive experiments demonstrate that\nAlignDiT significantly outperforms existing methods across multiple benchmarks\nin terms of quality, synchronization, and speaker similarity. Moreover,\nAlignDiT exhibits strong generalization capability across various multimodal\ntasks, such as video-to-speech synthesis and visual forced alignment,\nconsistently achieving state-of-the-art performance. The demo page is available\nat https://mm.kaist.ac.kr/projects/AlignDiT ."}
{"id": "2504.20501", "pdf": "https://arxiv.org/pdf/2504.20501", "abs": "https://arxiv.org/abs/2504.20501", "authors": ["Jia Wang", "Yunan Mei", "Jiarui Liu", "Xin Fan"], "title": "SAM-Guided Robust Representation Learning for One-Shot 3D Medical Image Segmentation", "categories": ["eess.IV", "cs.CV"], "comment": null, "summary": "One-shot medical image segmentation (MIS) is crucial for medical analysis due\nto the burden of medical experts on manual annotation. The recent emergence of\nthe segment anything model (SAM) has demonstrated remarkable adaptation in MIS\nbut cannot be directly applied to one-shot medical image segmentation (MIS) due\nto its reliance on labor-intensive user interactions and the high computational\ncost. To cope with these limitations, we propose a novel SAM-guided robust\nrepresentation learning framework, named RRL-MedSAM, to adapt SAM to one-shot\n3D MIS, which exploits the strong generalization capabilities of the SAM\nencoder to learn better feature representation. We devise a dual-stage\nknowledge distillation (DSKD) strategy to distill general knowledge between\nnatural and medical images from the foundation model to train a lightweight\nencoder, and then adopt a mutual exponential moving average (mutual-EMA) to\nupdate the weights of the general lightweight encoder and medical-specific\nencoder. Specifically, pseudo labels from the registration network are used to\nperform mutual supervision for such two encoders. Moreover, we introduce an\nauto-prompting (AP) segmentation decoder which adopts the mask generated from\nthe general lightweight model as a prompt to assist the medical-specific model\nin boosting the final segmentation performance. Extensive experiments conducted\non three public datasets, i.e., OASIS, CT-lung demonstrate that the proposed\nRRL-MedSAM outperforms state-of-the-art one-shot MIS methods for both\nsegmentation and registration tasks. Especially, our lightweight encoder uses\nonly 3\\% of the parameters compared to the encoder of SAM-Base."}
{"id": "2504.20097", "pdf": "https://arxiv.org/pdf/2504.20097", "abs": "https://arxiv.org/abs/2504.20097", "authors": ["Junran Guo", "Tonglin Mu", "Keyuan Li", "Jianing Li", "Ziyang Luo", "Ye Chen", "Xiaodong Fan", "Jinquan Huang", "Minjie Liu", "Jinbei Zhang", "Ruoyang Qi", "Naiting Gu", "Shihai Sun"], "title": "Long-Distance Field Demonstration of Imaging-Free Drone Identification in Intracity Environments", "categories": ["cs.CV", "quant-ph"], "comment": "15 pages, 9 figures", "summary": "Detecting small objects, such as drones, over long distances presents a\nsignificant challenge with broad implications for security, surveillance,\nenvironmental monitoring, and autonomous systems. Traditional imaging-based\nmethods rely on high-resolution image acquisition, but are often constrained by\nrange, power consumption, and cost. In contrast, data-driven\nsingle-photon-single-pixel light detection and ranging\n(\\text{D\\textsuperscript{2}SP\\textsuperscript{2}-LiDAR}) provides an\nimaging-free alternative, directly enabling target identification while\nreducing system complexity and cost. However, its detection range has been\nlimited to a few hundred meters. Here, we introduce a novel integration of\nresidual neural networks (ResNet) with\n\\text{D\\textsuperscript{2}SP\\textsuperscript{2}-LiDAR}, incorporating a refined\nobservation model to extend the detection range to 5~\\si{\\kilo\\meter} in an\nintracity environment while enabling high-accuracy identification of drone\nposes and types. Experimental results demonstrate that our approach not only\noutperforms conventional imaging-based recognition systems, but also achieves\n94.93\\% pose identification accuracy and 97.99\\% type classification accuracy,\neven under weak signal conditions with long distances and low signal-to-noise\nratios (SNRs). These findings highlight the potential of imaging-free methods\nfor robust long-range detection of small targets in real-world scenarios."}
{"id": "2504.20109", "pdf": "https://arxiv.org/pdf/2504.20109", "abs": "https://arxiv.org/abs/2504.20109", "authors": ["Rajeev Gupta", "Suhani Gupta", "Ronak Parikh", "Divya Gupta", "Amir Javaheri", "Jairaj Singh Shaktawat"], "title": "Personalized Artificial General Intelligence (AGI) via Neuroscience-Inspired Continuous Learning Systems", "categories": ["cs.AI", "cs.LG"], "comment": "39 pages, 16 figures", "summary": "Artificial Intelligence has made remarkable advancements in recent years,\nprimarily driven by increasingly large deep learning models. However, achieving\ntrue Artificial General Intelligence (AGI) demands fundamentally new\narchitectures rather than merely scaling up existing models. Current approaches\nlargely depend on expanding model parameters, which improves task-specific\nperformance but falls short in enabling continuous, adaptable, and generalized\nlearning. Achieving AGI capable of continuous learning and personalization on\nresource-constrained edge devices is an even bigger challenge.\n  This paper reviews the state of continual learning and neuroscience-inspired\nAI, and proposes a novel architecture for Personalized AGI that integrates\nbrain-like learning mechanisms for edge deployment. We review literature on\ncontinuous lifelong learning, catastrophic forgetting, and edge AI, and discuss\nkey neuroscience principles of human learning, including Synaptic Pruning,\nHebbian plasticity, Sparse Coding, and Dual Memory Systems, as inspirations for\nAI systems. Building on these insights, we outline an AI architecture that\nfeatures complementary fast-and-slow learning modules, synaptic\nself-optimization, and memory-efficient model updates to support on-device\nlifelong adaptation.\n  Conceptual diagrams of the proposed architecture and learning processes are\nprovided. We address challenges such as catastrophic forgetting, memory\nefficiency, and system scalability, and present application scenarios for\nmobile AI assistants and embodied AI systems like humanoid robots. We conclude\nwith key takeaways and future research directions toward truly continual,\npersonalized AGI on the edge. While the architecture is theoretical, it\nsynthesizes diverse findings and offers a roadmap for future implementation."}
{"id": "2504.20073", "pdf": "https://arxiv.org/pdf/2504.20073", "abs": "https://arxiv.org/abs/2504.20073", "authors": ["Zihan Wang", "Kangrui Wang", "Qineng Wang", "Pingyue Zhang", "Linjie Li", "Zhengyuan Yang", "Kefan Yu", "Minh Nhat Nguyen", "Licheng Liu", "Eli Gottlieb", "Monica Lam", "Yiping Lu", "Kyunghyun Cho", "Jiajun Wu", "Li Fei-Fei", "Lijuan Wang", "Yejin Choi", "Manling Li"], "title": "RAGEN: Understanding Self-Evolution in LLM Agents via Multi-Turn Reinforcement Learning", "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": null, "summary": "Training large language models (LLMs) as interactive agents presents unique\nchallenges including long-horizon decision making and interacting with\nstochastic environment feedback. While reinforcement learning (RL) has enabled\nprogress in static tasks, multi-turn agent RL training remains underexplored.\nWe propose StarPO (State-Thinking-Actions-Reward Policy Optimization), a\ngeneral framework for trajectory-level agent RL, and introduce RAGEN, a modular\nsystem for training and evaluating LLM agents. Our study on three stylized\nenvironments reveals three core findings. First, our agent RL training shows a\nrecurring mode of Echo Trap where reward variance cliffs and gradient spikes;\nwe address this with StarPO-S, a stabilized variant with trajectory filtering,\ncritic incorporation, and decoupled clipping. Second, we find the shaping of RL\nrollouts would benefit from diverse initial states, medium interaction\ngranularity and more frequent sampling. Third, we show that without\nfine-grained, reasoning-aware reward signals, agent reasoning hardly emerge\nthrough multi-turn RL and they may show shallow strategies or hallucinated\nthoughts. Code and environments are available at\nhttps://github.com/RAGEN-AI/RAGEN."}
{"id": "2504.20276", "pdf": "https://arxiv.org/pdf/2504.20276", "abs": "https://arxiv.org/abs/2504.20276", "authors": ["Dandan Chen Kaptur", "Yue Huang", "Xuejun Ryan Ji", "Yanhui Guo", "Bradley Kaptur"], "title": "Enhancing Systematic Reviews with Large Language Models: Using GPT-4 and Kimi", "categories": ["cs.CL", "stat.AP"], "comment": "13 pages, Paper presented at the National Council on Measurement in\n  Education (NCME) Conference, Denver, Colorado, in April 2025", "summary": "This research delved into GPT-4 and Kimi, two Large Language Models (LLMs),\nfor systematic reviews. We evaluated their performance by comparing\nLLM-generated codes with human-generated codes from a peer-reviewed systematic\nreview on assessment. Our findings suggested that the performance of LLMs\nfluctuates by data volume and question complexity for systematic reviews."}
{"id": "2408.03581", "pdf": "https://arxiv.org/pdf/2408.03581", "abs": "https://arxiv.org/abs/2408.03581", "authors": ["Lior Madmoni", "Zamir Ben-Hur", "Jacob Donley", "Vladimir Tourbabin", "Boaz Rafaely"], "title": "Design and Analysis of Binaural Signal Matching with Arbitrary Microphone Arrays and Listener Head Rotations", "categories": ["eess.AS"], "comment": "Published on EURASIP Journal on audio speech and music processing", "summary": "Binaural reproduction is rapidly becoming a topic of great interest in the\nresearch community, especially with the surge of new and popular devices, such\nas virtual reality headsets, smart glasses, and head-tracked headphones. In\norder to immerse the listener in a virtual or remote environment with such\ndevices, it is essential to generate realistic and accurate binaural signals.\nThis is challenging, especially since the microphone arrays mounted on these\ndevices are typically composed of an arbitrarily-arranged small number of\nmicrophones, which impedes the use of standard audio formats like Ambisonics,\nand provides limited spatial resolution. The binaural signal matching (BSM)\nmethod was developed recently to overcome these challenges. While it produced\nbinaural signals with low error using relatively simple arrays, its performance\ndegraded significantly when head rotation was introduced. This paper aims to\ndevelop the BSM method further and overcome its limitations. For this purpose,\nthe method is first analyzed in detail, and a design framework that guarantees\naccurate binaural reproduction for relatively complex acoustic environments is\npresented. Next, it is shown that the BSM accuracy may significantly degrade at\nhigh frequencies, and thus, a perceptually motivated extension to the method is\nproposed, based on a magnitude least-squares (MagLS) formulation. These\ninsights and developments are then analyzed with the help of an extensive\nsimulation study of a simple six-microphone semi-circular array. It is further\nshown that the BSM-MagLS method can be very useful in compensating for head\nrotations with this array. Finally, a listening experiment is conducted with a\nfour-microphone array on a pair of glasses in a reverberant speech environment\nand including head rotations, where it is shown that BSM-MagLS can indeed\nproduce binaural signals with a high perceived quality."}
{"id": "2504.20117", "pdf": "https://arxiv.org/pdf/2504.20117", "abs": "https://arxiv.org/abs/2504.20117", "authors": ["Shubham Gandhi", "Dhruv Shah", "Manasi Patwardhan", "Lovekesh Vig", "Gautam Shroff"], "title": "ResearchCodeAgent: An LLM Multi-Agent System for Automated Codification of Research Methodologies", "categories": ["cs.SE", "cs.AI", "cs.CL", "cs.MA"], "comment": null, "summary": "In this paper we introduce ResearchCodeAgent, a novel multi-agent system\nleveraging large language models (LLMs) agents to automate the codification of\nresearch methodologies described in machine learning literature. The system\nbridges the gap between high-level research concepts and their practical\nimplementation, allowing researchers auto-generating code of existing research\npapers for benchmarking or building on top-of existing methods specified in the\nliterature with availability of partial or complete starter code.\nResearchCodeAgent employs a flexible agent architecture with a comprehensive\naction suite, enabling context-aware interactions with the research\nenvironment. The system incorporates a dynamic planning mechanism, utilizing\nboth short and long-term memory to adapt its approach iteratively. We evaluate\nResearchCodeAgent on three distinct machine learning tasks with distinct task\ncomplexity and representing different parts of the ML pipeline: data\naugmentation, optimization, and data batching. Our results demonstrate the\nsystem's effectiveness and generalizability, with 46.9% of generated code being\nhigh-quality and error-free, and 25% showing performance improvements over\nbaseline implementations. Empirical analysis shows an average reduction of\n57.9% in coding time compared to manual implementation. We observe higher gains\nfor more complex tasks. ResearchCodeAgent represents a significant step towards\nautomating the research implementation process, potentially accelerating the\npace of machine learning research."}
{"id": "2405.15863", "pdf": "https://arxiv.org/pdf/2405.15863", "abs": "https://arxiv.org/abs/2405.15863", "authors": ["Chang Li", "Ruoyu Wang", "Lijuan Liu", "Jun Du", "Yixuan Sun", "Zilu Guo", "Zhenrong Zhang", "Yuan Jiang", "Jianqing Gao", "Feng Ma"], "title": "QA-MDT: Quality-aware Masked Diffusion Transformer for Enhanced Music Generation", "categories": ["cs.SD", "cs.AI", "eess.AS"], "comment": "2025 International Joint Conference on Artificial Intelligence", "summary": "Text-to-music (TTM) generation, which converts textual descriptions into\naudio, opens up innovative avenues for multimedia creation. Achieving high\nquality and diversity in this process demands extensive, high-quality data,\nwhich are often scarce in available datasets. Most open-source datasets\nfrequently suffer from issues like low-quality waveforms and low text-audio\nconsistency, hindering the advancement of music generation models. To address\nthese challenges, we propose a novel quality-aware training paradigm for\ngenerating high-quality, high-musicality music from large-scale,\nquality-imbalanced datasets. Additionally, by leveraging unique properties in\nthe latent space of musical signals, we adapt and implement a masked diffusion\ntransformer (MDT) model for the TTM task, showcasing its capacity for quality\ncontrol and enhanced musicality. Furthermore, we introduce a three-stage\ncaption refinement approach to address low-quality captions' issue. Experiments\nshow state-of-the-art (SOTA) performance on benchmark datasets including\nMusicCaps and the Song-Describer Dataset with both objective and subjective\nmetrics. Demo audio samples are available at https://qa-mdt.github.io/, code\nand pretrained checkpoints are open-sourced at\nhttps://github.com/ivcylc/OpenMusic."}
{"id": "2504.20669", "pdf": "https://arxiv.org/pdf/2504.20669", "abs": "https://arxiv.org/abs/2504.20669", "authors": ["Joy Battocchio", "Stefano Dell'Anna", "Andrea Montibeller", "Giulia Boato"], "title": "Advance Fake Video Detection via Vision Transformers", "categories": ["cs.CV", "cs.AI", "cs.MM"], "comment": null, "summary": "Recent advancements in AI-based multimedia generation have enabled the\ncreation of hyper-realistic images and videos, raising concerns about their\npotential use in spreading misinformation. The widespread accessibility of\ngenerative techniques, which allow for the production of fake multimedia from\nprompts or existing media, along with their continuous refinement, underscores\nthe urgent need for highly accurate and generalizable AI-generated media\ndetection methods, underlined also by new regulations like the European Digital\nAI Act. In this paper, we draw inspiration from Vision Transformer (ViT)-based\nfake image detection and extend this idea to video. We propose an {original}\n%innovative framework that effectively integrates ViT embeddings over time to\nenhance detection performance. Our method shows promising accuracy,\ngeneralization, and few-shot learning capabilities across a new, large and\ndiverse dataset of videos generated using five open source generative\ntechniques from the state-of-the-art, as well as a separate dataset containing\nvideos produced by proprietary generative methods."}
{"id": "2504.20504", "pdf": "https://arxiv.org/pdf/2504.20504", "abs": "https://arxiv.org/abs/2504.20504", "authors": ["Yutong Du", "Zicheng Liu", "Miao Cao", "Zupeng Liang", "Yali Zong", "Changyou Li"], "title": "Quality-factor inspired deep neural network solver for solving inverse scattering problems", "categories": ["eess.IV", "cs.LG", "physics.comp-ph"], "comment": null, "summary": "Deep neural networks have been applied to address electromagnetic inverse\nscattering problems (ISPs) and shown superior imaging performances, which can\nbe affected by the training dataset, the network architecture and the applied\nloss function. Here, the quality of data samples is cared and valued by the\ndefined quality factor. Based on the quality factor, the composition of the\ntraining dataset is optimized. The network architecture is integrated with the\nresidual connections and channel attention mechanism to improve feature\nextraction. A loss function that incorporates data-fitting error,\nphysical-information constraints and the desired feature of the solution is\ndesigned and analyzed to suppress the background artifacts and improve the\nreconstruction accuracy. Various numerical analysis are performed to\ndemonstrate the superiority of the proposed quality-factor inspired deep neural\nnetwork (QuaDNN) solver and the imaging performance is finally verified by\nexperimental imaging test."}
{"id": "2504.20104", "pdf": "https://arxiv.org/pdf/2504.20104", "abs": "https://arxiv.org/abs/2504.20104", "authors": ["Luiz F. P. Southier", "Marcelo Filipak", "Luiz A. Zanlorensi", "Ildefonso Wasilevski", "Fabio Favarim", "Jefferson T. Oliva", "Marcelo Teixeira", "Dalcimar Casanova"], "title": "An on-production high-resolution longitudinal neonatal fingerprint database in Brazil", "categories": ["cs.CV"], "comment": null, "summary": "The neonatal period is critical for survival, requiring accurate and early\nidentification to enable timely interventions such as vaccinations, HIV\ntreatment, and nutrition programs. Biometric solutions offer potential for\nchild protection by helping to prevent baby swaps, locate missing children, and\nsupport national identity systems. However, developing effective biometric\nidentification systems for newborns remains a major challenge due to the\nphysiological variability caused by finger growth, weight changes, and skin\ntexture alterations during early development. Current literature has attempted\nto address these issues by applying scaling factors to emulate growth-induced\ndistortions in minutiae maps, but such approaches fail to capture the complex\nand non-linear growth patterns of infants. A key barrier to progress in this\ndomain is the lack of comprehensive, longitudinal biometric datasets capturing\nthe evolution of neonatal fingerprints over time. This study addresses this gap\nby focusing on designing and developing a high-quality biometric database of\nneonatal fingerprints, acquired at multiple early life stages. The dataset is\nintended to support the training and evaluation of machine learning models\naimed at emulating the effects of growth on biometric features. We hypothesize\nthat such a dataset will enable the development of more robust and accurate\nDeep Learning-based models, capable of predicting changes in the minutiae map\nwith higher fidelity than conventional scaling-based methods. Ultimately, this\neffort lays the groundwork for more reliable biometric identification systems\ntailored to the unique developmental trajectory of newborns."}
{"id": "2504.20113", "pdf": "https://arxiv.org/pdf/2504.20113", "abs": "https://arxiv.org/abs/2504.20113", "authors": ["Lingbo Li", "Anuradha Mathrani", "Teo Susnjak"], "title": "Transforming Evidence Synthesis: A Systematic Review of the Evolution of Automated Meta-Analysis in the Age of AI", "categories": ["cs.AI", "cs.LG"], "comment": null, "summary": "Exponential growth in scientific literature has heightened the demand for\nefficient evidence-based synthesis, driving the rise of the field of Automated\nMeta-analysis (AMA) powered by natural language processing and machine\nlearning. This PRISMA systematic review introduces a structured framework for\nassessing the current state of AMA, based on screening 978 papers from 2006 to\n2024, and analyzing 54 studies across diverse domains. Findings reveal a\npredominant focus on automating data processing (57%), such as extraction and\nstatistical modeling, while only 17% address advanced synthesis stages. Just\none study (2%) explored preliminary full-process automation, highlighting a\ncritical gap that limits AMA's capacity for comprehensive synthesis. Despite\nrecent breakthroughs in large language models (LLMs) and advanced AI, their\nintegration into statistical modeling and higher-order synthesis, such as\nheterogeneity assessment and bias evaluation, remains underdeveloped. This has\nconstrained AMA's potential for fully autonomous meta-analysis. From our\ndataset spanning medical (67%) and non-medical (33%) applications, we found\nthat AMA has exhibited distinct implementation patterns and varying degrees of\neffectiveness in actually improving efficiency, scalability, and\nreproducibility. While automation has enhanced specific meta-analytic tasks,\nachieving seamless, end-to-end automation remains an open challenge. As AI\nsystems advance in reasoning and contextual understanding, addressing these\ngaps is now imperative. Future efforts must focus on bridging automation across\nall meta-analysis stages, refining interpretability, and ensuring\nmethodological robustness to fully realize AMA's potential for scalable,\ndomain-agnostic synthesis."}
{"id": "2504.20078", "pdf": "https://arxiv.org/pdf/2504.20078", "abs": "https://arxiv.org/abs/2504.20078", "authors": ["Kalyan Cherukuri", "Aarav Lala"], "title": "Low-Rank Matrix Approximation for Neural Network Compression", "categories": ["cs.LG", "cs.CC"], "comment": null, "summary": "Deep Neural Networks (DNNs) are often constrained by their large memories and\ncomputational restrictions. In this paper, we introduce a novel adaptive-rank\nSingular Value Decomposition (ARSVD) that dynamically chooses the rank increase\nof the fully connected layers below a certain threshold in energy expenditure.\nUnlike conventional SVD compression methods that apply a fixed rank reduction\nin all layers, our ARSVD method uses energy distribution to adaptively select\nrank per layer while retaining accuracy. This is done for each layer in an\neffort to use as much energy as possible while maintaining the lowest accuracy\nloss. Such accuracy-adaptive approaches outperform traditional static rank\nreduction methods by providing an improved balance between compression and\nmodel performance. We first train a simple Multi-Layer Perceptron (MLP) on the\nMNIST, CIFAR-10, and CIFAR-100 dataset and evaluate its performance using\naccuracy and F1-score. After applying ARSVD, our results demonstrate that the\ntechnique can achieve substantial model compression without compromising\nclassification accuracy. These results illustrate the usefulness of ARSVD in\ncomputing scenarios where both computational and memory resources are scarce."}
{"id": "2504.20304", "pdf": "https://arxiv.org/pdf/2504.20304", "abs": "https://arxiv.org/abs/2504.20304", "authors": ["Xiulin Yang", "Zhuoxuan Ju", "Lanni Bu", "Zoey Liu", "Nathan Schneider"], "title": "UD-English-CHILDES: A Collected Resource of Gold and Silver Universal Dependencies Trees for Child Language Interactions", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "CHILDES is a widely used resource of transcribed child and child-directed\nspeech. This paper introduces UD-English-CHILDES, the first officially released\nUniversal Dependencies (UD) treebank derived from previously\ndependency-annotated CHILDES data with consistent and unified annotation\nguidelines. Our corpus harmonizes annotations from 11 children and their\ncaregivers, totaling over 48k sentences. We validate existing gold-standard\nannotations under the UD v2 framework and provide an additional 1M\nsilver-standard sentences, offering a consistent resource for computational and\nlinguistic research."}
{"id": "2504.08524", "pdf": "https://arxiv.org/pdf/2504.08524", "abs": "https://arxiv.org/abs/2504.08524", "authors": ["Na Li", "Chuke Wang", "Yu Gu", "Zhifeng Li"], "title": "Mitigating Timbre Leakage with Universal Semantic Mapping Residual Block for Voice Conversion", "categories": ["eess.AS", "cs.AI", "cs.SD"], "comment": null, "summary": "Voice conversion (VC) transforms source speech into a target voice by\npreserving the content. However, timbre information from the source speaker is\ninherently embedded in the content representations, causing significant timbre\nleakage and reducing similarity to the target speaker. To address this, we\nintroduce a residual block to a content extractor. The residual block consists\nof two weighted branches: 1) universal semantic dictionary based Content\nFeature Re-expression (CFR) module, supplying timbre-free content\nrepresentation. 2) skip connection to the original content layer, providing\ncomplementary fine-grained information. In the CFR module, each dictionary\nentry in the universal semantic dictionary represents a phoneme class, computed\nstatistically using speech from multiple speakers, creating a stable,\nspeaker-independent semantic set. We introduce a CFR method to obtain\ntimbre-free content representations by expressing each content frame as a\nweighted linear combination of dictionary entries using corresponding phoneme\nposteriors as weights. Extensive experiments across various VC frameworks\ndemonstrate that our approach effectively mitigates timbre leakage and\nsignificantly improves similarity to the target speaker."}
{"id": "2504.20927", "pdf": "https://arxiv.org/pdf/2504.20927", "abs": "https://arxiv.org/abs/2504.20927", "authors": ["Shahbaz P Qadri Syed", "He Bai"], "title": "Exploiting inter-agent coupling information for efficient reinforcement learning of cooperative LQR", "categories": ["eess.SY", "cs.LG", "cs.MA", "cs.SY", "math.OC"], "comment": "Accepted at Learning for Dynamics and Control (L4DC), 2025", "summary": "Developing scalable and efficient reinforcement learning algorithms for\ncooperative multi-agent control has received significant attention over the\npast years. Existing literature has proposed inexact decompositions of local\nQ-functions based on empirical information structures between the agents. In\nthis paper, we exploit inter-agent coupling information and propose a\nsystematic approach to exactly decompose the local Q-function of each agent. We\ndevelop an approximate least square policy iteration algorithm based on the\nproposed decomposition and identify two architectures to learn the local\nQ-function for each agent. We establish that the worst-case sample complexity\nof the decomposition is equal to the centralized case and derive necessary and\nsufficient graphical conditions on the inter-agent couplings to achieve better\nsample efficiency. We demonstrate the improved sample efficiency and\ncomputational efficiency on numerical examples."}
{"id": "2504.18950", "pdf": "https://arxiv.org/pdf/2504.18950", "abs": "https://arxiv.org/abs/2504.18950", "authors": ["Erfan Loweimi", "Mengjie Qian", "Kate Knill", "Mark Gales"], "title": "Speaker Retrieval in the Wild: Challenges, Effectiveness and Robustness", "categories": ["cs.SD", "cs.IR", "cs.LG", "eess.AS"], "comment": "13 pages, 10 figures, 10 tables, 76 references", "summary": "There is a growing abundance of publicly available or company-owned\naudio/video archives, highlighting the increasing importance of efficient\naccess to desired content and information retrieval from these archives. This\npaper investigates the challenges, solutions, effectiveness, and robustness of\nspeaker retrieval systems developed \"in the wild\" which involves addressing two\nprimary challenges: extraction of task-relevant labels from limited metadata\nfor system development and evaluation, as well as the unconstrained acoustic\nconditions encountered in the archive, ranging from quiet studios to adverse\nnoisy environments. While we focus on the publicly-available BBC Rewind archive\n(spanning 1948 to 1979), our framework addresses the broader issue of speaker\nretrieval on extensive and possibly aged archives with no control over the\ncontent and acoustic conditions. Typically, these archives offer a brief and\ngeneral file description, mostly inadequate for specific applications like\nspeaker retrieval, and manual annotation of such large-scale archives is\nunfeasible. We explore various aspects of system development (e.g., speaker\ndiarisation, embedding extraction, query selection) and analyse the challenges,\npossible solutions, and their functionality. To evaluate the performance, we\nconduct systematic experiments in both clean setup and against various\ndistortions simulating real-world applications. Our findings demonstrate the\neffectiveness and robustness of the developed speaker retrieval systems,\nestablishing the versatility and scalability of the proposed framework for a\nwide range of applications beyond the BBC Rewind corpus."}
{"id": "2504.19458", "pdf": "https://arxiv.org/pdf/2504.19458", "abs": "https://arxiv.org/abs/2504.19458", "authors": ["Taoyu Su", "Jiawei Sheng", "Duohe Ma", "Xiaodong Li", "Juwei Yue", "Mengxiao Song", "Yingkai Tang", "Tingwen Liu"], "title": "Mitigating Modality Bias in Multi-modal Entity Alignment from a Causal Perspective", "categories": ["cs.MM", "cs.CL", "cs.IR"], "comment": "Accepted by SIGIR 2025, 11 pages, 10 figures, 4 tables,", "summary": "Multi-Modal Entity Alignment (MMEA) aims to retrieve equivalent entities from\ndifferent Multi-Modal Knowledge Graphs (MMKGs), a critical information\nretrieval task. Existing studies have explored various fusion paradigms and\nconsistency constraints to improve the alignment of equivalent entities, while\noverlooking that the visual modality may not always contribute positively.\nEmpirically, entities with low-similarity images usually generate\nunsatisfactory performance, highlighting the limitation of overly relying on\nvisual features. We believe the model can be biased toward the visual modality,\nleading to a shortcut image-matching task. To address this, we propose a\ncounterfactual debiasing framework for MMEA, termed CDMEA, which investigates\nvisual modality bias from a causal perspective. Our approach aims to leverage\nboth visual and graph modalities to enhance MMEA while suppressing the direct\ncausal effect of the visual modality on model predictions. By estimating the\nTotal Effect (TE) of both modalities and excluding the Natural Direct Effect\n(NDE) of the visual modality, we ensure that the model predicts based on the\nTotal Indirect Effect (TIE), effectively utilizing both modalities and reducing\nvisual modality bias. Extensive experiments on 9 benchmark datasets show that\nCDMEA outperforms 14 state-of-the-art methods, especially in low-similarity,\nhigh-noise, and low-resource data scenarios."}
{"id": "2504.20892", "pdf": "https://arxiv.org/pdf/2504.20892", "abs": "https://arxiv.org/abs/2504.20892", "authors": ["Zhenduo Shang", "Thomas Blumensath"], "title": "Imaging on the Edge: Mapping Object Corners and Edges with Stereo X-ray Tomography", "categories": ["eess.IV"], "comment": null, "summary": "X-ray computed tomography is a powerful tool for volumetric imaging, where\nthree-dimensional (3D) images are generated from a large number of individual\nX-ray projection images. Collecting the required number of low noise projection\nimages is however time-consuming and so the technique is not currently\napplicable when spatial information needs to be collected with high temporal\nresolution, such as in the study of dynamic processes. In our previous work,\ninspired by stereo vision, we developed stereo X-ray imaging methods that\noperate with only two X-ray projection images. Previously we have shown how\nthis allowed us to map point and line fiducial markers into 3D space at\nsignificantly faster temporal resolutions. In this paper, we make two further\ncontributions. Firstly, instead of utilising internal fiducial markers, we\ndemonstrate the applicability of the method to the 3D mapping of sharp object\ncorners, a problem of interest in measuring the deformation of manufactured\ncomponents under different loads. Furthermore, we demonstrate how the approach\ncan be applied to real stereo X-ray data, even in settings where we do not have\nthe annotated real training data that was required for the training of our\nprevious Machine Learning approach. This is achieved by substituting the real\ndata with a relatively simple synthetic training dataset designed to mimic key\naspects of the real data."}
{"id": "2504.20111", "pdf": "https://arxiv.org/pdf/2504.20111", "abs": "https://arxiv.org/abs/2504.20111", "authors": ["Anubhav Jain", "Yuya Kobayashi", "Naoki Murata", "Yuhta Takida", "Takashi Shibuya", "Yuki Mitsufuji", "Niv Cohen", "Nasir Memon", "Julian Togelius"], "title": "Forging and Removing Latent-Noise Diffusion Watermarks Using a Single Image", "categories": ["cs.CV"], "comment": null, "summary": "Watermarking techniques are vital for protecting intellectual property and\npreventing fraudulent use of media. Most previous watermarking schemes designed\nfor diffusion models embed a secret key in the initial noise. The resulting\npattern is often considered hard to remove and forge into unrelated images. In\nthis paper, we propose a black-box adversarial attack without presuming access\nto the diffusion model weights. Our attack uses only a single watermarked\nexample and is based on a simple observation: there is a many-to-one mapping\nbetween images and initial noises. There are regions in the clean image latent\nspace pertaining to each watermark that get mapped to the same initial noise\nwhen inverted. Based on this intuition, we propose an adversarial attack to\nforge the watermark by introducing perturbations to the images such that we can\nenter the region of watermarked images. We show that we can also apply a\nsimilar approach for watermark removal by learning perturbations to exit this\nregion. We report results on multiple watermarking schemes (Tree-Ring, RingID,\nWIND, and Gaussian Shading) across two diffusion models (SDv1.4 and SDv2.0).\nOur results demonstrate the effectiveness of the attack and expose\nvulnerabilities in the watermarking methods, motivating future research on\nimproving them."}
{"id": "2504.20278", "pdf": "https://arxiv.org/pdf/2504.20278", "abs": "https://arxiv.org/abs/2504.20278", "authors": ["Haoyu Yang", "Kamyar Azizzadenesheli", "Haoxing Ren"], "title": "Deep Physics Prior for First Order Inverse Optimization", "categories": ["cs.AI"], "comment": "10 pages, 5 figure. Under Review", "summary": "Inverse design optimization aims to infer system parameters from observed\nsolutions, posing critical challenges across domains such as semiconductor\nmanufacturing, structural engineering, materials science, and fluid dynamics.\nThe lack of explicit mathematical representations in many systems complicates\nthis process and makes the first order optimization impossible. Mainstream\napproaches, including generative AI and Bayesian optimization, address these\nchallenges but have limitations. Generative AI is computationally expensive,\nwhile Bayesian optimization, relying on surrogate models, suffers from\nscalability, sensitivity to priors, and noise issues, often leading to\nsuboptimal solutions. This paper introduces Deep Physics Prior (DPP), a novel\nmethod enabling first-order gradient-based inverse optimization with surrogate\nmachine learning models. By leveraging pretrained auxiliary Neural Operators,\nDPP enforces prior distribution constraints to ensure robust and meaningful\nsolutions. This approach is particularly effective when prior data and\nobservation distributions are unknown."}
{"id": "2504.20079", "pdf": "https://arxiv.org/pdf/2504.20079", "abs": "https://arxiv.org/abs/2504.20079", "authors": ["Xuan Rao", "Bo Zhao", "Derong Liu", "Cesare Alippi"], "title": "FX-DARTS: Designing Topology-unconstrained Architectures with Differentiable Architecture Search and Entropy-based Super-network Shrinking", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Strong priors are imposed on the search space of Differentiable Architecture\nSearch (DARTS), such that cells of the same type share the same topological\nstructure and each intermediate node retains two operators from distinct nodes.\nWhile these priors reduce optimization difficulties and improve the\napplicability of searched architectures, they hinder the subsequent development\nof automated machine learning (Auto-ML) and prevent the optimization algorithm\nfrom exploring more powerful neural networks through improved architectural\nflexibility. This paper aims to reduce these prior constraints by eliminating\nrestrictions on cell topology and modifying the discretization mechanism for\nsuper-networks. Specifically, the Flexible DARTS (FX-DARTS) method, which\nleverages an Entropy-based Super-Network Shrinking (ESS) framework, is\npresented to address the challenges arising from the elimination of prior\nconstraints. Notably, FX-DARTS enables the derivation of neural architectures\nwithout strict prior rules while maintaining the stability in the enlarged\nsearch space. Experimental results on image classification benchmarks\ndemonstrate that FX-DARTS is capable of exploring a set of neural architectures\nwith competitive trade-offs between performance and computational complexity\nwithin a single search procedure."}
{"id": "2504.20323", "pdf": "https://arxiv.org/pdf/2504.20323", "abs": "https://arxiv.org/abs/2504.20323", "authors": ["Chao-Lin Liu", "Po-Hsien Wu", "Yi-Ting Yu"], "title": "Labeling Case Similarity based on Co-Citation of Legal Articles in Judgment Documents with Empirical Dispute-Based Evaluation", "categories": ["cs.CL", "cs.AI", "cs.DL", "cs.IR", "cs.LG"], "comment": "16 pages, 9 figures, 2 tables, the Nineteenth International Workshop\n  on Juris-Informatics (JURISIN 2025), associated with the Seventeenth JSAI\n  International Symposium on AI (JSAI-isAI 2025)", "summary": "This report addresses the challenge of limited labeled datasets for\ndeveloping legal recommender systems, particularly in specialized domains like\nlabor disputes. We propose a new approach leveraging the co-citation of legal\narticles within cases to establish similarity and enable algorithmic\nannotation. This method draws a parallel to the concept of case co-citation,\nutilizing cited precedents as indicators of shared legal issues. To evaluate\nthe labeled results, we employ a system that recommends similar cases based on\nplaintiffs' accusations, defendants' rebuttals, and points of disputes. The\nevaluation demonstrates that the recommender, with finetuned text embedding\nmodels and a reasonable BiLSTM module can recommend labor cases whose\nsimilarity was measured by the co-citation of the legal articles. This research\ncontributes to the development of automated annotation techniques for legal\ndocuments, particularly in areas with limited access to comprehensive legal\ndatabases."}
{"id": "2504.19062", "pdf": "https://arxiv.org/pdf/2504.19062", "abs": "https://arxiv.org/abs/2504.19062", "authors": ["Yu Zhang", "Wenxiang Guo", "Changhao Pan", "Zhiyuan Zhu", "Ruiqi Li", "Jingyu Lu", "Rongjie Huang", "Ruiyuan Zhang", "Zhiqing Hong", "Ziyue Jiang", "Zhou Zhao"], "title": "Versatile Framework for Song Generation with Prompt-based Control", "categories": ["eess.AS", "cs.CL", "cs.SD"], "comment": null, "summary": "Song generation focuses on producing controllable high-quality songs based on\nvarious prompts. However, existing methods struggle to generate vocals and\naccompaniments with prompt-based control and proper alignment. Additionally,\nthey fall short in supporting various tasks. To address these challenges, we\nintroduce VersBand, a multi-task song generation framework for synthesizing\nhigh-quality, aligned songs with prompt-based control. VersBand comprises these\nprimary models: 1) VocalBand, a decoupled model, leverages the flow-matching\nmethod for generating singing styles, pitches, and mel-spectrograms, allowing\nfast, high-quality vocal generation with style control. 2) AccompBand, a\nflow-based transformer model, incorporates the Band-MOE, selecting suitable\nexperts for enhanced quality, alignment, and control. This model allows for\ngenerating controllable, high-quality accompaniments aligned with vocals. 3)\nTwo generation models, LyricBand for lyrics and MelodyBand for melodies,\ncontribute to the comprehensive multi-task song generation system, allowing for\nextensive control based on multiple prompts. Experimental results demonstrate\nthat VersBand performs better over baseline models across multiple song\ngeneration tasks using objective and subjective metrics. Audio samples are\navailable at https://aaronz345.github.io/VersBandDemo."}
{"id": "2504.20947", "pdf": "https://arxiv.org/pdf/2504.20947", "abs": "https://arxiv.org/abs/2504.20947", "authors": ["Norah K. Alghamdi", "Shinkyu Park"], "title": "Opinion-Driven Decision-Making for Multi-Robot Navigation through Narrow Corridors", "categories": ["cs.RO", "cs.MA"], "comment": null, "summary": "We propose an opinion-driven navigation framework for multi-robot traversal\nthrough a narrow corridor. Our approach leverages a multi-agent decision-making\nmodel known as the Nonlinear Opinion Dynamics (NOD) to address the narrow\ncorridor passage problem, formulated as a multi-robot navigation game. By\nintegrating the NOD model with a multi-robot path planning algorithm, we\ndemonstrate that the framework effectively reduces the likelihood of deadlocks\nduring corridor traversal. To ensure scalability with an increasing number of\nrobots, we introduce a game reduction technique that enables efficient\ncoordination in larger groups. Extensive simulation studies are conducted to\nvalidate the effectiveness of the proposed approach."}
{"id": "2504.19595", "pdf": "https://arxiv.org/pdf/2504.19595", "abs": "https://arxiv.org/abs/2504.19595", "authors": ["Pietro Bongini", "Sara Mandelli", "Andrea Montibeller", "Mirko Casu", "Orazio Pontorno", "Claudio Vittorio Ragaglia", "Luca Zanchetta", "Mattia Aquilina", "Taiba Majid Wani", "Luca Guarnera", "Benedetta Tondi", "Giulia Boato", "Paolo Bestagini", "Irene Amerini", "Francesco De Natale", "Sebastiano Battiato", "Mauro Barni"], "title": "WILD: a new in-the-Wild Image Linkage Dataset for synthetic image attribution", "categories": ["cs.MM", "cs.AI", "cs.CV"], "comment": null, "summary": "Synthetic image source attribution is an open challenge, with an increasing\nnumber of image generators being released yearly. The complexity and the sheer\nnumber of available generative techniques, as well as the scarcity of\nhigh-quality open source datasets of diverse nature for this task, make\ntraining and benchmarking synthetic image source attribution models very\nchallenging. WILD is a new in-the-Wild Image Linkage Dataset designed to\nprovide a powerful training and benchmarking tool for synthetic image\nattribution models. The dataset is built out of a closed set of 10 popular\ncommercial generators, which constitutes the training base of attribution\nmodels, and an open set of 10 additional generators, simulating a real-world\nin-the-wild scenario. Each generator is represented by 1,000 images, for a\ntotal of 10,000 images in the closed set and 10,000 images in the open set.\nHalf of the images are post-processed with a wide range of operators. WILD\nallows benchmarking attribution models in a wide range of tasks, including\nclosed and open set identification and verification, and robust attribution\nwith respect to post-processing and adversarial attacks. Models trained on WILD\nare expected to benefit from the challenging scenario represented by the\ndataset itself. Moreover, an assessment of seven baseline methodologies on\nclosed and open set attribution is presented, including robustness tests with\nrespect to post-processing."}
{"id": "2504.20108", "pdf": "https://arxiv.org/pdf/2504.20108", "abs": "https://arxiv.org/abs/2504.20108", "authors": ["Stephen Ekaputra Limantoro", "Jhe-Hao Lin", "Chih-Yu Wang", "Yi-Lung Tsai", "Hong-Han Shuai", "Ching-Chun Huang", "Wen-Huang Cheng"], "title": "Swapped Logit Distillation via Bi-level Teacher Alignment", "categories": ["cs.LG", "eess.IV", "eess.SP"], "comment": "Accepted to Multimedia Systems 2025", "summary": "Knowledge distillation (KD) compresses the network capacity by transferring\nknowledge from a large (teacher) network to a smaller one (student). It has\nbeen mainstream that the teacher directly transfers knowledge to the student\nwith its original distribution, which can possibly lead to incorrect\npredictions. In this article, we propose a logit-based distillation via swapped\nlogit processing, namely Swapped Logit Distillation (SLD). SLD is proposed\nunder two assumptions: (1) the wrong prediction occurs when the prediction\nlabel confidence is not the maximum; (2) the \"natural\" limit of probability\nremains uncertain as the best value addition to the target cannot be\ndetermined. To address these issues, we propose a swapped logit processing\nscheme. Through this approach, we find that the swap method can be effectively\nextended to teacher and student outputs, transforming into two teachers. We\nfurther introduce loss scheduling to boost the performance of two teachers'\nalignment. Extensive experiments on image classification tasks demonstrate that\nSLD consistently performs best among previous state-of-the-art methods."}
{"id": "2504.20178", "pdf": "https://arxiv.org/pdf/2504.20178", "abs": "https://arxiv.org/abs/2504.20178", "authors": ["Zhe Cui", "Yuli Li", "Le-Nam Tran"], "title": "A Transformer-based Multimodal Fusion Model for Efficient Crowd Counting Using Visual and Wireless Signals", "categories": ["cs.CV", "cs.LG"], "comment": "This paper was accepted at IEEE WCNC 2025", "summary": "Current crowd-counting models often rely on single-modal inputs, such as\nvisual images or wireless signal data, which can result in significant\ninformation loss and suboptimal recognition performance. To address these\nshortcomings, we propose TransFusion, a novel multimodal fusion-based\ncrowd-counting model that integrates Channel State Information (CSI) with image\ndata. By leveraging the powerful capabilities of Transformer networks,\nTransFusion effectively combines these two distinct data modalities, enabling\nthe capture of comprehensive global contextual information that is critical for\naccurate crowd estimation. However, while transformers are well capable of\ncapturing global features, they potentially fail to identify finer-grained,\nlocal details essential for precise crowd counting. To mitigate this, we\nincorporate Convolutional Neural Networks (CNNs) into the model architecture,\nenhancing its ability to extract detailed local features that complement the\nglobal context provided by the Transformer. Extensive experimental evaluations\ndemonstrate that TransFusion achieves high accuracy with minimal counting\nerrors while maintaining superior efficiency."}
{"id": "2504.20294", "pdf": "https://arxiv.org/pdf/2504.20294", "abs": "https://arxiv.org/abs/2504.20294", "authors": ["William P. McCarthy", "Saujas Vaduguru", "Karl D. D. Willis", "Justin Matejka", "Judith E. Fan", "Daniel Fried", "Yewen Pu"], "title": "mrCAD: Multimodal Refinement of Computer-aided Designs", "categories": ["cs.AI", "cs.CL", "cs.HC"], "comment": "the first two authors contributed equally", "summary": "A key feature of human collaboration is the ability to iteratively refine the\nconcepts we have communicated. In contrast, while generative AI excels at the\n\\textit{generation} of content, it often struggles to make specific\nlanguage-guided \\textit{modifications} of its prior outputs. To bridge the gap\nbetween how humans and machines perform edits, we present mrCAD, a dataset of\nmultimodal instructions in a communication game. In each game, players created\ncomputer aided designs (CADs) and refined them over several rounds to match\nspecific target designs. Only one player, the Designer, could see the target,\nand they must instruct the other player, the Maker, using text, drawing, or a\ncombination of modalities. mrCAD consists of 6,082 communication games, 15,163\ninstruction-execution rounds, played between 1,092 pairs of human players. We\nanalyze the dataset and find that generation and refinement instructions differ\nin their composition of drawing and text. Using the mrCAD task as a benchmark,\nwe find that state-of-the-art VLMs are better at following generation\ninstructions than refinement instructions. These results lay a foundation for\nanalyzing and modeling a multimodal language of refinement that is not\nrepresented in previous datasets."}
{"id": "2504.20080", "pdf": "https://arxiv.org/pdf/2504.20080", "abs": "https://arxiv.org/abs/2504.20080", "authors": ["Xuan Rao", "Bo Zhao", "Derong Liu"], "title": "DNAD: Differentiable Neural Architecture Distillation", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "To meet the demand for designing efficient neural networks with appropriate\ntrade-offs between model performance (e.g., classification accuracy) and\ncomputational complexity, the differentiable neural architecture distillation\n(DNAD) algorithm is developed based on two cores, namely search by deleting and\nsearch by imitating. Primarily, to derive neural architectures in a space where\ncells of the same type no longer share the same topology, the super-network\nprogressive shrinking (SNPS) algorithm is developed based on the framework of\ndifferentiable architecture search (DARTS), i.e., search by deleting. Unlike\nconventional DARTS-based approaches which yield neural architectures with\nsimple structures and derive only one architecture during the search procedure,\nSNPS is able to derive a Pareto-optimal set of architectures with flexible\nstructures by forcing the dynamic super-network shrink from a dense structure\nto a sparse one progressively. Furthermore, since knowledge distillation (KD)\nhas shown great effectiveness to train a compact network with the assistance of\nan over-parameterized model, we integrate SNPS with KD to formulate the DNAD\nalgorithm, i.e., search by imitating. By minimizing behavioral differences\nbetween the super-network and teacher network, the over-fitting of one-level\nDARTS is avoided and well-performed neural architectures are derived.\nExperiments on CIFAR-10 and ImageNet classification tasks demonstrate that both\nSNPS and DNAD are able to derive a set of architectures which achieve similar\nor lower error rates with fewer parameters and FLOPs. Particularly, DNAD\nachieves the top-1 error rate of 23.7% on ImageNet classification with a model\nof 6.0M parameters and 598M FLOPs, which outperforms most DARTS-based methods."}
{"id": "2504.20355", "pdf": "https://arxiv.org/pdf/2504.20355", "abs": "https://arxiv.org/abs/2504.20355", "authors": ["Yash Jain", "Vishal Chowdhary"], "title": "Local Prompt Optimization", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "Accepted as Oral at NAACL 2025 (Main Conference)", "summary": "In recent years, the use of prompts to guide the output of Large Language\nModels have increased dramatically. However, even the best of experts struggle\nto choose the correct words to stitch up a prompt for the desired task. To\nsolve this, LLM driven prompt optimization emerged as an important problem.\nExisting prompt optimization methods optimize a prompt globally, where in all\nthe prompt tokens have to be optimized over a large vocabulary while solving a\ncomplex task. The large optimization space (tokens) leads to insufficient\nguidance for a better prompt. In this work, we introduce Local Prompt\nOptimization (LPO) that integrates with any general automatic prompt\nengineering method. We identify the optimization tokens in a prompt and nudge\nthe LLM to focus only on those tokens in its optimization step. We observe\nremarkable performance improvements on Math Reasoning (GSM8k and MultiArith)\nand BIG-bench Hard benchmarks across various automatic prompt engineering\nmethods. Further, we show that LPO converges to the optimal prompt faster than\nglobal methods."}
{"id": "2310.03903", "pdf": "https://arxiv.org/pdf/2310.03903", "abs": "https://arxiv.org/abs/2310.03903", "authors": ["Saaket Agashe", "Yue Fan", "Anthony Reyna", "Xin Eric Wang"], "title": "LLM-Coordination: Evaluating and Analyzing Multi-agent Coordination Abilities in Large Language Models", "categories": ["cs.CL", "cs.MA"], "comment": null, "summary": "Large Language Models (LLMs) have demonstrated emergent common-sense\nreasoning and Theory of Mind (ToM) capabilities, making them promising\ncandidates for developing coordination agents. This study introduces the\nLLM-Coordination Benchmark, a novel benchmark for analyzing LLMs in the context\nof Pure Coordination Settings, where agents must cooperate to maximize gains.\nOur benchmark evaluates LLMs through two distinct tasks. The first is Agentic\nCoordination, where LLMs act as proactive participants in four pure\ncoordination games. The second is Coordination Question Answering (CoordQA),\nwhich tests LLMs on 198 multiple-choice questions across these games to\nevaluate three key abilities: Environment Comprehension, ToM Reasoning, and\nJoint Planning. Results from Agentic Coordination experiments reveal that\nLLM-Agents excel in multi-agent coordination settings where decision-making\nprimarily relies on environmental variables but face challenges in scenarios\nrequiring active consideration of partners' beliefs and intentions. The CoordQA\nexperiments further highlight significant room for improvement in LLMs' Theory\nof Mind reasoning and joint planning capabilities. Zero-Shot Coordination (ZSC)\nexperiments in the Agentic Coordination setting demonstrate that LLM agents,\nunlike RL methods, exhibit robustness to unseen partners. These findings\nindicate the potential of LLMs as Agents in pure coordination setups and\nunderscore areas for improvement. Code Available at\nhttps://github.com/eric-ai-lab/llm_coordination."}
{"id": "2504.20203", "pdf": "https://arxiv.org/pdf/2504.20203", "abs": "https://arxiv.org/abs/2504.20203", "authors": ["Vladyslav Polushko", "Damjan Hatic", "Ronald Rösch", "Thomas März", "Markus Rauhut", "Andreas Weinmann"], "title": "Remote Sensing Imagery for Flood Detection: Exploration of Augmentation Strategies", "categories": ["cs.CV", "eess.IV"], "comment": null, "summary": "Floods cause serious problems around the world. Responding quickly and\neffectively requires accurate and timely information about the affected areas.\nThe effective use of Remote Sensing images for accurate flood detection\nrequires specific detection methods. Typically, Deep Neural Networks are\nemployed, which are trained on specific datasets. For the purpose of river\nflood detection in RGB imagery, we use the BlessemFlood21 dataset. We here\nexplore the use of different augmentation strategies, ranging from basic\napproaches to more complex techniques, including optical distortion. By\nidentifying effective strategies, we aim to refine the training process of\nstate-of-the-art Deep Learning segmentation networks."}
{"id": "2504.20179", "pdf": "https://arxiv.org/pdf/2504.20179", "abs": "https://arxiv.org/abs/2504.20179", "authors": ["Jingjing Wang", "Dan Zhang", "Joshua Luo", "Yin Yang", "Feng Luo"], "title": "Integration Flow Models", "categories": ["cs.CV", "cs.AI", "cs.LG"], "comment": null, "summary": "Ordinary differential equation (ODE) based generative models have emerged as\na powerful approach for producing high-quality samples in many applications.\nHowever, the ODE-based methods either suffer the discretization error of\nnumerical solvers of ODE, which restricts the quality of samples when only a\nfew NFEs are used, or struggle with training instability. In this paper, we\nproposed Integration Flow, which directly learns the integral of ODE-based\ntrajectory paths without solving the ODE functions. Moreover, Integration Flow\nexplicitly incorporates the target state $\\mathbf{x}_0$ as the anchor state in\nguiding the reverse-time dynamics. We have theoretically proven this can\ncontribute to both stability and accuracy. To the best of our knowledge,\nIntegration Flow is the first model with a unified structure to estimate\nODE-based generative models and the first to show the exact straightness of\n1-Rectified Flow without reflow. Through theoretical analysis and empirical\nevaluations, we show that Integration Flows achieve improved performance when\nit is applied to existing ODE-based models, such as diffusion models, Rectified\nFlows, and PFGM++. Specifically, Integration Flow achieves one-step generation\non CIFAR10 with FIDs of 2.86 for the Variance Exploding (VE) diffusion model,\n3.36 for rectified flow without reflow, and 2.91 for PFGM++; and on ImageNet\nwith FIDs of 4.09 for VE diffusion model, 4.35 for rectified flow without\nreflow and 4.15 for PFGM++."}
{"id": "2504.20318", "pdf": "https://arxiv.org/pdf/2504.20318", "abs": "https://arxiv.org/abs/2504.20318", "authors": ["Ryan Xiao Wang", "Felipe Trevizan"], "title": "Leveraging Action Relational Structures for Integrated Learning and Planning", "categories": ["cs.AI"], "comment": "Extended version of ICAPS 2025 paper", "summary": "Recent advances in planning have explored using learning methods to help\nplanning. However, little attention has been given to adapting search\nalgorithms to work better with learning systems. In this paper, we introduce\npartial-space search, a new search space for classical planning that leverages\nthe relational structure of actions given by PDDL action schemas -- a structure\noverlooked by traditional planning approaches. Partial-space search provides a\nmore granular view of the search space and allows earlier pruning of poor\nactions compared to state-space search. To guide partial-space search, we\nintroduce action set heuristics that evaluate sets of actions in a state. We\ndescribe how to automatically convert existing heuristics into action set\nheuristics. We also train action set heuristics from scratch using large\ntraining datasets from partial-space search. Our new planner, LazyLifted,\nexploits our better integrated search and learning heuristics and outperforms\nthe state-of-the-art ML-based heuristic on IPC 2023 learning track (LT)\nbenchmarks. We also show the efficiency of LazyLifted on high-branching factor\ntasks and show that it surpasses LAMA in the combined IPC 2023 LT and\nhigh-branching factor benchmarks."}
{"id": "2504.20096", "pdf": "https://arxiv.org/pdf/2504.20096", "abs": "https://arxiv.org/abs/2504.20096", "authors": ["Damien Martins Gomes"], "title": "Towards Practical Second-Order Optimizers in Deep Learning: Insights from Fisher Information Analysis", "categories": ["cs.LG", "math.OC"], "comment": "Master's Thesis in Computer Science (Deep Learning Dynamics -\n  Optimization)", "summary": "First-order optimization methods remain the standard for training deep neural\nnetworks (DNNs). Optimizers like Adam incorporate limited curvature information\nby preconditioning the stochastic gradient with a diagonal matrix. Despite the\nwidespread adoption of first-order methods, second-order optimization\nalgorithms often exhibit superior convergence compared to methods like Adam and\nSGD. However, their practicality in training DNNs is still limited by a\nsignificantly higher per-iteration computational cost compared to first-order\nmethods. In this thesis, we present AdaFisher, a novel adaptive second-order\noptimizer that leverages a diagonal block-Kronecker approximation of the Fisher\ninformation matrix to adaptively precondition gradients. AdaFisher aims to\nbridge the gap between the improved convergence and generalization of\nsecond-order methods and the computational efficiency needed for training DNNs.\nDespite the traditionally slower speed of second-order optimizers, AdaFisher is\neffective for tasks such as image classification and language modeling,\nexhibiting remarkable stability and robustness during hyperparameter tuning. We\ndemonstrate that AdaFisher outperforms state-of-the-art optimizers in both\naccuracy and convergence speed. The code is available from\nhttps://github.com/AtlasAnalyticsLab/AdaFisher."}
{"id": "2504.20356", "pdf": "https://arxiv.org/pdf/2504.20356", "abs": "https://arxiv.org/abs/2504.20356", "authors": ["Maria Khelli", "Samuel Cahyawijaya", "Ayu Purwarianti", "Genta Indra Winata"], "title": "What Causes Knowledge Loss in Multilingual Language Models?", "categories": ["cs.CL"], "comment": null, "summary": "Cross-lingual transfer in natural language processing (NLP) models enhances\nmultilingual performance by leveraging shared linguistic knowledge. However,\ntraditional methods that process all data simultaneously often fail to mimic\nreal-world scenarios, leading to challenges like catastrophic forgetting, where\nfine-tuning on new tasks degrades performance on previously learned ones. Our\nstudy explores this issue in multilingual contexts, focusing on linguistic\ndifferences affecting representational learning rather than just model\nparameters. We experiment with 52 languages using LoRA adapters of varying\nranks to evaluate non-shared, partially shared, and fully shared parameters.\nOur aim is to see if parameter sharing through adapters can mitigate forgetting\nwhile preserving prior knowledge. We find that languages using non-Latin\nscripts are more susceptible to catastrophic forgetting, whereas those written\nin Latin script facilitate more effective cross-lingual transfer."}
{"id": "2403.14562", "pdf": "https://arxiv.org/pdf/2403.14562", "abs": "https://arxiv.org/abs/2403.14562", "authors": ["Maxime Peyrard", "Martin Josifoski", "Robert West"], "title": "Agentic AI: The Era of Semantic Decoding", "categories": ["cs.CL", "cs.AI", "cs.HC", "cs.MA"], "comment": "25 pages, 3 figures", "summary": "Recent work demonstrated great promise in the idea of orchestrating\ncollaborations between LLMs, human input, and various tools to address the\ninherent limitations of LLMs. We propose a novel perspective called semantic\ndecoding, which frames these collaborative processes as optimization procedures\nin semantic space. Specifically, we conceptualize LLMs as semantic processors\nthat manipulate meaningful pieces of information that we call semantic tokens\n(known thoughts). LLMs are among a large pool of other semantic processors,\nincluding humans and tools, such as search engines or code executors.\nCollectively, semantic processors engage in dynamic exchanges of semantic\ntokens to progressively construct high-utility outputs. We refer to these\norchestrated interactions among semantic processors, optimizing and searching\nin semantic space, as semantic decoding algorithms. This concept draws a direct\nparallel to the well-studied problem of syntactic decoding, which involves\ncrafting algorithms to best exploit auto-regressive language models for\nextracting high-utility sequences of syntactic tokens. By focusing on the\nsemantic level and disregarding syntactic details, we gain a fresh perspective\non the engineering of AI systems, enabling us to imagine systems with much\ngreater complexity and capabilities. In this position paper, we formalize the\ntransition from syntactic to semantic tokens as well as the analogy between\nsyntactic and semantic decoding. Subsequently, we explore the possibilities of\noptimizing within the space of semantic tokens via semantic decoding\nalgorithms. We conclude with a list of research opportunities and questions\narising from this fresh perspective. The semantic decoding perspective offers a\npowerful abstraction for search and optimization directly in the space of\nmeaningful concepts, with semantic tokens as the fundamental units of a new\ntype of computation."}
{"id": "2504.20383", "pdf": "https://arxiv.org/pdf/2504.20383", "abs": "https://arxiv.org/abs/2504.20383", "authors": ["Shiyin Jiang", "Zhenghao Chen", "Minghao Han", "Xingyu Zhou", "Leheng Zhang", "Shuhang Gu"], "title": "Neural Stereo Video Compression with Hybrid Disparity Compensation", "categories": ["cs.CV", "eess.IV"], "comment": null, "summary": "Disparity compensation represents the primary strategy in stereo video\ncompression (SVC) for exploiting cross-view redundancy. These mechanisms can be\nbroadly categorized into two types: one that employs explicit horizontal\nshifting, and another that utilizes an implicit cross-attention mechanism to\nreduce cross-view disparity redundancy. In this work, we propose a hybrid\ndisparity compensation (HDC) strategy that leverages explicit pixel\ndisplacement as a robust prior feature to simplify optimization and perform\nimplicit cross-attention mechanisms for subsequent warping operations, thereby\ncapturing a broader range of disparity information. Specifically, HDC first\ncomputes a similarity map by fusing the horizontally shifted cross-view\nfeatures to capture pixel displacement information. This similarity map is then\nnormalized into an \"explicit pixel-wise attention score\" to perform the\ncross-attention mechanism, implicitly aligning features from one view to\nanother. Building upon HDC, we introduce a novel end-to-end optimized neural\nstereo video compression framework, which integrates HDC-based modules into key\ncoding operations, including cross-view feature extraction and reconstruction\n(HDC-FER) and cross-view entropy modeling (HDC-EM). Extensive experiments on\nSVC benchmarks, including KITTI 2012, KITTI 2015, and Nagoya, which cover both\nautonomous driving and general scenes, demonstrate that our framework\noutperforms both neural and traditional SVC methodologies."}
{"id": "2504.20199", "pdf": "https://arxiv.org/pdf/2504.20199", "abs": "https://arxiv.org/abs/2504.20199", "authors": ["Juntian Zhang", "Chuanqi cheng", "Yuhan Liu", "Wei Liu", "Jian Luan", "Rui Yan"], "title": "Weaving Context Across Images: Improving Vision-Language Models through Focus-Centric Visual Chains", "categories": ["cs.CV", "cs.AI", "cs.CL"], "comment": null, "summary": "Vision-language models (VLMs) achieve remarkable success in single-image\ntasks. However, real-world scenarios often involve intricate multi-image\ninputs, leading to a notable performance decline as models struggle to\ndisentangle critical information scattered across complex visual features. In\nthis work, we propose Focus-Centric Visual Chain, a novel paradigm that\nenhances VLMs'perception, comprehension, and reasoning abilities in multi-image\nscenarios. To facilitate this paradigm, we propose Focus-Centric Data\nSynthesis, a scalable bottom-up approach for synthesizing high-quality data\nwith elaborate reasoning paths. Through this approach, We construct VISC-150K,\na large-scale dataset with reasoning data in the form of Focus-Centric Visual\nChain, specifically designed for multi-image tasks. Experimental results on\nseven multi-image benchmarks demonstrate that our method achieves average\nperformance gains of 3.16% and 2.24% across two distinct model architectures,\nwithout compromising the general vision-language capabilities. our study\nrepresents a significant step toward more robust and capable vision-language\nsystems that can handle complex visual scenarios."}
{"id": "2504.20340", "pdf": "https://arxiv.org/pdf/2504.20340", "abs": "https://arxiv.org/abs/2504.20340", "authors": ["Khoi Trinh", "Scott Seidenberger", "Raveen Wijewickrama", "Murtuza Jadliwala", "Anindya Maiti"], "title": "A Picture is Worth a Thousand Prompts? Efficacy of Iterative Human-Driven Prompt Refinement in Image Regeneration Tasks", "categories": ["cs.AI", "cs.CV", "cs.HC"], "comment": null, "summary": "With AI-generated content becoming ubiquitous across the web, social media,\nand other digital platforms, it is vital to examine how such content are\ninspired and generated. The creation of AI-generated images often involves\nrefining the input prompt iteratively to achieve desired visual outcomes. This\nstudy focuses on the relatively underexplored concept of image regeneration\nusing AI, in which a human operator attempts to closely recreate a specific\ntarget image by iteratively refining their prompt. Image regeneration is\ndistinct from normal image generation, which lacks any predefined visual\nreference. A separate challenge lies in determining whether existing image\nsimilarity metrics (ISMs) can provide reliable, objective feedback in iterative\nworkflows, given that we do not fully understand if subjective human judgments\nof similarity align with these metrics. Consequently, we must first validate\ntheir alignment with human perception before assessing their potential as a\nfeedback mechanism in the iterative prompt refinement process. To address these\nresearch gaps, we present a structured user study evaluating how iterative\nprompt refinement affects the similarity of regenerated images relative to\ntheir targets, while also examining whether ISMs capture the same improvements\nperceived by human observers. Our findings suggest that incremental prompt\nadjustments substantially improve alignment, verified through both subjective\nevaluations and quantitative measures, underscoring the broader potential of\niterative workflows to enhance generative AI content creation across various\napplication domains."}
{"id": "2504.20099", "pdf": "https://arxiv.org/pdf/2504.20099", "abs": "https://arxiv.org/abs/2504.20099", "authors": ["Inmaculada Santamaria-Valenzuela", "Victor Rodriguez-Fernandez", "Javier Huertas-Tato", "Jong Hyuk Park", "David Camacho"], "title": "Decoding Latent Spaces: Assessing the Interpretability of Time Series Foundation Models for Visual Analytics", "categories": ["cs.LG", "cs.AI", "stat.ML"], "comment": "Currently under review at the International Journal of Interactive\n  Multimedia and Artificial Intelligence (IJIMAI)", "summary": "The present study explores the interpretability of latent spaces produced by\ntime series foundation models, focusing on their potential for visual analysis\ntasks. Specifically, we evaluate the MOMENT family of models, a set of\ntransformer-based, pre-trained architectures for multivariate time series tasks\nsuch as: imputation, prediction, classification, and anomaly detection. We\nevaluate the capacity of these models on five datasets to capture the\nunderlying structures in time series data within their latent space projection\nand validate whether fine tuning improves the clarity of the resulting\nembedding spaces. Notable performance improvements in terms of loss reduction\nwere observed after fine tuning. Visual analysis shows limited improvement in\nthe interpretability of the embeddings, requiring further work. Results suggest\nthat, although Time Series Foundation Models such as MOMENT are robust, their\nlatent spaces may require additional methodological refinements to be\nadequately interpreted, such as alternative projection techniques, loss\nfunctions, or data preprocessing strategies. Despite the limitations of MOMENT,\nfoundation models supose a big reduction in execution time and so a great\nadvance for interactive visual analytics."}
{"id": "2504.20371", "pdf": "https://arxiv.org/pdf/2504.20371", "abs": "https://arxiv.org/abs/2504.20371", "authors": ["Zhibo Man", "Yuanmeng Chen", "Yujie Zhang", "Yufeng Chen", "Jinan Xu"], "title": "DMDTEval: An Evaluation and Analysis of LLMs on Disambiguation in Multi-domain Translation", "categories": ["cs.CL"], "comment": null, "summary": "Currently, Large Language Models (LLMs) have achieved remarkable results in\nmachine translation. However, their performance in multi-domain translation\n(MDT) is less satisfactory; the meanings of words can vary across different\ndomains, highlighting the significant ambiguity inherent in MDT. Therefore,\nevaluating the disambiguation ability of LLMs in MDT remains an open problem.\nTo this end, we present an evaluation and analysis of LLMs on disambiguation in\nmulti-domain translation (DMDTEval), our systematic evaluation framework\nconsisting of three critical aspects: (1) we construct a translation test set\nwith multi-domain ambiguous word annotation, (2) we curate a diverse set of\ndisambiguation prompting templates, and (3) we design precise disambiguation\nmetrics, and study the efficacy of various prompting strategies on multiple\nstate-of-the-art LLMs. Our extensive experiments reveal a number of crucial\nfindings that we believe will pave the way and also facilitate further research\nin the critical area of improving the disambiguation of LLMs."}
{"id": "2206.08928", "pdf": "https://arxiv.org/pdf/2206.08928", "abs": "https://arxiv.org/abs/2206.08928", "authors": ["Amit Kohli", "Anastasios N. Angelopoulos", "David McAllister", "Esther Whang", "Sixian You", "Kyrollos Yanny", "Federico M. Gasparoli", "Bo-Jui Chang", "Reto Fiolka", "Laura Waller"], "title": "Ring deconvolution microscopy: exploiting symmetry for efficient spatially varying aberration correction", "categories": ["eess.IV"], "comment": "32 pages, 14 figures. The Version of Record of this article is\n  published in Nature Methods, and is available online at\n  https://doi.org/10.1038/s41592-025-02684-5", "summary": "The most ubiquitous form of computational aberration correction for\nmicroscopy is deconvolution. However, deconvolution relies on the assumption\nthat the point spread function is the same across the entire field-of-view.\nThis assumption is often inadequate, but space-variant deblurring techniques\ngenerally require impractical amounts of calibration and computation. We\npresent a new imaging pipeline that leverages symmetry to provide simple and\nfast spatially-varying aberration correction. Our ring deconvolution microscopy\n(RDM) method leverages the rotational symmetry of most microscopes and cameras,\nand naturally extends to sheet deconvolution in the case of lateral symmetry.\nWe formally derive theory and algorithms for image recovery and additionally\npropose a neural network based on Seidel coefficients as a fast alternative, as\nwell as extension of RDM to blind deconvolution. We demonstrate significant\nimprovements in speed and image quality as compared to standard deconvolution\nand existing spatially-varying deconvolution across a diverse range of\nmicroscope modalities, including miniature microscopy, multicolor fluorescence\nmicroscopy, point-scanning multimode fiber micro-endoscopy, and light-sheet\nfluorescence microscopy. Our approach enables near-isotropic, subcellular\nresolution in each of these applications."}
{"id": "2504.20222", "pdf": "https://arxiv.org/pdf/2504.20222", "abs": "https://arxiv.org/abs/2504.20222", "authors": ["Naoko Sawada", "Pedro Miraldo", "Suhas Lohit", "Tim K. Marks", "Moitreya Chatterjee"], "title": "FreBIS: Frequency-Based Stratification for Neural Implicit Surface Representations", "categories": ["cs.CV"], "comment": "Accepted to CVPR 2025 CV4Metaverse Workshop", "summary": "Neural implicit surface representation techniques are in high demand for\nadvancing technologies in augmented reality/virtual reality, digital twins,\nautonomous navigation, and many other fields. With their ability to model\nobject surfaces in a scene as a continuous function, such techniques have made\nremarkable strides recently, especially over classical 3D surface\nreconstruction methods, such as those that use voxels or point clouds. However,\nthese methods struggle with scenes that have varied and complex surfaces\nprincipally because they model any given scene with a single encoder network\nthat is tasked to capture all of low through high-surface frequency information\nin the scene simultaneously. In this work, we propose a novel, neural implicit\nsurface representation approach called FreBIS to overcome this challenge.\nFreBIS works by stratifying the scene based on the frequency of surfaces into\nmultiple frequency levels, with each level (or a group of levels) encoded by a\ndedicated encoder. Moreover, FreBIS encourages these encoders to capture\ncomplementary information by promoting mutual dissimilarity of the encoded\nfeatures via a novel, redundancy-aware weighting module. Empirical evaluations\non the challenging BlendedMVS dataset indicate that replacing the standard\nencoder in an off-the-shelf neural surface reconstruction method with our\nfrequency-stratified encoders yields significant improvements. These\nenhancements are evident both in the quality of the reconstructed 3D surfaces\nand in the fidelity of their renderings from any viewpoint."}
{"id": "2504.20406", "pdf": "https://arxiv.org/pdf/2504.20406", "abs": "https://arxiv.org/abs/2504.20406", "authors": ["Paiheng Xu", "Gang Wu", "Xiang Chen", "Tong Yu", "Chang Xiao", "Franck Dernoncourt", "Tianyi Zhou", "Wei Ai", "Viswanathan Swaminathan"], "title": "Skill Discovery for Software Scripting Automation via Offline Simulations with LLMs", "categories": ["cs.AI", "cs.SE"], "comment": null, "summary": "Scripting interfaces enable users to automate tasks and customize software\nworkflows, but creating scripts traditionally requires programming expertise\nand familiarity with specific APIs, posing barriers for many users. While Large\nLanguage Models (LLMs) can generate code from natural language queries, runtime\ncode generation is severely limited due to unverified code, security risks,\nlonger response times, and higher computational costs. To bridge the gap, we\npropose an offline simulation framework to curate a software-specific skillset,\na collection of verified scripts, by exploiting LLMs and publicly available\nscripting guides. Our framework comprises two components: (1) task creation,\nusing top-down functionality guidance and bottom-up API synergy exploration to\ngenerate helpful tasks; and (2) skill generation with trials, refining and\nvalidating scripts based on execution feedback. To efficiently navigate the\nextensive API landscape, we introduce a Graph Neural Network (GNN)-based link\nprediction model to capture API synergy, enabling the generation of skills\ninvolving underutilized APIs and expanding the skillset's diversity.\nExperiments with Adobe Illustrator demonstrate that our framework significantly\nimproves automation success rates, reduces response time, and saves runtime\ntoken costs compared to traditional runtime code generation. This is the first\nattempt to use software scripting interfaces as a testbed for LLM-based\nsystems, highlighting the advantages of leveraging execution feedback in a\ncontrolled environment and offering valuable insights into aligning AI\ncapabilities with user needs in specialized software domains."}
{"id": "2504.20102", "pdf": "https://arxiv.org/pdf/2504.20102", "abs": "https://arxiv.org/abs/2504.20102", "authors": ["Qingzhi Yu", "Shuai Yan", "Wenfeng Dai", "Xiang Cheng"], "title": "HyboWaveNet: Hyperbolic Graph Neural Networks with Multi-Scale Wavelet Transform for Protein-Protein Interaction Prediction", "categories": ["cs.LG", "cs.AI", "q-bio.QM"], "comment": "9 pages", "summary": "Protein-protein interactions (PPIs) are fundamental for deciphering cellular\nfunctions,disease pathways,and drug discovery.Although existing neural networks\nand machine learning methods have achieved high accuracy in PPI\nprediction,their black-box nature leads to a lack of causal interpretation of\nthe prediction results and difficulty in capturing hierarchical geometries and\nmulti-scale dynamic interaction patterns among proteins.To address these\nchallenges, we propose HyboWaveNet,a novel deep learning framework that\ncollaborates with hyperbolic graphical neural networks (HGNNs) and multiscale\ngraphical wavelet transform for robust PPI prediction. Mapping protein features\nto Lorentz space simulates hierarchical topological relationships among\nbiomolecules via a hyperbolic distance metric,enabling node feature\nrepresentations that better fit biological a priori.HyboWaveNet inherently\nsimulates hierarchical and scale-free biological relationships, while the\nintegration of wavelet transforms enables adaptive extraction of local and\nglobal interaction features across different resolutions. Our framework\ngenerates node feature representations via a graph neural network under the\nLorenz model and generates pairs of positive samples under multiple different\nviews for comparative learning, followed by further feature extraction via\nmulti-scale graph wavelet transforms to predict potential PPIs. Experiments on\npublic datasets show that HyboWaveNet improves over both existing\nstate-of-the-art methods. We also demonstrate through ablation experimental\nstudies that the multi-scale graph wavelet transform module improves the\npredictive performance and generalization ability of HyboWaveNet. This work\nlinks geometric deep learning and signal processing to advance PPI prediction,\nproviding a principled approach for analyzing complex biological systems"}
{"id": "2504.20444", "pdf": "https://arxiv.org/pdf/2504.20444", "abs": "https://arxiv.org/abs/2504.20444", "authors": ["Mika Hämäläinen"], "title": "On Psychology of AI -- Does Primacy Effect Affect ChatGPT and Other LLMs?", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "We study the primacy effect in three commercial LLMs: ChatGPT, Gemini and\nClaude. We do this by repurposing the famous experiment Asch (1946) conducted\nusing human subjects. The experiment is simple, given two candidates with equal\ndescriptions which one is preferred if one description has positive adjectives\nfirst before negative ones and another description has negative adjectives\nfollowed by positive ones. We test this in two experiments. In one experiment,\nLLMs are given both candidates simultaneously in the same prompt, and in\nanother experiment, LLMs are given both candidates separately. We test all the\nmodels with 200 candidate pairs. We found that, in the first experiment,\nChatGPT preferred the candidate with positive adjectives listed first, while\nGemini preferred both equally often. Claude refused to make a choice. In the\nsecond experiment, ChatGPT and Claude were most likely to rank both candidates\nequally. In the case where they did not give an equal rating, both showed a\nclear preference to a candidate that had negative adjectives listed first.\nGemini was most likely to prefer a candidate with negative adjectives listed\nfirst."}
{"id": "2502.12181", "pdf": "https://arxiv.org/pdf/2502.12181", "abs": "https://arxiv.org/abs/2502.12181", "authors": ["Melane Navaratnarajah", "Sophie A. Martin", "David A. Kelly", "Nathan Blake", "Hana Chockler"], "title": "3D ReX: Causal Explanations in 3D Neuroimaging Classification", "categories": ["eess.IV", "cs.AI", "cs.CV", "cs.LG"], "comment": "Presented in the 2nd Workshop on Imageomics (Imageomics-AAAI-25),\n  Discovering Biological Knowledge from Images using AI, held as part of\n  AAAI-2025", "summary": "Explainability remains a significant problem for AI models in medical\nimaging, making it challenging for clinicians to trust AI-driven predictions.\nWe introduce 3D ReX, the first causality-based post-hoc explainability tool for\n3D models. 3D ReX uses the theory of actual causality to generate\nresponsibility maps which highlight the regions most crucial to the model's\ndecision. We test 3D ReX on a stroke detection model, providing insight into\nthe spatial distribution of features relevant to stroke."}
{"id": "2504.20234", "pdf": "https://arxiv.org/pdf/2504.20234", "abs": "https://arxiv.org/abs/2504.20234", "authors": ["Bartosz Ptak", "Marek Kraft"], "title": "Improving trajectory continuity in drone-based crowd monitoring using a set of minimal-cost techniques and deep discriminative correlation filters", "categories": ["cs.CV", "cs.RO"], "comment": "Preprint submitted to the Expert Systems with Applications journal", "summary": "Drone-based crowd monitoring is the key technology for applications in\nsurveillance, public safety, and event management. However, maintaining\ntracking continuity and consistency remains a significant challenge.\nTraditional detection-assignment tracking methods struggle with false\npositives, false negatives, and frequent identity switches, leading to degraded\ncounting accuracy and making in-depth analysis impossible. This paper\nintroduces a point-oriented online tracking algorithm that improves trajectory\ncontinuity and counting reliability in drone-based crowd monitoring. Our method\nbuilds on the Simple Online and Real-time Tracking (SORT) framework, replacing\nthe original bounding-box assignment with a point-distance metric. The\nalgorithm is enhanced with three cost-effective techniques: camera motion\ncompensation, altitude-aware assignment, and classification-based trajectory\nvalidation. Further, Deep Discriminative Correlation Filters (DDCF) that re-use\nspatial feature maps from localisation algorithms for increased computational\nefficiency through neural network resource sharing are integrated to refine\nobject tracking by reducing noise and handling missed detections. The proposed\nmethod is evaluated on the DroneCrowd and newly shared UP-COUNT-TRACK datasets,\ndemonstrating substantial improvements in tracking metrics, reducing counting\nerrors to 23% and 15%, respectively. The results also indicate a significant\nreduction of identity switches while maintaining high tracking accuracy,\noutperforming baseline online trackers and even an offline greedy optimisation\nmethod."}
{"id": "2504.20426", "pdf": "https://arxiv.org/pdf/2504.20426", "abs": "https://arxiv.org/abs/2504.20426", "authors": ["Jiapeng Wang", "Jinhao Jiang", "Zhiqiang Zhang", "Jun Zhou", "Wayne Xin Zhao"], "title": "RV-Syn: Rational and Verifiable Mathematical Reasoning Data Synthesis based on Structured Function Library", "categories": ["cs.AI"], "comment": null, "summary": "The advancement of reasoning capabilities in Large Language Models (LLMs)\nrequires substantial amounts of high-quality reasoning data, particularly in\nmathematics. Existing data synthesis methods, such as data augmentation from\nannotated training sets or direct question generation based on relevant\nknowledge points and documents, have expanded datasets but face challenges in\nmastering the inner logic of the problem during generation and ensuring the\nverifiability of the solutions. To address these issues, we propose RV-Syn, a\nnovel Rational and Verifiable mathematical Synthesis approach. RV-Syn\nconstructs a structured mathematical operation function library based on\ninitial seed problems and generates computational graphs as solutions by\ncombining Python-formatted functions from this library. These graphs are then\nback-translated into complex problems. Based on the constructed computation\ngraph, we achieve solution-guided logic-aware problem generation. Furthermore,\nthe executability of the computational graph ensures the verifiability of the\nsolving process. Experimental results show that RV-Syn surpasses existing\nsynthesis methods, including those involving human-generated problems,\nachieving greater efficient data scaling. This approach provides a scalable\nframework for generating high-quality reasoning datasets."}
{"id": "2504.20106", "pdf": "https://arxiv.org/pdf/2504.20106", "abs": "https://arxiv.org/abs/2504.20106", "authors": ["Ren-Wei Liang", "Chin-Ting Hsu", "Chan-Hung Yu", "Saransh Agrawal", "Shih-Cheng Huang", "Shang-Tse Chen", "Kuan-Hao Huang", "Shao-Hua Sun"], "title": "Adaptive Helpfulness-Harmlessness Alignment with Preference Vectors", "categories": ["cs.LG", "cs.AI"], "comment": "22 pages, 5 figures, 9 tables", "summary": "Ensuring that large language models (LLMs) are both helpful and harmless is a\ncritical challenge, as overly strict constraints can lead to excessive\nrefusals, while permissive models risk generating harmful content. Existing\napproaches, such as reinforcement learning from human feedback (RLHF) and\ndirect preference optimization (DPO), attempt to balance these trade-offs but\nsuffer from performance conflicts, limited controllability, and poor\nextendability. To address these issues, we propose Preference Vector, a novel\nframework inspired by task arithmetic. Instead of optimizing multiple\npreferences within a single objective, we train separate models on individual\npreferences, extract behavior shifts as preference vectors, and dynamically\nmerge them at test time. This modular approach enables fine-grained,\nuser-controllable preference adjustments and facilitates seamless integration\nof new preferences without retraining. Experiments show that our proposed\nPreference Vector framework improves helpfulness without excessive\nconservatism, allows smooth control over preference trade-offs, and supports\nscalable multi-preference alignment."}
{"id": "2504.20451", "pdf": "https://arxiv.org/pdf/2504.20451", "abs": "https://arxiv.org/abs/2504.20451", "authors": ["Daniel Lee", "Harsh Sharma", "Jieun Han", "Sunny Jeong", "Alice Oh", "Vered Shwartz"], "title": "Team ACK at SemEval-2025 Task 2: Beyond Word-for-Word Machine Translation for English-Korean Pairs", "categories": ["cs.CL", "cs.IR"], "comment": "Accepted at SemEval-2025 Workshop (ACL 2025)", "summary": "Translating knowledge-intensive and entity-rich text between English and\nKorean requires transcreation to preserve language-specific and cultural\nnuances beyond literal, phonetic or word-for-word conversion. We evaluate 13\nmodels (LLMs and MT models) using automatic metrics and human assessment by\nbilingual annotators. Our findings show LLMs outperform traditional MT systems\nbut struggle with entity translation requiring cultural adaptation. By\nconstructing an error taxonomy, we identify incorrect responses and entity name\nerrors as key issues, with performance varying by entity type and popularity\nlevel. This work exposes gaps in automatic evaluation metrics and hope to\nenable future work in completing culturally-nuanced machine translation."}
{"id": "2503.04821", "pdf": "https://arxiv.org/pdf/2503.04821", "abs": "https://arxiv.org/abs/2503.04821", "authors": ["Zelin Meng", "Takanori Fukao"], "title": "RGB-Thermal Infrared Fusion for Robust Depth Estimation in Complex Environments", "categories": ["eess.IV", "cs.AI", "cs.CV"], "comment": "7 pages, 2 figures", "summary": "Depth estimation in complex real-world scenarios is a challenging task,\nespecially when relying solely on a single modality such as visible light or\nthermal infrared (THR) imagery. This paper proposes a novel multimodal depth\nestimation model, RTFusion, which enhances depth estimation accuracy and\nrobustness by integrating the complementary strengths of RGB and THR data. The\nRGB modality provides rich texture and color information, while the THR\nmodality captures thermal patterns, ensuring stability under adverse lighting\nconditions such as extreme illumination. The model incorporates a unique fusion\nmechanism, EGFusion, consisting of the Mutual Complementary Attention (MCA)\nmodule for cross-modal feature alignment and the Edge Saliency Enhancement\nModule (ESEM) to improve edge detail preservation. Comprehensive experiments on\nthe MS2 and ViViD++ datasets demonstrate that the proposed model consistently\nproduces high-quality depth maps across various challenging environments,\nincluding nighttime, rainy, and high-glare conditions. The experimental results\nhighlight the potential of the proposed method in applications requiring\nreliable depth estimation, such as autonomous driving, robotics, and augmented\nreality."}
{"id": "2504.20241", "pdf": "https://arxiv.org/pdf/2504.20241", "abs": "https://arxiv.org/abs/2504.20241", "authors": ["Kamirul Kamirul", "Odysseas Pappas", "Alin Achim"], "title": "Physics-Informed Diffusion Models for SAR Ship Wake Generation from Text Prompts", "categories": ["cs.CV"], "comment": "4 pages; Submitted Machine Intelligence for GeoAnalytics and Remote\n  Sensing (MIGARS) - 2025", "summary": "Detecting ship presence via wake signatures in SAR imagery is attracting\nconsiderable research interest, but limited annotated data availability poses\nsignificant challenges for supervised learning. Physics-based simulations are\ncommonly used to address this data scarcity, although they are slow and\nconstrain end-to-end learning. In this work, we explore a new direction for\nmore efficient and end-to-end SAR ship wake simulation using a diffusion model\ntrained on data generated by a physics-based simulator. The training dataset is\nbuilt by pairing images produced by the simulator with text prompts derived\nfrom simulation parameters. Experimental result show that the model generates\nrealistic Kelvin wake patterns and achieves significantly faster inference than\nthe physics-based simulator. These results highlight the potential of diffusion\nmodels for fast and controllable wake image generation, opening new\npossibilities for end-to-end downstream tasks in maritime SAR analysis."}
{"id": "2504.20445", "pdf": "https://arxiv.org/pdf/2504.20445", "abs": "https://arxiv.org/abs/2504.20445", "authors": ["Tianqing Zhang", "Zixin Zhu", "Kairong Yu", "Hongwei Wang"], "title": "Head-Tail-Aware KL Divergence in Knowledge Distillation for Spiking Neural Networks", "categories": ["cs.AI"], "comment": "Accepted by IJCNN2025", "summary": "Spiking Neural Networks (SNNs) have emerged as a promising approach for\nenergy-efficient and biologically plausible computation. However, due to\nlimitations in existing training methods and inherent model constraints, SNNs\noften exhibit a performance gap when compared to Artificial Neural Networks\n(ANNs). Knowledge distillation (KD) has been explored as a technique to\ntransfer knowledge from ANN teacher models to SNN student models to mitigate\nthis gap. Traditional KD methods typically use Kullback-Leibler (KL) divergence\nto align output distributions. However, conventional KL-based approaches fail\nto fully exploit the unique characteristics of SNNs, as they tend to\noveremphasize high-probability predictions while neglecting low-probability\nones, leading to suboptimal generalization. To address this, we propose\nHead-Tail Aware Kullback-Leibler (HTA-KL) divergence, a novel KD method for\nSNNs. HTA-KL introduces a cumulative probability-based mask to dynamically\ndistinguish between high- and low-probability regions. It assigns adaptive\nweights to ensure balanced knowledge transfer, enhancing the overall\nperformance. By integrating forward KL (FKL) and reverse KL (RKL) divergence,\nour method effectively align both head and tail regions of the distribution. We\nevaluate our methods on CIFAR-10, CIFAR-100 and Tiny ImageNet datasets. Our\nmethod outperforms existing methods on most datasets with fewer timesteps."}
{"id": "2504.20110", "pdf": "https://arxiv.org/pdf/2504.20110", "abs": "https://arxiv.org/abs/2504.20110", "authors": ["Yu-hsuan Chen", "Jing Bi", "Cyril Ngo Ngoc", "Victor Oancea", "Jonathan Cagan", "Levent Burak Kara"], "title": "Attention to Detail: Fine-Scale Feature Preservation-Oriented Geometric Pre-training for AI-Driven Surrogate Modeling", "categories": ["cs.LG"], "comment": null, "summary": "AI-driven surrogate modeling has become an increasingly effective alternative\nto physics-based simulations for 3D design, analysis, and manufacturing. These\nmodels leverage data-driven methods to predict physical quantities\ntraditionally requiring computationally expensive simulations. However, the\nscarcity of labeled CAD-to-simulation datasets has driven recent advancements\nin self-supervised and foundation models, where geometric representation\nlearning is performed offline and later fine-tuned for specific downstream\ntasks. While these approaches have shown promise, their effectiveness is\nlimited in applications requiring fine-scale geometric detail preservation.\nThis work introduces a self-supervised geometric representation learning method\ndesigned to capture fine-scale geometric features from non-parametric 3D\nmodels. Unlike traditional end-to-end surrogate models, this approach decouples\ngeometric feature extraction from downstream physics tasks, learning a latent\nspace embedding guided by geometric reconstruction losses. Key elements include\nthe essential use of near-zero level sampling and the innovative batch-adaptive\nattention-weighted loss function, which enhance the encoding of intricate\ndesign features. The proposed method is validated through case studies in\nstructural mechanics, demonstrating strong performance in capturing design\nfeatures and enabling accurate few-shot physics predictions. Comparisons with\ntraditional parametric surrogate modeling highlight its potential to bridge the\ngap between geometric and physics-based representations, providing an effective\nsolution for surrogate modeling in data-scarce scenarios."}
{"id": "2504.20469", "pdf": "https://arxiv.org/pdf/2504.20469", "abs": "https://arxiv.org/abs/2504.20469", "authors": ["Enfa Fane", "Mihai Surdeanu", "Eduardo Blanco", "Steven R. Corman"], "title": "Fane at SemEval-2025 Task 10: Zero-Shot Entity Framing with Large Language Models", "categories": ["cs.CL", "cs.CY", "I.2.7"], "comment": "Accepted to The 19th International Workshop on Semantic Evaluation\n  (Semeval 2025)", "summary": "Understanding how news narratives frame entities is crucial for studying\nmedia's impact on societal perceptions of events. In this paper, we evaluate\nthe zero-shot capabilities of large language models (LLMs) in classifying\nframing roles. Through systematic experimentation, we assess the effects of\ninput context, prompting strategies, and task decomposition. Our findings show\nthat a hierarchical approach of first identifying broad roles and then\nfine-grained roles, outperforms single-step classification. We also demonstrate\nthat optimal input contexts and prompts vary across task levels, highlighting\nthe need for subtask-specific strategies. We achieve a Main Role Accuracy of\n89.4% and an Exact Match Ratio of 34.5%, demonstrating the effectiveness of our\napproach. Our findings emphasize the importance of tailored prompt design and\ninput context optimization for improving LLM performance in entity framing."}
{"id": "2504.18251", "pdf": "https://arxiv.org/pdf/2504.18251", "abs": "https://arxiv.org/abs/2504.18251", "authors": ["Md Jahidul Islam"], "title": "Adaptive Weight Modified Riesz Mean Filter For High-Density Salt and Pepper Noise Removal", "categories": ["eess.IV"], "comment": "This is a preprint. Submitted to Journal of Electrical Systems and\n  Information Technology (Springer)", "summary": "This paper introduces a novel filter, the Adaptive Weight Modified Riesz Mean\nFilter (AWMRmF), designed for the effective removal of high-density salt and\npepper noise (SPN). AWMRmF integrates a pixel weight function and adaptivity\ncondition inspired by the Different Adaptive Modified Riesz Mean Filter\n(DAMRmF). In my simulations, I evaluated the performance of AWMRmF against\nestablished filters such as Adaptive Frequency Median Filter (AFMF), Adaptive\nWeighted Mean Filter (AWMF), Adaptive Cesaro Mean Filter (ACmF), Adaptive Riesz\nMean Filter (ARmF), and Improved Adaptive Weighted Mean Filter (IAWMF). The\nassessment was conducted on 26 typical test images, varying noise levels from\n60% to 95%. The findings indicate that, in terms of both Peak Signal to Noise\nRatio (PSNR) and Structural Similarity (SSIM) metrics, AWMRmF outperformed\nother state-of-the-art filters. Furthermore, AWMRmF demonstrated superior\nperformance in mean PSNR and SSIM results as well."}
{"id": "2504.20288", "pdf": "https://arxiv.org/pdf/2504.20288", "abs": "https://arxiv.org/abs/2504.20288", "authors": ["Shinnosuke Saito", "Takashi Matsubara"], "title": "Image Interpolation with Score-based Riemannian Metrics of Diffusion Models", "categories": ["cs.CV"], "comment": null, "summary": "Diffusion models excel in content generation by implicitly learning the data\nmanifold, yet they lack a practical method to leverage this manifold - unlike\nother deep generative models equipped with latent spaces. This paper introduces\na novel framework that treats the data space of pre-trained diffusion models as\na Riemannian manifold, with a metric derived from the score function.\nExperiments with MNIST and Stable Diffusion show that this geometry-aware\napproach yields image interpolations that are more realistic, less noisy, and\nmore faithful to prompts than existing methods, demonstrating its potential for\nimproved content generation and editing."}
{"id": "2504.20462", "pdf": "https://arxiv.org/pdf/2504.20462", "abs": "https://arxiv.org/abs/2504.20462", "authors": ["Qi Wang", "Xiao Zhang", "Mingyi Li", "Yuan Yuan", "Mengbai Xiao", "Fuzhen Zhuang", "Dongxiao Yu"], "title": "TAMO:Fine-Grained Root Cause Analysis via Tool-Assisted LLM Agent with Multi-Modality Observation Data", "categories": ["cs.AI"], "comment": null, "summary": "With the development of distributed systems, microservices and cloud native\ntechnologies have become central to modern enterprise software development.\nDespite bringing significant advantages, these technologies also increase\nsystem complexity and operational challenges. Traditional root cause analysis\n(RCA) struggles to achieve automated fault response, heavily relying on manual\nintervention. In recent years, large language models (LLMs) have made\nbreakthroughs in contextual inference and domain knowledge integration,\nproviding new solutions for Artificial Intelligence for Operations (AIOps).\nHowever, Existing LLM-based approaches face three key challenges: text input\nconstraints, dynamic service dependency hallucinations, and context window\nlimitations. To address these issues, we propose a tool-assisted LLM agent with\nmulti-modality observation data, namely TAMO, for fine-grained RCA. It unifies\nmulti-modal observational data into time-aligned representations to extract\nconsistent features and employs specialized root cause localization and fault\nclassification tools for perceiving the contextual environment. This approach\novercomes the limitations of LLM in handling real-time changing service\ndependencies and raw observational data and guides LLM to generate repair\nstrategies aligned with system contexts by structuring key information into a\nprompt. Experimental results show that TAMO performs well in root cause\nanalysis when dealing with public datasets characterized by heterogeneity and\ncommon fault types, demonstrating its effectiveness."}
{"id": "2504.20112", "pdf": "https://arxiv.org/pdf/2504.20112", "abs": "https://arxiv.org/abs/2504.20112", "authors": ["Chowdhury Mohammad Abid Rahman", "Aldo H. Romero", "Prashnna K. Gyawali"], "title": "Supervised Pretraining for Material Property Prediction", "categories": ["cs.LG", "cond-mat.mtrl-sci", "cs.AI"], "comment": "21 pages, 7 figures, 2 algorithms, 6 tables", "summary": "Accurate prediction of material properties facilitates the discovery of novel\nmaterials with tailored functionalities. Deep learning models have recently\nshown superior accuracy and flexibility in capturing structure-property\nrelationships. However, these models often rely on supervised learning, which\nrequires large, well-annotated datasets an expensive and time-consuming\nprocess. Self-supervised learning (SSL) offers a promising alternative by\npretraining on large, unlabeled datasets to develop foundation models that can\nbe fine-tuned for material property prediction. In this work, we propose\nsupervised pretraining, where available class information serves as surrogate\nlabels to guide learning, even when downstream tasks involve unrelated material\nproperties. We evaluate this strategy on two state-of-the-art SSL models and\nintroduce a novel framework for supervised pretraining. To further enhance\nrepresentation learning, we propose a graph-based augmentation technique that\ninjects noise to improve robustness without structurally deforming material\ngraphs. The resulting foundation models are fine-tuned for six challenging\nmaterial property predictions, achieving significant performance gains over\nbaselines, ranging from 2% to 6.67% improvement in mean absolute error (MAE)\nand establishing a new benchmark in material property prediction. This study\nrepresents the first exploration of supervised pertaining with surrogate labels\nin material property prediction, advancing methodology and application in the\nfield."}
{"id": "2504.20484", "pdf": "https://arxiv.org/pdf/2504.20484", "abs": "https://arxiv.org/abs/2504.20484", "authors": ["Linjuan Wu", "Haoran Wei", "Huan Lin", "Tianhao Li", "Baosong Yang", "Weiming Lu"], "title": "Enhancing LLM Language Adaption through Cross-lingual In-Context Pre-training", "categories": ["cs.CL"], "comment": "12 pages, 6 figures, Under Review", "summary": "Large language models (LLMs) exhibit remarkable multilingual capabilities\ndespite English-dominated pre-training, attributed to cross-lingual mechanisms\nduring pre-training. Existing methods for enhancing cross-lingual transfer\nremain constrained by parallel resources, suffering from limited linguistic and\ndomain coverage. We propose Cross-lingual In-context Pre-training (CrossIC-PT),\na simple and scalable approach that enhances cross-lingual transfer by\nleveraging semantically related bilingual texts via simple next-word\nprediction. We construct CrossIC-PT samples by interleaving semantic-related\nbilingual Wikipedia documents into a single context window. To access window\nsize constraints, we implement a systematic segmentation policy to split long\nbilingual document pairs into chunks while adjusting the sliding window\nmechanism to preserve contextual coherence. We further extend data availability\nthrough a semantic retrieval framework to construct CrossIC-PT samples from\nweb-crawled corpus. Experimental results demonstrate that CrossIC-PT improves\nmultilingual performance on three models (Llama-3.1-8B, Qwen2.5-7B, and\nQwen2.5-1.5B) across six target languages, yielding performance gains of 3.79%,\n3.99%, and 1.95%, respectively, with additional improvements after data\naugmentation."}
{"id": "2504.19203", "pdf": "https://arxiv.org/pdf/2504.19203", "abs": "https://arxiv.org/abs/2504.19203", "authors": ["Ehsan Karami", "Hamid Soltanian-Zadeh"], "title": "Improving Generalization in MRI-Based Deep Learning Models for Total Knee Replacement Prediction", "categories": ["eess.IV", "cs.CV"], "comment": null, "summary": "Knee osteoarthritis (KOA) is a common joint disease that causes pain and\nmobility issues. While MRI-based deep learning models have demonstrated\nsuperior performance in predicting total knee replacement (TKR) and disease\nprogression, their generalizability remains challenging, particularly when\napplied to imaging data from different sources. In this study, we have shown\nthat replacing batch normalization with instance normalization, using data\naugmentation, and applying contrastive loss improves model generalization in a\nbaseline deep learning model for knee osteoarthritis (KOA) prediction. We\ntrained and evaluated our model using MRI data from the Osteoarthritis\nInitiative (OAI) database, considering sagittal fat-suppressed\nintermediate-weighted turbo spin-echo (FS-IW-TSE) images as the source domain\nand sagittal fat-suppressed three-dimensional (3D) dual-echo in steady state\n(DESS) images as the target domain. The results demonstrate a statistically\nsignificant improvement in classification accuracy across both domains, with\nour approach outperforming the baseline model."}
{"id": "2504.20303", "pdf": "https://arxiv.org/pdf/2504.20303", "abs": "https://arxiv.org/abs/2504.20303", "authors": ["Junlin Guo", "James R. Zimmer-Dauphinee", "Jordan M. Nieusma", "Siqi Lu", "Quan Liu", "Ruining Deng", "Can Cui", "Jialin Yue", "Yizhe Lin", "Tianyuan Yao", "Juming Xiong", "Junchao Zhu", "Chongyu Qu", "Yuechen Yang", "Mitchell Wilkes", "Xiao Wang", "Parker VanValkenburgh", "Steven A. Wernke", "Yuankai Huo"], "title": "DeepAndes: A Self-Supervised Vision Foundation Model for Multi-Spectral Remote Sensing Imagery of the Andes", "categories": ["cs.CV"], "comment": null, "summary": "By mapping sites at large scales using remotely sensed data, archaeologists\ncan generate unique insights into long-term demographic trends, inter-regional\nsocial networks, and past adaptations to climate change. Remote sensing surveys\ncomplement field-based approaches, and their reach can be especially great when\ncombined with deep learning and computer vision techniques. However,\nconventional supervised deep learning methods face challenges in annotating\nfine-grained archaeological features at scale. While recent vision foundation\nmodels have shown remarkable success in learning large-scale remote sensing\ndata with minimal annotations, most off-the-shelf solutions are designed for\nRGB images rather than multi-spectral satellite imagery, such as the 8-band\ndata used in our study. In this paper, we introduce DeepAndes, a\ntransformer-based vision foundation model trained on three million\nmulti-spectral satellite images, specifically tailored for Andean archaeology.\nDeepAndes incorporates a customized DINOv2 self-supervised learning algorithm\noptimized for 8-band multi-spectral imagery, marking the first foundation model\ndesigned explicitly for the Andes region. We evaluate its image understanding\nperformance through imbalanced image classification, image instance retrieval,\nand pixel-level semantic segmentation tasks. Our experiments show that\nDeepAndes achieves superior F1 scores, mean average precision, and Dice scores\nin few-shot learning scenarios, significantly outperforming models trained from\nscratch or pre-trained on smaller datasets. This underscores the effectiveness\nof large-scale self-supervised pre-training in archaeological remote sensing.\nCodes will be available on https://github.com/geopacha/DeepAndes."}
{"id": "2504.20464", "pdf": "https://arxiv.org/pdf/2504.20464", "abs": "https://arxiv.org/abs/2504.20464", "authors": ["Jiahao Li", "Kaer Huang"], "title": "A Summary on GUI Agents with Foundation Models Enhanced by Reinforcement Learning", "categories": ["cs.AI"], "comment": null, "summary": "Graphical User Interface (GUI) agents, driven by Multi-modal Large Language\nModels (MLLMs), have emerged as a promising paradigm for enabling intelligent\ninteraction with digital systems. This paper provides a structured summary of\nrecent advances in GUI agents, focusing on architectures enhanced by\nReinforcement Learning (RL). We first formalize GUI agent tasks as Markov\nDecision Processes and discuss typical execution environments and evaluation\nmetrics. We then review the modular architecture of (M)LLM-based GUI agents,\ncovering Perception, Planning, and Acting modules, and trace their evolution\nthrough representative works. Furthermore, we categorize GUI agent training\nmethodologies into Prompt-based, Supervised Fine-Tuning (SFT)-based, and\nRL-based approaches, highlighting the progression from simple prompt\nengineering to dynamic policy learning via RL. Our summary illustrates how\nrecent innovations in multimodal perception, decision reasoning, and adaptive\naction generation have significantly improved the generalization and robustness\nof GUI agents in complex real-world environments. We conclude by identifying\nkey challenges and future directions for building more capable and reliable GUI\nagents."}
{"id": "2504.20121", "pdf": "https://arxiv.org/pdf/2504.20121", "abs": "https://arxiv.org/abs/2504.20121", "authors": ["Alireza Kazemi", "Helia Rezvani", "Mahsa Baktashmotlagh"], "title": "Benchmarking Transferability: A Framework for Fair and Robust Evaluation", "categories": ["cs.LG"], "comment": null, "summary": "Transferability scores aim to quantify how well a model trained on one domain\ngeneralizes to a target domain. Despite numerous methods proposed for measuring\ntransferability, their reliability and practical usefulness remain\ninconclusive, often due to differing experimental setups, datasets, and\nassumptions. In this paper, we introduce a comprehensive benchmarking framework\ndesigned to systematically evaluate transferability scores across diverse\nsettings. Through extensive experiments, we observe variations in how different\nmetrics perform under various scenarios, suggesting that current evaluation\npractices may not fully capture each method's strengths and limitations. Our\nfindings underscore the value of standardized assessment protocols, paving the\nway for more reliable transferability measures and better-informed model\nselection in cross-domain applications. Additionally, we achieved a 3.5\\%\nimprovement using our proposed metric for the head-training fine-tuning\nexperimental setup. Our code is available in this repository:\nhttps://github.com/alizkzm/pert_robust_platform."}
{"id": "2504.20500", "pdf": "https://arxiv.org/pdf/2504.20500", "abs": "https://arxiv.org/abs/2504.20500", "authors": ["Huimin Lu", "Masaru Isonuma", "Junichiro Mori", "Ichiro Sakata"], "title": "UniDetox: Universal Detoxification of Large Language Models via Dataset Distillation", "categories": ["cs.CL", "cs.LG"], "comment": "Accepted at ICLR 2025 (poster)", "summary": "We present UniDetox, a universally applicable method designed to mitigate\ntoxicity across various large language models (LLMs). Previous detoxification\nmethods are typically model-specific, addressing only individual models or\nmodel families, and require careful hyperparameter tuning due to the trade-off\nbetween detoxification efficacy and language modeling performance. In contrast,\nUniDetox provides a detoxification technique that can be universally applied to\na wide range of LLMs without the need for separate model-specific tuning.\nSpecifically, we propose a novel and efficient dataset distillation technique\nfor detoxification using contrastive decoding. This approach distills\ndetoxifying representations in the form of synthetic text data, enabling\nuniversal detoxification of any LLM through fine-tuning with the distilled\ntext. Our experiments demonstrate that the detoxifying text distilled from\nGPT-2 can effectively detoxify larger models, including OPT, Falcon, and\nLLaMA-2. Furthermore, UniDetox eliminates the need for separate hyperparameter\ntuning for each model, as a single hyperparameter configuration can be\nseamlessly applied across different models. Additionally, analysis of the\ndetoxifying text reveals a reduction in politically biased content, providing\ninsights into the attributes necessary for effective detoxification of LLMs."}
{"id": "2504.20306", "pdf": "https://arxiv.org/pdf/2504.20306", "abs": "https://arxiv.org/abs/2504.20306", "authors": ["Teja Krishna Cherukuri", "Nagur Shareef Shaik", "Sribhuvan Reddy Yellu", "Jun-Won Chung", "Dong Hye Ye"], "title": "Dynamic Contextual Attention Network: Transforming Spatial Representations into Adaptive Insights for Endoscopic Polyp Diagnosis", "categories": ["cs.CV"], "comment": "Accepted at 47th Annual International Conference of the IEEE\n  Engineering in Medicine and Biology Society (EMBC) 2025", "summary": "Colorectal polyps are key indicators for early detection of colorectal\ncancer. However, traditional endoscopic imaging often struggles with accurate\npolyp localization and lacks comprehensive contextual awareness, which can\nlimit the explainability of diagnoses. To address these issues, we propose the\nDynamic Contextual Attention Network (DCAN). This novel approach transforms\nspatial representations into adaptive contextual insights, using an attention\nmechanism that enhances focus on critical polyp regions without explicit\nlocalization modules. By integrating contextual awareness into the\nclassification process, DCAN improves decision interpretability and overall\ndiagnostic performance. This advancement in imaging could lead to more reliable\ncolorectal cancer detection, enabling better patient outcomes."}
{"id": "2504.20505", "pdf": "https://arxiv.org/pdf/2504.20505", "abs": "https://arxiv.org/abs/2504.20505", "authors": ["Xi Chen", "Julien Cumin", "Fano Ramparany", "Dominique Vaufreydaz"], "title": "MuRAL: A Multi-Resident Ambient Sensor Dataset Annotated with Natural Language for Activities of Daily Living", "categories": ["cs.AI"], "comment": null, "summary": "Recent advances in Large Language Models (LLMs) have shown promising\npotential for human activity recognition (HAR) using ambient sensors,\nespecially through natural language reasoning and zero-shot learning. However,\nexisting datasets such as CASAS, ARAS, and MARBLE were not originally designed\nwith LLMs in mind and therefore lack the contextual richness, complexity, and\nannotation granularity required to fully exploit LLM capabilities. In this\npaper, we introduce MuRAL, the first Multi-Resident Ambient sensor dataset with\nnatural Language, comprising over 21 hours of multi-user sensor data collected\nfrom 21 sessions in a smart-home environment. MuRAL is annotated with\nfine-grained natural language descriptions, resident identities, and high-level\nactivity labels, all situated in dynamic, realistic multi-resident settings. We\nbenchmark MuRAL using state-of-the-art LLMs for three core tasks: subject\nassignment, action description, and activity classification. Our results\ndemonstrate that while LLMs can provide rich semantic interpretations of\nambient data, current models still face challenges in handling multi-user\nambiguity and under-specified sensor contexts. We release MuRAL to support\nfuture research on LLM-powered, explainable, and socially aware activity\nunderstanding in smart environments. For access to the dataset, please reach\nout to us via the provided contact information. A direct link for dataset\nretrieval will be made available at this location in due course."}
{"id": "2504.20131", "pdf": "https://arxiv.org/pdf/2504.20131", "abs": "https://arxiv.org/abs/2504.20131", "authors": ["Antonio A. Ginart", "Naveen Kodali", "Jason Lee", "Caiming Xiong", "Silvio Savarese", "John R. Emmons"], "title": "LZ Penalty: An information-theoretic repetition penalty for autoregressive language models", "categories": ["cs.LG", "cs.AI", "cs.IT", "math.IT"], "comment": "Preprint (draft)", "summary": "We introduce the LZ penalty, a penalty specialized for reducing degenerate\nrepetitions in autoregressive language models without loss of capability. The\npenalty is based on the codelengths in the LZ77 universal lossless compression\nalgorithm. Through the lens of the prediction-compression duality, decoding the\nLZ penalty has the interpretation of sampling from the residual distribution\nafter removing the information that is highly compressible. We demonstrate the\nLZ penalty enables state-of-the-art open-source reasoning models to operate\nwith greedy (temperature zero) decoding without loss of capability and without\ninstances of degenerate repetition. Both the industry-standard frequency\npenalty and repetition penalty are ineffective, incurring degenerate repetition\nrates of up to 4%."}
{"id": "2504.20547", "pdf": "https://arxiv.org/pdf/2504.20547", "abs": "https://arxiv.org/abs/2504.20547", "authors": ["Jesus Lovon", "Thouria Ben-Haddi", "Jules Di Scala", "Jose G. Moreno", "Lynda Tamine"], "title": "Revisiting the MIMIC-IV Benchmark: Experiments Using Language Models for Electronic Health Records", "categories": ["cs.CL"], "comment": null, "summary": "The lack of standardized evaluation benchmarks in the medical domain for text\ninputs can be a barrier to widely adopting and leveraging the potential of\nnatural language models for health-related downstream tasks. This paper\nrevisited an openly available MIMIC-IV benchmark for electronic health records\n(EHRs) to address this issue. First, we integrate the MIMIC-IV data within the\nHugging Face datasets library to allow an easy share and use of this\ncollection. Second, we investigate the application of templates to convert EHR\ntabular data to text. Experiments using fine-tuned and zero-shot LLMs on the\nmortality of patients task show that fine-tuned text-based models are\ncompetitive against robust tabular classifiers. In contrast, zero-shot LLMs\nstruggle to leverage EHR representations. This study underlines the potential\nof text-based approaches in the medical field and highlights areas for further\nimprovement."}
{"id": "2504.20322", "pdf": "https://arxiv.org/pdf/2504.20322", "abs": "https://arxiv.org/abs/2504.20322", "authors": ["Sumit Mamtani", "Yash Thesia"], "title": "Fine Grain Classification: Connecting Meta using Cross-Contrastive pre-training", "categories": ["cs.CV", "cs.LG"], "comment": "9 pages, 4 figures. Submitted to arXiv", "summary": "Fine-grained visual classification aims to recognize objects belonging to\nmultiple subordinate categories within a super-category. However, this remains\na challenging problem, as appearance information alone is often insufficient to\naccurately differentiate between fine-grained visual categories. To address\nthis, we propose a novel and unified framework that leverages meta-information\nto assist fine-grained identification. We tackle the joint learning of visual\nand meta-information through cross-contrastive pre-training. In the first\nstage, we employ three encoders for images, text, and meta-information,\naligning their projected embeddings to achieve better representations. We then\nfine-tune the image and meta-information encoders for the classification task.\nExperiments on the NABirds dataset demonstrate that our framework effectively\nutilizes meta-information to enhance fine-grained recognition performance. With\nthe addition of meta-information, our framework surpasses the current baseline\non NABirds by 7.83%. Furthermore, it achieves an accuracy of 84.44% on the\nNABirds dataset, outperforming many existing state-of-the-art approaches that\nutilize meta-information."}
{"id": "2504.20595", "pdf": "https://arxiv.org/pdf/2504.20595", "abs": "https://arxiv.org/abs/2504.20595", "authors": ["Rulin Shao", "Rui Qiao", "Varsha Kishore", "Niklas Muennighoff", "Xi Victoria Lin", "Daniela Rus", "Bryan Kian Hsiang Low", "Sewon Min", "Wen-tau Yih", "Pang Wei Koh", "Luke Zettlemoyer"], "title": "ReasonIR: Training Retrievers for Reasoning Tasks", "categories": ["cs.AI", "cs.CL", "cs.IR", "cs.LG"], "comment": "Our code is released at\n  \\url{https://github.com/facebookresearch/ReasonIR}", "summary": "We present ReasonIR-8B, the first retriever specifically trained for general\nreasoning tasks. Existing retrievers have shown limited gains on reasoning\ntasks, in part because existing training datasets focus on short factual\nqueries tied to documents that straightforwardly answer them. We develop a\nsynthetic data generation pipeline that, for each document, our pipeline\ncreates a challenging and relevant query, along with a plausibly related but\nultimately unhelpful hard negative. By training on a mixture of our synthetic\ndata and existing public data, ReasonIR-8B achieves a new state-of-the-art of\n29.9 nDCG@10 without reranker and 36.9 nDCG@10 with reranker on BRIGHT, a\nwidely-used reasoning-intensive information retrieval (IR) benchmark. When\napplied to RAG tasks, ReasonIR-8B improves MMLU and GPQA performance by 6.4%\nand 22.6% respectively, relative to the closed-book baseline, outperforming\nother retrievers and search engines. In addition, ReasonIR-8B uses test-time\ncompute more effectively: on BRIGHT, its performance consistently increases\nwith longer and more information-rich rewritten queries; it continues to\noutperform other retrievers when combined with an LLM reranker. Our training\nrecipe is general and can be easily extended to future LLMs; to this end, we\nopen-source our code, data, and model."}
{"id": "2504.20172", "pdf": "https://arxiv.org/pdf/2504.20172", "abs": "https://arxiv.org/abs/2504.20172", "authors": ["Erik Jahn", "Karthik Karnik", "Leonard J. Schulman"], "title": "Causal Identification in Time Series Models", "categories": ["cs.LG", "cs.AI", "stat.ME", "stat.ML"], "comment": null, "summary": "In this paper, we analyze the applicability of the Causal Identification\nalgorithm to causal time series graphs with latent confounders. Since these\ngraphs extend over infinitely many time steps, deciding whether causal effects\nacross arbitrary time intervals are identifiable appears to require computation\non graph segments of unbounded size. Even for deciding the identifiability of\nintervention effects on variables that are close in time, no bound is known on\nhow many time steps in the past need to be considered. We give a first bound of\nthis kind that only depends on the number of variables per time step and the\nmaximum time lag of any direct or latent causal effect. More generally, we show\nthat applying the Causal Identification algorithm to a constant-size segment of\nthe time series graph is sufficient to decide identifiability of causal\neffects, even across unbounded time intervals."}
{"id": "2504.20552", "pdf": "https://arxiv.org/pdf/2504.20552", "abs": "https://arxiv.org/abs/2504.20552", "authors": ["Baz Roland", "Kristina Malyseva", "Anna Pappa", "Tristan Cazenave"], "title": "BrAIcht, a theatrical agent that speaks like Bertolt Brecht's characters", "categories": ["cs.CL"], "comment": null, "summary": "This project introduces BrAIcht, an AI conversational agent that creates\ndialogues in the distinctive style of the famous German playwright Bertolt\nBrecht. BrAIcht is fine-tuned using German LeoLM, a large language model with 7\nbillion parameters and a modified version of the base Llama2 suitable for\nGerman language tasks. For fine-tuning, 29 plays of Bertolt Brecht and 907 of\nother German plays that are stylistically similar to Bertolt Brecht are used to\nform a more di-erse dataset. Due to the limited memory capacity, a\nparameterefficient fine-tuning technique called QLoRA is implemented to train\nthe large language model. The results, based on BLEU score and perplexity, show\nvery promising performance of BrAIcht in generating dialogues in the style of\nBertolt Brecht."}
{"id": "2504.20343", "pdf": "https://arxiv.org/pdf/2504.20343", "abs": "https://arxiv.org/abs/2504.20343", "authors": ["Amaan Izhar", "Nurul Japar", "Norisma Idris", "Ting Dang"], "title": "MicarVLMoE: A Modern Gated Cross-Aligned Vision-Language Mixture of Experts Model for Medical Image Captioning and Report Generation", "categories": ["cs.CV"], "comment": "Accepted by IJCNN 2025, 8 pages, 8 figures, 3 tables", "summary": "Medical image reporting (MIR) aims to generate structured clinical\ndescriptions from radiological images. Existing methods struggle with\nfine-grained feature extraction, multimodal alignment, and generalization\nacross diverse imaging types, often relying on vanilla transformers and\nfocusing primarily on chest X-rays. We propose MicarVLMoE, a vision-language\nmixture-of-experts model with gated cross-aligned fusion, designed to address\nthese limitations. Our architecture includes: (i) a multiscale vision encoder\n(MSVE) for capturing anatomical details at varying resolutions, (ii) a\nmultihead dual-branch latent attention (MDLA) module for vision-language\nalignment through latent bottleneck representations, and (iii) a modulated\nmixture-of-experts (MoE) decoder for adaptive expert specialization. We extend\nMIR to CT scans, retinal imaging, MRI scans, and gross pathology images,\nreporting state-of-the-art results on COVCTR, MMR, PGROSS, and ROCO datasets.\nExtensive experiments and ablations confirm improved clinical accuracy,\ncross-modal alignment, and model interpretability. Code is available at\nhttps://github.com/AI-14/micar-vl-moe."}
{"id": "2504.20624", "pdf": "https://arxiv.org/pdf/2504.20624", "abs": "https://arxiv.org/abs/2504.20624", "authors": ["Zihan Niu", "Zheyong Xie", "Shaosheng Cao", "Chonggang Lu", "Zheyu Ye", "Tong Xu", "Zuozhu Liu", "Yan Gao", "Jia Chen", "Zhe Xu", "Yi Wu", "Yao Hu"], "title": "PaRT: Enhancing Proactive Social Chatbots with Personalized Real-Time Retrieval", "categories": ["cs.AI"], "comment": null, "summary": "Social chatbots have become essential intelligent companions in daily\nscenarios ranging from emotional support to personal interaction. However,\nconventional chatbots with passive response mechanisms usually rely on users to\ninitiate or sustain dialogues by bringing up new topics, resulting in\ndiminished engagement and shortened dialogue duration. In this paper, we\npresent PaRT, a novel framework enabling context-aware proactive dialogues for\nsocial chatbots through personalized real-time retrieval and generation.\nSpecifically, PaRT first integrates user profiles and dialogue context into a\nlarge language model (LLM), which is initially prompted to refine user queries\nand recognize their underlying intents for the upcoming conversation. Guided by\nrefined intents, the LLM generates personalized dialogue topics, which then\nserve as targeted queries to retrieve relevant passages from RedNote. Finally,\nwe prompt LLMs with summarized passages to generate knowledge-grounded and\nengagement-optimized responses. Our approach has been running stably in a\nreal-world production environment for more than 30 days, achieving a 21.77\\%\nimprovement in the average duration of dialogues."}
{"id": "2504.20187", "pdf": "https://arxiv.org/pdf/2504.20187", "abs": "https://arxiv.org/abs/2504.20187", "authors": ["Weihao Sun", "Heeseung Bang", "Andreas A. Malikopoulos"], "title": "AI Recommendation Systems for Lane-Changing Using Adherence-Aware Reinforcement Learning", "categories": ["cs.LG", "cs.AI", "cs.SY", "eess.SY"], "comment": "6 pages, 5 figures, conference", "summary": "In this paper, we present an adherence-aware reinforcement learning (RL)\napproach aimed at seeking optimal lane-changing recommendations within a\nsemi-autonomous driving environment to enhance a single vehicle's travel\nefficiency. The problem is framed within a Markov decision process setting and\nis addressed through an adherence-aware deep Q network, which takes into\naccount the partial compliance of human drivers with the recommended actions.\nThis approach is evaluated within CARLA's driving environment under realistic\nscenarios."}
{"id": "2504.20581", "pdf": "https://arxiv.org/pdf/2504.20581", "abs": "https://arxiv.org/abs/2504.20581", "authors": ["Iwona Christop", "Tomasz Kuczyński", "Marek Kubis"], "title": "ClonEval: An Open Voice Cloning Benchmark", "categories": ["cs.CL"], "comment": null, "summary": "We present a novel benchmark for voice cloning text-to-speech models. The\nbenchmark consists of an evaluation protocol, an open-source library for\nassessing the performance of voice cloning models, and an accompanying\nleaderboard. The paper discusses design considerations and presents a detailed\ndescription of the evaluation procedure. The usage of the software library is\nexplained, along with the organization of results on the leaderboard."}
{"id": "2504.20362", "pdf": "https://arxiv.org/pdf/2504.20362", "abs": "https://arxiv.org/abs/2504.20362", "authors": ["Qinhua Xie", "Hao Tang"], "title": "TTTFusion: A Test-Time Training-Based Strategy for Multimodal Medical Image Fusion in Surgical Robots", "categories": ["cs.CV"], "comment": null, "summary": "With the increasing use of surgical robots in clinical practice, enhancing\ntheir ability to process multimodal medical images has become a key research\nchallenge. Although traditional medical image fusion methods have made progress\nin improving fusion accuracy, they still face significant challenges in\nreal-time performance, fine-grained feature extraction, and edge\npreservation.In this paper, we introduce TTTFusion, a Test-Time Training\n(TTT)-based image fusion strategy that dynamically adjusts model parameters\nduring inference to efficiently fuse multimodal medical images. By adapting the\nmodel during the test phase, our method optimizes the parameters based on the\ninput image data, leading to improved accuracy and better detail preservation\nin the fusion results.Experimental results demonstrate that TTTFusion\nsignificantly enhances the fusion quality of multimodal images compared to\ntraditional fusion methods, particularly in fine-grained feature extraction and\nedge preservation. This approach not only improves image fusion accuracy but\nalso offers a novel technical solution for real-time image processing in\nsurgical robots."}
{"id": "2504.20628", "pdf": "https://arxiv.org/pdf/2504.20628", "abs": "https://arxiv.org/abs/2504.20628", "authors": ["Marta Kryven", "Cole Wyeth", "Aidan Curtis", "Kevin Ellis"], "title": "Cognitive maps are generative programs", "categories": ["cs.AI", "cs.ET"], "comment": "9 pages, 4 figures, to be published in Cognitive Sciences Society\n  proceedings", "summary": "Making sense of the world and acting in it relies on building simplified\nmental representations that abstract away aspects of reality. This principle of\ncognitive mapping is universal to agents with limited resources. Living\norganisms, people, and algorithms all face the problem of forming functional\nrepresentations of their world under various computing constraints. In this\nwork, we explore the hypothesis that human resource-efficient planning may\narise from representing the world as predictably structured. Building on the\nmetaphor of concepts as programs, we propose that cognitive maps can take the\nform of generative programs that exploit predictability and redundancy, in\ncontrast to directly encoding spatial layouts. We use a behavioral experiment\nto show that people who navigate in structured spaces rely on modular planning\nstrategies that align with programmatic map representations. We describe a\ncomputational model that predicts human behavior in a variety of structured\nscenarios. This model infers a small distribution over possible programmatic\ncognitive maps conditioned on human prior knowledge of the world, and uses this\ndistribution to generate resource-efficient plans. Our models leverages a Large\nLanguage Model as an embedding of human priors, implicitly learned through\ntraining on a vast corpus of human data. Our model demonstrates improved\ncomputational efficiency, requires drastically less memory, and outperforms\nunstructured planning algorithms with cognitive constraints at predicting human\nbehavior, suggesting that human planning strategies rely on programmatic\ncognitive maps."}
{"id": "2504.20193", "pdf": "https://arxiv.org/pdf/2504.20193", "abs": "https://arxiv.org/abs/2504.20193", "authors": ["Zhe Cui", "Shuxian Zhang", "Kangzhi Lou", "Le-Nam Tran"], "title": "ProFi-Net: Prototype-based Feature Attention with Curriculum Augmentation for WiFi-based Gesture Recognition", "categories": ["cs.LG"], "comment": "This paper was accepted at The 9th APWeb-WAIM joint International\n  Conference on Web and Big Data", "summary": "This paper presents ProFi-Net, a novel few-shot learning framework for\nWiFi-based gesture recognition that overcomes the challenges of limited\ntraining data and sparse feature representations. ProFi-Net employs a\nprototype-based metric learning architecture enhanced with a feature-level\nattention mechanism, which dynamically refines the Euclidean distance by\nemphasizing the most discriminative feature dimensions. Additionally, our\napproach introduces a curriculum-inspired data augmentation strategy\nexclusively on the query set. By progressively incorporating Gaussian noise of\nincreasing magnitude, the model is exposed to a broader range of challenging\nvariations, thereby improving its generalization and robustness to overfitting.\nExtensive experiments conducted across diverse real-world environments\ndemonstrate that ProFi-Net significantly outperforms conventional prototype\nnetworks and other state-of-the-art few-shot learning methods in terms of\nclassification accuracy and training efficiency."}
{"id": "2504.20605", "pdf": "https://arxiv.org/pdf/2504.20605", "abs": "https://arxiv.org/abs/2504.20605", "authors": ["Mihai Nadas", "Laura Diosan", "Andrei Piscoran", "Andreea Tomescu"], "title": "TF1-EN-3M: Three Million Synthetic Moral Fables for Training Small, Open Language Models", "categories": ["cs.CL"], "comment": null, "summary": "Moral stories are a time-tested vehicle for transmitting values, yet modern\nNLP lacks a large, structured corpus that couples coherent narratives with\nexplicit ethical lessons. We close this gap with TF1-EN-3M, the first open\ndataset of three million English-language fables generated exclusively by\ninstruction-tuned models no larger than 8B parameters. Each story follows a\nsix-slot scaffold (character -> trait -> setting -> conflict -> resolution ->\nmoral), produced through a combinatorial prompt engine that guarantees genre\nfidelity while covering a broad thematic space.\n  A hybrid evaluation pipeline blends (i) a GPT-based critic that scores\ngrammar, creativity, moral clarity, and template adherence with (ii)\nreference-free diversity and readability metrics. Among ten open-weight\ncandidates, an 8B-parameter Llama-3 variant delivers the best quality-speed\ntrade-off, producing high-scoring fables on a single consumer GPU (<24 GB VRAM)\nat approximately 13.5 cents per 1,000 fables.\n  We release the dataset, generation code, evaluation scripts, and full\nmetadata under a permissive license, enabling exact reproducibility and cost\nbenchmarking. TF1-EN-3M opens avenues for research in instruction following,\nnarrative intelligence, value alignment, and child-friendly educational AI,\ndemonstrating that large-scale moral storytelling no longer requires\nproprietary giant models."}
{"id": "2504.20376", "pdf": "https://arxiv.org/pdf/2504.20376", "abs": "https://arxiv.org/abs/2504.20376", "authors": ["Shiqian Zhao", "Jiayang Liu", "Yiming Li", "Runyi Hu", "Xiaojun Jia", "Wenshu Fan", "Xinfeng Li", "Jie Zhang", "Wei Dong", "Tianwei Zhang", "Luu Anh Tuan"], "title": "Inception: Jailbreak the Memory Mechanism of Text-to-Image Generation Systems", "categories": ["cs.CV", "cs.CR"], "comment": "17 pages, 8 figures", "summary": "Currently, the memory mechanism has been widely and successfully exploited in\nonline text-to-image (T2I) generation systems ($e.g.$, DALL$\\cdot$E 3) for\nalleviating the growing tokenization burden and capturing key information in\nmulti-turn interactions. Despite its practicality, its security analyses have\nfallen far behind. In this paper, we reveal that this mechanism exacerbates the\nrisk of jailbreak attacks. Different from previous attacks that fuse the unsafe\ntarget prompt into one ultimate adversarial prompt, which can be easily\ndetected or may generate non-unsafe images due to under- or over-optimization,\nwe propose Inception, the first multi-turn jailbreak attack against the memory\nmechanism in real-world text-to-image generation systems. Inception embeds the\nmalice at the inception of the chat session turn by turn, leveraging the\nmechanism that T2I generation systems retrieve key information in their memory.\nSpecifically, Inception mainly consists of two modules. It first segments the\nunsafe prompt into chunks, which are subsequently fed to the system in multiple\nturns, serving as pseudo-gradients for directive optimization. Specifically, we\ndevelop a series of segmentation policies that ensure the images generated are\nsemantically consistent with the target prompt. Secondly, after segmentation,\nto overcome the challenge of the inseparability of minimum unsafe words, we\npropose recursion, a strategy that makes minimum unsafe words subdivisible.\nCollectively, segmentation and recursion ensure that all the request prompts\nare benign but can lead to malicious outcomes. We conduct experiments on the\nreal-world text-to-image generation system ($i.e.$, DALL$\\cdot$E 3) to validate\nthe effectiveness of Inception. The results indicate that Inception surpasses\nthe state-of-the-art by a 14\\% margin in attack success rate."}
{"id": "2504.20676", "pdf": "https://arxiv.org/pdf/2504.20676", "abs": "https://arxiv.org/abs/2504.20676", "authors": ["Shrisha Rao"], "title": "The Limits of AI Explainability: An Algorithmic Information Theory Approach", "categories": ["cs.AI", "cs.CY", "cs.IT", "math.IT", "68Q30, 68T01", "I.2.0; H.1.1; K.4.1"], "comment": null, "summary": "This paper establishes a theoretical foundation for understanding the\nfundamental limits of AI explainability through algorithmic information theory.\nWe formalize explainability as the approximation of complex models by simpler\nones, quantifying both approximation error and explanation complexity using\nKolmogorov complexity. Our key theoretical contributions include: (1) a\ncomplexity gap theorem proving that any explanation significantly simpler than\nthe original model must differ from it on some inputs; (2) precise bounds\nshowing that explanation complexity grows exponentially with input dimension\nbut polynomially with error tolerance for Lipschitz functions; and (3) a\ncharacterization of the gap between local and global explainability,\ndemonstrating that local explanations can be significantly simpler while\nmaintaining accuracy in relevant regions. We further establish a regulatory\nimpossibility theorem proving that no governance framework can simultaneously\npursue unrestricted AI capabilities, human-interpretable explanations, and\nnegligible error. These results highlight considerations likely to be relevant\nto the design, evaluation, and oversight of explainable AI systems."}
{"id": "2504.20197", "pdf": "https://arxiv.org/pdf/2504.20197", "abs": "https://arxiv.org/abs/2504.20197", "authors": ["Aryeh Brill"], "title": "Representation Learning on a Random Lattice", "categories": ["cs.LG", "cond-mat.dis-nn", "cs.AI"], "comment": "Published in Proceedings of ILIAD (2024),\n  https://www.iliadconference.com/proceedings", "summary": "Decomposing a deep neural network's learned representations into\ninterpretable features could greatly enhance its safety and reliability. To\nbetter understand features, we adopt a geometric perspective, viewing them as a\nlearned coordinate system for mapping an embedded data distribution. We\nmotivate a model of a generic data distribution as a random lattice and analyze\nits properties using percolation theory. Learned features are categorized into\ncontext, component, and surface features. The model is qualitatively consistent\nwith recent findings in mechanistic interpretability and suggests directions\nfor future research."}
{"id": "2504.20609", "pdf": "https://arxiv.org/pdf/2504.20609", "abs": "https://arxiv.org/abs/2504.20609", "authors": ["Xinyu Yao", "Mengdi Wang", "Bo Chen", "Xiaobing Zhao"], "title": "WenyanGPT: A Large Language Model for Classical Chinese Tasks", "categories": ["cs.CL"], "comment": null, "summary": "Classical Chinese, as the core carrier of Chinese culture, plays a crucial\nrole in the inheritance and study of ancient literature. However, existing\nnatural language processing models primarily optimize for Modern Chinese,\nresulting in inadequate performance on Classical Chinese. This paper presents a\ncomprehensive solution for Classical Chinese language processing. By continuing\npre-training and instruction fine-tuning on the LLaMA3-8B-Chinese model, we\nconstruct a large language model, WenyanGPT, which is specifically designed for\nClassical Chinese tasks. Additionally, we develop an evaluation benchmark\ndataset, WenyanBENCH. Experimental results on WenyanBENCH demonstrate that\nWenyanGPT significantly outperforms current advanced LLMs in various Classical\nChinese tasks. We make the model's training data, instruction fine-tuning\ndata\\footnote, and evaluation benchmark dataset publicly available to promote\nfurther research and development in the field of Classical Chinese processing."}
{"id": "2504.20378", "pdf": "https://arxiv.org/pdf/2504.20378", "abs": "https://arxiv.org/abs/2504.20378", "authors": ["Jiang Wu", "Rui Li", "Yu Zhu", "Rong Guo", "Jinqiu Sun", "Yanning Zhang"], "title": "Sparse2DGS: Geometry-Prioritized Gaussian Splatting for Surface Reconstruction from Sparse Views", "categories": ["cs.CV"], "comment": "CVPR 2025", "summary": "We present a Gaussian Splatting method for surface reconstruction using\nsparse input views. Previous methods relying on dense views struggle with\nextremely sparse Structure-from-Motion points for initialization. While\nlearning-based Multi-view Stereo (MVS) provides dense 3D points, directly\ncombining it with Gaussian Splatting leads to suboptimal results due to the\nill-posed nature of sparse-view geometric optimization. We propose Sparse2DGS,\nan MVS-initialized Gaussian Splatting pipeline for complete and accurate\nreconstruction. Our key insight is to incorporate the geometric-prioritized\nenhancement schemes, allowing for direct and robust geometric learning under\nill-posed conditions. Sparse2DGS outperforms existing methods by notable\nmargins while being ${2}\\times$ faster than the NeRF-based fine-tuning\napproach."}
{"id": "2504.20756", "pdf": "https://arxiv.org/pdf/2504.20756", "abs": "https://arxiv.org/abs/2504.20756", "authors": ["Moirangthem Tiken Singh"], "title": "Graph-Based Fault Diagnosis for Rotating Machinery: Adaptive Segmentation and Structural Feature Integration", "categories": ["cs.AI"], "comment": null, "summary": "This paper proposes a novel graph-based framework for robust and\ninterpretable multiclass fault diagnosis in rotating machinery. The method\nintegrates entropy-optimized signal segmentation, time-frequency feature\nextraction, and graph-theoretic modeling to transform vibration signals into\nstructured representations suitable for classification. Graph metrics, such as\naverage shortest path length, modularity, and spectral gap, are computed and\ncombined with local features to capture global and segment-level fault\ncharacteristics. The proposed method achieves high diagnostic accuracy when\nevaluated on two benchmark datasets, the CWRU bearing dataset (under 0-3 HP\nloads) and the SU gearbox and bearing datasets (under different speed-load\nconfigurations). Classification scores reach up to 99.8% accuracy on Case\nWestern Reserve University (CWRU) and 100% accuracy on the Southeast University\ndatasets using a logistic regression classifier. Furthermore, the model\nexhibits strong noise resilience, maintaining over 95.4% accuracy at high noise\nlevels (standard deviation = 0.5), and demonstrates excellent cross-domain\ntransferability with up to 99.7% F1-score in load-transfer scenarios. Compared\nto traditional techniques, this approach requires no deep learning\narchitecture, enabling lower complexity while ensuring interpretability. The\nresults confirm the method's scalability, reliability, and potential for\nreal-time deployment in industrial diagnostics."}
{"id": "2504.20213", "pdf": "https://arxiv.org/pdf/2504.20213", "abs": "https://arxiv.org/abs/2504.20213", "authors": ["Yuan Xia", "Akanksha Atrey", "Fadoua Khmaissia", "Kedar S. Namjoshi"], "title": "Can Large Language Models Learn Formal Logic? A Data-Driven Training and Evaluation Framework", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "This paper investigates the logical reasoning capabilities of large language\nmodels (LLMs). For a precisely defined yet tractable formulation, we choose the\nconceptually simple but technically complex task of constructing proofs in\nBoolean logic. A trained LLM receives as input a set of assumptions and a goal,\nand produces as output a proof that formally derives the goal from the\nassumptions. Incorrect proofs are caught by an automated proof checker. A\ncritical obstacle for training is the scarcity of real-world proofs. We propose\nan efficient, randomized procedure for synthesizing valid proofs and introduce\nTemplate Transformation, a data augmentation technique that enhances the\nmodel's ability to handle complex logical expressions. The central evaluation\nquestion is whether an LLM has indeed learned to reason. We propose tests to\nmeasure the reasoning ability of a black-box LLM. By these measures,\nexperiments demonstrate strong reasoning capabilities for assertions with short\nproofs, which decline with proof complexity. Notably, template transformation\nimproves accuracy even for smaller models, suggesting its effectiveness across\nmodel scales."}
{"id": "2504.20643", "pdf": "https://arxiv.org/pdf/2504.20643", "abs": "https://arxiv.org/abs/2504.20643", "authors": ["Moran Mizrahi", "Chen Shani", "Gabriel Stanovsky", "Dan Jurafsky", "Dafna Shahaf"], "title": "Cooking Up Creativity: A Cognitively-Inspired Approach for Enhancing LLM Creativity through Structured Representations", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "10 pages, 8 figures", "summary": "Large Language Models (LLMs) excel at countless tasks, yet struggle with\ncreativity. In this paper, we introduce a novel approach that couples LLMs with\nstructured representations and cognitively inspired manipulations to generate\nmore creative and diverse ideas. Our notion of creativity goes beyond\nsuperficial token-level variations; rather, we explicitly recombine structured\nrepresentations of existing ideas, allowing our algorithm to effectively\nexplore the more abstract landscape of ideas. We demonstrate our approach in\nthe culinary domain with DishCOVER, a model that generates creative recipes.\nExperiments comparing our model's results to those of GPT-4o show greater\ndiversity. Domain expert evaluations reveal that our outputs, which are mostly\ncoherent and feasible culinary creations, significantly surpass GPT-4o in terms\nof novelty, thus outperforming it in creative generation. We hope our work\ninspires further research into structured creativity in AI."}
{"id": "2504.20379", "pdf": "https://arxiv.org/pdf/2504.20379", "abs": "https://arxiv.org/abs/2504.20379", "authors": ["Jongwon Lee", "Timothy Bretl"], "title": "GSFeatLoc: Visual Localization Using Feature Correspondence on 3D Gaussian Splatting", "categories": ["cs.CV", "cs.RO"], "comment": null, "summary": "In this paper, we present a method for localizing a query image with respect\nto a precomputed 3D Gaussian Splatting (3DGS) scene representation. First, the\nmethod uses 3DGS to render a synthetic RGBD image at some initial pose\nestimate. Second, it establishes 2D-2D correspondences between the query image\nand this synthetic image. Third, it uses the depth map to lift the 2D-2D\ncorrespondences to 2D-3D correspondences and solves a perspective-n-point (PnP)\nproblem to produce a final pose estimate. Results from evaluation across three\nexisting datasets with 38 scenes and over 2,700 test images show that our\nmethod significantly reduces both inference time (by over two orders of\nmagnitude, from more than 10 seconds to as fast as 0.1 seconds) and estimation\nerror compared to baseline methods that use photometric loss minimization.\nResults also show that our method tolerates large errors in the initial pose\nestimate of up to 55{\\deg} in rotation and 1.1 units in translation (normalized\nby scene scale), achieving final pose errors of less than 5{\\deg} in rotation\nand 0.05 units in translation on 90% of images from the Synthetic NeRF and\nMip-NeRF360 datasets and on 42% of images from the more challenging Tanks and\nTemples dataset."}
{"id": "2504.20784", "pdf": "https://arxiv.org/pdf/2504.20784", "abs": "https://arxiv.org/abs/2504.20784", "authors": ["Malte Luttermann", "Jan Speller", "Marcel Gehrke", "Tanya Braun", "Ralf Möller", "Mattis Hartwig"], "title": "Approximate Lifted Model Construction", "categories": ["cs.AI", "cs.DS", "cs.LG"], "comment": "Extended version of paper accepted to the Proceedings of the 34th\n  International Joint Conference on Artificial Intelligence (IJCAI-2025)", "summary": "Probabilistic relational models such as parametric factor graphs enable\nefficient (lifted) inference by exploiting the indistinguishability of objects.\nIn lifted inference, a representative of indistinguishable objects is used for\ncomputations. To obtain a relational (i.e., lifted) representation, the\nAdvanced Colour Passing (ACP) algorithm is the state of the art. The ACP\nalgorithm, however, requires underlying distributions, encoded as\npotential-based factorisations, to exactly match to identify and exploit\nindistinguishabilities. Hence, ACP is unsuitable for practical applications\nwhere potentials learned from data inevitably deviate even if associated\nobjects are indistinguishable. To mitigate this problem, we introduce the\n$\\varepsilon$-Advanced Colour Passing ($\\varepsilon$-ACP) algorithm, which\nallows for a deviation of potentials depending on a hyperparameter\n$\\varepsilon$. $\\varepsilon$-ACP efficiently uncovers and exploits\nindistinguishabilities that are not exact. We prove that the approximation\nerror induced by $\\varepsilon$-ACP is strictly bounded and our experiments show\nthat the approximation error is close to zero in practice."}
{"id": "2504.20249", "pdf": "https://arxiv.org/pdf/2504.20249", "abs": "https://arxiv.org/abs/2504.20249", "authors": ["W. Diab", "M. Al-Kobaisi"], "title": "Temporal Neural Operator for Modeling Time-Dependent Physical Phenomena", "categories": ["cs.LG"], "comment": null, "summary": "Neural Operators (NOs) are machine learning models designed to solve partial\ndifferential equations (PDEs) by learning to map between function spaces.\nNeural Operators such as the Deep Operator Network (DeepONet) and the Fourier\nNeural Operator (FNO) have demonstrated excellent generalization properties\nwhen mapping between spatial function spaces. However, they struggle in mapping\nthe temporal dynamics of time-dependent PDEs, especially for time steps not\nexplicitly seen during training. This limits their temporal accuracy as they do\nnot leverage these dynamics in the training process. In addition, most NOs tend\nto be prohibitively costly to train, especially for higher-dimensional PDEs. In\nthis paper, we propose the Temporal Neural Operator (TNO), an efficient neural\noperator specifically designed for spatio-temporal operator learning for\ntime-dependent PDEs. TNO achieves this by introducing a temporal-branch to the\nDeepONet framework, leveraging the best architectural design choices from\nseveral other NOs, and a combination of training strategies including Markov\nassumption, teacher forcing, temporal bundling, and the flexibility to\ncondition the output on the current state or past states. Through extensive\nbenchmarking and an ablation study on a diverse set of example problems we\ndemonstrate the TNO long range temporal extrapolation capabilities, robustness\nto error accumulation, resolution invariance, and flexibility to handle\nmultiple input functions."}
{"id": "2504.20668", "pdf": "https://arxiv.org/pdf/2504.20668", "abs": "https://arxiv.org/abs/2504.20668", "authors": ["Ivan Vykopal", "Martin Hyben", "Robert Moro", "Michal Gregor", "Jakub Simko"], "title": "A Generative-AI-Driven Claim Retrieval System Capable of Detecting and Retrieving Claims from Social Media Platforms in Multiple Languages", "categories": ["cs.CL"], "comment": null, "summary": "Online disinformation poses a global challenge, placing significant demands\non fact-checkers who must verify claims efficiently to prevent the spread of\nfalse information. A major issue in this process is the redundant verification\nof already fact-checked claims, which increases workload and delays responses\nto newly emerging claims. This research introduces an approach that retrieves\npreviously fact-checked claims, evaluates their relevance to a given input, and\nprovides supplementary information to support fact-checkers. Our method employs\nlarge language models (LLMs) to filter irrelevant fact-checks and generate\nconcise summaries and explanations, enabling fact-checkers to faster assess\nwhether a claim has been verified before. In addition, we evaluate our approach\nthrough both automatic and human assessments, where humans interact with the\ndeveloped tool to review its effectiveness. Our results demonstrate that LLMs\nare able to filter out many irrelevant fact-checks and, therefore, reduce\neffort and streamline the fact-checking process."}
{"id": "2504.20384", "pdf": "https://arxiv.org/pdf/2504.20384", "abs": "https://arxiv.org/abs/2504.20384", "authors": ["Yanan Guo", "Wenhui Dong", "Jun Song", "Shiding Zhu", "Xuan Zhang", "Hanqing Yang", "Yingbo Wang", "Yang Du", "Xianing Chen", "Bo Zheng"], "title": "FiLA-Video: Spatio-Temporal Compression for Fine-Grained Long Video Understanding", "categories": ["cs.CV"], "comment": "8 pages, 6 figures", "summary": "Recent advancements in video understanding within visual large language\nmodels (VLLMs) have led to notable progress. However, the complexity of video\ndata and contextual processing limitations still hinder long-video\ncomprehension. A common approach is video feature compression to reduce token\ninput to large language models, yet many methods either fail to prioritize\nessential features, leading to redundant inter-frame information, or introduce\ncomputationally expensive modules.To address these issues, we propose\nFiLA(Fine-grained Vision Language Model)-Video, a novel framework that\nleverages a lightweight dynamic-weight multi-frame fusion strategy, which\nadaptively integrates multiple frames into a single representation while\npreserving key video information and reducing computational costs. To enhance\nframe selection for fusion, we introduce a keyframe selection strategy,\neffectively identifying informative frames from a larger pool for improved\nsummarization. Additionally, we present a simple yet effective long-video\ntraining data generation strategy, boosting model performance without extensive\nmanual annotation. Experimental results demonstrate that FiLA-Video achieves\nsuperior efficiency and accuracy in long-video comprehension compared to\nexisting methods."}
{"id": "2504.20797", "pdf": "https://arxiv.org/pdf/2504.20797", "abs": "https://arxiv.org/abs/2504.20797", "authors": ["Renye Zhang", "Yimin Yin", "Jinghua Zhang"], "title": "Partitioned Memory Storage Inspired Few-Shot Class-Incremental learning", "categories": ["cs.AI"], "comment": null, "summary": "Current mainstream deep learning techniques exhibit an over-reliance on\nextensive training data and a lack of adaptability to the dynamic world,\nmarking a considerable disparity from human intelligence. To bridge this gap,\nFew-Shot Class-Incremental Learning (FSCIL) has emerged, focusing on continuous\nlearning of new categories with limited samples without forgetting old\nknowledge. Existing FSCIL studies typically use a single model to learn\nknowledge across all sessions, inevitably leading to the stability-plasticity\ndilemma. Unlike machines, humans store varied knowledge in different cerebral\ncortices. Inspired by this characteristic, our paper aims to develop a method\nthat learns independent models for each session. It can inherently prevent\ncatastrophic forgetting. During the testing stage, our method integrates\nUncertainty Quantification (UQ) for model deployment. Our method provides a\nfresh viewpoint for FSCIL and demonstrates the state-of-the-art performance on\nCIFAR-100 and mini-ImageNet datasets."}
{"id": "2504.20250", "pdf": "https://arxiv.org/pdf/2504.20250", "abs": "https://arxiv.org/abs/2504.20250", "authors": ["Kun Yang", "Nikhil Krishnan", "Sanjeev R. Kulkarni"], "title": "Financial Data Analysis with Robust Federated Logistic Regression", "categories": ["cs.LG", "q-fin.GN", "q-fin.ST", "stat.AP", "stat.ML"], "comment": null, "summary": "In this study, we focus on the analysis of financial data in a federated\nsetting, wherein data is distributed across multiple clients or locations, and\nthe raw data never leaves the local devices. Our primary focus is not only on\nthe development of efficient learning frameworks (for protecting user data\nprivacy) in the field of federated learning but also on the importance of\ndesigning models that are easier to interpret. In addition, we care about the\nrobustness of the framework to outliers. To achieve these goals, we propose a\nrobust federated logistic regression-based framework that strives to strike a\nbalance between these goals. To verify the feasibility of our proposed\nframework, we carefully evaluate its performance not only on independently\nidentically distributed (IID) data but also on non-IID data, especially in\nscenarios involving outliers. Extensive numerical results collected from\nmultiple public datasets demonstrate that our proposed method can achieve\ncomparable performance to those of classical centralized algorithms, such as\nLogistical Regression, Decision Tree, and K-Nearest Neighbors, in both binary\nand multi-class classification tasks."}
{"id": "2504.20679", "pdf": "https://arxiv.org/pdf/2504.20679", "abs": "https://arxiv.org/abs/2504.20679", "authors": ["Wing Yan Li", "Zeqiang Wang", "Jon Johnson", "Suparna De"], "title": "Are Information Retrieval Approaches Good at Harmonising Longitudinal Survey Questions in Social Science?", "categories": ["cs.CL", "cs.IR"], "comment": null, "summary": "Automated detection of semantically equivalent questions in longitudinal\nsocial science surveys is crucial for long-term studies informing empirical\nresearch in the social, economic, and health sciences. Retrieving equivalent\nquestions faces dual challenges: inconsistent representation of theoretical\nconstructs (i.e. concept/sub-concept) across studies as well as between\nquestion and response options, and the evolution of vocabulary and structure in\nlongitudinal text. To address these challenges, our multi-disciplinary\ncollaboration of computer scientists and survey specialists presents a new\ninformation retrieval (IR) task of identifying concept (e.g. Housing, Job,\netc.) equivalence across question and response options to harmonise\nlongitudinal population studies. This paper investigates multiple unsupervised\napproaches on a survey dataset spanning 1946-2020, including probabilistic\nmodels, linear probing of language models, and pre-trained neural networks\nspecialised for IR. We show that IR-specialised neural models achieve the\nhighest overall performance with other approaches performing comparably.\nAdditionally, the re-ranking of the probabilistic model's results with neural\nmodels only introduces modest improvements of 0.07 at most in F1-score.\nQualitative post-hoc evaluation by survey specialists shows that models\ngenerally have a low sensitivity to questions with high lexical overlap,\nparticularly in cases where sub-concepts are mismatched. Altogether, our\nanalysis serves to further research on harmonising longitudinal studies in\nsocial science."}
{"id": "2504.20409", "pdf": "https://arxiv.org/pdf/2504.20409", "abs": "https://arxiv.org/abs/2504.20409", "authors": ["Jingfeng Guo", "Jinnan Chen", "Weikai Chen", "Zhenyu Sun", "Lanjiong Li", "Baozhu Zhao", "Lingting Zhu", "Xin Wang", "Qi Liu"], "title": "GarmentX: Autoregressive Parametric Representations for High-Fidelity 3D Garment Generation", "categories": ["cs.CV"], "comment": null, "summary": "This work presents GarmentX, a novel framework for generating diverse,\nhigh-fidelity, and wearable 3D garments from a single input image. Traditional\ngarment reconstruction methods directly predict 2D pattern edges and their\nconnectivity, an overly unconstrained approach that often leads to severe\nself-intersections and physically implausible garment structures. In contrast,\nGarmentX introduces a structured and editable parametric representation\ncompatible with GarmentCode, ensuring that the decoded sewing patterns always\nform valid, simulation-ready 3D garments while allowing for intuitive\nmodifications of garment shape and style. To achieve this, we employ a masked\nautoregressive model that sequentially predicts garment parameters, leveraging\nautoregressive modeling for structured generation while mitigating\ninconsistencies in direct pattern prediction. Additionally, we introduce\nGarmentX dataset, a large-scale dataset of 378,682 garment parameter-image\npairs, constructed through an automatic data generation pipeline that\nsynthesizes diverse and high-quality garment images conditioned on parametric\ngarment representations. Through integrating our method with GarmentX dataset,\nwe achieve state-of-the-art performance in geometric fidelity and input image\nalignment, significantly outperforming prior approaches. We will release\nGarmentX dataset upon publication."}
{"id": "2504.20828", "pdf": "https://arxiv.org/pdf/2504.20828", "abs": "https://arxiv.org/abs/2504.20828", "authors": ["Azam Ikram", "Xiang Li", "Sameh Elnikety", "Saurabh Bagchi"], "title": "Ascendra: Dynamic Request Prioritization for Efficient LLM Serving", "categories": ["cs.AI"], "comment": null, "summary": "The rapid advancement of Large Language Models (LLMs) has driven the need for\nmore efficient serving strategies. In this context, efficiency refers to the\nproportion of requests that meet their Service Level Objectives (SLOs),\nparticularly for Time To First Token (TTFT) and Time Between Tokens (TBT).\nHowever, existing systems often prioritize one metric at the cost of the other.\nWe present Ascendra, an LLM serving system designed to meet both TTFT and TBT\nSLOs simultaneously. The core insight behind Ascendra is that a request's\nurgency evolves as it approaches its deadline. To leverage this, Ascendra\npartitions GPU resources into two types of instances: low-priority and\nhigh-priority. Low-priority instances maximize throughput by processing\nrequests out of arrival order, but at the risk of request starvation. To\naddress this, Ascendra employs a performance model to predict requests at risk\nof missing their SLOs and proactively offloads them to high-priority instances.\nHigh-priority instances are optimized for low-latency execution and handle\nurgent requests nearing their deadlines. This partitioned architecture enables\nAscendra to effectively balance high throughput and low latency. Extensive\nevaluation shows that Ascendra improves system throughput by up to 1.7x\ncompared to vLLM and Sarathi-Serve while meeting both TTFT and TBT SLOs."}
{"id": "2504.20271", "pdf": "https://arxiv.org/pdf/2504.20271", "abs": "https://arxiv.org/abs/2504.20271", "authors": ["Henk Tillman", "Dan Mossing"], "title": "Investigating task-specific prompts and sparse autoencoders for activation monitoring", "categories": ["cs.LG"], "comment": "18 pages, 13 figures", "summary": "Language models can behave in unexpected and unsafe ways, and so it is\nvaluable to monitor their outputs. Internal activations of language models\nencode additional information that could be useful for this. The baseline\napproach for activation monitoring is some variation of linear probing on a\nparticular layer: starting from a labeled dataset, train a logistic regression\nclassifier on that layer's activations. Recent work has proposed several\napproaches which may improve on naive linear probing, by leveraging additional\ncomputation. One class of techniques, which we call \"prompted probing,\"\nleverages test time computation to improve monitoring by (1) prompting the\nmodel with a description of the monitoring task, and (2) applying a learned\nlinear probe to resulting activations. Another class of techniques uses\ncomputation at train time: training sparse autoencoders offline to identify an\ninterpretable basis for the activations, and e.g. max-pooling activations\nacross tokens using that basis before applying a linear probe. However, one can\nalso prompt the model with a description of the monitoring task and use its\noutput directly. We develop and test novel refinements of these methods and\ncompare them against each other. We find asking the model zero-shot is a\nreasonable baseline when inference-time compute is not limited; however,\nactivation probing methods can substantially outperform this baseline given\nsufficient training data. Specifically, we recommend prompted probing when\ninference-time compute is available, due to its superior data efficiency and\ngood generalization performance. Alternatively, if inference-time compute is\nlimited, we find SAE-based probing methods outperform raw activation probing."}
{"id": "2504.20699", "pdf": "https://arxiv.org/pdf/2504.20699", "abs": "https://arxiv.org/abs/2504.20699", "authors": ["Evangelia Gogoulou", "Shorouq Zahra", "Liane Guillou", "Luise Dürlich", "Joakim Nivre"], "title": "Can LLMs Detect Intrinsic Hallucinations in Paraphrasing and Machine Translation?", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "A frequently observed problem with LLMs is their tendency to generate output\nthat is nonsensical, illogical, or factually incorrect, often referred to\nbroadly as hallucination. Building on the recently proposed HalluciGen task for\nhallucination detection and generation, we evaluate a suite of open-access LLMs\non their ability to detect intrinsic hallucinations in two conditional\ngeneration tasks: translation and paraphrasing. We study how model performance\nvaries across tasks and language and we investigate the impact of model size,\ninstruction tuning, and prompt choice. We find that performance varies across\nmodels but is consistent across prompts. Finally, we find that NLI models\nperform comparably well, suggesting that LLM-based detectors are not the only\nviable option for this specific task."}
{"id": "2504.20419", "pdf": "https://arxiv.org/pdf/2504.20419", "abs": "https://arxiv.org/abs/2504.20419", "authors": ["Konstantinos I. Roumeliotis", "Ranjan Sapkota", "Manoj Karkee", "Nikolaos D. Tselikas", "Dimitrios K. Nasiopoulos"], "title": "Plant Disease Detection through Multimodal Large Language Models and Convolutional Neural Networks", "categories": ["cs.CV"], "comment": null, "summary": "Automation in agriculture plays a vital role in addressing challenges related\nto crop monitoring and disease management, particularly through early detection\nsystems. This study investigates the effectiveness of combining multimodal\nLarge Language Models (LLMs), specifically GPT-4o, with Convolutional Neural\nNetworks (CNNs) for automated plant disease classification using leaf imagery.\nLeveraging the PlantVillage dataset, we systematically evaluate model\nperformance across zero-shot, few-shot, and progressive fine-tuning scenarios.\nA comparative analysis between GPT-4o and the widely used ResNet-50 model was\nconducted across three resolutions (100, 150, and 256 pixels) and two plant\nspecies (apple and corn). Results indicate that fine-tuned GPT-4o models\nachieved slightly better performance compared to the performance of ResNet-50,\nachieving up to 98.12% classification accuracy on apple leaf images, compared\nto 96.88% achieved by ResNet-50, with improved generalization and near-zero\ntraining loss. However, zero-shot performance of GPT-4o was significantly\nlower, underscoring the need for minimal training. Additional evaluations on\ncross-resolution and cross-plant generalization revealed the models'\nadaptability and limitations when applied to new domains. The findings\nhighlight the promise of integrating multimodal LLMs into automated disease\ndetection pipelines, enhancing the scalability and intelligence of precision\nagriculture systems while reducing the dependence on large, labeled datasets\nand high-resolution sensor infrastructure. Large Language Models, Vision\nLanguage Models, LLMs and CNNs, Disease Detection with Vision Language Models,\nVLMs"}
{"id": "2504.20846", "pdf": "https://arxiv.org/pdf/2504.20846", "abs": "https://arxiv.org/abs/2504.20846", "authors": ["Robert F. Downey", "S. S. Ravi"], "title": "Disjunctive and Conjunctive Normal Form Explanations of Clusters Using Auxiliary Information", "categories": ["cs.AI", "I.2"], "comment": null, "summary": "We consider generating post-hoc explanations of clusters generated from\nvarious datasets using auxiliary information which was not used by clustering\nalgorithms. Following terminology used in previous work, we refer to the\nauxiliary information as tags. Our focus is on two forms of explanations,\nnamely disjunctive form (where the explanation for a cluster consists of a set\nof tags) and a two-clause conjunctive normal form (CNF) explanation (where the\nexplanation consists of two sets of tags, combined through the AND operator).\nWe use integer linear programming (ILP) as well as heuristic methods to\ngenerate these explanations. We experiment with a variety of datasets and\ndiscuss the insights obtained from our explanations. We also present\nexperimental results regarding the scalability of our explanation methods."}
{"id": "2504.20277", "pdf": "https://arxiv.org/pdf/2504.20277", "abs": "https://arxiv.org/abs/2504.20277", "authors": ["Yigit Berkay Uslu", "Samar Hadou", "Shirin Saeedi Bidokhti", "Alejandro Ribeiro"], "title": "Generative Diffusion Models for Resource Allocation in Wireless Networks", "categories": ["cs.LG", "eess.SP"], "comment": null, "summary": "This paper proposes a supervised training algorithm for learning stochastic\nresource allocation policies with generative diffusion models (GDMs). We\nformulate the allocation problem as the maximization of an ergodic utility\nfunction subject to ergodic Quality of Service (QoS) constraints. Given samples\nfrom a stochastic expert policy that yields a near-optimal solution to the\nproblem, we train a GDM policy to imitate the expert and generate new samples\nfrom the optimal distribution. We achieve near-optimal performance through\nsequential execution of the generated samples. To enable generalization to a\nfamily of network configurations, we parameterize the backward diffusion\nprocess with a graph neural network (GNN) architecture. We present numerical\nresults in a case study of power control in multi-user interference networks."}
{"id": "2504.20703", "pdf": "https://arxiv.org/pdf/2504.20703", "abs": "https://arxiv.org/abs/2504.20703", "authors": ["Foteini Papadopoulou", "Osman Mutlu", "Neris Özen", "Bas H. M. van der Velden", "Iris Hendrickx", "Ali Hürriyetoğlu"], "title": "BrightCookies at SemEval-2025 Task 9: Exploring Data Augmentation for Food Hazard Classification", "categories": ["cs.CL"], "comment": null, "summary": "This paper presents our system developed for the SemEval-2025 Task 9: The\nFood Hazard Detection Challenge. The shared task's objective is to evaluate\nexplainable classification systems for classifying hazards and products in two\nlevels of granularity from food recall incident reports. In this work, we\npropose text augmentation techniques as a way to improve poor performance on\nminority classes and compare their effect for each category on various\ntransformer and machine learning models. We explore three word-level data\naugmentation techniques, namely synonym replacement, random word swapping, and\ncontextual word insertion. The results show that transformer models tend to\nhave a better overall performance. None of the three augmentation techniques\nconsistently improved overall performance for classifying hazards and products.\nWe observed a statistically significant improvement (P < 0.05) in the\nfine-grained categories when using the BERT model to compare the baseline with\neach augmented model. Compared to the baseline, the contextual words insertion\naugmentation improved the accuracy of predictions for the minority hazard\nclasses by 6%. This suggests that targeted augmentation of minority classes can\nimprove the performance of transformer models."}
{"id": "2504.20435", "pdf": "https://arxiv.org/pdf/2504.20435", "abs": "https://arxiv.org/abs/2504.20435", "authors": ["Love Panta", "Suraj Prasai", "Karishma Malla Vaidya", "Shyam Shrestha", "Suresh Manandhar"], "title": "AI Assisted Cervical Cancer Screening for Cytology Samples in Developing Countries", "categories": ["cs.CV"], "comment": null, "summary": "Cervical cancer remains a significant health challenge, with high incidence\nand mortality rates, particularly in transitioning countries. Conventional\nLiquid-Based Cytology(LBC) is a labor-intensive process, requires expert\npathologists and is highly prone to errors, highlighting the need for more\nefficient screening methods. This paper introduces an innovative approach that\nintegrates low-cost biological microscopes with our simple and efficient AI\nalgorithms for automated whole-slide analysis. Our system uses a motorized\nmicroscope to capture cytology images, which are then processed through an AI\npipeline involving image stitching, cell segmentation, and classification. We\nutilize the lightweight UNet-based model involving human-in-the-loop approach\nto train our segmentation model with minimal ROIs. CvT-based classification\nmodel, trained on the SIPaKMeD dataset, accurately categorizes five cell types.\nOur framework offers enhanced accuracy and efficiency in cervical cancer\nscreening compared to various state-of-art methods, as demonstrated by\ndifferent evaluation metrics."}
{"id": "2504.20879", "pdf": "https://arxiv.org/pdf/2504.20879", "abs": "https://arxiv.org/abs/2504.20879", "authors": ["Shivalika Singh", "Yiyang Nan", "Alex Wang", "Daniel D'Souza", "Sayash Kapoor", "Ahmet Üstün", "Sanmi Koyejo", "Yuntian Deng", "Shayne Longpre", "Noah Smith", "Beyza Ermis", "Marzieh Fadaee", "Sara Hooker"], "title": "The Leaderboard Illusion", "categories": ["cs.AI", "cs.CL", "cs.LG", "stat.ME"], "comment": "68 pages, 18 figures, 9 tables", "summary": "Measuring progress is fundamental to the advancement of any scientific field.\nAs benchmarks play an increasingly central role, they also grow more\nsusceptible to distortion. Chatbot Arena has emerged as the go-to leaderboard\nfor ranking the most capable AI systems. Yet, in this work we identify\nsystematic issues that have resulted in a distorted playing field. We find that\nundisclosed private testing practices benefit a handful of providers who are\nable to test multiple variants before public release and retract scores if\ndesired. We establish that the ability of these providers to choose the best\nscore leads to biased Arena scores due to selective disclosure of performance\nresults. At an extreme, we identify 27 private LLM variants tested by Meta in\nthe lead-up to the Llama-4 release. We also establish that proprietary closed\nmodels are sampled at higher rates (number of battles) and have fewer models\nremoved from the arena than open-weight and open-source alternatives. Both\nthese policies lead to large data access asymmetries over time. Providers like\nGoogle and OpenAI have received an estimated 19.2% and 20.4% of all data on the\narena, respectively. In contrast, a combined 83 open-weight models have only\nreceived an estimated 29.7% of the total data. We show that access to Chatbot\nArena data yields substantial benefits; even limited additional data can result\nin relative performance gains of up to 112% on the arena distribution, based on\nour conservative estimates. Together, these dynamics result in overfitting to\nArena-specific dynamics rather than general model quality. The Arena builds on\nthe substantial efforts of both the organizers and an open community that\nmaintains this valuable evaluation platform. We offer actionable\nrecommendations to reform the Chatbot Arena's evaluation framework and promote\nfairer, more transparent benchmarking for the field"}
{"id": "2504.20282", "pdf": "https://arxiv.org/pdf/2504.20282", "abs": "https://arxiv.org/abs/2504.20282", "authors": ["Michael A. Helcig", "Stefan Nastic"], "title": "FedCCL: Federated Clustered Continual Learning Framework for Privacy-focused Energy Forecasting", "categories": ["cs.LG"], "comment": null, "summary": "Privacy-preserving distributed model training is crucial for modern machine\nlearning applications, yet existing Federated Learning approaches struggle with\nheterogeneous data distributions and varying computational capabilities.\nTraditional solutions either treat all participants uniformly or require costly\ndynamic clustering during training, leading to reduced efficiency and delayed\nmodel specialization. We present FedCCL (Federated Clustered Continual\nLearning), a framework specifically designed for environments with static\norganizational characteristics but dynamic client availability. By combining\nstatic pre-training clustering with an adapted asynchronous FedAvg algorithm,\nFedCCL enables new clients to immediately profit from specialized models\nwithout prior exposure to their data distribution, while maintaining reduced\ncoordination overhead and resilience to client disconnections. Our approach\nimplements an asynchronous Federated Learning protocol with a three-tier model\ntopology - global, cluster-specific, and local models - that efficiently\nmanages knowledge sharing across heterogeneous participants. Evaluation using\nphotovoltaic installations across central Europe demonstrates that FedCCL's\nlocation-based clustering achieves an energy prediction error of 3.93%\n(+-0.21%), while maintaining data privacy and showing that the framework\nmaintains stability for population-independent deployments, with 0.14\npercentage point degradation in performance for new installations. The results\ndemonstrate that FedCCL offers an effective framework for privacy-preserving\ndistributed learning, maintaining high accuracy and adaptability even with\ndynamic participant populations."}
{"id": "2504.20708", "pdf": "https://arxiv.org/pdf/2504.20708", "abs": "https://arxiv.org/abs/2504.20708", "authors": ["Hasan Abed Al Kader Hammoud", "Hani Itani", "Bernard Ghanem"], "title": "Beyond the Last Answer: Your Reasoning Trace Uncovers More than You Think", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "Preprint", "summary": "Large Language Models (LLMs) leverage step-by-step reasoning to solve complex\nproblems. Standard evaluation practice involves generating a complete reasoning\ntrace and assessing the correctness of the final answer presented at its\nconclusion. In this paper, we challenge the reliance on the final answer by\nposing the following two questions: Does the final answer reliably represent\nthe model's optimal conclusion? Can alternative reasoning paths yield different\nresults? To answer these questions, we analyze intermediate reasoning steps,\ntermed subthoughts, and propose a method based on our findings. Our approach\ninvolves segmenting a reasoning trace into sequential subthoughts based on\nlinguistic cues. We start by prompting the model to generate continuations from\nthe end-point of each intermediate subthought. We extract a potential answer\nfrom every completed continuation originating from different subthoughts. We\nfind that aggregating these answers by selecting the most frequent one (the\nmode) often yields significantly higher accuracy compared to relying solely on\nthe answer derived from the original complete trace. Analyzing the consistency\namong the answers derived from different subthoughts reveals characteristics\nthat correlate with the model's confidence and correctness, suggesting\npotential for identifying less reliable answers. Our experiments across various\nLLMs and challenging mathematical reasoning datasets (AIME2024 and AIME2025)\nshow consistent accuracy improvements, with gains reaching up to 13\\% and 10\\%\nrespectively. Implementation is available at:\nhttps://github.com/hammoudhasan/SubthoughtReasoner."}
{"id": "2504.20438", "pdf": "https://arxiv.org/pdf/2504.20438", "abs": "https://arxiv.org/abs/2504.20438", "authors": ["Ziyang Xu", "Kangsheng Duan", "Xiaolei Shen", "Zhifeng Ding", "Wenyu Liu", "Xiaohu Ruan", "Xiaoxin Chen", "Xinggang Wang"], "title": "PixelHacker: Image Inpainting with Structural and Semantic Consistency", "categories": ["cs.CV"], "comment": null, "summary": "Image inpainting is a fundamental research area between image editing and\nimage generation. Recent state-of-the-art (SOTA) methods have explored novel\nattention mechanisms, lightweight architectures, and context-aware modeling,\ndemonstrating impressive performance. However, they often struggle with complex\nstructure (e.g., texture, shape, spatial relations) and semantics (e.g., color\nconsistency, object restoration, and logical correctness), leading to artifacts\nand inappropriate generation. To address this challenge, we design a simple yet\neffective inpainting paradigm called latent categories guidance, and further\npropose a diffusion-based model named PixelHacker. Specifically, we first\nconstruct a large dataset containing 14 million image-mask pairs by annotating\nforeground and background (potential 116 and 21 categories, respectively).\nThen, we encode potential foreground and background representations separately\nthrough two fixed-size embeddings, and intermittently inject these features\ninto the denoising process via linear attention. Finally, by pre-training on\nour dataset and fine-tuning on open-source benchmarks, we obtain PixelHacker.\nExtensive experiments show that PixelHacker comprehensively outperforms the\nSOTA on a wide range of datasets (Places2, CelebA-HQ, and FFHQ) and exhibits\nremarkable consistency in both structure and semantics. Project page at\nhttps://hustvl.github.io/projects/PixelHacker."}
{"id": "2504.20898", "pdf": "https://arxiv.org/pdf/2504.20898", "abs": "https://arxiv.org/abs/2504.20898", "authors": ["Hasan Md Tusfiqur Alam", "Devansh Srivastav", "Abdulrahman Mohamed Selim", "Md Abdul Kadir", "Md Moktadiurl Hoque Shuvo", "Daniel Sonntag"], "title": "CBM-RAG: Demonstrating Enhanced Interpretability in Radiology Report Generation with Multi-Agent RAG and Concept Bottleneck Models", "categories": ["cs.AI", "cs.CV", "cs.IR"], "comment": "Accepted in the 17th ACM SIGCHI Symposium on Engineering Interactive\n  Computing Systems (EICS 2025)", "summary": "Advancements in generative Artificial Intelligence (AI) hold great promise\nfor automating radiology workflows, yet challenges in interpretability and\nreliability hinder clinical adoption. This paper presents an automated\nradiology report generation framework that combines Concept Bottleneck Models\n(CBMs) with a Multi-Agent Retrieval-Augmented Generation (RAG) system to bridge\nAI performance with clinical explainability. CBMs map chest X-ray features to\nhuman-understandable clinical concepts, enabling transparent disease\nclassification. Meanwhile, the RAG system integrates multi-agent collaboration\nand external knowledge to produce contextually rich, evidence-based reports.\nOur demonstration showcases the system's ability to deliver interpretable\npredictions, mitigate hallucinations, and generate high-quality, tailored\nreports with an interactive interface addressing accuracy, trust, and usability\nchallenges. This framework provides a pathway to improving diagnostic\nconsistency and empowering radiologists with actionable insights."}
{"id": "2504.20293", "pdf": "https://arxiv.org/pdf/2504.20293", "abs": "https://arxiv.org/abs/2504.20293", "authors": ["Stefan Kober"], "title": "Radius-Guided Post-Clustering for Shape-Aware, Scalable Refinement of k-Means Results", "categories": ["cs.LG"], "comment": "9 pages, 2 figures, 4 tables. Open-source code available at:\n  https://github.com/stefankober/radius-guided-kmeans", "summary": "Traditional k-means clustering underperforms on non-convex shapes and\nrequires the number of clusters k to be specified in advance. We propose a\nsimple geometric enhancement: after standard k-means, each cluster center is\nassigned a radius (the distance to its farthest assigned point), and clusters\nwhose radii overlap are merged. This post-processing step loosens the\nrequirement for exact k: as long as k is overestimated (but not excessively),\nthe method can often reconstruct non-convex shapes through meaningful merges.\nWe also show that this approach supports recursive partitioning: clustering can\nbe performed independently on tiled regions of the feature space, then globally\nmerged, making the method scalable and suitable for distributed systems.\nImplemented as a lightweight post-processing step atop scikit-learn's k-means,\nthe algorithm performs well on benchmark datasets, achieving high accuracy with\nminimal additional computation."}
{"id": "2504.20734", "pdf": "https://arxiv.org/pdf/2504.20734", "abs": "https://arxiv.org/abs/2504.20734", "authors": ["Woongyeong Yeo", "Kangsan Kim", "Soyeong Jeong", "Jinheon Baek", "Sung Ju Hwang"], "title": "UniversalRAG: Retrieval-Augmented Generation over Multiple Corpora with Diverse Modalities and Granularities", "categories": ["cs.CL", "cs.AI", "cs.CV", "cs.IR", "cs.LG"], "comment": "Project page : https://universalrag.github.io", "summary": "Retrieval-Augmented Generation (RAG) has shown substantial promise in\nimproving factual accuracy by grounding model responses with external knowledge\nrelevant to queries. However, most existing RAG approaches are limited to a\ntext-only corpus, and while recent efforts have extended RAG to other\nmodalities such as images and videos, they typically operate over a single\nmodality-specific corpus. In contrast, real-world queries vary widely in the\ntype of knowledge they require, which a single type of knowledge source cannot\naddress. To address this, we introduce UniversalRAG, a novel RAG framework\ndesigned to retrieve and integrate knowledge from heterogeneous sources with\ndiverse modalities and granularities. Specifically, motivated by the\nobservation that forcing all modalities into a unified representation space\nderived from a single combined corpus causes a modality gap, where the\nretrieval tends to favor items from the same modality as the query, we propose\na modality-aware routing mechanism that dynamically identifies the most\nappropriate modality-specific corpus and performs targeted retrieval within it.\nAlso, beyond modality, we organize each modality into multiple granularity\nlevels, enabling fine-tuned retrieval tailored to the complexity and scope of\nthe query. We validate UniversalRAG on 8 benchmarks spanning multiple\nmodalities, showing its superiority over modality-specific and unified\nbaselines."}
{"id": "2504.20466", "pdf": "https://arxiv.org/pdf/2504.20466", "abs": "https://arxiv.org/abs/2504.20466", "authors": ["Woo Yi Yang", "Jiarui Wang", "Sijing Wu", "Huiyu Duan", "Yuxin Zhu", "Liu Yang", "Kang Fu", "Guangtao Zhai", "Xiongkuo Min"], "title": "LMM4Gen3DHF: Benchmarking and Evaluating Multimodal 3D Human Face Generation with LMMs", "categories": ["cs.CV"], "comment": null, "summary": "The rapid advancement in generative artificial intelligence have enabled the\ncreation of 3D human faces (HFs) for applications including media production,\nvirtual reality, security, healthcare, and game development, etc. However,\nassessing the quality and realism of these AI-generated 3D human faces remains\na significant challenge due to the subjective nature of human perception and\ninnate perceptual sensitivity to facial features. To this end, we conduct a\ncomprehensive study on the quality assessment of AI-generated 3D human faces.\nWe first introduce Gen3DHF, a large-scale benchmark comprising 2,000 videos of\nAI-Generated 3D Human Faces along with 4,000 Mean Opinion Scores (MOS)\ncollected across two dimensions, i.e., quality and authenticity, 2,000\ndistortion-aware saliency maps and distortion descriptions. Based on Gen3DHF,\nwe propose LMME3DHF, a Large Multimodal Model (LMM)-based metric for Evaluating\n3DHF capable of quality and authenticity score prediction, distortion-aware\nvisual question answering, and distortion-aware saliency prediction.\nExperimental results show that LMME3DHF achieves state-of-the-art performance,\nsurpassing existing methods in both accurately predicting quality scores for\nAI-generated 3D human faces and effectively identifying distortion-aware\nsalient regions and distortion types, while maintaining strong alignment with\nhuman perceptual judgments. Both the Gen3DHF database and the LMME3DHF will be\nreleased upon the publication."}
{"id": "2504.20921", "pdf": "https://arxiv.org/pdf/2504.20921", "abs": "https://arxiv.org/abs/2504.20921", "authors": ["Polycarp Nalela"], "title": "Leveraging Generative AI Through Prompt Engineering and Rigorous Validation to Create Comprehensive Synthetic Datasets for AI Training in Healthcare", "categories": ["cs.AI"], "comment": null, "summary": "Access to high-quality medical data is often restricted due to privacy\nconcerns, posing significant challenges for training artificial intelligence\n(AI) algorithms within Electronic Health Record (EHR) applications. In this\nstudy, prompt engineering with the GPT-4 API was employed to generate\nhigh-quality synthetic datasets aimed at overcoming this limitation. The\ngenerated data encompassed a comprehensive array of patient admission\ninformation, including healthcare provider details, hospital departments,\nwards, bed assignments, patient demographics, emergency contacts, vital signs,\nimmunizations, allergies, medical histories, appointments, hospital visits,\nlaboratory tests, diagnoses, treatment plans, medications, clinical notes,\nvisit logs, discharge summaries, and referrals. To ensure data quality and\nintegrity, advanced validation techniques were implemented utilizing models\nsuch as BERT's Next Sentence Prediction for sentence coherence, GPT-2 for\noverall plausibility, RoBERTa for logical consistency, autoencoders for anomaly\ndetection, and conducted diversity analysis. Synthetic data that met all\nvalidation criteria were integrated into a comprehensive PostgreSQL database,\nserving as the data management system for the EHR application. This approach\ndemonstrates that leveraging generative AI models with rigorous validation can\neffectively produce high-quality synthetic medical data, facilitating the\ntraining of AI algorithms while addressing privacy concerns associated with\nreal patient data."}
{"id": "2504.20295", "pdf": "https://arxiv.org/pdf/2504.20295", "abs": "https://arxiv.org/abs/2504.20295", "authors": ["Mohammadhossein Homaei", "Victor Gonzalez Morales", "Oscar Mogollon-Gutierrez", "Andres Caro"], "title": "The Dark Side of Digital Twins: Adversarial Attacks on AI-Driven Water Forecasting", "categories": ["cs.LG", "cs.AI", "cs.CR"], "comment": "7 Pages, 7 Figures", "summary": "Digital twins (DTs) are improving water distribution systems by using\nreal-time data, analytics, and prediction models to optimize operations. This\npaper presents a DT platform designed for a Spanish water supply network,\nutilizing Long Short-Term Memory (LSTM) networks to predict water consumption.\nHowever, machine learning models are vulnerable to adversarial attacks, such as\nthe Fast Gradient Sign Method (FGSM) and Projected Gradient Descent (PGD).\nThese attacks manipulate critical model parameters, injecting subtle\ndistortions that degrade forecasting accuracy. To further exploit these\nvulnerabilities, we introduce a Learning Automata (LA) and Random LA-based\napproach that dynamically adjusts perturbations, making adversarial attacks\nmore difficult to detect. Experimental results show that this approach\nsignificantly impacts prediction reliability, causing the Mean Absolute\nPercentage Error (MAPE) to rise from 26% to over 35%. Moreover, adaptive attack\nstrategies amplify this effect, highlighting cybersecurity risks in AI-driven\nDTs. These findings emphasize the urgent need for robust defenses, including\nadversarial training, anomaly detection, and secure data pipelines."}
{"id": "2504.20752", "pdf": "https://arxiv.org/pdf/2504.20752", "abs": "https://arxiv.org/abs/2504.20752", "authors": ["Roman Abramov", "Felix Steinbauer", "Gjergji Kasneci"], "title": "Grokking in the Wild: Data Augmentation for Real-World Multi-Hop Reasoning with Transformers", "categories": ["cs.CL", "cs.AI", "cs.LG", "I.2.7; I.2.6; I.2.3; I.7"], "comment": null, "summary": "Transformers have achieved great success in numerous NLP tasks but continue\nto exhibit notable gaps in multi-step factual reasoning, especially when\nreal-world knowledge is sparse. Recent advances in grokking have demonstrated\nthat neural networks can transition from memorizing to perfectly generalizing\nonce they detect underlying logical patterns - yet these studies have primarily\nused small, synthetic tasks. In this paper, for the first time, we extend\ngrokking to real-world factual data and address the challenge of dataset\nsparsity by augmenting existing knowledge graphs with carefully designed\nsynthetic data to raise the ratio $\\phi_r$ of inferred facts to atomic facts\nabove the threshold required for grokking. Surprisingly, we find that even\nfactually incorrect synthetic data can strengthen emergent reasoning circuits\nrather than degrade accuracy, as it forces the model to rely on relational\nstructure rather than memorization. When evaluated on multi-hop reasoning\nbenchmarks, our approach achieves up to 95-100% accuracy on 2WikiMultiHopQA -\nsubstantially improving over strong baselines and matching or exceeding current\nstate-of-the-art results. We further provide an in-depth analysis of how\nincreasing $\\phi_r$ drives the formation of generalizing circuits inside\nTransformers. Our findings suggest that grokking-based data augmentation can\nunlock implicit multi-hop reasoning capabilities, opening the door to more\nrobust and interpretable factual reasoning in large-scale language models."}
{"id": "2504.20468", "pdf": "https://arxiv.org/pdf/2504.20468", "abs": "https://arxiv.org/abs/2504.20468", "authors": ["Yuanchen Wu", "Lu Zhang", "Hang Yao", "Junlong Du", "Ke Yan", "Shouhong Ding", "Yunsheng Wu", "Xiaoqiang Li"], "title": "Antidote: A Unified Framework for Mitigating LVLM Hallucinations in Counterfactual Presupposition and Object Perception", "categories": ["cs.CV"], "comment": "Accepted to CVPR 2025", "summary": "Large Vision-Language Models (LVLMs) have achieved impressive results across\nvarious cross-modal tasks. However, hallucinations, i.e., the models generating\ncounterfactual responses, remain a challenge. Though recent studies have\nattempted to alleviate object perception hallucinations, they focus on the\nmodels' response generation, and overlooking the task question itself. This\npaper discusses the vulnerability of LVLMs in solving counterfactual\npresupposition questions (CPQs), where the models are prone to accept the\npresuppositions of counterfactual objects and produce severe hallucinatory\nresponses. To this end, we introduce \"Antidote\", a unified, synthetic\ndata-driven post-training framework for mitigating both types of hallucination\nabove. It leverages synthetic data to incorporate factual priors into questions\nto achieve self-correction, and decouple the mitigation process into a\npreference optimization problem. Furthermore, we construct \"CP-Bench\", a novel\nbenchmark to evaluate LVLMs' ability to correctly handle CPQs and produce\nfactual responses. Applied to the LLaVA series, Antidote can simultaneously\nenhance performance on CP-Bench by over 50%, POPE by 1.8-3.3%, and CHAIR & SHR\nby 30-50%, all without relying on external supervision from stronger LVLMs or\nhuman feedback and introducing noticeable catastrophic forgetting issues."}
{"id": "2504.20924", "pdf": "https://arxiv.org/pdf/2504.20924", "abs": "https://arxiv.org/abs/2504.20924", "authors": ["Beomjun Kim", "Kangyeon Kim", "Sunwoo Kim", "Heejin Ahn"], "title": "A Domain-Agnostic Scalable AI Safety Ensuring Framework", "categories": ["cs.AI"], "comment": "Experimental supplementary material will be available before May 22\n  23:59PM AOE", "summary": "Ensuring the safety of AI systems has recently emerged as a critical priority\nfor real-world deployment, particularly in physical AI applications. Current\napproaches to AI safety typically address predefined domain-specific safety\nconditions, limiting their ability to generalize across contexts.\n  We propose a novel AI safety framework that ensures AI systems comply with\n\\textbf{any user-defined constraint}, with \\textbf{any desired probability},\nand across \\textbf{various domains}.\n  In this framework, we combine an AI component (e.g., neural network) with an\noptimization problem to produce responses that minimize objectives while\nsatisfying user-defined constraints with probabilities exceeding user-defined\nthresholds. For credibility assessment of the AI component, we propose\n\\textit{internal test data}, a supplementary set of safety-labeled data, and a\n\\textit{conservative testing} methodology that provides statistical validity of\nusing internal test data. We also present an approximation method of a loss\nfunction and how to compute its gradient for training.\n  We mathematically prove that probabilistic constraint satisfaction is\nguaranteed under specific, mild conditions and prove a scaling law between\nsafety and the number of internal test data. We demonstrate our framework's\neffectiveness through experiments in diverse domains: demand prediction for\nproduction decision, safe reinforcement learning within the SafetyGym\nsimulator, and guarding AI chatbot outputs. Through these experiments, we\ndemonstrate that our method guarantees safety for user-specified constraints,\noutperforms {for \\textbf{up to several order of magnitudes}} existing methods\nin low safety threshold regions, and scales effectively with respect to the\nsize of internal test data."}
{"id": "2504.20307", "pdf": "https://arxiv.org/pdf/2504.20307", "abs": "https://arxiv.org/abs/2504.20307", "authors": ["Hui Chen", "Xuhui Fan", "Zhangkai Wu", "Longbing Cao"], "title": "FigBO: A Generalized Acquisition Function Framework with Look-Ahead Capability for Bayesian Optimization", "categories": ["cs.LG"], "comment": null, "summary": "Bayesian optimization is a powerful technique for optimizing\nexpensive-to-evaluate black-box functions, consisting of two main components: a\nsurrogate model and an acquisition function. In recent years, myopic\nacquisition functions have been widely adopted for their simplicity and\neffectiveness. However, their lack of look-ahead capability limits their\nperformance. To address this limitation, we propose FigBO, a generalized\nacquisition function that incorporates the future impact of candidate points on\nglobal information gain. FigBO is a plug-and-play method that can integrate\nseamlessly with most existing myopic acquisition functions. Theoretically, we\nanalyze the regret bound and convergence rate of FigBO when combined with the\nmyopic base acquisition function expected improvement (EI), comparing them to\nthose of standard EI. Empirically, extensive experimental results across\ndiverse tasks demonstrate that FigBO achieves state-of-the-art performance and\nsignificantly faster convergence compared to existing methods."}
{"id": "2504.20769", "pdf": "https://arxiv.org/pdf/2504.20769", "abs": "https://arxiv.org/abs/2504.20769", "authors": ["Wenxiao Wang", "Parsa Hosseini", "Soheil Feizi"], "title": "Chain-of-Defensive-Thought: Structured Reasoning Elicits Robustness in Large Language Models against Reference Corruption", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": null, "summary": "Chain-of-thought prompting has demonstrated great success in facilitating the\nreasoning abilities of large language models. In this work, we explore how\nthese enhanced reasoning abilities can be exploited to improve the robustness\nof large language models in tasks that are not necessarily reasoning-focused.\nIn particular, we show how a wide range of large language models exhibit\nsignificantly improved robustness against reference corruption using a simple\nmethod called chain-of-defensive-thought, where only a few exemplars with\nstructured and defensive reasoning are provided as demonstrations. Empirically,\nthe improvements can be astounding, especially given the simplicity and\napplicability of the method. For example, in the Natural Questions task, the\naccuracy of GPT-4o degrades from 60% to as low as 3% with standard prompting\nwhen 1 out of 10 references provided is corrupted with prompt injection\nattacks. In contrast, GPT-4o using chain-of-defensive-thought prompting\nmaintains an accuracy of 50%."}
{"id": "2504.20496", "pdf": "https://arxiv.org/pdf/2504.20496", "abs": "https://arxiv.org/abs/2504.20496", "authors": ["Shuo Sun", "Torsten Sattler", "Malcolm Mielle", "Achim J. Lilienthal", "Martin Magnusson"], "title": "Large-scale visual SLAM for in-the-wild videos", "categories": ["cs.CV"], "comment": "fix the overview figure", "summary": "Accurate and robust 3D scene reconstruction from casual, in-the-wild videos\ncan significantly simplify robot deployment to new environments. However,\nreliable camera pose estimation and scene reconstruction from such\nunconstrained videos remains an open challenge. Existing visual-only SLAM\nmethods perform well on benchmark datasets but struggle with real-world footage\nwhich often exhibits uncontrolled motion including rapid rotations and pure\nforward movements, textureless regions, and dynamic objects. We analyze the\nlimitations of current methods and introduce a robust pipeline designed to\nimprove 3D reconstruction from casual videos. We build upon recent deep visual\nodometry methods but increase robustness in several ways. Camera intrinsics are\nautomatically recovered from the first few frames using structure-from-motion.\nDynamic objects and less-constrained areas are masked with a predictive model.\nAdditionally, we leverage monocular depth estimates to regularize bundle\nadjustment, mitigating errors in low-parallax situations. Finally, we integrate\nplace recognition and loop closure to reduce long-term drift and refine both\nintrinsics and pose estimates through global bundle adjustment. We demonstrate\nlarge-scale contiguous 3D models from several online videos in various\nenvironments. In contrast, baseline methods typically produce locally\ninconsistent results at several points, producing separate segments or\ndistorted maps. In lieu of ground-truth pose data, we evaluate map consistency,\nexecution time and visual accuracy of re-rendered NeRF models. Our proposed\nsystem establishes a new baseline for visual reconstruction from casual\nuncontrolled videos found online, demonstrating more consistent reconstructions\nover longer sequences of in-the-wild videos than previously achieved."}
{"id": "2504.20930", "pdf": "https://arxiv.org/pdf/2504.20930", "abs": "https://arxiv.org/abs/2504.20930", "authors": ["Ziqing Fan", "Cheng Liang", "Chaoyi Wu", "Ya Zhang", "Yanfeng Wang", "Weidi Xie"], "title": "ChestX-Reasoner: Advancing Radiology Foundation Models with Reasoning through Step-by-Step Verification", "categories": ["cs.AI", "cs.CL", "cs.CV"], "comment": null, "summary": "Recent advances in reasoning-enhanced large language models (LLMs) and\nmultimodal LLMs (MLLMs) have significantly improved performance in complex\ntasks, yet medical AI models often overlook the structured reasoning processes\ninherent in clinical practice. In this work, we present ChestX-Reasoner, a\nradiology diagnosis MLLM designed to leverage process supervision mined\ndirectly from clinical reports, reflecting the step-by-step reasoning followed\nby radiologists. We construct a large dataset by extracting and refining\nreasoning chains from routine radiology reports. Our two-stage training\nframework combines supervised fine-tuning and reinforcement learning guided by\nprocess rewards to better align model reasoning with clinical standards. We\nintroduce RadRBench-CXR, a comprehensive benchmark featuring 59K visual\nquestion answering samples with 301K clinically validated reasoning steps, and\npropose RadRScore, a metric evaluating reasoning factuality, completeness, and\neffectiveness. ChestX-Reasoner outperforms existing medical and general-domain\nMLLMs in both diagnostic accuracy and reasoning ability, achieving 16%, 5.9%,\nand 18% improvements in reasoning ability compared to the best medical MLLM,\nthe best general MLLM, and its base model, respectively, as well as 3.3%, 24%,\nand 27% improvements in outcome accuracy. All resources are open-sourced to\nfacilitate further research in medical reasoning MLLMs."}
{"id": "2504.20310", "pdf": "https://arxiv.org/pdf/2504.20310", "abs": "https://arxiv.org/abs/2504.20310", "authors": ["Greg Gluch", "Shafi Goldwasser"], "title": "A Cryptographic Perspective on Mitigation vs. Detection in Machine Learning", "categories": ["cs.LG", "cs.AI", "cs.CR"], "comment": "29 pages", "summary": "In this paper, we initiate a cryptographically inspired theoretical study of\ndetection versus mitigation of adversarial inputs produced by attackers of\nMachine Learning algorithms during inference time.\n  We formally define defense by detection (DbD) and defense by mitigation\n(DbM). Our definitions come in the form of a 3-round protocol between two\nresource-bounded parties: a trainer/defender and an attacker. The attacker aims\nto produce inference-time inputs that fool the training algorithm. We define\ncorrectness, completeness, and soundness properties to capture successful\ndefense at inference time while not degrading (too much) the performance of the\nalgorithm on inputs from the training distribution.\n  We first show that achieving DbD and achieving DbM are equivalent for ML\nclassification tasks. Surprisingly, this is not the case for ML generative\nlearning tasks, where there are many possible correct outputs that can be\ngenerated for each input. We show a separation between DbD and DbM by\nexhibiting a generative learning task for which is possible to defend by\nmitigation but is provably impossible to defend by detection under the\nassumption that the Identity-Based Fully Homomorphic Encryption (IB-FHE),\npublicly-verifiable zero-knowledge Succinct Non-Interactive Arguments of\nKnowledge (zk-SNARK) and Strongly Unforgeable Signatures exist. The mitigation\nphase uses significantly fewer samples than the initial training algorithm."}
{"id": "2504.20771", "pdf": "https://arxiv.org/pdf/2504.20771", "abs": "https://arxiv.org/abs/2504.20771", "authors": ["Haitao Wu", "Zongbo Han", "Huaxi Huang", "Changqing Zhang"], "title": "Turing Machine Evaluation for Large Language Model", "categories": ["cs.CL"], "comment": null, "summary": "With the rapid development and widespread application of Large Language\nModels (LLMs), rigorous evaluation has become particularly crucial. This\nresearch adopts a novel perspective, focusing on evaluating the core\ncomputational reasoning ability of LLMs, defined as the capacity of model to\naccurately understand rules, and execute logically computing operations. This\ncapability assesses the reliability of LLMs as precise executors, and is\ncritical to advanced tasks such as complex code generation and multi-step\nproblem-solving. We propose an evaluation framework based on Universal Turing\nMachine (UTM) simulation. This framework requires LLMs to strictly follow\ninstructions and track dynamic states, such as tape content and read/write head\nposition, during multi-step computations. To enable standardized evaluation, we\ndeveloped TMBench, a benchmark for systematically studying the computational\nreasoning capabilities of LLMs. TMBench provides several key advantages,\nincluding knowledge-agnostic evaluation, adjustable difficulty, foundational\ncoverage through Turing machine encoding, and unlimited capacity for instance\ngeneration, ensuring scalability as models continue to evolve. We find that\nmodel performance on TMBench correlates strongly with performance on other\nrecognized reasoning benchmarks (Pearson correlation coefficient is 0.73),\nclearly demonstrating that computational reasoning is a significant dimension\nfor measuring the deep capabilities of LLMs. Code and data are available at\nhttps://github.com/HaitaoWuTJU/Turing-Machine-Bench."}
{"id": "2504.20498", "pdf": "https://arxiv.org/pdf/2504.20498", "abs": "https://arxiv.org/abs/2504.20498", "authors": ["Jianhong Han", "Yupei Wang", "Liang Chen"], "title": "Style-Adaptive Detection Transformer for Single-Source Domain Generalized Object Detection", "categories": ["cs.CV"], "comment": "Manuscript submitted to IEEE Transactions on Multimedia", "summary": "Single-source Domain Generalization (SDG) in object detection aims to develop\na detector using only data from a source domain that can exhibit strong\ngeneralization capability when applied to unseen target domains. Existing\nmethods are built upon CNN-based detectors and primarily improve robustness by\nemploying carefully designed data augmentation strategies integrated with\nfeature alignment techniques. However, data augmentation methods have inherent\ndrawbacks; they are only effective when the augmented sample distribution\napproximates or covers the unseen scenarios, thus failing to enhance\ngeneralization across all unseen domains. Furthermore, while the recent\nDetection Transformer (DETR) has demonstrated superior generalization\ncapability in domain adaptation tasks due to its efficient global information\nextraction, its potential in SDG tasks remains unexplored. To this end, we\nintroduce a strong DETR-based detector named the Style-Adaptive Detection\nTransformer (SA-DETR) for SDG in object detection. Specifically, we present a\ndomain style adapter that projects the style representation of the unseen\ntarget domain into the training domain, enabling dynamic style adaptation.\nThen, we propose an object-aware contrastive learning module to guide the\ndetector in extracting domain-invariant features through contrastive learning.\nBy using object-aware gating masks to constrain feature aggregation in both\nspatial and semantic dimensions, this module achieves cross-domain contrast of\ninstance-level features, thereby enhancing generalization. Extensive\nexperiments demonstrate the superior performance and generalization capability\nof SA-DETR across five different weather scenarios. Code is released at\nhttps://github.com/h751410234/SA-DETR."}
{"id": "2504.20980", "pdf": "https://arxiv.org/pdf/2504.20980", "abs": "https://arxiv.org/abs/2504.20980", "authors": ["Neil F. Johnson", "Frank Yingjie Huo"], "title": "Jekyll-and-Hyde Tipping Point in an AI's Behavior", "categories": ["cs.AI", "cs.CY", "nlin.AO", "physics.comp-ph", "physics.soc-ph"], "comment": null, "summary": "Trust in AI is undermined by the fact that there is no science that predicts\n-- or that can explain to the public -- when an LLM's output (e.g. ChatGPT) is\nlikely to tip mid-response to become wrong, misleading, irrelevant or\ndangerous. With deaths and trauma already being blamed on LLMs, this\nuncertainty is even pushing people to treat their 'pet' LLM more politely to\n'dissuade' it (or its future Artificial General Intelligence offspring) from\nsuddenly turning on them. Here we address this acute need by deriving from\nfirst principles an exact formula for when a Jekyll-and-Hyde tipping point\noccurs at LLMs' most basic level. Requiring only secondary school mathematics,\nit shows the cause to be the AI's attention spreading so thin it suddenly\nsnaps. This exact formula provides quantitative predictions for how the\ntipping-point can be delayed or prevented by changing the prompt and the AI's\ntraining. Tailored generalizations will provide policymakers and the public\nwith a firm platform for discussing any of AI's broader uses and risks, e.g. as\na personal counselor, medical advisor, decision-maker for when to use force in\na conflict situation. It also meets the need for clear and transparent answers\nto questions like ''should I be polite to my LLM?''"}
{"id": "2504.20314", "pdf": "https://arxiv.org/pdf/2504.20314", "abs": "https://arxiv.org/abs/2504.20314", "authors": ["Qitao Tan", "Sung-En Chang", "Rui Xia", "Huidong Ji", "Chence Yang", "Ci Zhang", "Jun Liu", "Zheng Zhan", "Zhou Zou", "Yanzhi Wang", "Jin Lu", "Geng Yuan"], "title": "Perturbation-efficient Zeroth-order Optimization for Hardware-friendly On-device Training", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Zeroth-order (ZO) optimization is an emerging deep neural network (DNN)\ntraining paradigm that offers computational simplicity and memory savings.\nHowever, this seemingly promising approach faces a significant and long-ignored\nchallenge. ZO requires generating a substantial number of Gaussian random\nnumbers, which poses significant difficulties and even makes it infeasible for\nhardware platforms, such as FPGAs and ASICs. In this paper, we identify this\ncritical issue, which arises from the mismatch between algorithm and hardware\ndesigners. To address this issue, we proposed PeZO, a perturbation-efficient ZO\nframework. Specifically, we design random number reuse strategies to\nsignificantly reduce the demand for random number generation and introduce a\nhardware-friendly adaptive scaling method to replace the costly Gaussian\ndistribution with a uniform distribution. Our experiments show that PeZO\nreduces the required LUTs and FFs for random number generation by 48.6\\% and\n12.7\\%, and saves at maximum 86\\% power consumption, all without compromising\ntraining performance, making ZO optimization feasible for on-device training.\nTo the best of our knowledge, we are the first to explore the potential of\non-device ZO optimization, providing valuable insights for future research."}
{"id": "2504.20839", "pdf": "https://arxiv.org/pdf/2504.20839", "abs": "https://arxiv.org/abs/2504.20839", "authors": ["D. -F. Qin"], "title": "Universal language model with the intervention of quantum theory", "categories": ["cs.CL", "quant-ph"], "comment": null, "summary": "This paper examines language modeling based on the theory of quantum\nmechanics. It focuses on the introduction of quantum mechanics into the\nsymbol-meaning pairs of language in order to build a representation model of\nnatural language. At the same time, it is realized that word embedding, which\nis widely used as a basic technique for statistical language modeling, can be\nexplained and improved by the mathematical framework of quantum mechanics. On\nthis basis, this paper continues to try to use quantum statistics and other\nrelated theories to study the mathematical representation, natural evolution\nand statistical properties of natural language. It is also assumed that the\nsource of such quantum properties is the physicality of information. The\nfeasibility of using quantum theory to model natural language is pointed out\nthrough the construction of a experimental code. The paper discusses, in terms\nof applications, the possible help of the theory in constructing generative\nmodels that are popular nowadays. A preliminary discussion of future\napplications of the theory to quantum computers is also presented."}
{"id": "2504.20509", "pdf": "https://arxiv.org/pdf/2504.20509", "abs": "https://arxiv.org/abs/2504.20509", "authors": ["Yichu Xu", "Di Wang", "Hongzan Jiao", "Lefei Zhang", "Liangpei Zhang"], "title": "MambaMoE: Mixture-of-Spectral-Spatial-Experts State Space Model for Hyperspectral Image Classification", "categories": ["cs.CV"], "comment": null, "summary": "The Mamba model has recently demonstrated strong potential in hyperspectral\nimage (HSI) classification, owing to its ability to perform context modeling\nwith linear computational complexity. However, existing Mamba-based methods\nusually neglect the spectral and spatial directional characteristics related to\nheterogeneous objects in hyperspectral scenes, leading to limited\nclassification performance. To address these issues, we propose MambaMoE, a\nnovel spectral-spatial mixture-of-experts framework, representing the first\nMoE-based approach in the HSI classification community. Specifically, we design\na Mixture of Mamba Expert Block (MoMEB) that leverages sparse expert activation\nto enable adaptive spectral-spatial modeling. Furthermore, we introduce an\nuncertainty-guided corrective learning (UGCL) strategy to encourage the model's\nattention toward complex regions prone to prediction ambiguity. Extensive\nexperiments on multiple public HSI benchmarks demonstrate that MambaMoE\nachieves state-of-the-art performance in both accuracy and efficiency compared\nto existing advanced approaches, especially for Mamba-based methods. Code will\nbe released."}
{"id": "2504.20983", "pdf": "https://arxiv.org/pdf/2504.20983", "abs": "https://arxiv.org/abs/2504.20983", "authors": ["Giuseppe De Giacomo", "Gianmarco Parretti", "Shufang Zhu"], "title": "LTLf Adaptive Synthesis for Multi-Tier Goals in Nondeterministic Domains", "categories": ["cs.AI"], "comment": null, "summary": "We study a variant of LTLf synthesis that synthesizes adaptive strategies for\nachieving a multi-tier goal, consisting of multiple increasingly challenging\nLTLf objectives in nondeterministic planning domains. Adaptive strategies are\nstrategies that at any point of their execution (i) enforce the satisfaction of\nas many objectives as possible in the multi-tier goal, and (ii) exploit\npossible cooperation from the environment to satisfy as many as possible of the\nremaining ones. This happens dynamically: if the environment cooperates (ii)\nand an objective becomes enforceable (i), then our strategies will enforce it.\nWe provide a game-theoretic technique to compute adaptive strategies that is\nsound and complete. Notably, our technique is polynomial, in fact quadratic, in\nthe number of objectives. In other words, it handles multi-tier goals with only\na minor overhead compared to standard LTLf synthesis."}
{"id": "2504.20319", "pdf": "https://arxiv.org/pdf/2504.20319", "abs": "https://arxiv.org/abs/2504.20319", "authors": ["Huchen Yang", "Xinghao Dong", "Jin-Long Wu"], "title": "Bayesian Experimental Design for Model Discrepancy Calibration: An Auto-Differentiable Ensemble Kalman Inversion Approach", "categories": ["cs.LG"], "comment": null, "summary": "Bayesian experimental design (BED) offers a principled framework for\noptimizing data acquisition by leveraging probabilistic inference. However,\npractical implementations of BED are often compromised by model discrepancy,\ni.e., the mismatch between predictive models and true physical systems, which\ncan potentially lead to biased parameter estimates. While data-driven\napproaches have been recently explored to characterize the model discrepancy,\nthe resulting high-dimensional parameter space poses severe challenges for both\nBayesian updating and design optimization. In this work, we propose a hybrid\nBED framework enabled by auto-differentiable ensemble Kalman inversion (AD-EKI)\nthat addresses these challenges by providing a computationally efficient,\ngradient-free alternative to estimate the information gain for high-dimensional\nnetwork parameters. The AD-EKI allows a differentiable evaluation of the\nutility function in BED and thus facilitates the use of standard gradient-based\nmethods for design optimization. In the proposed hybrid framework, we\niteratively optimize experimental designs, decoupling the inference of\nlow-dimensional physical parameters handled by standard BED methods, from the\nhigh-dimensional model discrepancy handled by AD-EKI. The identified optimal\ndesigns for the model discrepancy enable us to systematically collect\ninformative data for its calibration. The performance of the proposed method is\nstudied by a classical convection-diffusion BED example, and the hybrid\nframework enabled by AD-EKI efficiently identifies informative data to\ncalibrate the model discrepancy and robustly infers the unknown physical\nparameters in the modeled system. Besides addressing the challenges of BED with\nmodel discrepancy, AD-EKI also potentially fosters efficient and scalable\nframeworks in many other areas with bilevel optimization, such as meta-learning\nand structure optimization."}
{"id": "2504.20849", "pdf": "https://arxiv.org/pdf/2504.20849", "abs": "https://arxiv.org/abs/2504.20849", "authors": ["Anum Afzal", "Alexandre Mercier", "Florian Matthes"], "title": "JaccDiv: A Metric and Benchmark for Quantifying Diversity of Generated Marketing Text in the Music Industry", "categories": ["cs.CL"], "comment": null, "summary": "Online platforms are increasingly interested in using Data-to-Text\ntechnologies to generate content and help their users. Unfortunately,\ntraditional generative methods often fall into repetitive patterns, resulting\nin monotonous galleries of texts after only a few iterations. In this paper, we\ninvestigate LLM-based data-to-text approaches to automatically generate\nmarketing texts that are of sufficient quality and diverse enough for broad\nadoption. We leverage Language Models such as T5, GPT-3.5, GPT-4, and LLaMa2 in\nconjunction with fine-tuning, few-shot, and zero-shot approaches to set a\nbaseline for diverse marketing texts. We also introduce a metric JaccDiv to\nevaluate the diversity of a set of texts. This research extends its relevance\nbeyond the music industry, proving beneficial in various fields where\nrepetitive automated content generation is prevalent."}
{"id": "2504.20510", "pdf": "https://arxiv.org/pdf/2504.20510", "abs": "https://arxiv.org/abs/2504.20510", "authors": ["Irina Ruzavina", "Lisa Sophie Theis", "Jesse Lemeer", "Rutger de Groen", "Leo Ebeling", "Andrej Hulak", "Jouaria Ali", "Guangzhi Tang", "Rico Mockel"], "title": "SteelBlastQC: Shot-blasted Steel Surface Dataset with Interpretable Detection of Surface Defects", "categories": ["cs.CV", "cs.NE"], "comment": "Accepted by IJCNN 2025", "summary": "Automating the quality control of shot-blasted steel surfaces is crucial for\nimproving manufacturing efficiency and consistency. This study presents a\ndataset of 1654 labeled RGB images (512x512) of steel surfaces, classified as\neither \"ready for paint\" or \"needs shot-blasting.\" The dataset captures\nreal-world surface defects, including discoloration, welding lines, scratches\nand corrosion, making it well-suited for training computer vision models.\nAdditionally, three classification approaches were evaluated: Compact\nConvolutional Transformers (CCT), Support Vector Machines (SVM) with ResNet-50\nfeature extraction, and a Convolutional Autoencoder (CAE). The supervised\nmethods (CCT and SVM) achieve 95% classification accuracy on the test set, with\nCCT leveraging transformer-based attention mechanisms and SVM offering a\ncomputationally efficient alternative. The CAE approach, while less effective,\nestablishes a baseline for unsupervised quality control. We present\ninterpretable decision-making by all three neural networks, allowing industry\nusers to visually pinpoint problematic regions and understand the model's\nrationale. By releasing the dataset and baseline codes, this work aims to\nsupport further research in defect detection, advance the development of\ninterpretable computer vision models for quality control, and encourage the\nadoption of automated inspection systems in industrial applications."}
{"id": "2504.20047", "pdf": "https://arxiv.org/pdf/2504.20047", "abs": "https://arxiv.org/abs/2504.20047", "authors": ["Mohammad S. Ahmad", "Zan A. Naeem", "Michaël Aupetit", "Ahmed Elmagarmid", "Mohamed Eltabakh", "Xiasong Ma", "Mourad Ouzzani", "Chaoyi Ruan"], "title": "HCT-QA: A Benchmark for Question Answering on Human-Centric Tables", "categories": ["cs.IR", "cs.AI", "cs.DB"], "comment": "12 pages", "summary": "Tabular data embedded within PDF files, web pages, and other document formats\nare prevalent across numerous sectors such as government, engineering, science,\nand business. These human-centric tables (HCTs) possess a unique combination of\nhigh business value, intricate layouts, limited operational power at scale, and\nsometimes serve as the only data source for critical insights. However, their\ncomplexity poses significant challenges to traditional data extraction,\nprocessing, and querying methods. While current solutions focus on transforming\nthese tables into relational formats for SQL queries, they fall short in\nhandling the diverse and complex layouts of HCTs and hence being amenable to\nquerying. This paper describes HCT-QA, an extensive benchmark of HCTs, natural\nlanguage queries, and related answers on thousands of tables. Our dataset\nincludes 2,188 real-world HCTs with 9,835 QA pairs and 4,679 synthetic tables\nwith 67.5K QA pairs. While HCTs can be potentially processed by different type\nof query engines, in this paper, we focus on Large Language Models as potential\nengines and assess their ability in processing and querying such tables."}
{"id": "2504.20375", "pdf": "https://arxiv.org/pdf/2504.20375", "abs": "https://arxiv.org/abs/2504.20375", "authors": ["Ellis R. Crabtree", "Dimitris G. Giovanis", "Nikolaos Evangelou", "Juan M. Bello-Rivas", "Ioannis G. Kevrekidis"], "title": "Generative Learning for Slow Manifolds and Bifurcation Diagrams", "categories": ["cs.LG", "math.DS", "37M20, 37M21, 68T07, 35B32"], "comment": "17 pages, 13 figures, 1 table", "summary": "In dynamical systems characterized by separation of time scales, the\napproximation of so called ``slow manifolds'', on which the long term dynamics\nlie, is a useful step for model reduction. Initializing on such slow manifolds\nis a useful step in modeling, since it circumvents fast transients, and is\ncrucial in multiscale algorithms alternating between fine scale (fast) and\ncoarser scale (slow) simulations. In a similar spirit, when one studies the\ninfinite time dynamics of systems depending on parameters, the system\nattractors (e.g., its steady states) lie on bifurcation diagrams. Sampling\nthese manifolds gives us representative attractors (here, steady states of ODEs\nor PDEs) at different parameter values. Algorithms for the systematic\nconstruction of these manifolds are required parts of the ``traditional''\nnumerical nonlinear dynamics toolkit.\n  In more recent years, as the field of Machine Learning develops, conditional\nscore-based generative models (cSGMs) have demonstrated capabilities in\ngenerating plausible data from target distributions that are conditioned on\nsome given label. It is tempting to exploit such generative models to produce\nsamples of data distributions conditioned on some quantity of interest (QoI).\nIn this work, we present a framework for using cSGMs to quickly (a) initialize\non a low-dimensional (reduced-order) slow manifold of a multi-time-scale system\nconsistent with desired value(s) of a QoI (a ``label'') on the manifold, and\n(b) approximate steady states in a bifurcation diagram consistent with a (new,\nout-of-sample) parameter value. This conditional sampling can help uncover the\ngeometry of the reduced slow-manifold and/or approximately ``fill in'' missing\nsegments of steady states in a bifurcation diagram."}
{"id": "2504.20922", "pdf": "https://arxiv.org/pdf/2504.20922", "abs": "https://arxiv.org/abs/2504.20922", "authors": ["Miguel Nogales", "Matteo Gambella", "Manuel Roveri"], "title": "DYNAMAX: Dynamic computing for Transformers and Mamba based architectures", "categories": ["cs.CL", "cs.AI", "cs.LG", "68T50 (Primary), 68T07 (Secondary)"], "comment": "Accepted to IJCNN 2025", "summary": "Early exits (EEs) offer a promising approach to reducing computational costs\nand latency by dynamically terminating inference once a satisfactory prediction\nconfidence on a data sample is achieved. Although many works integrate EEs into\nencoder-only Transformers, their application to decoder-only architectures and,\nmore importantly, Mamba models, a novel family of state-space architectures in\nthe LLM realm, remains insufficiently explored. This work introduces DYNAMAX,\nthe first framework to exploit the unique properties of Mamba architectures for\nearly exit mechanisms. We not only integrate EEs into Mamba but also repurpose\nMamba as an efficient EE classifier for both Mamba-based and transformer-based\nLLMs, showcasing its versatility. Our experiments employ the Mistral 7B\ntransformer compared to the Codestral 7B Mamba model, using data sets such as\nTruthfulQA, CoQA, and TriviaQA to evaluate computational savings, accuracy, and\nconsistency. The results highlight the adaptability of Mamba as a powerful EE\nclassifier and its efficiency in balancing computational cost and performance\nquality across NLP tasks. By leveraging Mamba's inherent design for dynamic\nprocessing, we open pathways for scalable and efficient inference in embedded\napplications and resource-constrained environments. This study underscores the\ntransformative potential of Mamba in redefining dynamic computing paradigms for\nLLMs."}
{"id": "2504.20518", "pdf": "https://arxiv.org/pdf/2504.20518", "abs": "https://arxiv.org/abs/2504.20518", "authors": ["Zhongqi Wang", "Jie Zhang", "Shiguang Shan", "Xilin Chen"], "title": "Dynamic Attention Analysis for Backdoor Detection in Text-to-Image Diffusion Models", "categories": ["cs.CV"], "comment": null, "summary": "Recent studies have revealed that text-to-image diffusion models are\nvulnerable to backdoor attacks, where attackers implant stealthy textual\ntriggers to manipulate model outputs. Previous backdoor detection methods\nprimarily focus on the static features of backdoor samples. However, a vital\nproperty of diffusion models is their inherent dynamism. This study introduces\na novel backdoor detection perspective named Dynamic Attention Analysis (DAA),\nshowing that these dynamic characteristics serve as better indicators for\nbackdoor detection. Specifically, by examining the dynamic evolution of\ncross-attention maps, we observe that backdoor samples exhibit distinct feature\nevolution patterns at the $<$EOS$>$ token compared to benign samples. To\nquantify these dynamic anomalies, we first introduce DAA-I, which treats the\ntokens' attention maps as spatially independent and measures dynamic feature\nusing the Frobenius norm. Furthermore, to better capture the interactions\nbetween attention maps and refine the feature, we propose a dynamical\nsystem-based approach, referred to as DAA-S. This model formulates the spatial\ncorrelations among attention maps using a graph-based state equation and we\ntheoretically analyze the global asymptotic stability of this method. Extensive\nexperiments across five representative backdoor attack scenarios demonstrate\nthat our approach significantly surpasses existing detection methods, achieving\nan average F1 Score of 79.49% and an AUC of 87.67%. The code is available at\nhttps://github.com/Robin-WZQ/DAA."}
{"id": "2504.20059", "pdf": "https://arxiv.org/pdf/2504.20059", "abs": "https://arxiv.org/abs/2504.20059", "authors": ["Joey Chan", "Qiao Jin", "Nicholas Wan", "Charalampos S. Floudas", "Elisabetta Xue", "Zhiyong Lu"], "title": "Recommending Clinical Trials for Online Patient Cases using Artificial Intelligence", "categories": ["cs.IR", "cs.AI", "cs.CL"], "comment": "10 pages with 2 figures and 2 tables", "summary": "Clinical trials are crucial for assessing new treatments; however,\nrecruitment challenges - such as limited awareness, complex eligibility\ncriteria, and referral barriers - hinder their success. With the growth of\nonline platforms, patients increasingly turn to social media and health\ncommunities for support, research, and advocacy, expanding recruitment pools\nand established enrollment pathways. Recognizing this potential, we utilized\nTrialGPT, a framework that leverages a large language model (LLM) as its\nbackbone, to match 50 online patient cases (collected from published case\nreports and a social media website) to clinical trials and evaluate performance\nagainst traditional keyword-based searches. Our results show that TrialGPT\noutperforms traditional methods by 46% in identifying eligible trials, with\neach patient, on average, being eligible for around 7 trials. Additionally, our\noutreach efforts to case authors and trial organizers regarding these\npatient-trial matches yielded highly positive feedback, which we present from\nboth perspectives."}
{"id": "2504.20390", "pdf": "https://arxiv.org/pdf/2504.20390", "abs": "https://arxiv.org/abs/2504.20390", "authors": ["Fangfang Li", "Quanxue Gao"], "title": "Manifold Clustering with Schatten p-norm Maximization", "categories": ["cs.LG"], "comment": null, "summary": "Manifold clustering, with its exceptional ability to capture complex data\nstructures, holds a pivotal position in cluster analysis. However, existing\nmethods often focus only on finding the optimal combination between K-means and\nmanifold learning, and overlooking the consistency between the data structure\nand labels. To address this issue, we deeply explore the relationship between\nK-means and manifold learning, and on this basis, fuse them to develop a new\nclustering framework. Specifically, the algorithm uses labels to guide the\nmanifold structure and perform clustering on it, which ensures the consistency\nbetween the data structure and labels. Furthermore, in order to naturally\nmaintain the class balance in the clustering process, we maximize the Schatten\np-norm of labels, and provide a theoretical proof to support this.\nAdditionally, our clustering framework is designed to be flexible and\ncompatible with many types of distance functions, which facilitates efficient\nprocessing of nonlinear separable data. The experimental results of several\ndatabases confirm the superiority of our proposed model."}
{"id": "2504.20946", "pdf": "https://arxiv.org/pdf/2504.20946", "abs": "https://arxiv.org/abs/2504.20946", "authors": ["Tyler McDonald", "Ali Emami"], "title": "Trace-of-Thought: Enhanced Arithmetic Problem Solving via Reasoning Distillation From Large to Small Language Models", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "As Large Language Models (LLMs) continue to be leveraged for daily tasks,\nprompt engineering remains an active field of contribution within computational\nlinguistics, particularly in domains requiring specialized knowledge such as\narithmetic reasoning. While these LLMs are optimized for a variety of tasks,\ntheir exhaustive employment may become computationally or financially\ncumbersome for small teams. Additionally, complete reliance on proprietary,\nclosed-source models often limits customization and adaptability, posing\nsignificant challenges in research and application scalability. Instead, by\nleveraging open-source models at or below 7 billion parameters, we can optimize\nour resource usage while still observing remarkable gains over standard\nprompting approaches. To cultivate this notion, we introduce Trace-of-Thought\nPrompting, a simple, zero-shot prompt engineering method that instructs LLMs to\ncreate observable subproblems using critical problem-solving, specifically\ndesigned to enhance arithmetic reasoning capabilities. When applied to\nopen-source models in tandem with GPT-4, we observe that Trace-of-Thought not\nonly allows novel insight into the problem-solving process but also introduces\nperformance gains as large as 125% on language models at or below 7 billion\nparameters. This approach underscores the potential of open-source initiatives\nin democratizing AI research and improving the accessibility of high-quality\ncomputational linguistics applications."}
{"id": "2504.20525", "pdf": "https://arxiv.org/pdf/2504.20525", "abs": "https://arxiv.org/abs/2504.20525", "authors": ["Huan Zheng", "Wencheng Han", "Tianyi Yan", "Cheng-zhong Xu", "Jianbing Shen"], "title": "Geometry-aware Temporal Aggregation Network for Monocular 3D Lane Detection", "categories": ["cs.CV"], "comment": null, "summary": "Monocular 3D lane detection aims to estimate 3D position of lanes from\nfrontal-view (FV) images. However, current monocular 3D lane detection methods\nsuffer from two limitations, including inaccurate geometric information of the\npredicted 3D lanes and difficulties in maintaining lane integrity. To address\nthese issues, we seek to fully exploit the potential of multiple input frames.\nFirst, we aim at enhancing the ability to perceive the geometry of scenes by\nleveraging temporal geometric consistency. Second, we strive to improve the\nintegrity of lanes by revealing more instance information from temporal\nsequences. Therefore, we propose a novel Geometry-aware Temporal Aggregation\nNetwork (GTA-Net) for monocular 3D lane detection. On one hand, we develop the\nTemporal Geometry Enhancement Module (TGEM), which exploits geometric\nconsistency across successive frames, facilitating effective geometry\nperception. On the other hand, we present the Temporal Instance-aware Query\nGeneration (TIQG), which strategically incorporates temporal cues into query\ngeneration, thereby enabling the exploration of comprehensive instance\ninformation. Experiments demonstrate that our GTA-Net achieves SoTA results,\nsurpassing existing monocular 3D lane detection solutions."}
{"id": "2504.20074", "pdf": "https://arxiv.org/pdf/2504.20074", "abs": "https://arxiv.org/abs/2504.20074", "authors": ["Khurram Khalil", "Khaza Anuarul Hoque"], "title": "EPSILON: Adaptive Fault Mitigation in Approximate Deep Neural Network using Statistical Signatures", "categories": ["cs.DC", "cs.AI", "cs.LG"], "comment": "Accepted at the International Joint Conference on Neural Networks\n  (IJCNN), June 30th - July 5th, 2025 in Rome, Italy", "summary": "The increasing adoption of approximate computing in deep neural network\naccelerators (AxDNNs) promises significant energy efficiency gains. However,\npermanent faults in AxDNNs can severely degrade their performance compared to\ntheir accurate counterparts (AccDNNs). Traditional fault detection and\nmitigation approaches, while effective for AccDNNs, introduce substantial\noverhead and latency, making them impractical for energy-constrained real-time\ndeployment. To address this, we introduce EPSILON, a lightweight framework that\nleverages pre-computed statistical signatures and layer-wise importance metrics\nfor efficient fault detection and mitigation in AxDNNs. Our framework\nintroduces a novel non-parametric pattern-matching algorithm that enables\nconstant-time fault detection without interrupting normal execution while\ndynamically adapting to different network architectures and fault patterns.\nEPSILON maintains model accuracy by intelligently adjusting mitigation\nstrategies based on a statistical analysis of weight distribution and layer\ncriticality while preserving the energy benefits of approximate computing.\nExtensive evaluations across various approximate multipliers, AxDNN\narchitectures, popular datasets (MNIST, CIFAR-10, CIFAR-100, ImageNet-1k), and\nfault scenarios demonstrate that EPSILON maintains 80.05\\% accuracy while\noffering 22\\% improvement in inference time and 28\\% improvement in energy\nefficiency, establishing EPSILON as a practical solution for deploying reliable\nAxDNNs in safety-critical edge applications."}
{"id": "2504.20408", "pdf": "https://arxiv.org/pdf/2504.20408", "abs": "https://arxiv.org/abs/2504.20408", "authors": ["Jae Yong Lee", "Gwang Jae Jung", "Byung Chan Lim", "Hyung Ju Hwang"], "title": "FourierSpecNet: Neural Collision Operator Approximation Inspired by the Fourier Spectral Method for Solving the Boltzmann Equation", "categories": ["cs.LG", "cs.AI", "cs.NA", "math.NA", "physics.comp-ph", "68T20, 35Q20, 35B40, 82C40"], "comment": "27 pages, 11 figures", "summary": "The Boltzmann equation, a fundamental model in kinetic theory, describes the\nevolution of particle distribution functions through a nonlinear,\nhigh-dimensional collision operator. However, its numerical solution remains\ncomputationally demanding, particularly for inelastic collisions and\nhigh-dimensional velocity domains. In this work, we propose the Fourier Neural\nSpectral Network (FourierSpecNet), a hybrid framework that integrates the\nFourier spectral method with deep learning to approximate the collision\noperator in Fourier space efficiently. FourierSpecNet achieves\nresolution-invariant learning and supports zero-shot super-resolution, enabling\naccurate predictions at unseen resolutions without retraining. Beyond empirical\nvalidation, we establish a consistency result showing that the trained operator\nconverges to the spectral solution as the discretization is refined. We\nevaluate our method on several benchmark cases, including Maxwellian and\nhard-sphere molecular models, as well as inelastic collision scenarios. The\nresults demonstrate that FourierSpecNet offers competitive accuracy while\nsignificantly reducing computational cost compared to traditional spectral\nsolvers. Our approach provides a robust and scalable alternative for solving\nthe Boltzmann equation across both elastic and inelastic regimes."}
{"id": "2504.20951", "pdf": "https://arxiv.org/pdf/2504.20951", "abs": "https://arxiv.org/abs/2504.20951", "authors": ["Maryna Vyshnyvetska"], "title": "Information Gravity: A Field-Theoretic Model for Token Selection in Large Language Models", "categories": ["cs.CL"], "comment": "12 pages, 1 figure", "summary": "We propose a theoretical model called \"information gravity\" to describe the\ntext generation process in large language models (LLMs). The model uses\nphysical apparatus from field theory and spacetime geometry to formalize the\ninteraction between user queries and the probability distribution of generated\ntokens. A query is viewed as an object with \"information mass\" that curves the\nsemantic space of the model, creating gravitational potential wells that\n\"attract\" tokens during generation. This model offers a mechanism to explain\nseveral observed phenomena in LLM behavior, including hallucinations (emerging\nfrom low-density semantic voids), sensitivity to query formulation (due to\nsemantic field curvature changes), and the influence of sampling temperature on\noutput diversity."}
{"id": "2504.20530", "pdf": "https://arxiv.org/pdf/2504.20530", "abs": "https://arxiv.org/abs/2504.20530", "authors": ["Wenxuan Liu", "Xian Zhong", "Zhuo Zhou", "Siyuan Yang", "Chia-Wen Lin", "Alex Chichung Kot"], "title": "Beyond the Horizon: Decoupling UAVs Multi-View Action Recognition via Partial Order Transfer", "categories": ["cs.CV"], "comment": "11 pages", "summary": "Action recognition in unmanned aerial vehicles (UAVs) poses unique challenges\ndue to significant view variations along the vertical spatial axis. Unlike\ntraditional ground-based settings, UAVs capture actions from a wide range of\naltitudes, resulting in considerable appearance discrepancies. We introduce a\nmulti-view formulation tailored to varying UAV altitudes and empirically\nobserve a partial order among views, where recognition accuracy consistently\ndecreases as the altitude increases. This motivates a novel approach that\nexplicitly models the hierarchical structure of UAV views to improve\nrecognition performance across altitudes. To this end, we propose the Partial\nOrder Guided Multi-View Network (POG-MVNet), designed to address drastic view\nvariations by effectively leveraging view-dependent information across\ndifferent altitude levels. The framework comprises three key components: a View\nPartition (VP) module, which uses the head-to-body ratio to group views by\naltitude; an Order-aware Feature Decoupling (OFD) module, which disentangles\naction-relevant and view-specific features under partial order guidance; and an\nAction Partial Order Guide (APOG), which leverages the partial order to\ntransfer informative knowledge from easier views to support learning in more\nchallenging ones. We conduct experiments on Drone-Action, MOD20, and UAV\ndatasets, demonstrating that POG-MVNet significantly outperforms competing\nmethods. For example, POG-MVNet achieves a 4.7% improvement on Drone-Action\ndataset and a 3.5% improvement on UAV dataset compared to state-of-the-art\nmethods ASAT and FAR. The code for POG-MVNet will be made available soon."}
{"id": "2504.20083", "pdf": "https://arxiv.org/pdf/2504.20083", "abs": "https://arxiv.org/abs/2504.20083", "authors": ["Thuong Dang", "Qiqi Chen"], "title": "A model and package for German ColBERT", "categories": ["cs.IR", "cs.AI"], "comment": null, "summary": "In this work, we introduce a German version for ColBERT, a late interaction\nmulti-dense vector retrieval method, with a focus on RAG applications. We also\npresent the main features of our package for ColBERT models, supporting both\nretrieval and fine-tuning workflows."}
{"id": "2504.20411", "pdf": "https://arxiv.org/pdf/2504.20411", "abs": "https://arxiv.org/abs/2504.20411", "authors": ["Amartya Mukherjee", "Ruizhi Deng", "He Zhao", "Yuzhen Mao", "Leonid Sigal", "Frederick Tung"], "title": "ADiff4TPP: Asynchronous Diffusion Models for Temporal Point Processes", "categories": ["cs.LG"], "comment": null, "summary": "This work introduces a novel approach to modeling temporal point processes\nusing diffusion models with an asynchronous noise schedule. At each step of the\ndiffusion process, the noise schedule injects noise of varying scales into\ndifferent parts of the data. With a careful design of the noise schedules,\nearlier events are generated faster than later ones, thus providing stronger\nconditioning for forecasting the more distant future. We derive an objective to\neffectively train these models for a general family of noise schedules based on\nconditional flow matching. Our method models the joint distribution of the\nlatent representations of events in a sequence and achieves state-of-the-art\nresults in predicting both the next inter-event time and event type on\nbenchmark datasets. Additionally, it flexibly accommodates varying lengths of\nobservation and prediction windows in different forecasting settings by\nadjusting the starting and ending points of the generation process. Finally,\nour method shows superior performance in long-horizon prediction tasks,\noutperforming existing baseline methods."}
{"id": "2504.20964", "pdf": "https://arxiv.org/pdf/2504.20964", "abs": "https://arxiv.org/abs/2504.20964", "authors": ["Shangyu Li", "Juyong Jiang", "Tiancheng Zhao", "Jiasi Shen"], "title": "OSVBench: Benchmarking LLMs on Specification Generation Tasks for Operating System Verification", "categories": ["cs.CL", "cs.AI", "cs.OS", "cs.PL", "cs.SE"], "comment": null, "summary": "We introduce OSVBench, a new benchmark for evaluating Large Language Models\n(LLMs) in generating complete specification code pertaining to operating system\nkernel verification tasks. The benchmark first defines the specification\ngeneration problem into a program synthesis problem within a confined scope of\nsyntax and semantics by providing LLMs with the programming model. The LLMs are\nrequired to understand the provided verification assumption and the potential\nsyntax and semantics space to search for, then generate the complete\nspecification for the potentially buggy operating system code implementation\nunder the guidance of the high-level functional description of the operating\nsystem. This benchmark is built upon a real-world operating system kernel,\nHyperkernel, and consists of 245 complex specification generation tasks in\ntotal, each is a long context task of about 20k-30k tokens. Our comprehensive\nevaluation of 12 LLMs exhibits the limited performance of the current LLMs on\nthe specification generation tasks for operating system verification.\nSignificant disparities in their performance on the benchmark highlight\ndifferences in their ability to handle long-context code generation tasks. The\nevaluation toolkit and benchmark are available at\nhttps://github.com/lishangyu-hkust/OSVBench."}
{"id": "2504.20541", "pdf": "https://arxiv.org/pdf/2504.20541", "abs": "https://arxiv.org/abs/2504.20541", "authors": ["Daniele Pannone", "Danilo Avola"], "title": "Autoencoder Models for Point Cloud Environmental Synthesis from WiFi Channel State Information: A Preliminary Study", "categories": ["cs.CV", "cs.LG"], "comment": null, "summary": "This paper introduces a deep learning framework for generating point clouds\nfrom WiFi Channel State Information data. We employ a two-stage autoencoder\napproach: a PointNet autoencoder with convolutional layers for point cloud\ngeneration, and a Convolutional Neural Network autoencoder to map CSI data to a\nmatching latent space. By aligning these latent spaces, our method enables\naccurate environmental point cloud reconstruction from WiFi data. Experimental\nresults validate the effectiveness of our approach, highlighting its potential\nfor wireless sensing and environmental mapping applications."}
{"id": "2504.20092", "pdf": "https://arxiv.org/pdf/2504.20092", "abs": "https://arxiv.org/abs/2504.20092", "authors": ["Ali Rostami"], "title": "An Integrated Framework for Contextual Personalized LLM-Based Food Recommendation", "categories": ["cs.IR", "cs.AI"], "comment": "Doctorate Thesis, University of California, Irvine 2024", "summary": "Personalized food recommendation systems (Food-RecSys) critically\nunderperform due to fragmented component understanding and the failure of\nconventional machine learning with vast, imbalanced food data. While Large\nLanguage Models (LLMs) offer promise, current generic Recommendation as\nLanguage Processing (RLP) strategies lack the necessary specialization for the\nfood domain's complexity. This thesis tackles these deficiencies by first\nidentifying and analyzing the essential components for effective Food-RecSys.\nWe introduce two key innovations: a multimedia food logging platform for rich\ncontextual data acquisition and the World Food Atlas, enabling unique\ngeolocation-based food analysis previously unavailable. Building on this\nfoundation, we pioneer the Food Recommendation as Language Processing (F-RLP)\nframework - a novel, integrated approach specifically architected for the food\ndomain. F-RLP leverages LLMs in a tailored manner, overcoming the limitations\nof generic models and providing a robust infrastructure for effective,\ncontextual, and truly personalized food recommendations."}
{"id": "2504.20421", "pdf": "https://arxiv.org/pdf/2504.20421", "abs": "https://arxiv.org/abs/2504.20421", "authors": ["Michael Ito", "Danai Koutra", "Jenna Wiens"], "title": "Understanding GNNs and Homophily in Dynamic Node Classification", "categories": ["cs.LG"], "comment": "AISTATS 2025; version with full appendix", "summary": "Homophily, as a measure, has been critical to increasing our understanding of\ngraph neural networks (GNNs). However, to date this measure has only been\nanalyzed in the context of static graphs. In our work, we explore homophily in\ndynamic settings. Focusing on graph convolutional networks (GCNs), we\ndemonstrate theoretically that in dynamic settings, current GCN discriminative\nperformance is characterized by the probability that a node's future label is\nthe same as its neighbors' current labels. Based on this insight, we propose\ndynamic homophily, a new measure of homophily that applies in the dynamic\nsetting. This new measure correlates with GNN discriminative performance and\nsheds light on how to potentially design more powerful GNNs for dynamic graphs.\nLeveraging a variety of dynamic node classification datasets, we demonstrate\nthat popular GNNs are not robust to low dynamic homophily. Going forward, our\nwork represents an important step towards understanding homophily and GNN\nperformance in dynamic node classification."}
{"id": "2504.20972", "pdf": "https://arxiv.org/pdf/2504.20972", "abs": "https://arxiv.org/abs/2504.20972", "authors": ["Yifan Wei", "Xiaoyan Yu", "Ran Song", "Hao Peng", "Angsheng Li"], "title": "SetKE: Knowledge Editing for Knowledge Elements Overlap", "categories": ["cs.CL"], "comment": "The CR version will be updated subsequently", "summary": "Large Language Models (LLMs) excel in tasks such as retrieval and question\nanswering but require updates to incorporate new knowledge and reduce\ninaccuracies and hallucinations. Traditional updating methods, like fine-tuning\nand incremental learning, face challenges such as overfitting and high\ncomputational costs. Knowledge Editing (KE) provides a promising alternative\nbut often overlooks the Knowledge Element Overlap (KEO) phenomenon, where\nmultiple triplets share common elements, leading to editing conflicts. We\nidentify the prevalence of KEO in existing KE datasets and show its significant\nimpact on current KE methods, causing performance degradation in handling such\ntriplets. To address this, we propose a new formulation, Knowledge Set Editing\n(KSE), and introduce SetKE, a method that edits sets of triplets\nsimultaneously. Experimental results demonstrate that SetKE outperforms\nexisting methods in KEO scenarios on mainstream LLMs. Additionally, we\nintroduce EditSet, a dataset containing KEO triplets, providing a comprehensive\nbenchmark."}
{"id": "2504.20599", "pdf": "https://arxiv.org/pdf/2504.20599", "abs": "https://arxiv.org/abs/2504.20599", "authors": ["Qiaochu Wang", "Chufeng Xiao", "Manfred Lau", "Hongbo Fu"], "title": "PartHOI: Part-based Hand-Object Interaction Transfer via Generalized Cylinders", "categories": ["cs.CV"], "comment": "14 pages, 12 figures, this paper has been accepted by Computational\n  Visual Media Journal (CVMJ) but has not been published yet", "summary": "Learning-based methods to understand and model hand-object interactions (HOI)\nrequire a large amount of high-quality HOI data. One way to create HOI data is\nto transfer hand poses from a source object to another based on the objects'\ngeometry. However, current methods for transferring hand poses between objects\nrely on shape matching, limiting the ability to transfer poses across different\ncategories due to differences in their shapes and sizes. We observe that HOI\noften involves specific semantic parts of objects, which often have more\nconsistent shapes across categories. In addition, constructing size-invariant\ncorrespondences between these parts is important for cross-category transfer.\nBased on these insights, we introduce a novel method PartHOI for part-based HOI\ntransfer. Using a generalized cylinder representation to parameterize an object\nparts' geometry, PartHOI establishes a robust geometric correspondence between\nobject parts, and enables the transfer of contact points. Given the transferred\npoints, we optimize a hand pose to fit the target object well. Qualitative and\nquantitative results demonstrate that our method can generalize HOI transfers\nwell even for cross-category objects, and produce high-fidelity results that\nare superior to the existing methods."}
{"id": "2504.20093", "pdf": "https://arxiv.org/pdf/2504.20093", "abs": "https://arxiv.org/abs/2504.20093", "authors": ["Mohammad Baqar", "Rajat Khanda", "Saba Naqvi"], "title": "Self-Healing Software Systems: Lessons from Nature, Powered by AI", "categories": ["cs.SE", "cs.AI"], "comment": null, "summary": "As modern software systems grow in complexity and scale, their ability to\nautonomously detect, diagnose, and recover from failures becomes increasingly\nvital. Drawing inspiration from biological healing - where the human body\ndetects damage, signals the brain, and activates targeted recovery - this paper\nexplores the concept of self-healing software driven by artificial\nintelligence. We propose a novel framework that mimics this biological model\nsystem observability tools serve as sensory inputs, AI models function as the\ncognitive core for diagnosis and repair, and healing agents apply targeted code\nand test modifications. By combining log analysis, static code inspection, and\nAI-driven generation of patches or test updates, our approach aims to reduce\ndowntime, accelerate debugging, and enhance software resilience. We evaluate\nthe effectiveness of this model through case studies and simulations, comparing\nit against traditional manual debugging and recovery workflows. This work paves\nthe way toward intelligent, adaptive and self-reliant software systems capable\nof continuous healing, akin to living organisms."}
{"id": "2504.20430", "pdf": "https://arxiv.org/pdf/2504.20430", "abs": "https://arxiv.org/abs/2504.20430", "authors": ["Michael Ito", "Jiong Zhu", "Dexiong Chen", "Danai Koutra", "Jenna Wiens"], "title": "Learning Laplacian Positional Encodings for Heterophilous Graphs", "categories": ["cs.LG"], "comment": "AISTATS 2025; version with full appendix", "summary": "In this work, we theoretically demonstrate that current graph positional\nencodings (PEs) are not beneficial and could potentially hurt performance in\ntasks involving heterophilous graphs, where nodes that are close tend to have\ndifferent labels. This limitation is critical as many real-world networks\nexhibit heterophily, and even highly homophilous graphs can contain local\nregions of strong heterophily. To address this limitation, we propose Learnable\nLaplacian Positional Encodings (LLPE), a new PE that leverages the full\nspectrum of the graph Laplacian, enabling them to capture graph structure on\nboth homophilous and heterophilous graphs. Theoretically, we prove LLPE's\nability to approximate a general class of graph distances and demonstrate its\ngeneralization properties. Empirically, our evaluation on 12 benchmarks\ndemonstrates that LLPE improves accuracy across a variety of GNNs, including\ngraph transformers, by up to 35% and 14% on synthetic and real-world graphs,\nrespectively. Going forward, our work represents a significant step towards\ndeveloping PEs that effectively capture complex structures in heterophilous\ngraphs."}
{"id": "2504.20094", "pdf": "https://arxiv.org/pdf/2504.20094", "abs": "https://arxiv.org/abs/2504.20094", "authors": ["Zheng Hui", "Xiaokai Wei", "Yexi Jiang", "Kevin Gao", "Chen Wang", "Frank Ong", "Se-eun Yoon", "Rachit Pareek", "Michelle Gong"], "title": "MATCHA: Can Multi-Agent Collaboration Build a Trustworthy Conversational Recommender?", "categories": ["cs.IR", "cs.CL", "cs.HC"], "comment": null, "summary": "In this paper, we propose a multi-agent collaboration framework called MATCHA\nfor conversational recommendation system, leveraging large language models\n(LLMs) to enhance personalization and user engagement. Users can request\nrecommendations via free-form text and receive curated lists aligned with their\ninterests, preferences, and constraints. Our system introduces specialized\nagents for intent analysis, candidate generation, ranking, re-ranking,\nexplainability, and safeguards. These agents collaboratively improve\nrecommendations accuracy, diversity, and safety. On eight metrics, our model\nachieves superior or comparable performance to the current state-of-the-art.\nThrough comparisons with six baseline models, our approach addresses key\nchallenges in conversational recommendation systems for game recommendations,\nincluding: (1) handling complex, user-specific requests, (2) enhancing\npersonalization through multi-agent collaboration, (3) empirical evaluation and\ndeployment, and (4) ensuring safe and trustworthy interactions."}
{"id": "2504.20602", "pdf": "https://arxiv.org/pdf/2504.20602", "abs": "https://arxiv.org/abs/2504.20602", "authors": ["Siwei Wang", "Zhiwei Chen", "Liujuan Cao", "Rongrong Ji"], "title": "Purifying, Labeling, and Utilizing: A High-Quality Pipeline for Small Object Detection", "categories": ["cs.CV"], "comment": null, "summary": "Small object detection is a broadly investigated research task and is\ncommonly conceptualized as a \"pipeline-style\" engineering process. In the\nupstream, images serve as raw materials for processing in the detection\npipeline, where pre-trained models are employed to generate initial feature\nmaps. In the midstream, an assigner selects training positive and negative\nsamples. Subsequently, these samples and features are fed into the downstream\nfor classification and regression. Previous small object detection methods\noften focused on improving isolated stages of the pipeline, thereby neglecting\nholistic optimization and consequently constraining overall performance gains.\nTo address this issue, we have optimized three key aspects, namely Purifying,\nLabeling, and Utilizing, in this pipeline, proposing a high-quality Small\nobject detection framework termed PLUSNet. Specifically, PLUSNet comprises\nthree sequential components: the Hierarchical Feature Purifier (HFP) for\npurifying upstream features, the Multiple Criteria Label Assignment (MCLA) for\nimproving the quality of midstream training samples, and the Frequency\nDecoupled Head (FDHead) for more effectively exploiting information to\naccomplish downstream tasks. The proposed PLUS modules are readily integrable\ninto various object detectors, thus enhancing their detection capabilities in\nmulti-scale scenarios. Extensive experiments demonstrate the proposed PLUSNet\nconsistently achieves significant and consistent improvements across multiple\ndatasets for small object detection."}
{"id": "2504.20101", "pdf": "https://arxiv.org/pdf/2504.20101", "abs": "https://arxiv.org/abs/2504.20101", "authors": ["Fei Fang", "Yifan Hua", "Shengze Wang", "Ruilin Zhou", "Yi Liu", "Chen Qian", "Xiaoxue Zhang"], "title": "GenTorrent: Scaling Large Language Model Serving with An Overley Network", "categories": ["cs.DC", "cs.AI"], "comment": null, "summary": "While significant progress has been made in research and development on\nopen-source and cost-efficient large-language models (LLMs), serving\nscalability remains a critical challenge, particularly for small organizations\nand individuals seeking to deploy and test their LLM innovations. Inspired by\npeer-to-peer networks that leverage decentralized overlay nodes to increase\nthroughput and availability, we propose GenTorrent, an LLM serving overlay that\nharnesses computing resources from decentralized contributors. We identify four\nkey research problems inherent to enabling such a decentralized infrastructure:\n1) overlay network organization; 2) LLM communication privacy; 3) overlay\nforwarding for resource efficiency; and 4) verification of serving quality.\nThis work presents the first systematic study of these fundamental problems in\nthe context of decentralized LLM serving. Evaluation results from a prototype\nimplemented on a set of decentralized nodes demonstrate that GenTorrent\nachieves a latency reduction of over 50% compared to the baseline design\nwithout overlay forwarding. Furthermore, the security features introduce\nminimal overhead to serving latency and throughput. We believe this work\npioneers a new direction for democratizing and scaling future AI serving\ncapabilities."}
{"id": "2504.20437", "pdf": "https://arxiv.org/pdf/2504.20437", "abs": "https://arxiv.org/abs/2504.20437", "authors": ["DiJia Su", "Andrew Gu", "Jane Xu", "Yuandong Tian", "Jiawei Zhao"], "title": "GaLore 2: Large-Scale LLM Pre-Training by Gradient Low-Rank Projection", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Large language models (LLMs) have revolutionized natural language\nunderstanding and generation but face significant memory bottlenecks during\ntraining. GaLore, Gradient Low-Rank Projection, addresses this issue by\nleveraging the inherent low-rank structure of weight gradients, enabling\nsubstantial memory savings without sacrificing performance. Recent works\nfurther extend GaLore from various aspects, including low-bit quantization and\nhigher-order tensor structures. However, there are several remaining challenges\nfor GaLore, such as the computational overhead of SVD for subspace updates and\nthe integration with state-of-the-art training parallelization strategies\n(e.g., FSDP). In this paper, we present GaLore 2, an efficient and scalable\nGaLore framework that addresses these challenges and incorporates recent\nadvancements. In addition, we demonstrate the scalability of GaLore 2 by\npre-training Llama 7B from scratch using up to 500 billion training tokens,\nhighlighting its potential impact on real LLM pre-training scenarios."}
{"id": "2504.20456", "pdf": "https://arxiv.org/pdf/2504.20456", "abs": "https://arxiv.org/abs/2504.20456", "authors": ["Gabe Guo", "Stefano Ermon"], "title": "Reviving Any-Subset Autoregressive Models with Principled Parallel Sampling and Speculative Decoding", "categories": ["cs.LG", "cs.CL"], "comment": null, "summary": "In arbitrary-order language models, it is an open question how to sample\ntokens in parallel from the correct joint distribution. With discrete diffusion\nmodels, the more tokens they generate in parallel, the less their predicted\ndistributions adhere to the originally learned data distribution, as they rely\non a conditional independence assumption that only works with infinitesimally\nsmall timesteps. We find that a different class of models, any-subset\nautoregressive models (AS-ARMs), holds the solution. As implied by the name,\nAS-ARMs can generate tokens in any order, and in parallel. Moreover, AS-ARMs\nsupport parallelized joint probability density estimation, allowing them to\ncorrect their own parallel-generated token distributions, via our Any-Subset\nSpeculative Decoding (ASSD) algorithm. ASSD provably enables generation of\ntokens from the correct joint distribution, with the number of neural network\ncalls upper bounded by the number of tokens predicted. We empirically verify\nthat ASSD speeds up language generation, without sacrificing quality.\nFurthermore, we provide a mathematically justified scheme for training AS-ARMs\nfor generation, and show that AS-ARMs achieve state-of-the-art performance\namong sub-200M parameter models on infilling benchmark tasks, and nearly match\nthe performance of models 50X larger on code generation. Our theoretical and\nempirical results indicate that the once-forgotten AS-ARMs are a promising\ndirection of language modeling."}
{"id": "2504.20607", "pdf": "https://arxiv.org/pdf/2504.20607", "abs": "https://arxiv.org/abs/2504.20607", "authors": ["Hao Tian", "Rui Liu", "Wen Shen", "Yilong Hu", "Zhihao Zheng", "Xiaolin Qin"], "title": "EfficientHuman: Efficient Training and Reconstruction of Moving Human using Articulated 2D Gaussian", "categories": ["cs.CV"], "comment": "11 pages, 3 figures", "summary": "3D Gaussian Splatting (3DGS) has been recognized as a pioneering technique in\nscene reconstruction and novel view synthesis. Recent work on reconstructing\nthe 3D human body using 3DGS attempts to leverage prior information on human\npose to enhance rendering quality and improve training speed. However, it\nstruggles to effectively fit dynamic surface planes due to multi-view\ninconsistency and redundant Gaussians. This inconsistency arises because\nGaussian ellipsoids cannot accurately represent the surfaces of dynamic\nobjects, which hinders the rapid reconstruction of the dynamic human body.\nMeanwhile, the prevalence of redundant Gaussians means that the training time\nof these works is still not ideal for quickly fitting a dynamic human body. To\naddress these, we propose EfficientHuman, a model that quickly accomplishes the\ndynamic reconstruction of the human body using Articulated 2D Gaussian while\nensuring high rendering quality. The key innovation involves encoding Gaussian\nsplats as Articulated 2D Gaussian surfels in canonical space and then\ntransforming them to pose space via Linear Blend Skinning (LBS) to achieve\nefficient pose transformations. Unlike 3D Gaussians, Articulated 2D Gaussian\nsurfels can quickly conform to the dynamic human body while ensuring\nview-consistent geometries. Additionally, we introduce a pose calibration\nmodule and an LBS optimization module to achieve precise fitting of dynamic\nhuman poses, enhancing the model's performance. Extensive experiments on the\nZJU-MoCap dataset demonstrate that EfficientHuman achieves rapid 3D dynamic\nhuman reconstruction in less than a minute on average, which is 20 seconds\nfaster than the current state-of-the-art method, while also reducing the number\nof redundant Gaussians."}
{"id": "2504.20103", "pdf": "https://arxiv.org/pdf/2504.20103", "abs": "https://arxiv.org/abs/2504.20103", "authors": ["Wenfeng Dai", "Yanhong Wang", "Shuai Yan", "Qingzhi Yu", "Xiang Cheng"], "title": "Heterogeneous network drug-target interaction prediction model based on graph wavelet transform and multi-level contrastive learning", "categories": ["q-bio.QM", "cs.AI", "cs.LG"], "comment": null, "summary": "Drug-target interaction (DTI) prediction is a core task in drug development\nand precision medicine in the biomedical field. However, traditional machine\nlearning methods generally have the black box problem, which makes it difficult\nto reveal the deep correlation between the model decision mechanism and the\ninteraction pattern between biological molecules. This study proposes a\nheterogeneous network drug target interaction prediction framework, integrating\ngraph neural network and multi scale signal processing technology to construct\na model with both efficient prediction and multi level interpretability. Its\ntechnical breakthroughs are mainly reflected in the following three\ndimensions:Local global feature collaborative perception module. Based on\nheterogeneous graph convolutional neural network (HGCN), a multi order neighbor\naggregation strategy is designed.Multi scale graph signal decomposition and\nbiological interpretation module. A deep hierarchical node feature transform\n(GWT) architecture is proposed.Contrastive learning combining multi dimensional\nperspectives and hierarchical representations. By comparing the learning\nmodels, the node representations from the two perspectives of HGCN and GWT are\naligned and fused, so that the model can integrate multi dimensional\ninformation and improve the prediction robustness. Experimental results show\nthat our framework shows excellent prediction performance on all datasets. This\nstudy provides a complete solution for drug target discovery from black box\nprediction to mechanism decoding, and its methodology has important reference\nvalue for modeling complex biomolecular interaction systems."}
{"id": "2504.20442", "pdf": "https://arxiv.org/pdf/2504.20442", "abs": "https://arxiv.org/abs/2504.20442", "authors": ["Yuchen Wang", "Pengfei Jia", "Zhitao Shu", "Keyan Liu", "Abdul Rashid Mohamed Shariff"], "title": "Multidimensional precipitation index prediction based on CNN-LSTM hybrid framework", "categories": ["cs.LG", "cs.HC"], "comment": null, "summary": "With the intensification of global climate change, accurate prediction of\nweather indicators is of great significance in disaster prevention and\nmitigation, agricultural production, and transportation. Precipitation, as one\nof the key meteorological indicators, plays a crucial role in water resource\nmanagement, agricultural production, and urban flood control. This study\nproposes a multidimensional precipitation index prediction model based on a\nCNN- LSTM hybrid framework, aiming to improve the accuracy of precipitation\nforecasts. The dataset is sourced from Pune, Maharashtra, India, covering\nmonthly mean precipitation data from 1972 to 2002. This dataset includes nearly\n31 years (1972-2002) of monthly average precipitation, reflecting the long-term\nfluctuations and seasonal variations of precipitation in the region. By\nanalyzing these time series data, the CNN-LSTM model effectively captures local\nfeatures and long-term dependencies. Experimental results show that the model\nachieves a root mean square error (RMSE) of 6.752, which demonstrates a\nsignificant advantage over traditional time series prediction methods in terms\nof prediction accuracy and generalization ability. Furthermore, this study\nprovides new research ideas for precipitation prediction. However, the model\nrequires high computational resources when dealing with large-scale datasets,\nand its predictive ability for multidimensional precipitation data still needs\nimprovement. Future research could extend the model to support and predict\nmultidimensional precipitation data, thereby promoting the development of more\naccurate and efficient meteorological prediction technologies."}
{"id": "2504.20458", "pdf": "https://arxiv.org/pdf/2504.20458", "abs": "https://arxiv.org/abs/2504.20458", "authors": ["Xiaolei Wang", "Chunxuan Xia", "Junyi Li", "Fanzhe Meng", "Lei Huang", "Jinpeng Wang", "Wayne Xin Zhao", "Ji-Rong Wen"], "title": "Search-Based Interaction For Conversation Recommendation via Generative Reward Model Based Simulated User", "categories": ["cs.IR", "cs.CL"], "comment": "Accepted by SIGIR 2025", "summary": "Conversational recommendation systems (CRSs) use multi-turn interaction to\ncapture user preferences and provide personalized recommendations. A\nfundamental challenge in CRSs lies in effectively understanding user\npreferences from conversations. User preferences can be multifaceted and\ncomplex, posing significant challenges for accurate recommendations even with\naccess to abundant external knowledge. While interaction with users can clarify\ntheir true preferences, frequent user involvement can lead to a degraded user\nexperience.\n  To address this problem, we propose a generative reward model based simulated\nuser, named GRSU, for automatic interaction with CRSs. The simulated user\nprovides feedback to the items recommended by CRSs, enabling them to better\ncapture intricate user preferences through multi-turn interaction. Inspired by\ngenerative reward models, we design two types of feedback actions for the\nsimulated user: i.e., generative item scoring, which offers coarse-grained\nfeedback, and attribute-based item critique, which provides fine-grained\nfeedback. To ensure seamless integration, these feedback actions are unified\ninto an instruction-based format, allowing the development of a unified\nsimulated user via instruction tuning on synthesized data. With this simulated\nuser, automatic multi-turn interaction with CRSs can be effectively conducted.\nFurthermore, to strike a balance between effectiveness and efficiency, we draw\ninspiration from the paradigm of reward-guided search in complex reasoning\ntasks and employ beam search for the interaction process. On top of this, we\npropose an efficient candidate ranking method to improve the recommendation\nresults derived from interaction. Extensive experiments on public datasets\ndemonstrate the effectiveness, efficiency, and transferability of our approach."}
{"id": "2504.20645", "pdf": "https://arxiv.org/pdf/2504.20645", "abs": "https://arxiv.org/abs/2504.20645", "authors": ["Weiqin Jiao", "Hao Cheng", "George Vosselman", "Claudio Persello"], "title": "LDPoly: Latent Diffusion for Polygonal Road Outline Extraction in Large-Scale Topographic Mapping", "categories": ["cs.CV"], "comment": null, "summary": "Polygonal road outline extraction from high-resolution aerial images is an\nimportant task in large-scale topographic mapping, where roads are represented\nas vectorized polygons, capturing essential geometric features with minimal\nvertex redundancy. Despite its importance, no existing method has been\nexplicitly designed for this task. While polygonal building outline extraction\nhas been extensively studied, the unique characteristics of roads, such as\nbranching structures and topological connectivity, pose challenges to these\nmethods. To address this gap, we introduce LDPoly, the first dedicated\nframework for extracting polygonal road outlines from high-resolution aerial\nimages. Our method leverages a novel Dual-Latent Diffusion Model with a\nChannel-Embedded Fusion Module, enabling the model to simultaneously generate\nroad masks and vertex heatmaps. A tailored polygonization method is then\napplied to obtain accurate vectorized road polygons with minimal vertex\nredundancy. We evaluate LDPoly on a new benchmark dataset, Map2ImLas, which\ncontains detailed polygonal annotations for various topographic objects in\nseveral Dutch regions. Our experiments include both in-region and cross-region\nevaluations, with the latter designed to assess the model's generalization\nperformance on unseen regions. Quantitative and qualitative results demonstrate\nthat LDPoly outperforms state-of-the-art polygon extraction methods across\nvarious metrics, including pixel-level coverage, vertex efficiency, polygon\nregularity, and road connectivity. We also design two new metrics to assess\npolygon simplicity and boundary smoothness. Moreover, this work represents the\nfirst application of diffusion models for extracting precise vectorized object\noutlines without redundant vertices from remote-sensing imagery, paving the way\nfor future advancements in this field."}
{"id": "2504.20105", "pdf": "https://arxiv.org/pdf/2504.20105", "abs": "https://arxiv.org/abs/2504.20105", "authors": ["Shuang Wang", "He Zhang", "Tianxing Wu", "Yueyou Zhang", "Wei Emma Zhang", "Quan Z. Sheng"], "title": "Electricity Cost Minimization for Multi-Workflow Allocation in Geo-Distributed Data Centers", "categories": ["cs.DC", "cs.AI"], "comment": "have been accepted by IEEE Transactions on Services Computing", "summary": "Worldwide, Geo-distributed Data Centers (GDCs) provide computing and storage\nservices for massive workflow applications, resulting in high electricity costs\nthat vary depending on geographical locations and time. How to reduce\nelectricity costs while satisfying the deadline constraints of workflow\napplications is important in GDCs, which is determined by the execution time of\nservers, power, and electricity price. Determining the completion time of\nworkflows with different server frequencies can be challenging, especially in\nscenarios with heterogeneous computing resources in GDCs. Moreover, the\nelectricity price is also different in geographical locations and may change\ndynamically. To address these challenges, we develop a geo-distributed system\narchitecture and propose an Electricity Cost aware Multiple Workflows\nScheduling algorithm (ECMWS) for servers of GDCs with fixed frequency and\npower. ECMWS comprises four stages, namely workflow sequencing, deadline\npartitioning, task sequencing, and resource allocation where two graph\nembedding models and a policy network are constructed to solve the Markov\nDecision Process (MDP). After statistically calibrating parameters and\nalgorithm components over a comprehensive set of workflow instances, the\nproposed algorithms are compared with the state-of-the-art methods over two\ntypes of workflow instances. The experimental results demonstrate that our\nproposed algorithm significantly outperforms other algorithms, achieving an\nimprovement of over 15\\% while maintaining an acceptable computational time.\nThe source codes are available at\nhttps://gitee.com/public-artifacts/ecmws-experiments."}
{"id": "2504.20446", "pdf": "https://arxiv.org/pdf/2504.20446", "abs": "https://arxiv.org/abs/2504.20446", "authors": ["Wenjing Xiao", "Wenhao Song", "Miaojiang Chen", "Ruikun Luo", "Min Chen"], "title": "FT-MoE: Sustainable-learning Mixture of Experts Model for Fault-Tolerant Computing with Multiple Tasks", "categories": ["cs.LG"], "comment": null, "summary": "Intelligent fault-tolerant (FT) computing has recently demonstrated\nsignificant advantages of predicting and diagnosing faults in advance, enabling\nreliable service delivery. However, due to heterogeneity of fault knowledge and\ncomplex dependence relationships of time series log data, existing deep\nlearning-based FT algorithms further improve detection performance relying on\nsingle neural network model with difficulty. To this end, we propose FT-MoE, a\nsustainable-learning mixture-of-experts model for fault-tolerant computing with\nmultiple tasks, which enables different parameters learning distinct fault\nknowledge to achieve high-reliability for service system. Firstly, we use\ndecoder-based transformer models to obtain fault prototype vectors of\ndecoupling long-distance dependencies. Followed by, we present a dual mixture\nof experts networks for high-accurate prediction for both fault detection and\nclassification tasks. Then, we design a two-stage optimization scheme of\noffline training and online tuning, which allows that in operation FT-MoE can\nalso keep learning to adapt to dynamic service environments. Finally, to verify\nthe effectiveness of FT-MoE, we conduct extensive experiments on the FT\nbenchmark. Experimental results show that FT-MoE achieves superior performance\ncompared to the state-of-the-art methods. Code will be available upon\npublication."}
{"id": "2504.20571", "pdf": "https://arxiv.org/pdf/2504.20571", "abs": "https://arxiv.org/abs/2504.20571", "authors": ["Yiping Wang", "Qing Yang", "Zhiyuan Zeng", "Liliang Ren", "Lucas Liu", "Baolin Peng", "Hao Cheng", "Xuehai He", "Kuan Wang", "Jianfeng Gao", "Weizhu Chen", "Shuohang Wang", "Simon Shaolei Du", "Yelong Shen"], "title": "Reinforcement Learning for Reasoning in Large Language Models with One Training Example", "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": "28 pages, 12 figures, link: https://github.com/ypwang61/One-Shot-RLVR", "summary": "We show that reinforcement learning with verifiable reward using one training\nexample (1-shot RLVR) is effective in incentivizing the math reasoning\ncapabilities of large language models (LLMs). Applying RLVR to the base model\nQwen2.5-Math-1.5B, we identify a single example that elevates model performance\non MATH500 from 36.0% to 73.6%, and improves the average performance across six\ncommon mathematical reasoning benchmarks from 17.6% to 35.7%. This result\nmatches the performance obtained using the 1.2k DeepScaleR subset (MATH500:\n73.6%, average: 35.9%), which includes the aforementioned example. Similar\nsubstantial improvements are observed across various models (Qwen2.5-Math-7B,\nLlama3.2-3B-Instruct, DeepSeek-R1-Distill-Qwen-1.5B), RL algorithms (GRPO and\nPPO), and different math examples (many of which yield approximately 30% or\ngreater improvement on MATH500 when employed as a single training example). In\naddition, we identify some interesting phenomena during 1-shot RLVR, including\ncross-domain generalization, increased frequency of self-reflection, and\nsustained test performance improvement even after the training accuracy has\nsaturated, a phenomenon we term post-saturation generalization. Moreover, we\nverify that the effectiveness of 1-shot RLVR primarily arises from the policy\ngradient loss, distinguishing it from the \"grokking\" phenomenon. We also show\nthe critical role of promoting exploration (e.g., by adding entropy loss with\nan appropriate coefficient) in 1-shot RLVR training. As a bonus, we observe\nthat applying entropy loss alone, without any outcome reward, significantly\nenhances Qwen2.5-Math-1.5B's performance on MATH500 by 27.4%. These findings\ncan inspire future work on RLVR data efficiency and encourage a re-examination\nof both recent progress and the underlying mechanisms in RLVR. Our code, model,\nand data are open source at https://github.com/ypwang61/One-Shot-RLVR"}
{"id": "2504.20648", "pdf": "https://arxiv.org/pdf/2504.20648", "abs": "https://arxiv.org/abs/2504.20648", "authors": ["Michael Ogezi", "Freda Shi"], "title": "SpaRE: Enhancing Spatial Reasoning in Vision-Language Models with Synthetic Data", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Vision-language models (VLMs) work well in tasks ranging from image\ncaptioning to visual question answering (VQA), yet they struggle with spatial\nreasoning, a key skill for understanding our physical world that humans excel\nat. We find that spatial relations are generally rare in widely used VL\ndatasets, with only a few being well represented, while most form a long tail\nof underrepresented relations. This gap leaves VLMs ill-equipped to handle\ndiverse spatial relationships. To bridge it, we construct a synthetic VQA\ndataset focused on spatial reasoning generated from hyper-detailed image\ndescriptions in Localized Narratives, DOCCI, and PixMo-Cap. Our dataset\nconsists of 455k samples containing 3.4 million QA pairs. Trained on this\ndataset, our Spatial-Reasoning Enhanced (SpaRE) VLMs show strong improvements\non spatial reasoning benchmarks, achieving up to a 49% performance gain on the\nWhat's Up benchmark, while maintaining strong results on general tasks. Our\nwork narrows the gap between human and VLM spatial reasoning and makes VLMs\nmore capable in real-world tasks such as robotics and navigation."}
{"id": "2504.20114", "pdf": "https://arxiv.org/pdf/2504.20114", "abs": "https://arxiv.org/abs/2504.20114", "authors": ["Zhonghao Li", "Kunpeng Zhang", "Jinghuai Ou", "Shuliang Liu", "Xuming Hu"], "title": "TreeHop: Generate and Filter Next Query Embeddings Efficiently for Multi-hop Question Answering", "categories": ["cs.IR", "cs.AI", "cs.HC", "cs.LG"], "comment": "9 pages", "summary": "Retrieval-augmented generation (RAG) systems face significant challenges in\nmulti-hop question answering (MHQA), where complex queries require synthesizing\ninformation across multiple document chunks. Existing approaches typically rely\non iterative LLM-based query rewriting and routing, resulting in high\ncomputational costs due to repeated LLM invocations and multi-stage processes.\nTo address these limitations, we propose TreeHop, an embedding-level framework\nwithout the need for LLMs in query refinement. TreeHop dynamically updates\nquery embeddings by fusing semantic information from prior queries and\nretrieved documents, enabling iterative retrieval through embedding-space\noperations alone. This method replaces the traditional\n\"Retrieve-Rewrite-Vectorize-Retrieve\" cycle with a streamlined\n\"Retrieve-Embed-Retrieve\" loop, significantly reducing computational overhead.\nMoreover, a rule-based stop criterion is introduced to further prune redundant\nretrievals, balancing efficiency and recall rate. Experimental results show\nthat TreeHop rivals advanced RAG methods across three open-domain MHQA\ndatasets, achieving comparable performance with only 5\\%-0.4\\% of the model\nparameter size and reducing the query latency by approximately 99\\% compared to\nconcurrent approaches. This makes TreeHop a faster and more cost-effective\nsolution for deployment in a range of knowledge-intensive applications. For\nreproducibility purposes, codes and data are available here:\nhttps://github.com/allen-li1231/TreeHop."}
{"id": "2504.20471", "pdf": "https://arxiv.org/pdf/2504.20471", "abs": "https://arxiv.org/abs/2504.20471", "authors": ["Baining Chen", "Yiming Zhang", "Yuqiao Han", "Ruyue Zhang", "Ruihuan Du", "Zhishuo Zhou", "Zhengdan Zhu", "Xun Liu", "Jiecheng Guo"], "title": "The Estimation of Continual Causal Effect for Dataset Shifting Streams", "categories": ["cs.LG", "cs.AI", "stat.ME"], "comment": null, "summary": "Causal effect estimation has been widely used in marketing optimization. The\nframework of an uplift model followed by a constrained optimization algorithm\nis popular in practice. To enhance performance in the online environment, the\nframework needs to be improved to address the complexities caused by temporal\ndataset shift. This paper focuses on capturing the dataset shift from user\nbehavior and domain distribution changing over time. We propose an Incremental\nCausal Effect with Proxy Knowledge Distillation (ICE-PKD) framework to tackle\nthis challenge. The ICE-PKD framework includes two components: (i) a\nmulti-treatment uplift network that eliminates confounding bias using\ncounterfactual regression; (ii) an incremental training strategy that adapts to\nthe temporal dataset shift by updating with the latest data and protects\ngeneralization via replay-based knowledge distillation. We also revisit the\nuplift modeling metrics and introduce a novel metric for more precise online\nevaluation in multiple treatment scenarios. Extensive experiments on both\nsimulated and online datasets show that the proposed framework achieves better\nperformance. The ICE-PKD framework has been deployed in the marketing system of\nHuaxiaozhu, a ride-hailing platform in China."}
{"id": "2504.20859", "pdf": "https://arxiv.org/pdf/2504.20859", "abs": "https://arxiv.org/abs/2504.20859", "authors": ["Guy Hadad", "Haggai Roitman", "Yotam Eshel", "Bracha Shapira", "Lior Rokach"], "title": "X-Cross: Dynamic Integration of Language Models for Cross-Domain Sequential Recommendation", "categories": ["cs.IR", "cs.AI", "cs.CL", "cs.LG"], "comment": "Accepted for publication in SIGIR '25", "summary": "As new products are emerging daily, recommendation systems are required to\nquickly adapt to possible new domains without needing extensive retraining.\nThis work presents ``X-Cross'' -- a novel cross-domain\nsequential-recommendation model that recommends products in new domains by\nintegrating several domain-specific language models; each model is fine-tuned\nwith low-rank adapters (LoRA). Given a recommendation prompt, operating layer\nby layer, X-Cross dynamically refines the representation of each source\nlanguage model by integrating knowledge from all other models. These refined\nrepresentations are propagated from one layer to the next, leveraging the\nactivations from each domain adapter to ensure domain-specific nuances are\npreserved while enabling adaptability across domains. Using Amazon datasets for\nsequential recommendation, X-Cross achieves performance comparable to a model\nthat is fine-tuned with LoRA, while using only 25% of the additional\nparameters. In cross-domain tasks, such as adapting from Toys domain to Tools,\nElectronics or Sports, X-Cross demonstrates robust performance, while requiring\nabout 50%-75% less fine-tuning data than LoRA to make fine-tuning effective.\nFurthermore, X-Cross achieves significant improvement in accuracy over\nalternative cross-domain baselines. Overall, X-Cross enables scalable and\nadaptive cross-domain recommendations, reducing computational overhead and\nproviding an efficient solution for data-constrained environments."}
{"id": "2504.20657", "pdf": "https://arxiv.org/pdf/2504.20657", "abs": "https://arxiv.org/abs/2504.20657", "authors": ["Alex Michie", "Simon J Doran"], "title": "Image deidentification in the XNAT ecosystem: use cases and solutions", "categories": ["cs.CV", "J.3"], "comment": "For submission to MELBA (Machine Learning for Biomedical Imaging)\n  special issue on the MIDI-B deidentification challenge\n  (https://www.synapse.org/Synapse:syn53065760). 11 pages, 1 fig, 2 tables; 1\n  supplementary data file (supplementary_tables_S1_S2_S3.xlsx) containing three\n  spreadsheet tabs", "summary": "XNAT is a server-based data management platform widely used in academia for\ncurating large databases of DICOM images for research projects. We describe in\ndetail a deidentification workflow for DICOM data using facilities in XNAT,\ntogether with independent tools in the XNAT \"ecosystem\". We list different\ncontexts in which deidentification might be needed, based on our prior\nexperience. The starting point for participation in the Medical Image\nDe-Identification Benchmark (MIDI-B) challenge was a set of pre-existing local\nmethodologies, which were adapted during the validation phase of the challenge.\nOur result in the test phase was 97.91\\%, considerably lower than our peers,\ndue largely to an arcane technical incompatibility of our methodology with the\nchallenge's Synapse platform, which prevented us receiving feedback during the\nvalidation phase. Post-submission, additional discrepancy reports from the\norganisers and via the MIDI-B Continuous Benchmarking facility, enabled us to\nimprove this score significantly to 99.61\\%. An entirely rule-based approach\nwas shown to be capable of removing all name-related information in the test\ncorpus, but exhibited failures in dealing fully with address data. Initial\nexperiments using published machine-learning models to remove addresses were\npartially successful but showed the models to be \"over-aggressive\" on other\ntypes of free-text data, leading to a slight overall degradation in performance\nto 99.54\\%. Future development will therefore focus on improving\naddress-recognition capabilities, but also on better removal of identifiable\ndata burned into the image pixels. Several technical aspects relating to the\n\"answer key\" are still under discussion with the challenge organisers, but we\nestimate that our percentage of genuine deidentification failures on the MIDI-B\ntest corpus currently stands at 0.19\\%. (Abridged from original for arXiv\nsubmission)"}
{"id": "2504.20115", "pdf": "https://arxiv.org/pdf/2504.20115", "abs": "https://arxiv.org/abs/2504.20115", "authors": ["Zijie Lin", "Yiqing Shen", "Qilin Cai", "He Sun", "Jinrui Zhou", "Mingjun Xiao"], "title": "AutoP2C: An LLM-Based Agent Framework for Code Repository Generation from Multimodal Content in Academic Papers", "categories": ["cs.SE", "cs.AI"], "comment": null, "summary": "Machine Learning (ML) research is spread through academic papers featuring\nrich multimodal content, including text, diagrams, and tabular results.\nHowever, translating these multimodal elements into executable code remains a\nchallenging and time-consuming process that requires substantial ML expertise.\nWe introduce ``Paper-to-Code'' (P2C), a novel task that transforms the\nmultimodal content of scientific publications into fully executable code\nrepositories, which extends beyond the existing formulation of code generation\nthat merely converts textual descriptions into isolated code snippets. To\nautomate the P2C process, we propose AutoP2C, a multi-agent framework based on\nlarge language models that processes both textual and visual content from\nresearch papers to generate complete code repositories. Specifically, AutoP2C\ncontains four stages: (1) repository blueprint extraction from established\ncodebases, (2) multimodal content parsing that integrates information from\ntext, equations, and figures, (3) hierarchical task decomposition for\nstructured code generation, and (4) iterative feedback-driven debugging to\nensure functionality and performance. Evaluation on a benchmark of eight\nresearch papers demonstrates the effectiveness of AutoP2C, which can\nsuccessfully generate executable code repositories for all eight papers, while\nOpenAI-o1 or DeepSeek-R1 can only produce runnable code for one paper. The code\nis available at https://github.com/shoushouyu/Automated-Paper-to-Code."}
{"id": "2504.20482", "pdf": "https://arxiv.org/pdf/2504.20482", "abs": "https://arxiv.org/abs/2504.20482", "authors": ["Chao Li", "Changhua Zhou", "Jia Chen"], "title": "Group Relative Knowledge Distillation: Learning from Teacher's Relational Inductive Bias", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Knowledge distillation typically transfers knowledge from a teacher model to\na student model by minimizing differences between their output distributions.\nHowever, existing distillation approaches largely focus on mimicking absolute\nprobabilities and neglect the valuable relational inductive biases embedded in\nthe teacher's relative predictions, leading to exposure bias. In this paper, we\npropose Group Relative Knowledge Distillation (GRKD), a novel framework that\ndistills teacher knowledge by learning the relative ranking among classes,\nrather than directly fitting the absolute distribution. Specifically, we\nintroduce a group relative loss that encourages the student model to preserve\nthe pairwise preference orderings provided by the teacher's outputs. Extensive\nexperiments on classification benchmarks demonstrate that GRKD achieves\nsuperior generalization compared to existing methods, especially in tasks\nrequiring fine-grained class differentiation. Our method provides a new\nperspective on exploiting teacher knowledge, focusing on relational structure\nrather than absolute likelihood."}
{"id": "2504.20938", "pdf": "https://arxiv.org/pdf/2504.20938", "abs": "https://arxiv.org/abs/2504.20938", "authors": ["Zhengfu He", "Junxuan Wang", "Rui Lin", "Xuyang Ge", "Wentao Shu", "Qiong Tang", "Junping Zhang", "Xipeng Qiu"], "title": "Towards Understanding the Nature of Attention with Low-Rank Sparse Decomposition", "categories": ["cs.LG", "cs.CL"], "comment": null, "summary": "We propose Low-Rank Sparse Attention (Lorsa), a sparse replacement model of\nTransformer attention layers to disentangle original Multi Head Self Attention\n(MHSA) into individually comprehensible components. Lorsa is designed to\naddress the challenge of attention superposition to understand\nattention-mediated interaction between features in different token positions.\nWe show that Lorsa heads find cleaner and finer-grained versions of previously\ndiscovered MHSA behaviors like induction heads, successor heads and attention\nsink behavior (i.e., heavily attending to the first token). Lorsa and Sparse\nAutoencoder (SAE) are both sparse dictionary learning methods applied to\ndifferent Transformer components, and lead to consistent findings in many ways.\nFor instance, we discover a comprehensive family of arithmetic-specific Lorsa\nheads, each corresponding to an atomic operation in Llama-3.1-8B. Automated\ninterpretability analysis indicates that Lorsa achieves parity with SAE in\ninterpretability while Lorsa exhibits superior circuit discovery properties,\nespecially for features computed collectively by multiple MHSA heads. We also\nconduct extensive experiments on architectural design ablation, Lorsa scaling\nlaw and error analysis."}
{"id": "2504.20670", "pdf": "https://arxiv.org/pdf/2504.20670", "abs": "https://arxiv.org/abs/2504.20670", "authors": ["Yao Xiao", "Tingfa Xu", "Yu Xin", "Jianan Li"], "title": "FBRT-YOLO: Faster and Better for Real-Time Aerial Image Detection", "categories": ["cs.CV"], "comment": "AAAI 2025", "summary": "Embedded flight devices with visual capabilities have become essential for a\nwide range of applications. In aerial image detection, while many existing\nmethods have partially addressed the issue of small target detection,\nchallenges remain in optimizing small target detection and balancing detection\naccuracy with efficiency. These issues are key obstacles to the advancement of\nreal-time aerial image detection. In this paper, we propose a new family of\nreal-time detectors for aerial image detection, named FBRT-YOLO, to address the\nimbalance between detection accuracy and efficiency. Our method comprises two\nlightweight modules: Feature Complementary Mapping Module (FCM) and\nMulti-Kernel Perception Unit(MKP), designed to enhance object perception for\nsmall targets in aerial images. FCM focuses on alleviating the problem of\ninformation imbalance caused by the loss of small target information in deep\nnetworks. It aims to integrate spatial positional information of targets more\ndeeply into the network,better aligning with semantic information in the deeper\nlayers to improve the localization of small targets. We introduce MKP, which\nleverages convolutions with kernels of different sizes to enhance the\nrelationships between targets of various scales and improve the perception of\ntargets at different scales. Extensive experimental results on three major\naerial image datasets, including Visdrone, UAVDT, and AI-TOD,demonstrate that\nFBRT-YOLO outperforms various real-time detectors in terms of performance and\nspeed."}
{"id": "2504.20118", "pdf": "https://arxiv.org/pdf/2504.20118", "abs": "https://arxiv.org/abs/2504.20118", "authors": ["Jinglin He", "Yunqi Guo", "Lai Kwan Lam", "Waikei Leung", "Lixing He", "Yuanan Jiang", "Chi Chiu Wang", "Guoliang Xing", "Hongkai Chen"], "title": "OpenTCM: A GraphRAG-Empowered LLM-based System for Traditional Chinese Medicine Knowledge Retrieval and Diagnosis", "categories": ["cs.IR", "cs.AI"], "comment": "8 pages, 4 figures", "summary": "Traditional Chinese Medicine (TCM) represents a rich repository of ancient\nmedical knowledge that continues to play an important role in modern\nhealthcare. Due to the complexity and breadth of the TCM literature, the\nintegration of AI technologies is critical for its modernization and broader\naccessibility. However, this integration poses considerable challenges,\nincluding the interpretation of obscure classical Chinese texts and the\nmodeling of intricate semantic relationships among TCM concepts. In this paper,\nwe develop OpenTCM, an LLM-based system that combines a domain-specific TCM\nknowledge graph and Graph-based Retrieval-Augmented Generation (GraphRAG).\nFirst, we extract more than 3.73 million classical Chinese characters from 68\ngynecological books in the Chinese Medical Classics Database, with the help of\nTCM and gynecology experts. Second, we construct a comprehensive\nmulti-relational knowledge graph comprising more than 48,000 entities and\n152,000 interrelationships, using customized prompts and Chinese-oriented LLMs\nsuch as DeepSeek and Kimi to ensure high-fidelity semantic understanding. Last,\nwe integrate OpenTCM with this knowledge graph, enabling high-fidelity\ningredient knowledge retrieval and diagnostic question-answering without model\nfine-tuning. Experimental evaluations demonstrate that our prompt design and\nmodel selection significantly improve knowledge graph quality, achieving a\nprecision of 98. 55% and an F1 score of 99. 55%. In addition, OpenTCM achieves\nmean expert scores of 4.5 in ingredient information retrieval and 3.8 in\ndiagnostic question-answering tasks, outperforming state-of-the-art solutions\nin real-world TCM use cases."}
{"id": "2504.20522", "pdf": "https://arxiv.org/pdf/2504.20522", "abs": "https://arxiv.org/abs/2504.20522", "authors": ["Gissel Velarde", "Tillman Weyde", "David Meredith"], "title": "Wavelet-Filtering of Symbolic Music Representations for Folk Tune Segmentation and Classification", "categories": ["cs.LG"], "comment": "7 pages, 4 figures, 2 tables, Proceedings of the Third International\n  Workshop on Folk Music Analysis (FMA2013)", "summary": "The aim of this study is to evaluate a machine-learning method in which\nsymbolic representations of folk songs are segmented and classified into tune\nfamilies with Haar-wavelet filtering. The method is compared with previously\nproposed Gestalt-based method. Melodies are represented as discrete symbolic\npitch-time signals. We apply the continuous wavelet transform (CWT) with the\nHaar wavelet at specific scales, obtaining filtered versions of melodies\nemphasizing their information at particular time-scales. We use the filtered\nsignal for representation and segmentation, using the wavelet coefficients'\nlocal maxima to indicate local boundaries and classify segments by means of\nk-nearest neighbours based on standard vector-metrics (Euclidean, cityblock),\nand compare the results to a Gestalt-based segmentation method and metrics\napplied directly to the pitch signal. We found that the wavelet based\nsegmentation and wavelet-filtering of the pitch signal lead to better\nclassification accuracy in cross-validated evaluation when the time-scale and\nother parameters are optimized."}
{"id": "2308.09138", "pdf": "https://arxiv.org/pdf/2308.09138", "abs": "https://arxiv.org/abs/2308.09138", "authors": ["Harsh Raj", "Vipul Gupta", "Domenic Rosati", "Subhabrata Majumdar"], "title": "Semantic Consistency for Assuring Reliability of Large Language Models", "categories": ["cs.CL", "cs.AI", "cs.CY"], "comment": "An updated version of this preprint is available at arXiv:2502.15924,\n  and has been accepted at the Transactions on Machine Learning Research", "summary": "Large Language Models (LLMs) exhibit remarkable fluency and competence across\nvarious natural language tasks. However, recent research has highlighted their\nsensitivity to variations in input prompts. To deploy LLMs in a safe and\nreliable manner, it is crucial for their outputs to be consistent when prompted\nwith expressions that carry the same meaning or intent. While some existing\nwork has explored how state-of-the-art LLMs address this issue, their\nevaluations have been confined to assessing lexical equality of single- or\nmulti-word answers, overlooking the consistency of generative text sequences.\nFor a more comprehensive understanding of the consistency of LLMs in open-ended\ntext generation scenarios, we introduce a general measure of semantic\nconsistency, and formulate multiple versions of this metric to evaluate the\nperformance of various LLMs. Our proposal demonstrates significantly higher\nconsistency and stronger correlation with human evaluations of output\nconsistency than traditional metrics based on lexical consistency. Finally, we\npropose a novel prompting strategy, called Ask-to-Choose (A2C), to enhance\nsemantic consistency. When evaluated for closed-book question answering based\non answer variations from the TruthfulQA benchmark, A2C increases accuracy\nmetrics for pretrained and finetuned LLMs by up to 47%, and semantic\nconsistency metrics for instruction-tuned models by up to 7-fold."}
{"id": "2504.20677", "pdf": "https://arxiv.org/pdf/2504.20677", "abs": "https://arxiv.org/abs/2504.20677", "authors": ["Paola Natalia Cañas", "Alexander Diez", "David Galvañ", "Marcos Nieto", "Igor Rodríguez"], "title": "Occlusion-aware Driver Monitoring System using the Driver Monitoring Dataset", "categories": ["cs.CV"], "comment": "Submitted for review to the IEEE International Conference on\n  Intelligent Transportation Systems (ITSC) 2025", "summary": "This paper presents a robust, occlusion-aware driver monitoring system (DMS)\nutilizing the Driver Monitoring Dataset (DMD). The system performs driver\nidentification, gaze estimation by regions, and face occlusion detection under\nvarying lighting conditions, including challenging low-light scenarios. Aligned\nwith EuroNCAP recommendations, the inclusion of occlusion detection enhances\nsituational awareness and system trustworthiness by indicating when the\nsystem's performance may be degraded. The system employs separate algorithms\ntrained on RGB and infrared (IR) images to ensure reliable functioning. We\ndetail the development and integration of these algorithms into a cohesive\npipeline, addressing the challenges of working with different sensors and\nreal-car implementation. Evaluation on the DMD and in real-world scenarios\ndemonstrates the effectiveness of the proposed system, highlighting the\nsuperior performance of RGB-based models and the pioneering contribution of\nrobust occlusion detection in DMS."}
{"id": "2504.20119", "pdf": "https://arxiv.org/pdf/2504.20119", "abs": "https://arxiv.org/abs/2504.20119", "authors": ["Lorenz Brehme", "Thomas Ströhle", "Ruth Breu"], "title": "Can LLMs Be Trusted for Evaluating RAG Systems? A Survey of Methods and Datasets", "categories": ["cs.IR", "cs.AI"], "comment": "8 Pages. This paper has been accepted for presentation at the IEEE\n  Swiss Conference on Data Science (SDS25)", "summary": "Retrieval-Augmented Generation (RAG) has advanced significantly in recent\nyears. The complexity of RAG systems, which involve multiple components-such as\nindexing, retrieval, and generation-along with numerous other parameters, poses\nsubstantial challenges for systematic evaluation and quality enhancement.\nPrevious research highlights that evaluating RAG systems is essential for\ndocumenting advancements, comparing configurations, and identifying effective\napproaches for domain-specific applications. This study systematically reviews\n63 academic articles to provide a comprehensive overview of state-of-the-art\nRAG evaluation methodologies, focusing on four key areas: datasets, retrievers,\nindexing and databases, and the generator component. We observe the feasibility\nof an automated evaluation approach for each component of a RAG system,\nleveraging an LLM capable of both generating evaluation datasets and conducting\nevaluations. In addition, we found that further practical research is essential\nto provide companies with clear guidance on the do's and don'ts of implementing\nand evaluating RAG systems. By synthesizing evaluation approaches for key RAG\ncomponents and emphasizing the creation and adaptation of domain-specific\ndatasets for benchmarking, we contribute to the advancement of systematic\nevaluation methods and the improvement of evaluation rigor for RAG systems.\nFurthermore, by examining the interplay between automated approaches leveraging\nLLMs and human judgment, we contribute to the ongoing discourse on balancing\nautomation and human input, clarifying their respective contributions,\nlimitations, and challenges in achieving robust and reliable evaluations."}
{"id": "2504.20535", "pdf": "https://arxiv.org/pdf/2504.20535", "abs": "https://arxiv.org/abs/2504.20535", "authors": ["Chris Child", "Lam Ngo"], "title": "DeeP-Mod: Deep Dynamic Programming based Environment Modelling using Feature Extraction", "categories": ["cs.LG"], "comment": null, "summary": "The DeeP-Mod framework builds an environment model using features from a Deep\nDynamic Programming Network (DDPN), trained via a Deep Q-Network (DQN). While\nDeep Q-Learning is effective in decision-making, state information is lost in\ndeeper DQN layers due to mixed state-action representations. We address this by\nusing Dynamic Programming (DP) to train a DDPN, where Value Iteration ensures\nthe output represents state values, not state-action pairs. Extracting features\nfrom the DDPN preserves state information, enabling task and action set\nindependence. We show that a reduced DDPN can be trained using features\nextracted from the original DDPN trained on an identical problem. This reduced\nDDPN achieves faster convergence under noise and outperforms the original DDPN.\nFinally, we introduce the DeeP-Mod framework, which creates an environment\nmodel using the evolution of features extracted from a DDPN in response to\nactions. A second DDPN, which learns directly from this feature model rather\nthan raw states, can learn an effective feature-value representation and thus\noptimal policy. A key advantage of DeeP-Mod is that an externally defined\nenvironment model is not needed at any stage, making DDPN applicable to a wide\nrange of environments."}
{"id": "2310.12059", "pdf": "https://arxiv.org/pdf/2310.12059", "abs": "https://arxiv.org/abs/2310.12059", "authors": ["Duc-Vu Nguyen", "Quoc-Nam Nguyen"], "title": "Evaluating the Symbol Binding Ability of Large Language Models for Multiple-Choice Questions in Vietnamese General Education", "categories": ["cs.CL"], "comment": "Accepted at SoICT 2023", "summary": "In this paper, we evaluate the ability of large language models (LLMs) to\nperform multiple choice symbol binding (MCSB) for multiple choice question\nanswering (MCQA) tasks in zero-shot, one-shot, and few-shot settings. We focus\non Vietnamese, with fewer challenging MCQA datasets than in English. The two\nexisting datasets, ViMMRC 1.0 and ViMMRC 2.0, focus on literature. Recent\nresearch in Vietnamese natural language processing (NLP) has focused on the\nVietnamese National High School Graduation Examination (VNHSGE) from 2019 to\n2023 to evaluate ChatGPT. However, these studies have mainly focused on how\nChatGPT solves the VNHSGE step by step. We aim to create a novel and\nhigh-quality dataset by providing structured guidelines for typing LaTeX\nformulas for mathematics, physics, chemistry, and biology. This dataset can be\nused to evaluate the MCSB ability of LLMs and smaller language models (LMs)\nbecause it is typed in a strict LaTeX style. We focus on predicting the\ncharacter (A, B, C, or D) that is the most likely answer to a question, given\nthe context of the question. Our evaluation of six well-known LLMs, namely\nBLOOMZ-7.1B-MT, LLaMA-2-7B, LLaMA-2-70B, GPT-3, GPT-3.5, and GPT-4.0, on the\nViMMRC 1.0 and ViMMRC 2.0 benchmarks and our proposed dataset shows promising\nresults on the MCSB ability of LLMs for Vietnamese. The dataset is available\nfor research purposes only."}
{"id": "2504.20682", "pdf": "https://arxiv.org/pdf/2504.20682", "abs": "https://arxiv.org/abs/2504.20682", "authors": ["Long Liu", "Cihui Yang"], "title": "OG-HFYOLO :Orientation gradient guidance and heterogeneous feature fusion for deformation table cell instance segmentation", "categories": ["cs.CV"], "comment": null, "summary": "Table structure recognition is a key task in document analysis. However, the\ngeometric deformation in deformed tables causes a weak correlation between\ncontent information and structure, resulting in downstream tasks not being able\nto obtain accurate content information. To obtain fine-grained spatial\ncoordinates of cells, we propose the OG-HFYOLO model, which enhances the edge\nresponse by Gradient Orientation-aware Extractor, combines a Heterogeneous\nKernel Cross Fusion module and a scale-aware loss function to adapt to\nmulti-scale objective features, and introduces mask-driven non-maximal\nsuppression in the post-processing, which replaces the traditional bounding box\nsuppression mechanism. Furthermore, we also propose a data generator, filling\nthe gap in the dataset for fine-grained deformation table cell spatial\ncoordinate localization, and derive a large-scale dataset named Deformation\nWired Table (DWTAL). Experiments show that our proposed model demonstrates\nexcellent segmentation accuracy on all mainstream instance segmentation models.\nThe dataset and the source code are open source:\nhttps://github.com/justliulong/OGHFYOLO."}
{"id": "2504.20125", "pdf": "https://arxiv.org/pdf/2504.20125", "abs": "https://arxiv.org/abs/2504.20125", "authors": ["Michael Pekala", "Gregory Canal", "Samuel Barham", "Milena B. Graziano", "Morgan Trexler", "Leslie Hamilton", "Elizabeth Reilly", "Christopher D. Stiles"], "title": "Towards Large Language Models for Lunar Mission Planning and In Situ Resource Utilization", "categories": ["cs.DL", "cs.AI"], "comment": null, "summary": "A key factor for lunar mission planning is the ability to assess the local\navailability of raw materials. However, many potentially relevant measurements\nare scattered across a variety of scientific publications. In this paper we\nconsider the viability of obtaining lunar composition data by leveraging LLMs\nto rapidly process a corpus of scientific publications. While leveraging LLMs\nto obtain knowledge from scientific documents is not new, this particular\napplication presents interesting challenges due to the heterogeneity of lunar\nsamples and the nuances involved in their characterization. Accuracy and\nuncertainty quantification are particularly crucial since many materials\nproperties can be sensitive to small variations in composition. Our findings\nindicate that off-the-shelf LLMs are generally effective at extracting data\nfrom tables commonly found in these documents. However, there remains\nopportunity to further refine the data we extract in this initial approach; in\nparticular, to capture fine-grained mineralogy information and to improve\nperformance on more subtle/complex pieces of information."}
{"id": "2504.20566", "pdf": "https://arxiv.org/pdf/2504.20566", "abs": "https://arxiv.org/abs/2504.20566", "authors": ["Shunjie Wen", "Thomas Heinis", "Dong-Wan Choi"], "title": "Inclusive Training Separation and Implicit Knowledge Interaction for Balanced Online Class-Incremental Learning", "categories": ["cs.LG", "cs.AI"], "comment": "Under review", "summary": "Online class-incremental learning (OCIL) focuses on gradually learning new\nclasses (called plasticity) from a stream of data in a single-pass, while\nconcurrently preserving knowledge of previously learned classes (called\nstability). The primary challenge in OCIL lies in maintaining a good balance\nbetween the knowledge of old and new classes within the continually updated\nmodel. Most existing methods rely on explicit knowledge interaction through\nexperience replay, and often employ exclusive training separation to address\nbias problems. Nevertheless, it still remains a big challenge to achieve a\nwell-balanced learner, as these methods often exhibit either reduced plasticity\nor limited stability due to difficulties in continually integrating knowledge\nin the OCIL setting. In this paper, we propose a novel replay-based method,\ncalled Balanced Online Incremental Learning (BOIL), which can achieve both high\nplasticity and stability, thus ensuring more balanced performance in OCIL. Our\nBOIL method proposes an inclusive training separation strategy using dual\nclassifiers so that knowledge from both old and new classes can effectively be\nintegrated into the model, while introducing implicit approaches for\ntransferring knowledge across the two classifiers. Extensive experimental\nevaluations over three widely-used OCIL benchmark datasets demonstrate the\nsuperiority of BOIL, showing more balanced yet better performance compared to\nstate-of-the-art replay-based OCIL methods."}
{"id": "2407.15229", "pdf": "https://arxiv.org/pdf/2407.15229", "abs": "https://arxiv.org/abs/2407.15229", "authors": ["Kian Ahrabian", "Xihui Lin", "Barun Patra", "Vishrav Chaudhary", "Alon Benhaim", "Jay Pujara", "Xia Song"], "title": "A Practical Analysis of Human Alignment with *PO", "categories": ["cs.CL", "cs.AI"], "comment": "Accepted to NAACL 2025 findings papers. 9 pages, 7 figures, 3 tables", "summary": "At the forefront of state-of-the-art human alignment methods are preference\noptimization methods (*PO). Prior research has often concentrated on\nidentifying the best-performing method, typically involving a grid search over\nhyperparameters, which can be impractical for general practitioners. In this\npaper, we examine the robustness of existing state-of-the-art methods to\nvarying hyperparameters in a realistic out-of-distribution (OOD) scenario that\nmirrors real-world applications of human alignment. Our goal is to empirically\nfind the method that increases the likelihood of achieving better results\nthrough the lens of various metrics, such as KL divergence and response length.\nWe also introduce LN-DPO, a simple length-normalized version of DPO that is\nmore stable across hyperparameters, effectively reduces the average response\nlength, and improves performance. Our analysis of state-of-the-art\nreference-free (i.e., SimPO) and reference-dependent (i.e., DPO and LN-DPO)\nmethods reveals that they perform similarly at their peak (i.e., best possible\nscenario). However, we uncover that the pattern of change in performance\ngreatly varies as we move away from the best possible scenario."}
{"id": "2504.20685", "pdf": "https://arxiv.org/pdf/2504.20685", "abs": "https://arxiv.org/abs/2504.20685", "authors": ["Zesheng Wang", "Alexandre Bruckert", "Patrick Le Callet", "Guangtao Zhai"], "title": "Efficient Listener: Dyadic Facial Motion Synthesis via Action Diffusion", "categories": ["cs.CV", "cs.HC"], "comment": null, "summary": "Generating realistic listener facial motions in dyadic conversations remains\nchallenging due to the high-dimensional action space and temporal dependency\nrequirements. Existing approaches usually consider extracting 3D Morphable\nModel (3DMM) coefficients and modeling in the 3DMM space. However, this makes\nthe computational speed of the 3DMM a bottleneck, making it difficult to\nachieve real-time interactive responses. To tackle this problem, we propose\nFacial Action Diffusion (FAD), which introduces the diffusion methods from the\nfield of image generation to achieve efficient facial action generation. We\nfurther build the Efficient Listener Network (ELNet) specially designed to\naccommodate both the visual and audio information of the speaker as input.\nConsidering of FAD and ELNet, the proposed method learns effective listener\nfacial motion representations and leads to improvements of performance over the\nstate-of-the-art methods while reducing 99% computational time."}
{"id": "2504.20183", "pdf": "https://arxiv.org/pdf/2504.20183", "abs": "https://arxiv.org/abs/2504.20183", "authors": ["Niki van Stein", "Anna V. Kononova", "Haoran Yin", "Thomas Bäck"], "title": "BLADE: Benchmark suite for LLM-driven Automated Design and Evolution of iterative optimisation heuristics", "categories": ["cs.SE", "cs.AI", "cs.NE"], "comment": "9 pages, accepted at GECCO Workshop 2025", "summary": "The application of Large Language Models (LLMs) for Automated Algorithm\nDiscovery (AAD), particularly for optimisation heuristics, is an emerging field\nof research. This emergence necessitates robust, standardised benchmarking\npractices to rigorously evaluate the capabilities and limitations of LLM-driven\nAAD methods and the resulting generated algorithms, especially given the\nopacity of their design process and known issues with existing benchmarks. To\naddress this need, we introduce BLADE (Benchmark suite for LLM-driven Automated\nDesign and Evolution), a modular and extensible framework specifically designed\nfor benchmarking LLM-driven AAD methods in a continuous black-box optimisation\ncontext. BLADE integrates collections of benchmark problems (including MA-BBOB\nand SBOX-COST among others) with instance generators and textual descriptions\naimed at capability-focused testing, such as generalisation, specialisation and\ninformation exploitation. It offers flexible experimental setup options,\nstandardised logging for reproducibility and fair comparison, incorporates\nmethods for analysing the AAD process (e.g., Code Evolution Graphs and various\nvisualisation approaches) and facilitates comparison against human-designed\nbaselines through integration with established tools like IOHanalyser and\nIOHexplainer. BLADE provides an `out-of-the-box' solution to systematically\nevaluate LLM-driven AAD approaches. The framework is demonstrated through two\ndistinct use cases exploring mutation prompt strategies and function\nspecialisation."}
{"id": "2504.20568", "pdf": "https://arxiv.org/pdf/2504.20568", "abs": "https://arxiv.org/abs/2504.20568", "authors": ["Danilo Avola", "Federica Bruni", "Gian Luca Foresti", "Daniele Pannone", "Amedeo Ranaldi"], "title": "Digital Shielding for Cross-Domain Wi-Fi Signal Adaptation using Relativistic Average Generative Adversarial Network", "categories": ["cs.LG"], "comment": null, "summary": "Wi-Fi sensing uses radio-frequency signals from Wi-Fi devices to analyze\nenvironments, enabling tasks such as tracking people, detecting intrusions, and\nrecognizing gestures. The rise of this technology is driven by the IEEE\n802.11bf standard and growing demand for tools that can ensure privacy and\noperate through obstacles. However, the performance of Wi-Fi sensing is heavily\ninfluenced by environmental conditions, especially when extracting spatial and\ntemporal features from the surrounding scene. A key challenge is achieving\nrobust generalization across domains, ensuring stable performance even when the\nsensing environment changes significantly. This paper introduces a novel deep\nlearning model for cross-domain adaptation of Wi-Fi signals, inspired by\nphysical signal shielding. The model uses a Relativistic average Generative\nAdversarial Network (RaGAN) with Bidirectional Long Short-Term Memory (Bi-LSTM)\narchitectures for both the generator and discriminator. To simulate physical\nshielding, an acrylic box lined with electromagnetic shielding fabric was\nconstructed, mimicking a Faraday cage. Wi-Fi signal spectra were collected from\nvarious materials both inside (domain-free) and outside (domain-dependent) the\nbox to train the model. A multi-class Support Vector Machine (SVM) was trained\non domain-free spectra and tested on signals denoised by the RaGAN. The system\nachieved 96% accuracy and demonstrated strong material discrimination\ncapabilities, offering potential for use in security applications to identify\nconcealed objects based on their composition."}
{"id": "2408.00137", "pdf": "https://arxiv.org/pdf/2408.00137", "abs": "https://arxiv.org/abs/2408.00137", "authors": ["Sangwon Yu", "Jongyoon Song", "Bongkyu Hwang", "Hoyoung Kang", "Sooah Cho", "Junhwa Choi", "Seongho Joe", "Taehee Lee", "Youngjune L. Gwon", "Sungroh Yoon"], "title": "Correcting Negative Bias in Large Language Models through Negative Attention Score Alignment", "categories": ["cs.CL", "cs.AI"], "comment": "NAACL 2025 Oral", "summary": "A binary decision task, like yes-no questions or answer verification,\nreflects a significant real-world scenario such as where users look for\nconfirmation about the correctness of their decisions on specific issues. In\nthis work, we observe that language models exhibit a negative bias in the\nbinary decisions of complex reasoning tasks. Based on our observations and the\nrationale about attention-based model dynamics, we propose a negative attention\nscore (NAS) to systematically and quantitatively formulate negative bias. Based\non NAS, we identify attention heads that attend to negative tokens provided in\nthe instructions as answer candidate of binary decisions, regardless of the\nquestion in the prompt, and validate their association with the negative bias.\nAdditionally, we propose the negative attention score alignment (NASA) method,\nwhich is a parameter-efficient fine-tuning technique to address the extracted\nnegatively biased attention heads. Experimental results from various domains of\nreasoning tasks and large model search space demonstrate that NASA\nsignificantly reduces the gap between precision and recall caused by negative\nbias while preserving their generalization abilities."}
{"id": "2504.20690", "pdf": "https://arxiv.org/pdf/2504.20690", "abs": "https://arxiv.org/abs/2504.20690", "authors": ["Zechuan Zhang", "Ji Xie", "Yu Lu", "Zongxin Yang", "Yi Yang"], "title": "In-Context Edit: Enabling Instructional Image Editing with In-Context Generation in Large Scale Diffusion Transformer", "categories": ["cs.CV"], "comment": "Project Page: https://river-zhang.github.io/ICEdit-gh-pages/", "summary": "Instruction-based image editing enables robust image modification via natural\nlanguage prompts, yet current methods face a precision-efficiency tradeoff.\nFine-tuning methods demand significant computational resources and large\ndatasets, while training-free techniques struggle with instruction\ncomprehension and edit quality. We resolve this dilemma by leveraging\nlarge-scale Diffusion Transformer (DiT)' enhanced generation capacity and\nnative contextual awareness. Our solution introduces three contributions: (1)\nan in-context editing framework for zero-shot instruction compliance using\nin-context prompting, avoiding structural changes; (2) a LoRA-MoE hybrid tuning\nstrategy that enhances flexibility with efficient adaptation and dynamic expert\nrouting, without extensive retraining; and (3) an early filter inference-time\nscaling method using vision-language models (VLMs) to select better initial\nnoise early, improving edit quality. Extensive evaluations demonstrate our\nmethod's superiority: it outperforms state-of-the-art approaches while\nrequiring only 0.5% training data and 1% trainable parameters compared to\nconventional baselines. This work establishes a new paradigm that enables\nhigh-precision yet efficient instruction-guided editing. Codes and demos can be\nfound in https://river-zhang.github.io/ICEdit-gh-pages/."}
{"id": "2504.20196", "pdf": "https://arxiv.org/pdf/2504.20196", "abs": "https://arxiv.org/abs/2504.20196", "authors": ["Daye Nam", "Ahmed Omran", "Ambar Murillo", "Saksham Thakur", "Abner Araujo", "Marcel Blistein", "Alexander Frömmgen", "Vincent Hellendoorn", "Satish Chandra"], "title": "Prompting LLMs for Code Editing: Struggles and Remedies", "categories": ["cs.SE", "cs.AI", "cs.HC"], "comment": null, "summary": "Large Language Models (LLMs) are rapidly transforming software engineering,\nwith coding assistants embedded in an IDE becoming increasingly prevalent.\nWhile research has focused on improving the tools and understanding developer\nperceptions, a critical gap exists in understanding how developers actually use\nthese tools in their daily workflows, and, crucially, where they struggle. This\npaper addresses part of this gap through a multi-phased investigation of\ndeveloper interactions with an LLM-powered code editing and transformation\nfeature, Transform Code, in an IDE widely used at Google. First, we analyze\ntelemetry logs of the feature usage, revealing that frequent re-prompting can\nbe an indicator of developer struggles with using Transform Code. Second, we\nconduct a qualitative analysis of unsatisfactory requests, identifying five key\ncategories of information often missing from developer prompts. Finally, based\non these findings, we propose and evaluate a tool, AutoPrompter, for\nautomatically improving prompts by inferring missing information from the\nsurrounding code context, leading to a 27% improvement in edit correctness on\nour test set."}
{"id": "2504.20579", "pdf": "https://arxiv.org/pdf/2504.20579", "abs": "https://arxiv.org/abs/2504.20579", "authors": ["Praharsh Nanavati", "Ranjitha Prasad", "Karthikeyan Shanmugam"], "title": "Representation Learning Preserving Ignorability and Covariate Matching for Treatment Effects", "categories": ["cs.LG", "stat.ME"], "comment": null, "summary": "Estimating treatment effects from observational data is challenging due to\ntwo main reasons: (a) hidden confounding, and (b) covariate mismatch (control\nand treatment groups not having identical distributions). Long lines of works\nexist that address only either of these issues. To address the former,\nconventional techniques that require detailed knowledge in the form of causal\ngraphs have been proposed. For the latter, covariate matching and importance\nweighting methods have been used. Recently, there has been progress in\ncombining testable independencies with partial side information for tackling\nhidden confounding. A common framework to address both hidden confounding and\nselection bias is missing. We propose neural architectures that aim to learn a\nrepresentation of pre-treatment covariates that is a valid adjustment and also\nsatisfies covariate matching constraints. We combine two different neural\narchitectures: one based on gradient matching across domains created by\nsubsampling a suitable anchor variable that assumes causal side information,\nfollowed by the other, a covariate matching transformation. We prove that\napproximately invariant representations yield approximate valid adjustment sets\nwhich would enable an interval around the true causal effect. In contrast to\nusual sensitivity analysis, where an unknown nuisance parameter is varied, we\nhave a testable approximation yielding a bound on the effect estimate. We also\noutperform various baselines with respect to ATE and PEHE errors on causal\nbenchmarks that include IHDP, Jobs, Cattaneo, and an image-based Crowd\nManagement dataset."}
{"id": "2409.07394", "pdf": "https://arxiv.org/pdf/2409.07394", "abs": "https://arxiv.org/abs/2409.07394", "authors": ["Han Wang", "Archiki Prasad", "Elias Stengel-Eskin", "Mohit Bansal"], "title": "AdaCAD: Adaptively Decoding to Balance Conflicts between Contextual and Parametric Knowledge", "categories": ["cs.CL"], "comment": "NAACL 2025 (main conference), Code:\n  https://github.com/HanNight/AdaCAD", "summary": "Knowledge conflict arises from discrepancies between information in the\ncontext of a large language model (LLM) and the knowledge stored in its\nparameters. This can hurt performance when using standard decoding techniques,\nwhich tend to ignore the context. Existing test-time contrastive methods seek\nto address this by comparing the LLM's output distribution with and without the\ncontext and adjust the model according to the contrast between them. However,\nwe find that these methods frequently misjudge the degree of conflict and\nstruggle to handle instances that vary in their amount of conflict, with static\nmethods over-adjusting when conflict is absent. We propose a fine-grained,\ninstance-level approach called AdaCAD, which dynamically infers the weight of\nadjustment based on the degree of conflict, as measured by the Jensen-Shannon\ndivergence between distributions representing contextual and parametric\nknowledge. Across four LLMs, six question-answering (QA) and three\nsummarization datasets, we demonstrate that ADACAD consistently outperforms\nother decoding baselines with average QA accuracy gains of 14.21% (absolute)\nover a static contrastive baseline, and improves the factuality of summaries by\n6.19 (AlignScore). Lastly, we show that while contrastive baselines hurt\nperformance when conflict is absent, ADACAD mitigates these losses, making it\nmore applicable to real-world datasets in which some examples have conflict and\nothers do not."}
{"id": "2504.20800", "pdf": "https://arxiv.org/pdf/2504.20800", "abs": "https://arxiv.org/abs/2504.20800", "authors": ["Weizhen He", "Yunfeng Yan", "Shixiang Tang", "Yiheng Deng", "Yangyang Zhong", "Pengxin Luo", "Donglian Qi"], "title": "Adept: Annotation-Denoising Auxiliary Tasks with Discrete Cosine Transform Map and Keypoint for Human-Centric Pretraining", "categories": ["cs.CV"], "comment": null, "summary": "Human-centric perception is the core of diverse computer vision tasks and has\nbeen a long-standing research focus. However, previous research studied these\nhuman-centric tasks individually, whose performance is largely limited to the\nsize of the public task-specific datasets. Recent human-centric methods\nleverage the additional modalities, e.g., depth, to learn fine-grained semantic\ninformation, which limits the benefit of pretraining models due to their\nsensitivity to camera views and the scarcity of RGB-D data on the Internet.\nThis paper improves the data scalability of human-centric pretraining methods\nby discarding depth information and exploring semantic information of RGB\nimages in the frequency space by Discrete Cosine Transform (DCT). We further\npropose new annotation denoising auxiliary tasks with keypoints and DCT maps to\nenforce the RGB image extractor to learn fine-grained semantic information of\nhuman bodies. Our extensive experiments show that when pretrained on\nlarge-scale datasets (COCO and AIC datasets) without depth annotation, our\nmodel achieves better performance than state-of-the-art methods by +0.5 mAP on\nCOCO, +1.4 PCKh on MPII and -0.51 EPE on Human3.6M for pose estimation, by\n+4.50 mIoU on Human3.6M for human parsing, by -3.14 MAE on SHA and -0.07 MAE on\nSHB for crowd counting, by +1.1 F1 score on SHA and +0.8 F1 score on SHA for\ncrowd localization, and by +0.1 mAP on Market1501 and +0.8 mAP on MSMT for\nperson ReID. We also validate the effectiveness of our method on MPII+NTURGBD\ndatasets"}
{"id": "2504.20275", "pdf": "https://arxiv.org/pdf/2504.20275", "abs": "https://arxiv.org/abs/2504.20275", "authors": ["Mohammadhossein Homaei", "Victor Gonzalez Morales", "Oscar Mogollon Gutierrez", "Ruben Molano Gomez", "Andres Caro"], "title": "Smart Water Security with AI and Blockchain-Enhanced Digital Twins", "categories": ["cs.CR", "cs.AI", "cs.LG"], "comment": "8 Pages, 9 Figures", "summary": "Water distribution systems in rural areas face serious challenges such as a\nlack of real-time monitoring, vulnerability to cyberattacks, and unreliable\ndata handling. This paper presents an integrated framework that combines\nLoRaWAN-based data acquisition, a machine learning-driven Intrusion Detection\nSystem (IDS), and a blockchain-enabled Digital Twin (BC-DT) platform for secure\nand transparent water management. The IDS filters anomalous or spoofed data\nusing a Long Short-Term Memory (LSTM) Autoencoder and Isolation Forest before\nvalidated data is logged via smart contracts on a private Ethereum blockchain\nusing Proof of Authority (PoA) consensus. The verified data feeds into a\nreal-time DT model supporting leak detection, consumption forecasting, and\npredictive maintenance. Experimental results demonstrate that the system\nachieves over 80 transactions per second (TPS) with under 2 seconds of latency\nwhile remaining cost-effective and scalable for up to 1,000 smart meters. This\nwork demonstrates a practical and secure architecture for decentralized water\ninfrastructure in under-connected rural environments."}
{"id": "2504.20593", "pdf": "https://arxiv.org/pdf/2504.20593", "abs": "https://arxiv.org/abs/2504.20593", "authors": ["Rilind Sahitaj", "Paulius Sasnauskas", "Yiğit Yalın", "Debmalya Mandal", "Goran Radanović"], "title": "Independent Learning in Performative Markov Potential Games", "categories": ["cs.LG"], "comment": "AISTATS 2025, code available at\n  https://github.com/PauliusSasnauskas/performative-mpgs", "summary": "Performative Reinforcement Learning (PRL) refers to a scenario in which the\ndeployed policy changes the reward and transition dynamics of the underlying\nenvironment. In this work, we study multi-agent PRL by incorporating\nperformative effects into Markov Potential Games (MPGs). We introduce the\nnotion of a performatively stable equilibrium (PSE) and show that it always\nexists under a reasonable sensitivity assumption. We then provide convergence\nresults for state-of-the-art algorithms used to solve MPGs. Specifically, we\nshow that independent policy gradient ascent (IPGA) and independent natural\npolicy gradient (INPG) converge to an approximate PSE in the best-iterate\nsense, with an additional term that accounts for the performative effects.\nFurthermore, we show that INPG asymptotically converges to a PSE in the\nlast-iterate sense. As the performative effects vanish, we recover the\nconvergence rates from prior work. For a special case of our game, we provide\nfinite-time last-iterate convergence results for a repeated retraining\napproach, in which agents independently optimize a surrogate objective. We\nconduct extensive experiments to validate our theoretical findings."}
{"id": "2410.02102", "pdf": "https://arxiv.org/pdf/2410.02102", "abs": "https://arxiv.org/abs/2410.02102", "authors": ["Michael A. Lepori", "Michael C. Mozer", "Asma Ghandeharioun"], "title": "Racing Thoughts: Explaining Contextualization Errors in Large Language Models", "categories": ["cs.CL"], "comment": null, "summary": "The profound success of transformer-based language models can largely be\nattributed to their ability to integrate relevant contextual information from\nan input sequence in order to generate a response or complete a task. However,\nwe know very little about the algorithms that a model employs to implement this\ncapability, nor do we understand their failure modes. For example, given the\nprompt \"John is going fishing, so he walks over to the bank. Can he make an ATM\ntransaction?\", a model may incorrectly respond \"Yes\" if it has not properly\ncontextualized \"bank\" as a geographical feature, rather than a financial\ninstitution. We propose the LLM Race Conditions Hypothesis as an explanation of\ncontextualization errors of this form. This hypothesis identifies dependencies\nbetween tokens (e.g., \"bank\" must be properly contextualized before the final\ntoken, \"?\", integrates information from \"bank\"), and claims that\ncontextualization errors are a result of violating these dependencies. Using a\nvariety of techniques from mechanistic intepretability, we provide\ncorrelational and causal evidence in support of the hypothesis, and suggest\ninference-time interventions to address it."}
{"id": "2504.20829", "pdf": "https://arxiv.org/pdf/2504.20829", "abs": "https://arxiv.org/abs/2504.20829", "authors": ["Jiaxin Hong", "Sixu Chen", "Shuoyang Sun", "Hongyao Yu", "Hao Fang", "Yuqi Tan", "Bin Chen", "Shuhan Qi", "Jiawei Li"], "title": "GaussTrap: Stealthy Poisoning Attacks on 3D Gaussian Splatting for Targeted Scene Confusion", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "As 3D Gaussian Splatting (3DGS) emerges as a breakthrough in scene\nrepresentation and novel view synthesis, its rapid adoption in safety-critical\ndomains (e.g., autonomous systems, AR/VR) urgently demands scrutiny of\npotential security vulnerabilities. This paper presents the first systematic\nstudy of backdoor threats in 3DGS pipelines. We identify that adversaries may\nimplant backdoor views to induce malicious scene confusion during inference,\npotentially leading to environmental misperception in autonomous navigation or\nspatial distortion in immersive environments. To uncover this risk, we propose\nGuassTrap, a novel poisoning attack method targeting 3DGS models. GuassTrap\ninjects malicious views at specific attack viewpoints while preserving\nhigh-quality rendering in non-target views, ensuring minimal detectability and\nmaximizing potential harm. Specifically, the proposed method consists of a\nthree-stage pipeline (attack, stabilization, and normal training) to implant\nstealthy, viewpoint-consistent poisoned renderings in 3DGS, jointly optimizing\nattack efficacy and perceptual realism to expose security risks in 3D\nrendering. Extensive experiments on both synthetic and real-world datasets\ndemonstrate that GuassTrap can effectively embed imperceptible yet harmful\nbackdoor views while maintaining high-quality rendering in normal views,\nvalidating its robustness, adaptability, and practical applicability."}
{"id": "2504.20342", "pdf": "https://arxiv.org/pdf/2504.20342", "abs": "https://arxiv.org/abs/2504.20342", "authors": ["Shou-Tzu Han"], "title": "Narrative-Centered Emotional Reflection: Scaffolding Autonomous Emotional Literacy with AI", "categories": ["cs.HC", "cs.AI", "cs.CY", "H.5.2; H.1.2"], "comment": "10 pages, 5 figures, preliminary results, early-stage work intended\n  for future conference submission", "summary": "Reflexion is an AI-powered platform designed to enable structured emotional\nself-reflection at scale. By integrating real-time emotion detection, layered\nreflective prompting, and metaphorical storytelling generation, Reflexion\nempowers users to engage in autonomous emotional exploration beyond basic\nsentiment categorization. Grounded in theories of expressive writing, cognitive\nrestructuring, self-determination, and critical consciousness development, the\nsystem scaffolds a progressive journey from surface-level emotional recognition\ntoward value-aligned action planning. Initial pilot studies with diverse\nparticipants demonstrate positive outcomes in emotional articulation, cognitive\nreframing, and perceived psychological resilience. Reflexion represents a\npromising direction for scalable, theory-informed affective computing\ninterventions aimed at fostering emotional literacy and psychological growth\nacross educational, therapeutic, and public health contexts."}
{"id": "2504.20635", "pdf": "https://arxiv.org/pdf/2504.20635", "abs": "https://arxiv.org/abs/2504.20635", "authors": ["Bradley Segal", "Joshua Fieggen", "David Clifton", "Lei Clifton"], "title": "Bridging the Generalisation Gap: Synthetic Data Generation for Multi-Site Clinical Model Validation", "categories": ["cs.LG"], "comment": "7 pages, 4 figures", "summary": "Ensuring the generalisability of clinical machine learning (ML) models across\ndiverse healthcare settings remains a significant challenge due to variability\nin patient demographics, disease prevalence, and institutional practices.\nExisting model evaluation approaches often rely on real-world datasets, which\nare limited in availability, embed confounding biases, and lack the flexibility\nneeded for systematic experimentation. Furthermore, while generative models aim\nfor statistical realism, they often lack transparency and explicit control over\nfactors driving distributional shifts. In this work, we propose a novel\nstructured synthetic data framework designed for the controlled benchmarking of\nmodel robustness, fairness, and generalisability. Unlike approaches focused\nsolely on mimicking observed data, our framework provides explicit control over\nthe data generating process, including site-specific prevalence variations,\nhierarchical subgroup effects, and structured feature interactions. This\nenables targeted investigation into how models respond to specific\ndistributional shifts and potential biases. Through controlled experiments, we\ndemonstrate the framework's ability to isolate the impact of site variations,\nsupport fairness-aware audits, and reveal generalisation failures, particularly\nhighlighting how model complexity interacts with site-specific effects. This\nwork contributes a reproducible, interpretable, and configurable tool designed\nto advance the reliable deployment of ML in clinical settings."}
{"id": "2410.07103", "pdf": "https://arxiv.org/pdf/2410.07103", "abs": "https://arxiv.org/abs/2410.07103", "authors": ["Sangwon Yu", "Ik-hwan Kim", "Jongyoon Song", "Saehyung Lee", "Junsung Park", "Sungroh Yoon"], "title": "Unleashing Multi-Hop Reasoning Potential in Large Language Models through Repetition of Misordered Context", "categories": ["cs.CL"], "comment": "NAACL 2025 Findings", "summary": "Multi-hop reasoning, which requires multi-step reasoning based on the\nsupporting documents within a given context, remains challenging for large\nlanguage models (LLMs). LLMs often struggle to filter out irrelevant documents\nwithin the context, and their performance is sensitive to the absolute position\nof supporting documents within that context. In this paper, we identify an\nadditional challenge: LLMs' performance is also sensitive to the order,\nrelative position, in which the supporting documents are presented. We refer to\nthis as the misordered context problem. To address this issue, based on the\ntheoretical approach, we propose a simple yet effective method called context\nrepetition (CoRe), which involves prompting the model by repeatedly presenting\nthe context. This ensures that certain contiguous reasoning segments within\nsupporting documents are presented in the optimal order, effectively guiding\nthe model's reasoning in the appropriate direction. Applying CoRe, we improve\nthe F1 score by up to 30%p on multi-hop QA tasks and increase accuracy by up to\n70%p on a synthetic task. Additionally, CoRe helps mitigate the well-known\n\"lost-in-the-middle\" problem in LLMs and can be effectively combined with\nretrieval-based approaches utilizing Chain-of-Thought (CoT) reasoning."}
{"id": "2504.20830", "pdf": "https://arxiv.org/pdf/2504.20830", "abs": "https://arxiv.org/abs/2504.20830", "authors": ["Jianyu Wu", "Yizhou Wang", "Xiangyu Yue", "Xinzhu Ma", "Jingyang Guo", "Dongzhan Zhou", "Wanli Ouyang", "Shixiang Tang"], "title": "CMT: A Cascade MAR with Topology Predictor for Multimodal Conditional CAD Generation", "categories": ["cs.CV"], "comment": null, "summary": "While accurate and user-friendly Computer-Aided Design (CAD) is crucial for\nindustrial design and manufacturing, existing methods still struggle to achieve\nthis due to their over-simplified representations or architectures incapable of\nsupporting multimodal design requirements. In this paper, we attempt to tackle\nthis problem from both methods and datasets aspects. First, we propose a\ncascade MAR with topology predictor (CMT), the first multimodal framework for\nCAD generation based on Boundary Representation (B-Rep). Specifically, the\ncascade MAR can effectively capture the ``edge-counters-surface'' priors that\nare essential in B-Reps, while the topology predictor directly estimates\ntopology in B-Reps from the compact tokens in MAR. Second, to facilitate\nlarge-scale training, we develop a large-scale multimodal CAD dataset, mmABC,\nwhich includes over 1.3 million B-Rep models with multimodal annotations,\nincluding point clouds, text descriptions, and multi-view images. Extensive\nexperiments show the superior of CMT in both conditional and unconditional CAD\ngeneration tasks. For example, we improve Coverage and Valid ratio by +10.68%\nand +10.3%, respectively, compared to state-of-the-art methods on ABC in\nunconditional generation. CMT also improves +4.01 Chamfer on image conditioned\nCAD generation on mmABC. The dataset, code and pretrained network shall be\nreleased."}
{"id": "2504.20348", "pdf": "https://arxiv.org/pdf/2504.20348", "abs": "https://arxiv.org/abs/2504.20348", "authors": ["Varatheepan Paramanayakam", "Andreas Karatzas", "Iraklis Anagnostopoulos", "Dimitrios Stamoulis"], "title": "CarbonCall: Sustainability-Aware Function Calling for Large Language Models on Edge Devices", "categories": ["cs.PF", "cs.AI", "cs.SY", "eess.SY"], "comment": null, "summary": "Large Language Models (LLMs) enable real-time function calling in edge AI\nsystems but introduce significant computational overhead, leading to high power\nconsumption and carbon emissions. Existing methods optimize for performance\nwhile neglecting sustainability, making them inefficient for energy-constrained\nenvironments. We introduce CarbonCall, a sustainability-aware function-calling\nframework that integrates dynamic tool selection, carbon-aware execution, and\nquantized LLM adaptation. CarbonCall adjusts power thresholds based on\nreal-time carbon intensity forecasts and switches between model variants to\nsustain high tokens-per-second throughput under power constraints. Experiments\non an NVIDIA Jetson AGX Orin show that CarbonCall reduces carbon emissions by\nup to 52%, power consumption by 30%, and execution time by 30%, while\nmaintaining high efficiency."}
{"id": "2504.20642", "pdf": "https://arxiv.org/pdf/2504.20642", "abs": "https://arxiv.org/abs/2504.20642", "authors": ["Simon De Vos", "Jente Van Belle", "Andres Algaba", "Wouter Verbeke", "Sam Verboven"], "title": "Decision-centric fairness: Evaluation and optimization for resource allocation problems", "categories": ["cs.LG", "cs.CY"], "comment": null, "summary": "Data-driven decision support tools play an increasingly central role in\ndecision-making across various domains. In this work, we focus on binary\nclassification models for predicting positive-outcome scores and deciding on\nresource allocation, e.g., credit scores for granting loans or churn propensity\nscores for targeting customers with a retention campaign. Such models may\nexhibit discriminatory behavior toward specific demographic groups through\ntheir predicted scores, potentially leading to unfair resource allocation. We\nfocus on demographic parity as a fairness metric to compare the proportions of\ninstances that are selected based on their positive outcome scores across\ngroups. In this work, we propose a decision-centric fairness methodology that\ninduces fairness only within the decision-making region -- the range of\nrelevant decision thresholds on the score that may be used to decide on\nresource allocation -- as an alternative to a global fairness approach that\nseeks to enforce parity across the entire score distribution. By restricting\nthe induction of fairness to the decision-making region, the proposed\ndecision-centric approach avoids imposing overly restrictive constraints on the\nmodel, which may unnecessarily degrade the quality of the predicted scores. We\nempirically compare our approach to a global fairness approach on multiple\n(semi-synthetic) datasets to identify scenarios in which focusing on fairness\nwhere it truly matters, i.e., decision-centric fairness, proves beneficial."}
{"id": "2410.23463", "pdf": "https://arxiv.org/pdf/2410.23463", "abs": "https://arxiv.org/abs/2410.23463", "authors": ["Gabrielle Kaili-May Liu", "Bowen Shi", "Avi Caciularu", "Idan Szpektor", "Arman Cohan"], "title": "MDCure: A Scalable Pipeline for Multi-Document Instruction-Following", "categories": ["cs.CL", "cs.LG"], "comment": null, "summary": "Multi-document (MD) processing is crucial for LLMs to handle real-world tasks\nsuch as summarization and question-answering across large sets of documents.\nWhile LLMs have improved at processing long inputs, MD contexts still present\nunique difficulties, including management of inter-document dependencies,\nredundancy, and incoherent structures. To address this challenge, we introduce\nMDCure, a scalable and effective instruction data generation framework to\nenhance the MD capabilities of LLMs without the computational cost of\npre-training or reliance on human-annotated data. MDCure generates high-quality\nsynthetic MD instruction data over sets of articles via targeted prompts. We\nalso introduce MDCureRM, a cost-effective, MD-specific reward model to score\nand filter generated data based on their training utility for MD settings.\nMDCure is compatible with open- and closed-source models in addition to policy\noptimization methods such as PPO, enabling even small open-source models to\nsurpass proprietary LLMs as strong generators of high-quality MD instruction\ndata without further data filtering. With MDCure, we fine-tune a wide variety\nof LLMs up to 70B parameters in size from the FlanT5, Qwen2, and LLAMA3.1 model\nfamilies. Extensive evaluations on a wide range of MD and long-context\nbenchmarks spanning various tasks and domains show MDCure consistently improves\nperformance over pre-trained baselines and base models by up to 75.1%. Our\ncode, datasets, and models are available at https://github.com/yale-nlp/MDCure."}
{"id": "2504.20837", "pdf": "https://arxiv.org/pdf/2504.20837", "abs": "https://arxiv.org/abs/2504.20837", "authors": ["Julien Khlaut", "Elodie Ferreres", "Daniel Tordjman", "Hélène Philippe", "Tom Boeken", "Pierre Manceron", "Corentin Dancette"], "title": "RadSAM: Segmenting 3D radiological images with a 2D promptable model", "categories": ["cs.CV", "cs.AI", "cs.LG"], "comment": null, "summary": "Medical image segmentation is a crucial and time-consuming task in clinical\ncare, where mask precision is extremely important. The Segment Anything Model\n(SAM) offers a promising approach, as it provides an interactive interface\nbased on visual prompting and edition to refine an initial segmentation. This\nmodel has strong generalization capabilities, does not rely on predefined\nclasses, and adapts to diverse objects; however, it is pre-trained on natural\nimages and lacks the ability to process medical data effectively. In addition,\nthis model is built for 2D images, whereas a whole medical domain is based on\n3D images, such as CT and MRI. Recent adaptations of SAM for medical imaging\nare based on 2D models, thus requiring one prompt per slice to segment 3D\nobjects, making the segmentation process tedious. They also lack important\nfeatures such as editing. To bridge this gap, we propose RadSAM, a novel method\nfor segmenting 3D objects with a 2D model from a single prompt. In practice, we\ntrain a 2D model using noisy masks as initial prompts, in addition to bounding\nboxes and points. We then use this novel prompt type with an iterative\ninference pipeline to reconstruct the 3D mask slice-by-slice. We introduce a\nbenchmark to evaluate the model's ability to segment 3D objects in CT images\nfrom a single prompt and evaluate the models' out-of-domain transfer and\nedition capabilities. We demonstrate the effectiveness of our approach against\nstate-of-the-art models on this benchmark using the AMOS abdominal organ\nsegmentation dataset."}
{"id": "2504.20357", "pdf": "https://arxiv.org/pdf/2504.20357", "abs": "https://arxiv.org/abs/2504.20357", "authors": ["Jason Wang", "Basem Suleiman", "Muhammad Johan Alibasa"], "title": "Automated Unit Test Case Generation: A Systematic Literature Review", "categories": ["cs.SE", "cs.AI"], "comment": null, "summary": "Software is omnipresent within all factors of society. It is thus important\nto ensure that software are well tested to mitigate bad user experiences as\nwell as the potential for severe financial and human losses. Software testing\nis however expensive and absorbs valuable time and resources. As a result, the\nfield of automated software testing has grown of interest to researchers in\npast decades. In our review of present and past research papers, we have\nidentified an information gap in the areas of improvement for the Genetic\nAlgorithm and Particle Swarm Optimisation. A gap in knowledge in the current\nchallenges that face automated testing has also been identified. We therefore\npresent this systematic literature review in an effort to consolidate existing\nknowledge in regards to the evolutionary approaches as well as their\nimprovements and resulting limitations. These improvements include hybrid\nalgorithm combinations as well as interoperability with mutation testing and\nneural networks. We will also explore the main test criterion that are used in\nthese algorithms alongside the challenges currently faced in the field related\nto readability, mocking and more."}
{"id": "2504.20644", "pdf": "https://arxiv.org/pdf/2504.20644", "abs": "https://arxiv.org/abs/2504.20644", "authors": ["Ziqing Fan", "Siyuan Du", "Shengchao Hu", "Pingjie Wang", "Li Shen", "Ya Zhang", "Dacheng Tao", "Yanfeng Wang"], "title": "Combatting Dimensional Collapse in LLM Pre-Training Data via Diversified File Selection", "categories": ["cs.LG"], "comment": null, "summary": "Selecting high-quality pre-training data for large language models (LLMs) is\ncrucial for enhancing their overall performance under limited computation\nbudget, improving both training and sample efficiency. Recent advancements in\nfile selection primarily rely on using an existing or trained proxy model to\nassess the similarity of samples to a target domain, such as high quality\nsources BookCorpus and Wikipedia. However, upon revisiting these methods, the\ndomain-similarity selection criteria demonstrates a diversity dilemma,\ni.e.dimensional collapse in the feature space, improving performance on the\ndomain-related tasks but causing severe degradation on generic performance. To\nprevent collapse and enhance diversity, we propose a DiverSified File selection\nalgorithm (DiSF), which selects the most decorrelated text files in the feature\nspace. We approach this with a classical greedy algorithm to achieve more\nuniform eigenvalues in the feature covariance matrix of the selected texts,\nanalyzing its approximation to the optimal solution under a formulation of\n$\\gamma$-weakly submodular optimization problem. Empirically, we establish a\nbenchmark and conduct extensive experiments on the TinyLlama architecture with\nmodels from 120M to 1.1B parameters. Evaluating across nine tasks from the\nHarness framework, DiSF demonstrates a significant improvement on overall\nperformance. Specifically, DiSF saves 98.5% of 590M training files in\nSlimPajama, outperforming the full-data pre-training within a 50B training\nbudget, and achieving about 1.5x training efficiency and 5x data efficiency."}
{"id": "2410.24175", "pdf": "https://arxiv.org/pdf/2410.24175", "abs": "https://arxiv.org/abs/2410.24175", "authors": ["Yunjia Qi", "Hao Peng", "Xiaozhi Wang", "Bin Xu", "Lei Hou", "Juanzi Li"], "title": "Constraint Back-translation Improves Complex Instruction Following of Large Language Models", "categories": ["cs.CL", "cs.AI"], "comment": "14 pages, 6 figures", "summary": "Large language models (LLMs) struggle to follow instructions with complex\nconstraints in format, length, etc. Following the conventional\ninstruction-tuning practice, previous works conduct post-training on complex\ninstruction-response pairs generated by feeding complex instructions to\nadvanced LLMs. However, even advanced LLMs cannot follow complex instructions\nwell, thus limiting the quality of generated data. In this work, we find that\nexisting datasets inherently contain implicit complex constraints and propose a\nnovel data generation technique, constraint back-translation. Specifically, we\ntake the high-quality instruction-response pairs in existing datasets and only\nadopt advanced LLMs to add complex constraints already met by the responses to\nthe instructions, which naturally reduces costs and data noise. In the\nexperiments, we adopt Llama3-70B-Instruct to back-translate constraints and\ncreate a high-quality complex instruction-response dataset, named CRAB. We\npresent that post-training on CRAB improves multiple backbone LLMs' complex\ninstruction-following ability, evaluated on extensive instruction-following\nbenchmarks. We further find that constraint back-translation also serves as a\nuseful auxiliary training objective in post-training. Our code, data, and\nmodels will be released to facilitate future research."}
{"id": "2504.20860", "pdf": "https://arxiv.org/pdf/2504.20860", "abs": "https://arxiv.org/abs/2504.20860", "authors": ["Mainak Singha", "Subhankar Roy", "Sarthak Mehrotra", "Ankit Jha", "Moloud Abdar", "Biplab Banerjee", "Elisa Ricci"], "title": "FedMVP: Federated Multi-modal Visual Prompt Tuning for Vision-Language Models", "categories": ["cs.CV"], "comment": null, "summary": "Textual prompt tuning adapts Vision-Language Models (e.g., CLIP) in federated\nlearning by tuning lightweight input tokens (or prompts) on local client data,\nwhile keeping network weights frozen. Post training, only the prompts are\nshared by the clients with the central server for aggregation. However, textual\nprompt tuning often struggles with overfitting to known concepts and may be\noverly reliant on memorized text features, limiting its adaptability to unseen\nconcepts. To address this limitation, we propose Federated Multimodal Visual\nPrompt Tuning (FedMVP) that conditions the prompts on comprehensive contextual\ninformation -- image-conditioned features and textual attribute features of a\nclass -- that is multimodal in nature. At the core of FedMVP is a PromptFormer\nmodule that synergistically aligns textual and visual features through\ncross-attention, enabling richer contexual integration. The dynamically\ngenerated multimodal visual prompts are then input to the frozen vision encoder\nof CLIP, and trained with a combination of CLIP similarity loss and a\nconsistency loss. Extensive evaluation on 20 datasets spanning three\ngeneralization settings demonstrates that FedMVP not only preserves performance\non in-distribution classes and domains, but also displays higher\ngeneralizability to unseen classes and domains when compared to\nstate-of-the-art methods. Codes will be released upon acceptance."}
{"id": "2504.20412", "pdf": "https://arxiv.org/pdf/2504.20412", "abs": "https://arxiv.org/abs/2504.20412", "authors": ["Alex Mathai", "Chenxi Huang", "Suwei Ma", "Jihwan Kim", "Hailie Mitchell", "Aleksandr Nogikh", "Petros Maniatis", "Franjo Ivančić", "Junfeng Yang", "Baishakhi Ray"], "title": "CrashFixer: A crash resolution agent for the Linux kernel", "categories": ["cs.SE", "cs.AI", "cs.OS"], "comment": null, "summary": "Code large language models (LLMs) have shown impressive capabilities on a\nmultitude of software engineering tasks. In particular, they have demonstrated\nremarkable utility in the task of code repair. However, common benchmarks used\nto evaluate the performance of code LLMs are often limited to small-scale\nsettings. In this work, we build upon kGym, which shares a benchmark for\nsystem-level Linux kernel bugs and a platform to run experiments on the Linux\nkernel.\n  This paper introduces CrashFixer, the first LLM-based software repair agent\nthat is applicable to Linux kernel bugs. Inspired by the typical workflow of a\nkernel developer, we identify the key capabilities an expert developer\nleverages to resolve a kernel crash. Using this as our guide, we revisit the\nkGym platform and identify key system improvements needed to practically run\nLLM-based agents at the scale of the Linux kernel (50K files and 20M lines of\ncode). We implement these changes by extending kGym to create an improved\nplatform - called kGymSuite, which will be open-sourced. Finally, the paper\npresents an evaluation of various repair strategies for such complex kernel\nbugs and showcases the value of explicitly generating a hypothesis before\nattempting to fix bugs in complex systems such as the Linux kernel. We also\nevaluated CrashFixer's capabilities on still open bugs, and found at least two\npatch suggestions considered plausible to resolve the reported bug."}
{"id": "2504.20650", "pdf": "https://arxiv.org/pdf/2504.20650", "abs": "https://arxiv.org/abs/2504.20650", "authors": ["Adam Gudyś", "Cezary Maszczyk", "Joanna Badura", "Adam Grzelak", "Marek Sikora", "Łukasz Wróbel"], "title": "RuleKit 2: Faster and simpler rule learning", "categories": ["cs.LG"], "comment": "10 pages, 3 figures, 2 tables", "summary": "Rules offer an invaluable combination of predictive and descriptive\ncapabilities. Our package for rule-based data analysis, RuleKit, has proven its\neffectiveness in classification, regression, and survival problems. Here we\npresent its second version. New algorithms and optimized implementations of\nthose previously included, significantly improved the computational performance\nof our suite, reducing the analysis time of some data sets by two orders of\nmagnitude. The usability of RuleKit 2 is provided by two new components: Python\npackage and browser application with a graphical user interface. The former\ncomplies with scikit-learn, the most popular data mining library for Python,\nallowing RuleKit 2 to be straightforwardly integrated into existing data\nanalysis pipelines. RuleKit 2 is available at GitHub under GNU AGPL 3 license\n(https://github.com/adaa-polsl/RuleKit)"}
{"id": "2411.07127", "pdf": "https://arxiv.org/pdf/2411.07127", "abs": "https://arxiv.org/abs/2411.07127", "authors": ["Shengwei Xu", "Yuxuan Lu", "Grant Schoenebeck", "Yuqing Kong"], "title": "Benchmarking LLMs' Judgments with No Gold Standard", "categories": ["cs.CL", "cs.LG"], "comment": "The Thirteenth International Conference on Learning Representations\n  (ICLR 2025)", "summary": "We introduce the GEM (Generative Estimator for Mutual Information), an\nevaluation metric for assessing language generation by Large Language Models\n(LLMs), particularly in generating informative judgments, without the need for\na gold standard reference. GEM broadens the scenarios where we can benchmark\nLLM generation performance-from traditional ones, like machine translation and\nsummarization, where gold standard references are readily available, to\nsubjective tasks without clear gold standards, such as academic peer review.\n  GEM uses a generative model to estimate mutual information between candidate\nand reference responses, without requiring the reference to be a gold standard.\nIn experiments on a human-annotated dataset, GEM demonstrates competitive\ncorrelations with human scores compared to the state-of-the-art GPT-4o\nExaminer, and outperforms all other baselines. Additionally, GEM is more robust\nagainst strategic manipulations, such as rephrasing or elongation, which can\nartificially inflate scores under a GPT-4o Examiner.\n  We also present GRE-bench (Generating Review Evaluation Benchmark) which\nevaluates LLMs based on how well they can generate high-quality peer reviews\nfor academic research papers. Because GRE-bench is based upon GEM, it inherits\nits robustness properties. Additionally, GRE-bench circumvents data\ncontamination problems (or data leakage) by using the continuous influx of new\nopen-access research papers and peer reviews each year. We show GRE-bench\nresults of various popular LLMs on their peer review capabilities using the\nICLR2023 dataset."}
{"id": "2504.20865", "pdf": "https://arxiv.org/pdf/2504.20865", "abs": "https://arxiv.org/abs/2504.20865", "authors": ["Lorenzo Pellegrini", "Davide Cozzolino", "Serafino Pandolfini", "Davide Maltoni", "Matteo Ferrara", "Luisa Verdoliva", "Marco Prati", "Marco Ramilli"], "title": "AI-GenBench: A New Ongoing Benchmark for AI-Generated Image Detection", "categories": ["cs.CV"], "comment": "9 pages, 6 figures, 4 tables, code available:\n  https://github.com/MI-BioLab/AI-GenBench", "summary": "The rapid advancement of generative AI has revolutionized image creation,\nenabling high-quality synthesis from text prompts while raising critical\nchallenges for media authenticity. We present Ai-GenBench, a novel benchmark\ndesigned to address the urgent need for robust detection of AI-generated images\nin real-world scenarios. Unlike existing solutions that evaluate models on\nstatic datasets, Ai-GenBench introduces a temporal evaluation framework where\ndetection methods are incrementally trained on synthetic images, historically\nordered by their generative models, to test their ability to generalize to new\ngenerative models, such as the transition from GANs to diffusion models. Our\nbenchmark focuses on high-quality, diverse visual content and overcomes key\nlimitations of current approaches, including arbitrary dataset splits, unfair\ncomparisons, and excessive computational demands. Ai-GenBench provides a\ncomprehensive dataset, a standardized evaluation protocol, and accessible tools\nfor both researchers and non-experts (e.g., journalists, fact-checkers),\nensuring reproducibility while maintaining practical training requirements. By\nestablishing clear evaluation rules and controlled augmentation strategies,\nAi-GenBench enables meaningful comparison of detection methods and scalable\nsolutions. Code and data are publicly available to ensure reproducibility and\nto support the development of robust forensic detectors to keep pace with the\nrise of new synthetic generators."}
{"id": "2504.20434", "pdf": "https://arxiv.org/pdf/2504.20434", "abs": "https://arxiv.org/abs/2504.20434", "authors": ["Manish Bhattarai", "Miguel Cordova", "Javier Santos", "Dan O'Malley"], "title": "ARCS: Agentic Retrieval-Augmented Code Synthesis with Iterative Refinement", "categories": ["cs.SE", "cs.AI"], "comment": null, "summary": "In supercomputing, efficient and optimized code generation is essential to\nleverage high-performance systems effectively. We propose Agentic\nRetrieval-Augmented Code Synthesis (ARCS), an advanced framework for accurate,\nrobust, and efficient code generation, completion, and translation. ARCS\nintegrates Retrieval-Augmented Generation (RAG) with Chain-of-Thought (CoT)\nreasoning to systematically break down and iteratively refine complex\nprogramming tasks. An agent-based RAG mechanism retrieves relevant code\nsnippets, while real-time execution feedback drives the synthesis of candidate\nsolutions. This process is formalized as a state-action search tree\noptimization, balancing code correctness with editing efficiency. Evaluations\non the Geeks4Geeks and HumanEval benchmarks demonstrate that ARCS significantly\noutperforms traditional prompting methods in translation and generation\nquality. By enabling scalable and precise code synthesis, ARCS offers\ntransformative potential for automating and optimizing code development in\nsupercomputing applications, enhancing computational resource utilization."}
{"id": "2504.20656", "pdf": "https://arxiv.org/pdf/2504.20656", "abs": "https://arxiv.org/abs/2504.20656", "authors": ["Joshua Hatherley", "Anders Søgaard", "Angela Ballantyne", "Ruben Pauwels"], "title": "Federated learning, ethics, and the double black box problem in medical AI", "categories": ["cs.LG", "cs.AI", "cs.CY", "cs.HC"], "comment": null, "summary": "Federated learning (FL) is a machine learning approach that allows multiple\ndevices or institutions to collaboratively train a model without sharing their\nlocal data with a third-party. FL is considered a promising way to address\npatient privacy concerns in medical artificial intelligence. The ethical risks\nof medical FL systems themselves, however, have thus far been underexamined.\nThis paper aims to address this gap. We argue that medical FL presents a new\nvariety of opacity -- federation opacity -- that, in turn, generates a\ndistinctive double black box problem in healthcare AI. We highlight several\ninstances in which the anticipated benefits of medical FL may be exaggerated,\nand conclude by highlighting key challenges that must be overcome to make FL\nethically feasible in medicine."}
{"id": "2411.08243", "pdf": "https://arxiv.org/pdf/2411.08243", "abs": "https://arxiv.org/abs/2411.08243", "authors": ["Khaoula Chehbouni", "Jonathan Colaço Carr", "Yash More", "Jackie CK Cheung", "Golnoosh Farnadi"], "title": "Beyond the Safety Bundle: Auditing the Helpful and Harmless Dataset", "categories": ["cs.CL", "cs.CY"], "comment": "Prepared for conference submission", "summary": "In an effort to mitigate the harms of large language models (LLMs), learning\nfrom human feedback (LHF) has been used to steer LLMs towards outputs that are\nintended to be both less harmful and more helpful. Despite the widespread\nadoption of LHF in practice, the quality of this feedback and its effectiveness\nas a safety mitigation technique remain unclear. This study addresses these\nissues by auditing the widely-used Helpful and Harmless (HH) dataset by\nAnthropic. Our work includes: (1) a thorough investigation of the dataset's\ncontent through both manual and automated evaluation; (2) experiments\ndemonstrating the dataset's impact on models' safety; and (3) an analysis of\nthe 100 most influential papers citing this dataset. Through our audit, we\nshowcase how conceptualization failures and quality issues identified in the HH\ndataset can create additional harms by leading to disparate safety behaviors\nacross demographic groups. Our findings highlight the need for more nuanced,\ncontext-sensitive approaches to safety mitigation in LLMs."}
{"id": "2504.20872", "pdf": "https://arxiv.org/pdf/2504.20872", "abs": "https://arxiv.org/abs/2504.20872", "authors": ["Gilson Junior Soares", "Matheus Abrantes Cerqueira", "Jancarlo F. Gomes", "Laurent Najman", "Silvio Jamil F. Guimarães", "Alexandre Xavier Falcão"], "title": "FLIM-based Salient Object Detection Networks with Adaptive Decoders", "categories": ["cs.CV"], "comment": "This work has been submitted to the Journal of the Brazilian Computer\n  Society (JBCS)", "summary": "Salient Object Detection (SOD) methods can locate objects that stand out in\nan image, assign higher values to their pixels in a saliency map, and binarize\nthe map outputting a predicted segmentation mask. A recent tendency is to\ninvestigate pre-trained lightweight models rather than deep neural networks in\nSOD tasks, coping with applications under limited computational resources. In\nthis context, we have investigated lightweight networks using a methodology\nnamed Feature Learning from Image Markers (FLIM), which assumes that the\nencoder's kernels can be estimated from marker pixels on discriminative regions\nof a few representative images. This work proposes flyweight networks, hundreds\nof times lighter than lightweight models, for SOD by combining a FLIM encoder\nwith an adaptive decoder, whose weights are estimated for each input image by a\ngiven heuristic function. Such FLIM networks are trained from three to four\nrepresentative images only and without backpropagation, making the models\nsuitable for applications under labeled data constraints as well. We study five\nadaptive decoders; two of them are introduced here. Differently from the\nprevious ones that rely on one neuron per pixel with shared weights, the\nheuristic functions of the new adaptive decoders estimate the weights of each\nneuron per pixel. We compare FLIM models with adaptive decoders for two\nchallenging SOD tasks with three lightweight networks from the\nstate-of-the-art, two FLIM networks with decoders trained by backpropagation,\nand one FLIM network whose labeled markers define the decoder's weights. The\nexperiments demonstrate the advantages of the proposed networks over the\nbaselines, revealing the importance of further investigating such methods in\nnew applications."}
{"id": "2504.20452", "pdf": "https://arxiv.org/pdf/2504.20452", "abs": "https://arxiv.org/abs/2504.20452", "authors": ["Hai-Dang Kieu", "Delvin Ce Zhang", "Minh Duc Nguyen", "Min Xu", "Qiang Wu", "Dung D. Le"], "title": "Enhancing News Recommendation with Hierarchical LLM Prompting", "categories": ["cs.IR", "cs.AI"], "comment": null, "summary": "Personalized news recommendation systems often struggle to effectively\ncapture the complexity of user preferences, as they rely heavily on shallow\nrepresentations, such as article titles and abstracts. To address this problem,\nwe introduce a novel method, namely PNR-LLM, for Large Language Models for\nPersonalized News Recommendation. Specifically, PNR-LLM harnesses the\ngeneration capabilities of LLMs to enrich news titles and abstracts, and\nconsequently improves recommendation quality. PNR-LLM contains a novel module,\nNews Enrichment via LLMs, which generates deeper semantic information and\nrelevant entities from articles, transforming shallow contents into richer\nrepresentations. We further propose an attention mechanism to aggregate\nenriched semantic- and entity-level data, forming unified user and news\nembeddings that reveal a more accurate user-news match. Extensive experiments\non MIND datasets show that PNR-LLM outperforms state-of-the-art baselines.\nMoreover, the proposed data enrichment module is model-agnostic, and we\nempirically show that applying our proposed module to multiple existing models\ncan further improve their performance, verifying the advantage of our design."}
{"id": "2504.20660", "pdf": "https://arxiv.org/pdf/2504.20660", "abs": "https://arxiv.org/abs/2504.20660", "authors": ["Sahil Tomar", "Shamshe Alam", "Sandeep Kumar", "Amit Mathur"], "title": "Quantum-Enhanced Hybrid Reinforcement Learning Framework for Dynamic Path Planning in Autonomous Systems", "categories": ["cs.LG", "cs.ET", "cs.IT", "math.IT"], "comment": "16 pages", "summary": "In this paper, a novel quantum classical hybrid framework is proposed that\nsynergizes quantum with Classical Reinforcement Learning. By leveraging the\ninherent parallelism of quantum computing, the proposed approach generates\nrobust Q tables and specialized turn cost estimations, which are then\nintegrated with a classical Reinforcement Learning pipeline. The Classical\nQuantum fusion results in rapid convergence of training, reducing the training\ntime significantly and improved adaptability in scenarios featuring static,\ndynamic, and moving obstacles. Simulator based evaluations demonstrate\nsignificant enhancements in path efficiency, trajectory smoothness, and mission\nsuccess rates, underscoring the potential of framework for real time,\nautonomous navigation in complex and unpredictable environments. Furthermore,\nthe proposed framework was tested beyond simulations on practical scenarios,\nincluding real world map data such as the IIT Delhi campus, reinforcing its\npotential for real time, autonomous navigation in complex and unpredictable\nenvironments."}
{"id": "2411.09694", "pdf": "https://arxiv.org/pdf/2411.09694", "abs": "https://arxiv.org/abs/2411.09694", "authors": ["Julius Cheng", "Maike Züfle", "Vilém Zouhar", "Andreas Vlachos"], "title": "A Bayesian Optimization Approach to Machine Translation Reranking", "categories": ["cs.CL"], "comment": "NAACL 2025 camera ready", "summary": "Reranking a list of candidates from a machine translation system with an\nexternal scoring model and returning the highest-scoring candidate remains a\nsimple and effective method for improving the overall output quality.\nTranslation scoring models continue to grow in size, with the best models being\ncomparable to generation models. Thus, reranking can add substantial\ncomputational cost to the translation pipeline. In this work, we pose reranking\nas a Bayesian optimization (BayesOpt) problem. By strategically selecting\ncandidates to score based on a balance of exploration and exploitation, we show\nthat it is possible to find top-scoring candidates when scoring only a fraction\nof the candidate list. For instance, our method achieves the same CometKiwi\nscore using only 70 scoring evaluations compared a baseline system using 180.\nWe present a multi-fidelity setting for BayesOpt, where the candidates are\nfirst scored with a cheaper but noisier proxy scoring model, which further\nimproves the cost-performance tradeoff when using smaller but well-trained\ndistilled proxy scorers."}
{"id": "2504.20902", "pdf": "https://arxiv.org/pdf/2504.20902", "abs": "https://arxiv.org/abs/2504.20902", "authors": ["Quentin Guimard", "Moreno D'Incà", "Massimiliano Mancini", "Elisa Ricci"], "title": "Classifier-to-Bias: Toward Unsupervised Automatic Bias Detection for Visual Classifiers", "categories": ["cs.CV", "cs.AI", "cs.LG"], "comment": "CVPR 2025. Code: https://github.com/mardgui/C2B", "summary": "A person downloading a pre-trained model from the web should be aware of its\nbiases. Existing approaches for bias identification rely on datasets containing\nlabels for the task of interest, something that a non-expert may not have\naccess to, or may not have the necessary resources to collect: this greatly\nlimits the number of tasks where model biases can be identified. In this work,\nwe present Classifier-to-Bias (C2B), the first bias discovery framework that\nworks without access to any labeled data: it only relies on a textual\ndescription of the classification task to identify biases in the target\nclassification model. This description is fed to a large language model to\ngenerate bias proposals and corresponding captions depicting biases together\nwith task-specific target labels. A retrieval model collects images for those\ncaptions, which are then used to assess the accuracy of the model w.r.t. the\ngiven biases. C2B is training-free, does not require any annotations, has no\nconstraints on the list of biases, and can be applied to any pre-trained model\non any classification task. Experiments on two publicly available datasets show\nthat C2B discovers biases beyond those of the original datasets and outperforms\na recent state-of-the-art bias detection baseline that relies on task-specific\nannotations, being a promising first step toward addressing task-agnostic\nunsupervised bias detection."}
{"id": "2504.20493", "pdf": "https://arxiv.org/pdf/2504.20493", "abs": "https://arxiv.org/abs/2504.20493", "authors": ["Yu Cui", "Yujun Cai", "Yiwei Wang"], "title": "Token-Efficient Prompt Injection Attack: Provoking Cessation in LLM Reasoning via Adaptive Token Compression", "categories": ["cs.CR", "cs.AI"], "comment": null, "summary": "While reasoning large language models (LLMs) demonstrate remarkable\nperformance across various tasks, they also contain notable security\nvulnerabilities. Recent research has uncovered a \"thinking-stopped\"\nvulnerability in DeepSeek-R1, where model-generated reasoning tokens can\nforcibly interrupt the inference process, resulting in empty responses that\ncompromise LLM-integrated applications. However, existing methods triggering\nthis vulnerability require complex mathematical word problems with long\nprompts--even exceeding 5,000 tokens. To reduce the token cost and formally\ndefine this vulnerability, we propose a novel prompt injection attack named\n\"Reasoning Interruption Attack\", based on adaptive token compression. We\ndemonstrate that simple standalone arithmetic tasks can effectively trigger\nthis vulnerability, and the prompts based on such tasks exhibit simpler logical\nstructures than mathematical word problems. We develop a systematic approach to\nefficiently collect attack prompts and an adaptive token compression framework\nthat utilizes LLMs to automatically compress these prompts. Experiments show\nour compression framework significantly reduces prompt length while maintaining\neffective attack capabilities. We further investigate the attack's performance\nvia output prefix and analyze the underlying causes of the vulnerability,\nproviding valuable insights for improving security in reasoning LLMs."}
{"id": "2504.20666", "pdf": "https://arxiv.org/pdf/2504.20666", "abs": "https://arxiv.org/abs/2504.20666", "authors": ["Zhonghao Li", "Ji Shi", "Xinming Zhang", "Miao Zhang", "Bo Li"], "title": "SFi-Former: Sparse Flow Induced Attention for Graph Transformer", "categories": ["cs.LG"], "comment": "ICMR 2025", "summary": "Graph Transformers (GTs) have demonstrated superior performance compared to\ntraditional message-passing graph neural networks in many studies, especially\nin processing graph data with long-range dependencies. However, GTs tend to\nsuffer from weak inductive bias, overfitting and over-globalizing problems due\nto the dense attention. In this paper, we introduce SFi-attention, a novel\nattention mechanism designed to learn sparse pattern by minimizing an energy\nfunction based on network flows with l1-norm regularization, to relieve those\nissues caused by dense attention. Furthermore, SFi-Former is accordingly\ndevised which can leverage the sparse attention pattern of SFi-attention to\ngenerate sparse network flows beyond adjacency matrix of graph data.\nSpecifically, SFi-Former aggregates features selectively from other nodes\nthrough flexible adaptation of the sparse attention, leading to a more robust\nmodel. We validate our SFi-Former on various graph datasets, especially those\ngraph data exhibiting long-range dependencies. Experimental results show that\nour SFi-Former obtains competitive performance on GNN Benchmark datasets and\nSOTA performance on LongRange Graph Benchmark (LRGB) datasets. Additionally,\nour model gives rise to smaller generalization gaps, which indicates that it is\nless prone to over-fitting. Click here for codes."}
{"id": "2502.01563", "pdf": "https://arxiv.org/pdf/2502.01563", "abs": "https://arxiv.org/abs/2502.01563", "authors": ["Mingyu Jin", "Kai Mei", "Wujiang Xu", "Mingjie Sun", "Ruixiang Tang", "Mengnan Du", "Zirui Liu", "Yongfeng Zhang"], "title": "Massive Values in Self-Attention Modules are the Key to Contextual Knowledge Understanding", "categories": ["cs.CL"], "comment": null, "summary": "Large language models (LLMs) have achieved remarkable success in contextual\nknowledge understanding. In this paper, we show that these concentrated massive\nvalues consistently emerge in specific regions of attention queries (Q) and\nkeys (K) while not having such patterns in values (V) in various modern\ntransformer-based LLMs (Q, K, and V mean the representations output by the\nquery, key, and value layers respectively). Through extensive experiments, we\nfurther demonstrate that these massive values play a critical role in\ninterpreting contextual knowledge (knowledge obtained from the current context\nwindow) rather than in retrieving parametric knowledge stored within the\nmodel's parameters. Our further investigation of quantization strategies\nreveals that ignoring these massive values leads to a pronounced drop in\nperformance on tasks requiring rich contextual understanding, aligning with our\nanalysis. Finally, we trace the emergence of concentrated massive values and\nfind that such concentration is caused by Rotary Positional Encoding (RoPE),\nwhich has appeared since the first layers. These findings shed new light on how\nQ and K operate in LLMs and offer practical insights for model design and\noptimization. The Code is Available at\nhttps://github.com/MingyuJ666/Rope_with_LLM."}
{"id": "2504.20948", "pdf": "https://arxiv.org/pdf/2504.20948", "abs": "https://arxiv.org/abs/2504.20948", "authors": ["Yanghui Song", "Chengfu Yang"], "title": "DS_FusionNet: Dynamic Dual-Stream Fusion with Bidirectional Knowledge Distillation for Plant Disease Recognition", "categories": ["cs.CV"], "comment": null, "summary": "Given the severe challenges confronting the global growth security of\neconomic crops, precise identification and prevention of plant diseases has\nemerged as a critical issue in artificial intelligence-enabled agricultural\ntechnology. To address the technical challenges in plant disease recognition,\nincluding small-sample learning, leaf occlusion, illumination variations, and\nhigh inter-class similarity, this study innovatively proposes a Dynamic\nDual-Stream Fusion Network (DS_FusionNet). The network integrates a\ndual-backbone architecture, deformable dynamic fusion modules, and\nbidirectional knowledge distillation strategy, significantly enhancing\nrecognition accuracy. Experimental results demonstrate that DS_FusionNet\nachieves classification accuracies exceeding 90% using only 10% of the\nPlantDisease and CIFAR-10 datasets, while maintaining 85% accuracy on the\ncomplex PlantWild dataset, exhibiting exceptional generalization capabilities.\nThis research not only provides novel technical insights for fine-grained image\nclassification but also establishes a robust foundation for precise\nidentification and management of agricultural diseases."}
{"id": "2504.20520", "pdf": "https://arxiv.org/pdf/2504.20520", "abs": "https://arxiv.org/abs/2504.20520", "authors": ["Haowen Sun", "Han Wang", "Chengzhong Ma", "Shaolong Zhang", "Jiawei Ye", "Xingyu Chen", "Xuguang Lan"], "title": "PRISM: Projection-based Reward Integration for Scene-Aware Real-to-Sim-to-Real Transfer with Few Demonstrations", "categories": ["cs.RO", "cs.AI"], "comment": null, "summary": "Learning from few demonstrations to develop policies robust to variations in\nrobot initial positions and object poses is a problem of significant practical\ninterest in robotics. Compared to imitation learning, which often struggles to\ngeneralize from limited samples, reinforcement learning (RL) can autonomously\nexplore to obtain robust behaviors. Training RL agents through direct\ninteraction with the real world is often impractical and unsafe, while building\nsimulation environments requires extensive manual effort, such as designing\nscenes and crafting task-specific reward functions. To address these\nchallenges, we propose an integrated real-to-sim-to-real pipeline that\nconstructs simulation environments based on expert demonstrations by\nidentifying scene objects from images and retrieving their corresponding 3D\nmodels from existing libraries. We introduce a projection-based reward model\nfor RL policy training that is supervised by a vision-language model (VLM)\nusing human-guided object projection relationships as prompts, with the policy\nfurther fine-tuned using expert demonstrations. In general, our work focuses on\nthe construction of simulation environments and RL-based policy training,\nultimately enabling the deployment of reliable robotic control policies in\nreal-world scenarios."}
{"id": "2504.20667", "pdf": "https://arxiv.org/pdf/2504.20667", "abs": "https://arxiv.org/abs/2504.20667", "authors": ["Simone Piaggesi", "Riccardo Guidotti", "Fosca Giannotti", "Dino Pedreschi"], "title": "Explanations Go Linear: Interpretable and Individual Latent Encoding for Post-hoc Explainability", "categories": ["cs.LG"], "comment": null, "summary": "Post-hoc explainability is essential for understanding black-box machine\nlearning models. Surrogate-based techniques are widely used for local and\nglobal model-agnostic explanations but have significant limitations. Local\nsurrogates capture non-linearities but are computationally expensive and\nsensitive to parameters, while global surrogates are more efficient but\nstruggle with complex local behaviors. In this paper, we present ILLUME, a\nflexible and interpretable framework grounded in representation learning, that\ncan be integrated with various surrogate models to provide explanations for any\nblack-box classifier. Specifically, our approach combines a globally trained\nsurrogate with instance-specific linear transformations learned with a\nmeta-encoder to generate both local and global explanations. Through extensive\nempirical evaluations, we demonstrate the effectiveness of ILLUME in producing\nfeature attributions and decision rules that are not only accurate but also\nrobust and faithful to the black-box, thus providing a unified explanation\nframework that effectively addresses the limitations of traditional surrogate\nmethods."}
{"id": "2502.12836", "pdf": "https://arxiv.org/pdf/2502.12836", "abs": "https://arxiv.org/abs/2502.12836", "authors": ["Mohammad Feli", "Iman Azimi", "Pasi Liljeberg", "Amir M. Rahmani"], "title": "An LLM-Powered Agent for Physiological Data Analysis: A Case Study on PPG-based Heart Rate Estimation", "categories": ["cs.CL"], "comment": null, "summary": "Large language models (LLMs) are revolutionizing healthcare by improving\ndiagnosis, patient care, and decision support through interactive\ncommunication. More recently, they have been applied to analyzing physiological\ntime-series like wearable data for health insight extraction. Existing methods\nembed raw numerical sequences directly into prompts, which exceeds token limits\nand increases computational costs. Additionally, some studies integrated\nfeatures extracted from time-series in textual prompts or applied multimodal\napproaches. However, these methods often produce generic and unreliable outputs\ndue to LLMs' limited analytical rigor and inefficiency in interpreting\ncontinuous waveforms. In this paper, we develop an LLM-powered agent for\nphysiological time-series analysis aimed to bridge the gap in integrating LLMs\nwith well-established analytical tools. Built on the OpenCHA, an open-source\nLLM-powered framework, our agent powered by OpenAI's GPT-3.5-turbo model\nfeatures an orchestrator that integrates user interaction, data sources, and\nanalytical tools to generate accurate health insights. To evaluate its\neffectiveness, we implement a case study on heart rate (HR) estimation from\nPhotoplethysmogram (PPG) signals using a dataset of PPG and Electrocardiogram\n(ECG) recordings in a remote health monitoring study. The agent's performance\nis benchmarked against OpenAI GPT-4o-mini and GPT-4o, with ECG serving as the\ngold standard for HR estimation. Results demonstrate that our agent\nsignificantly outperforms benchmark models by achieving lower error rates and\nmore reliable HR estimations. The agent implementation is publicly available on\nGitHub."}
{"id": "2504.20970", "pdf": "https://arxiv.org/pdf/2504.20970", "abs": "https://arxiv.org/abs/2504.20970", "authors": ["Mete Erdogan", "Sebnem Demirtas"], "title": "SVD Based Least Squares for X-Ray Pneumonia Classification Using Deep Features", "categories": ["cs.CV", "cs.AI", "cs.LG"], "comment": "Preprint submitted to IEEE International Workshop on Machine Learning\n  for Signal Processing (MLSP), 2025", "summary": "Accurate and early diagnosis of pneumonia through X-ray imaging is essential\nfor effective treatment and improved patient outcomes. Recent advancements in\nmachine learning have enabled automated diagnostic tools that assist\nradiologists in making more reliable and efficient decisions. In this work, we\npropose a Singular Value Decomposition-based Least Squares (SVD-LS) framework\nfor multi-class pneumonia classification, leveraging powerful feature\nrepresentations from state-of-the-art self-supervised and transfer learning\nmodels. Rather than relying on computationally expensive gradient based\nfine-tuning, we employ a closed-form, non-iterative classification approach\nthat ensures efficiency without compromising accuracy. Experimental results\ndemonstrate that SVD-LS achieves competitive performance while offering\nsignificantly reduced computational costs, making it a viable alternative for\nreal-time medical imaging applications."}
{"id": "2504.20560", "pdf": "https://arxiv.org/pdf/2504.20560", "abs": "https://arxiv.org/abs/2504.20560", "authors": ["Francisco Sedeño", "Jamal Toutouh", "Francisco Chicano"], "title": "Generate more than one child in your co-evolutionary semi-supervised learning GAN", "categories": ["cs.NE", "cs.AI", "cs.LG"], "comment": "Submitted to The Leading European Event on Bio-Inspired AI (EvoStar\n  2025)", "summary": "Generative Adversarial Networks (GANs) are very useful methods to address\nsemi-supervised learning (SSL) datasets, thanks to their ability to generate\nsamples similar to real data. This approach, called SSL-GAN has attracted many\nresearchers in the last decade. Evolutionary algorithms have been used to guide\nthe evolution and training of SSL-GANs with great success. In particular,\nseveral co-evolutionary approaches have been applied where the two networks of\na GAN (the generator and the discriminator) are evolved in separate\npopulations. The co-evolutionary approaches published to date assume some\nspatial structure of the populations, based on the ideas of cellular\nevolutionary algorithms. They also create one single individual per generation\nand follow a generational replacement strategy in the evolution. In this paper,\nwe re-consider those algorithmic design decisions and propose a new\nco-evolutionary approach, called Co-evolutionary Elitist SSL-GAN (CE-SSLGAN),\nwith panmictic population, elitist replacement, and more than one individual in\nthe offspring. We evaluate the performance of our proposed method using three\nstandard benchmark datasets. The results show that creating more than one\noffspring per population and using elitism improves the results in comparison\nwith a classical SSL-GAN."}
{"id": "2504.20687", "pdf": "https://arxiv.org/pdf/2504.20687", "abs": "https://arxiv.org/abs/2504.20687", "authors": ["Jan Kapar", "Niklas Koenen", "Martin Jullum"], "title": "What's Wrong with Your Synthetic Tabular Data? Using Explainable AI to Evaluate Generative Models", "categories": ["cs.LG", "stat.ML"], "comment": "This is the accepted, post peer-reviewed version of the manuscript,\n  accepted for publication in the proceedings after the Third World Conference\n  on eXplainable Artificial Intelligence, XAI-2025. A link to the version of\n  record will be included here upon publication", "summary": "Evaluating synthetic tabular data is challenging, since they can differ from\nthe real data in so many ways. There exist numerous metrics of synthetic data\nquality, ranging from statistical distances to predictive performance, often\nproviding conflicting results. Moreover, they fail to explain or pinpoint the\nspecific weaknesses in the synthetic data. To address this, we apply\nexplainable AI (XAI) techniques to a binary detection classifier trained to\ndistinguish real from synthetic data. While the classifier identifies\ndistributional differences, XAI concepts such as feature importance and feature\neffects, analyzed through methods like permutation feature importance, partial\ndependence plots, Shapley values and counterfactual explanations, reveal why\nsynthetic data are distinguishable, highlighting inconsistencies, unrealistic\ndependencies, or missing patterns. This interpretability increases transparency\nin synthetic data evaluation and provides deeper insights beyond conventional\nmetrics, helping diagnose and improve synthetic data quality. We apply our\napproach to two tabular datasets and generative models, showing that it\nuncovers issues overlooked by standard evaluation techniques."}
{"id": "2502.17163", "pdf": "https://arxiv.org/pdf/2502.17163", "abs": "https://arxiv.org/abs/2502.17163", "authors": ["María Andrea Cruz Blandón", "Jayasimha Talur", "Bruno Charron", "Dong Liu", "Saab Mansour", "Marcello Federico"], "title": "MEMERAG: A Multilingual End-to-End Meta-Evaluation Benchmark for Retrieval Augmented Generation", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Automatic evaluation of retrieval augmented generation (RAG) systems relies\non fine-grained dimensions like faithfulness and relevance, as judged by expert\nhuman annotators. Meta-evaluation benchmarks support the development of\nautomatic evaluators that correlate well with human judgement. However,\nexisting benchmarks predominantly focus on English or use translated data,\nwhich fails to capture cultural nuances. A native approach provides a better\nrepresentation of the end user experience.\n  In this work, we develop a Multilingual End-to-end Meta-Evaluation RAG\nbenchmark (MEMERAG). Our benchmark builds on the popular MIRACL dataset, using\nnative-language questions and generating responses with diverse large language\nmodels (LLMs), which are then assessed by expert annotators for faithfulness\nand relevance. We describe our annotation process and show that it achieves\nhigh inter-annotator agreement. We then analyse the performance of the\nanswer-generating LLMs across languages as per the human evaluators. Finally we\napply the dataset to our main use-case which is to benchmark multilingual\nautomatic evaluators (LLM-as-a-judge). We show that our benchmark can reliably\nidentify improvements offered by advanced prompting techniques and LLMs. Our\ndataset is available at https://github.com/amazon-science/MEMERAG"}
{"id": "2504.20995", "pdf": "https://arxiv.org/pdf/2504.20995", "abs": "https://arxiv.org/abs/2504.20995", "authors": ["Haoyu Zhen", "Qiao Sun", "Hongxin Zhang", "Junyan Li", "Siyuan Zhou", "Yilun Du", "Chuang Gan"], "title": "TesserAct: Learning 4D Embodied World Models", "categories": ["cs.CV", "cs.RO"], "comment": "Project Page: https://tesseractworld.github.io/", "summary": "This paper presents an effective approach for learning novel 4D embodied\nworld models, which predict the dynamic evolution of 3D scenes over time in\nresponse to an embodied agent's actions, providing both spatial and temporal\nconsistency. We propose to learn a 4D world model by training on RGB-DN (RGB,\nDepth, and Normal) videos. This not only surpasses traditional 2D models by\nincorporating detailed shape, configuration, and temporal changes into their\npredictions, but also allows us to effectively learn accurate inverse dynamic\nmodels for an embodied agent. Specifically, we first extend existing robotic\nmanipulation video datasets with depth and normal information leveraging\noff-the-shelf models. Next, we fine-tune a video generation model on this\nannotated dataset, which jointly predicts RGB-DN (RGB, Depth, and Normal) for\neach frame. We then present an algorithm to directly convert generated RGB,\nDepth, and Normal videos into a high-quality 4D scene of the world. Our method\nensures temporal and spatial coherence in 4D scene predictions from embodied\nscenarios, enables novel view synthesis for embodied environments, and\nfacilitates policy learning that significantly outperforms those derived from\nprior video-based world models."}
{"id": "2504.20610", "pdf": "https://arxiv.org/pdf/2504.20610", "abs": "https://arxiv.org/abs/2504.20610", "authors": ["Michele Garetto", "Alessandro Cornacchia", "Franco Galante", "Emilio Leonardi", "Alessandro Nordio", "Alberto Tarable"], "title": "Information Retrieval in the Age of Generative AI: The RGB Model", "categories": ["cs.IR", "cs.AI", "cs.PF"], "comment": "To be presented at ACM SIGIR 25", "summary": "The advent of Large Language Models (LLMs) and generative AI is fundamentally\ntransforming information retrieval and processing on the Internet, bringing\nboth great potential and significant concerns regarding content authenticity\nand reliability. This paper presents a novel quantitative approach to shed\nlight on the complex information dynamics arising from the growing use of\ngenerative AI tools. Despite their significant impact on the digital ecosystem,\nthese dynamics remain largely uncharted and poorly understood. We propose a\nstochastic model to characterize the generation, indexing, and dissemination of\ninformation in response to new topics. This scenario particularly challenges\ncurrent LLMs, which often rely on real-time Retrieval-Augmented Generation\n(RAG) techniques to overcome their static knowledge limitations. Our findings\nsuggest that the rapid pace of generative AI adoption, combined with increasing\nuser reliance, can outpace human verification, escalating the risk of\ninaccurate information proliferation across digital resources. An in-depth\nanalysis of Stack Exchange data confirms that high-quality answers inevitably\nrequire substantial time and human effort to emerge. This underscores the\nconsiderable risks associated with generating persuasive text in response to\nnew questions and highlights the critical need for responsible development and\ndeployment of future generative AI tools."}
{"id": "2504.20733", "pdf": "https://arxiv.org/pdf/2504.20733", "abs": "https://arxiv.org/abs/2504.20733", "authors": ["Simon Klüttermann", "Tim Katzke", "Emmanuel Müller"], "title": "Unsupervised Surrogate Anomaly Detection", "categories": ["cs.LG", "cs.AI"], "comment": "13 pages + references and appendix = 35 pages", "summary": "In this paper, we study unsupervised anomaly detection algorithms that learn\na neural network representation, i.e. regular patterns of normal data, which\nanomalies are deviating from. Inspired by a similar concept in engineering, we\nrefer to our methodology as surrogate anomaly detection. We formalize the\nconcept of surrogate anomaly detection into a set of axioms required for\noptimal surrogate models and propose a new algorithm, named DEAN (Deep Ensemble\nANomaly detection), designed to fulfill these criteria. We evaluate DEAN on 121\nbenchmark datasets, demonstrating its competitive performance against 19\nexisting methods, as well as the scalability and reliability of our method."}
{"id": "2503.15358", "pdf": "https://arxiv.org/pdf/2503.15358", "abs": "https://arxiv.org/abs/2503.15358", "authors": ["Thomas Pickard", "Aline Villavicencio", "Maggie Mi", "Wei He", "Dylan Phelps", "Marco Idiart"], "title": "SemEval-2025 Task 1: AdMIRe -- Advancing Multimodal Idiomaticity Representation", "categories": ["cs.CL", "cs.CV", "I.2.7; I.4.m"], "comment": "Author accepted version; SemEval-2025 proceedings to appear at ACL\n  2025", "summary": "Idiomatic expressions present a unique challenge in NLP, as their meanings\nare often not directly inferable from their constituent words. Despite recent\nadvancements in Large Language Models (LLMs), idiomaticity remains a\nsignificant obstacle to robust semantic representation. We present datasets and\ntasks for SemEval-2025 Task 1: AdMiRe (Advancing Multimodal Idiomaticity\nRepresentation), which challenges the community to assess and improve models'\nability to interpret idiomatic expressions in multimodal contexts and in\nmultiple languages. Participants competed in two subtasks: ranking images based\non their alignment with idiomatic or literal meanings, and predicting the next\nimage in a sequence. The most effective methods achieved human-level\nperformance by leveraging pretrained LLMs and vision-language models in\nmixture-of-experts settings, with multiple queries used to smooth over the\nweaknesses in these models' representations of idiomaticity."}
{"id": "2504.20996", "pdf": "https://arxiv.org/pdf/2504.20996", "abs": "https://arxiv.org/abs/2504.20996", "authors": ["Sicheng Mo", "Thao Nguyen", "Xun Huang", "Siddharth Srinivasan Iyer", "Yijun Li", "Yuchen Liu", "Abhishek Tandon", "Eli Shechtman", "Krishna Kumar Singh", "Yong Jae Lee", "Bolei Zhou", "Yuheng Li"], "title": "X-Fusion: Introducing New Modality to Frozen Large Language Models", "categories": ["cs.CV"], "comment": "Project Page: https://sichengmo.github.io/XFusion/", "summary": "We propose X-Fusion, a framework that extends pretrained Large Language\nModels (LLMs) for multimodal tasks while preserving their language\ncapabilities. X-Fusion employs a dual-tower design with modality-specific\nweights, keeping the LLM's parameters frozen while integrating vision-specific\ninformation for both understanding and generation. Our experiments demonstrate\nthat X-Fusion consistently outperforms alternative architectures on both\nimage-to-text and text-to-image tasks. We find that incorporating\nunderstanding-focused data improves generation quality, reducing image data\nnoise enhances overall performance, and feature alignment accelerates\nconvergence for smaller models but has minimal impact on larger ones. Our\nfindings provide valuable insights into building efficient unified multimodal\nmodels."}
{"id": "2504.20612", "pdf": "https://arxiv.org/pdf/2504.20612", "abs": "https://arxiv.org/abs/2504.20612", "authors": ["Swaroop Dora", "Deven Lunkad", "Naziya Aslam", "S. Venkatesan", "Sandeep Kumar Shukla"], "title": "The Hidden Risks of LLM-Generated Web Application Code: A Security-Centric Evaluation of Code Generation Capabilities in Large Language Models", "categories": ["cs.CR", "cs.AI", "cs.ET"], "comment": "9 pages", "summary": "The rapid advancement of Large Language Models (LLMs) has enhanced software\ndevelopment processes, minimizing the time and effort required for coding and\nenhancing developer productivity. However, despite their potential benefits,\ncode generated by LLMs has been shown to generate insecure code in controlled\nenvironments, raising critical concerns about their reliability and security in\nreal-world applications. This paper uses predefined security parameters to\nevaluate the security compliance of LLM-generated code across multiple models,\nsuch as ChatGPT, DeepSeek, Claude, Gemini and Grok. The analysis reveals\ncritical vulnerabilities in authentication mechanisms, session management,\ninput validation and HTTP security headers. Although some models implement\nsecurity measures to a limited extent, none fully align with industry best\npractices, highlighting the associated risks in automated software development.\nOur findings underscore that human expertise is crucial to ensure secure\nsoftware deployment or review of LLM-generated code. Also, there is a need for\nrobust security assessment frameworks to enhance the reliability of\nLLM-generated code in real-world applications."}
{"id": "2504.20735", "pdf": "https://arxiv.org/pdf/2504.20735", "abs": "https://arxiv.org/abs/2504.20735", "authors": ["Tariq Qayyum", "Asadullah Tariq", "Muhammad Ali", "Mohamed Adel Serhani", "Zouheir Trabelsi", "Maite López-Sánchez"], "title": "Intelligent Task Offloading in VANETs: A Hybrid AI-Driven Approach for Low-Latency and Energy Efficiency", "categories": ["cs.LG", "cs.DC"], "comment": null, "summary": "Vehicular Ad-hoc Networks (VANETs) are integral to intelligent transportation\nsystems, enabling vehicles to offload computational tasks to nearby roadside\nunits (RSUs) and mobile edge computing (MEC) servers for real-time processing.\nHowever, the highly dynamic nature of VANETs introduces challenges, such as\nunpredictable network conditions, high latency, energy inefficiency, and task\nfailure. This research addresses these issues by proposing a hybrid AI\nframework that integrates supervised learning, reinforcement learning, and\nParticle Swarm Optimization (PSO) for intelligent task offloading and resource\nallocation. The framework leverages supervised models for predicting optimal\noffloading strategies, reinforcement learning for adaptive decision-making, and\nPSO for optimizing latency and energy consumption. Extensive simulations\ndemonstrate that the proposed framework achieves significant reductions in\nlatency and energy usage while improving task success rates and network\nthroughput. By offering an efficient, and scalable solution, this framework\nsets the foundation for enhancing real-time applications in dynamic vehicular\nenvironments."}
{"id": "2503.19878", "pdf": "https://arxiv.org/pdf/2503.19878", "abs": "https://arxiv.org/abs/2503.19878", "authors": ["Nengbo Wang", "Xiaotian Han", "Jagdip Singh", "Jing Ma", "Vipin Chaudhary"], "title": "CausalRAG: Integrating Causal Graphs into Retrieval-Augmented Generation", "categories": ["cs.CL", "cs.IR"], "comment": null, "summary": "Large language models (LLMs) have revolutionized natural language processing\n(NLP), particularly through Retrieval-Augmented Generation (RAG), which\nenhances LLM capabilities by integrating external knowledge. However,\ntraditional RAG systems face critical limitations, including disrupted\ncontextual integrity due to text chunking, and over-reliance on semantic\nsimilarity for retrieval. To address these issues, we propose CausalRAG, a\nnovel framework that incorporates causal graphs into the retrieval process. By\nconstructing and tracing causal relationships, CausalRAG preserves contextual\ncontinuity and improves retrieval precision, leading to more accurate and\ninterpretable responses. We evaluate CausalRAG against regular RAG and\ngraph-based RAG approaches, demonstrating its superiority across several\nmetrics. Our findings suggest that grounding retrieval in causal reasoning\nprovides a promising approach to knowledge-intensive tasks."}
{"id": "2504.20998", "pdf": "https://arxiv.org/pdf/2504.20998", "abs": "https://arxiv.org/abs/2504.20998", "authors": ["Thao Nguyen", "Krishna Kumar Singh", "Jing Shi", "Trung Bui", "Yong Jae Lee", "Yuheng Li"], "title": "YoChameleon: Personalized Vision and Language Generation", "categories": ["cs.CV", "cs.AI"], "comment": "CVPR 2025; Project page: https://thaoshibe.github.io/YoChameleon", "summary": "Large Multimodal Models (e.g., GPT-4, Gemini, Chameleon) have evolved into\npowerful tools with millions of users. However, they remain generic models and\nlack personalized knowledge of specific user concepts. Previous work has\nexplored personalization for text generation, yet it remains unclear how these\nmethods can be adapted to new modalities, such as image generation. In this\npaper, we introduce Yo'Chameleon, the first attempt to study personalization\nfor large multimodal models. Given 3-5 images of a particular concept,\nYo'Chameleon leverages soft-prompt tuning to embed subject-specific information\nto (i) answer questions about the subject and (ii) recreate pixel-level details\nto produce images of the subject in new contexts. Yo'Chameleon is trained with\n(i) a self-prompting optimization mechanism to balance performance across\nmultiple modalities, and (ii) a ``soft-positive\" image generation approach to\nenhance image quality in a few-shot setting."}
{"id": "2504.20634", "pdf": "https://arxiv.org/pdf/2504.20634", "abs": "https://arxiv.org/abs/2504.20634", "authors": ["Andrew Fitzgibbon", "Stephen Felix"], "title": "On Stochastic Rounding with Few Random Bits", "categories": ["math.NA", "cs.AI", "cs.LG", "cs.MS", "cs.NA"], "comment": "Published at ARITH 2025", "summary": "Large-scale numerical computations make increasing use of low-precision (LP)\nfloating point formats and mixed precision arithmetic, which can be enhanced by\nthe technique of stochastic rounding (SR), that is, rounding an intermediate\nhigh-precision value up or down randomly as a function of the value's distance\nto the two rounding candidates. Stochastic rounding requires, in addition to\nthe high-precision input value, a source of random bits. As the provision of\nhigh-quality random bits is an additional computational cost, it is of interest\nto require as few bits as possible while maintaining the desirable properties\nof SR in a given computation, or computational domain. This paper examines a\nnumber of possible implementations of few-bit stochastic rounding (FBSR), and\nshows how several natural implementations can introduce sometimes significant\nbias into the rounding process, which are not present in the case of\ninfinite-bit, infinite-precision examinations of these implementations. The\npaper explores the impact of these biases in machine learning examples, and\nhence opens another class of configuration parameters of which practitioners\nshould be aware when developing or adopting low-precision floating point. Code\nis available at\nhttp://github.com/graphcore-research/arith25-stochastic-rounding."}
{"id": "2504.20754", "pdf": "https://arxiv.org/pdf/2504.20754", "abs": "https://arxiv.org/abs/2504.20754", "authors": ["Hao Luan", "See-Kiong Ng", "Chun Kai Ling"], "title": "DDPS: Discrete Diffusion Posterior Sampling for Paths in Layered Graphs", "categories": ["cs.LG"], "comment": "To appear at Frontiers in Probabilistic Inference: Sampling meets\n  Learning (FPI) workshop at ICLR 2025.\n  https://openreview.net/forum?id=DBdkU0Ikzy", "summary": "Diffusion models form an important class of generative models today,\naccounting for much of the state of the art in cutting edge AI research. While\nnumerous extensions beyond image and video generation exist, few of such\napproaches address the issue of explicit constraints in the samples generated.\nIn this paper, we study the problem of generating paths in a layered graph (a\nvariant of a directed acyclic graph) using discrete diffusion models, while\nguaranteeing that our generated samples are indeed paths. Our approach utilizes\na simple yet effective representation for paths which we call the padded\nadjacency-list matrix (PALM). In addition, we show how to effectively perform\nclassifier guidance, which helps steer the sampled paths to specific preferred\nedges without any retraining of the diffusion model. Our preliminary results\nshow that empirically, our method outperforms alternatives which do not\nexplicitly account for path constraints."}
{"id": "2504.00799", "pdf": "https://arxiv.org/pdf/2504.00799", "abs": "https://arxiv.org/abs/2504.00799", "authors": ["Xi Wang", "Fanfei Meng", "Shiyang Zhang", "Lan Li"], "title": "Inaccuracy of an E-Dictionary and Its Influence on Chinese Language Users", "categories": ["cs.CL", "cs.HC", "H.5.2; I.2.7"], "comment": "The scope of the work has evolved significantly since initial\n  submission, and we are preparing a revised version that better reflects the\n  current direction of the research", "summary": "Electronic dictionaries have largely replaced paper dictionaries and become\ncentral tools for L2 learners seeking to expand their vocabulary. Users often\nassume these resources are reliable and rarely question the validity of the\ndefinitions provided. The accuracy of major E-dictionaries is seldom\nscrutinized, and little attention has been paid to how their corpora are\nconstructed. Research on dictionary use, particularly the limitations of\nelectronic dictionaries, remains scarce. This study adopts a combined method of\nexperimentation, user survey, and dictionary critique to examine Youdao, one of\nthe most widely used E-dictionaries in China. The experiment involved a\ntranslation task paired with retrospective reflection. Participants were asked\nto translate sentences containing words that are insufficiently or inaccurately\ndefined in Youdao. Their consultation behavior was recorded to analyze how\nfaulty definitions influenced comprehension. Results show that incomplete or\nmisleading definitions can cause serious misunderstandings. Additionally,\nstudents exhibited problematic consultation habits. The study further explores\nhow such flawed definitions originate, highlighting issues in data processing\nand the integration of AI and machine learning technologies in dictionary\nconstruction. The findings suggest a need for better training in dictionary\nliteracy for users, as well as improvements in the underlying AI models used to\nbuild E-dictionaries."}
{"id": "2504.20339", "pdf": "https://arxiv.org/pdf/2504.20339", "abs": "https://arxiv.org/abs/2504.20339", "authors": ["Cedric Le Gentil", "Leonardo Brizi", "Daniil Lisus", "Xinyuan Qiao", "Giorgio Grisetti", "Timothy D. Barfoot"], "title": "DRO: Doppler-Aware Direct Radar Odometry", "categories": ["cs.RO", "cs.CV"], "comment": "Accepted for presentation at RSS 2025", "summary": "A renaissance in radar-based sensing for mobile robotic applications is\nunderway. Compared to cameras or lidars, millimetre-wave radars have the\nability to `see' through thin walls, vegetation, and adversarial weather\nconditions such as heavy rain, fog, snow, and dust. In this paper, we propose a\nnovel SE(2) odometry approach for spinning frequency-modulated continuous-wave\nradars. Our method performs scan-to-local-map registration of the incoming\nradar data in a direct manner using all the radar intensity information without\nthe need for feature or point cloud extraction. The method performs locally\ncontinuous trajectory estimation and accounts for both motion and Doppler\ndistortion of the radar scans. If the radar possesses a specific frequency\nmodulation pattern that makes radial Doppler velocities observable, an\nadditional Doppler-based constraint is formulated to improve the velocity\nestimate and enable odometry in geometrically feature-deprived scenarios (e.g.,\nfeatureless tunnels). Our method has been validated on over 250km of on-road\ndata sourced from public datasets (Boreas and MulRan) and collected using our\nautomotive platform. With the aid of a gyroscope, it outperforms\nstate-of-the-art methods and achieves an average relative translation error of\n0.26% on the Boreas leaderboard. When using data with the appropriate\nDoppler-enabling frequency modulation pattern, the translation error is reduced\nto 0.18% in similar environments. We also benchmarked our algorithm using 1.5\nhours of data collected with a mobile robot in off-road environments with\nvarious levels of structure to demonstrate its versatility. Our real-time\nimplementation is publicly available: https://github.com/utiasASRL/dro."}
{"id": "2504.20673", "pdf": "https://arxiv.org/pdf/2504.20673", "abs": "https://arxiv.org/abs/2504.20673", "authors": ["Wenjing Yin", "Tianze Sun", "Yijiong Yu", "Jiawei Fang", "Guangyao Su", "Jiancheng Wang", "Zekun Wang", "Wei Wang", "Ran Chen", "Ziyun Dai", "Shuai Yuan", "Menghang Dong", "Peng Luo", "Dong Cao", "Da Lei", "Yajun Zhang", "Hao Chen", "Xiang Ma", "Yong Liu", "Weifeng Liu", "Yuanjian Xu", "Ji Pei"], "title": "CoCo-Bench: A Comprehensive Code Benchmark For Multi-task Large Language Model Evaluation", "categories": ["cs.SE", "cs.AI"], "comment": "Submitted to ACL 2025. Under review", "summary": "Large language models (LLMs) play a crucial role in software engineering,\nexcelling in tasks like code generation and maintenance. However, existing\nbenchmarks are often narrow in scope, focusing on a specific task and lack a\ncomprehensive evaluation framework that reflects real-world applications. To\naddress these gaps, we introduce CoCo-Bench (Comprehensive Code Benchmark),\ndesigned to evaluate LLMs across four critical dimensions: code understanding,\ncode generation, code modification, and code review. These dimensions capture\nessential developer needs, ensuring a more systematic and representative\nevaluation. CoCo-Bench includes multiple programming languages and varying task\ndifficulties, with rigorous manual review to ensure data quality and accuracy.\nEmpirical results show that CoCo-Bench aligns with existing benchmarks while\nuncovering significant variations in model performance, effectively\nhighlighting strengths and weaknesses. By offering a holistic and objective\nevaluation, CoCo-Bench provides valuable insights to guide future research and\ntechnological advancements in code-oriented LLMs, establishing a reliable\nbenchmark for the field."}
{"id": "2504.20770", "pdf": "https://arxiv.org/pdf/2504.20770", "abs": "https://arxiv.org/abs/2504.20770", "authors": ["Ji Shi", "Chengxun Xie", "Zhonghao Li", "Xinming Zhang", "Miao Zhang"], "title": "JTreeformer: Graph-Transformer via Latent-Diffusion Model for Molecular Generation", "categories": ["cs.LG", "cs.AI"], "comment": "9 pages, 6figures", "summary": "The discovery of new molecules based on the original chemical molecule\ndistributions is of great importance in medicine. The graph transformer, with\nits advantages of high performance and scalability compared to traditional\ngraph networks, has been widely explored in recent research for applications of\ngraph structures. However, current transformer-based graph decoders struggle to\neffectively utilize graph information, which limits their capacity to leverage\nonly sequences of nodes rather than the complex topological structures of\nmolecule graphs. This paper focuses on building a graph transformer-based\nframework for molecular generation, which we call \\textbf{JTreeformer} as it\ntransforms graph generation into junction tree generation. It combines GCN\nparallel with multi-head attention as the encoder. It integrates a directed\nacyclic GCN into a graph-based Transformer to serve as a decoder, which can\niteratively synthesize the entire molecule by leveraging information from the\npartially constructed molecular structure at each step. In addition, a\ndiffusion model is inserted in the latent space generated by the encoder, to\nenhance the efficiency and effectiveness of sampling further. The empirical\nresults demonstrate that our novel framework outperforms existing molecule\ngeneration methods, thus offering a promising tool to advance drug discovery\n(https://anonymous.4open.science/r/JTreeformer-C74C)."}
{"id": "2504.05239", "pdf": "https://arxiv.org/pdf/2504.05239", "abs": "https://arxiv.org/abs/2504.05239", "authors": ["Hang Li", "Yucheng Chu", "Kaiqi Yang", "Yasemin Copur-Gencturk", "Jiliang Tang"], "title": "LLM-based Automated Grading with Human-in-the-Loop", "categories": ["cs.CL"], "comment": null, "summary": "The rise of artificial intelligence (AI) technologies, particularly large\nlanguage models (LLMs), has brought significant advancements to the field of\neducation. Among various applications, automatic short answer grading (ASAG),\nwhich focuses on evaluating open-ended textual responses, has seen remarkable\nprogress with the introduction of LLMs. These models not only enhance grading\nperformance compared to traditional ASAG approaches but also move beyond simple\ncomparisons with predefined \"golden\" answers, enabling more sophisticated\ngrading scenarios, such as rubric-based evaluation. However, existing\nLLM-powered methods still face challenges in achieving human-level grading\nperformance in rubric-based assessments due to their reliance on fully\nautomated approaches. In this work, we explore the potential of LLMs in ASAG\ntasks by leveraging their interactive capabilities through a human-in-the-loop\n(HITL) approach. Our proposed framework, GradeHITL, utilizes the generative\nproperties of LLMs to pose questions to human experts, incorporating their\ninsights to refine grading rubrics dynamically. This adaptive process\nsignificantly improves grading accuracy, outperforming existing methods and\nbringing ASAG closer to human-level evaluation."}
{"id": "2504.20403", "pdf": "https://arxiv.org/pdf/2504.20403", "abs": "https://arxiv.org/abs/2504.20403", "authors": ["Hanxi Liu", "Yifang Men", "Zhouhui Lian"], "title": "Creating Your Editable 3D Photorealistic Avatar with Tetrahedron-constrained Gaussian Splatting", "categories": ["cs.GR", "cs.CV"], "comment": null, "summary": "Personalized 3D avatar editing holds significant promise due to its\nuser-friendliness and availability to applications such as AR/VR and virtual\ntry-ons. Previous studies have explored the feasibility of 3D editing, but\noften struggle to generate visually pleasing results, possibly due to the\nunstable representation learning under mixed optimization of geometry and\ntexture in complicated reconstructed scenarios. In this paper, we aim to\nprovide an accessible solution for ordinary users to create their editable 3D\navatars with precise region localization, geometric adaptability, and\nphotorealistic renderings. To tackle this challenge, we introduce a\nmeticulously designed framework that decouples the editing process into local\nspatial adaptation and realistic appearance learning, utilizing a hybrid\nTetrahedron-constrained Gaussian Splatting (TetGS) as the underlying\nrepresentation. TetGS combines the controllable explicit structure of\ntetrahedral grids with the high-precision rendering capabilities of 3D Gaussian\nSplatting and is optimized in a progressive manner comprising three stages: 3D\navatar instantiation from real-world monocular videos to provide accurate\npriors for TetGS initialization; localized spatial adaptation with explicitly\npartitioned tetrahedrons to guide the redistribution of Gaussian kernels; and\ngeometry-based appearance generation with a coarse-to-fine activation strategy.\nBoth qualitative and quantitative experiments demonstrate the effectiveness and\nsuperiority of our approach in generating photorealistic 3D editable avatars."}
{"id": "2504.20726", "pdf": "https://arxiv.org/pdf/2504.20726", "abs": "https://arxiv.org/abs/2504.20726", "authors": ["Hattan Althebeiti", "Mohammed Alkinoon", "Manar Mohaisen", "Saeed Salem", "DaeHun Nyang", "David Mohaisen"], "title": "Enhancing Vulnerability Reports with Automated and Augmented Description Summarization", "categories": ["cs.CR", "cs.AI", "cs.LG"], "comment": "12 pages, 3 tables, 12 figures. Accepted for publication in IEEE\n  Transactions on Big Data. Extended version of arXiv:2210.01260", "summary": "Public vulnerability databases, such as the National Vulnerability Database\n(NVD), document vulnerabilities and facilitate threat information sharing.\nHowever, they often suffer from short descriptions and outdated or insufficient\ninformation. In this paper, we introduce Zad, a system designed to enrich NVD\nvulnerability descriptions by leveraging external resources. Zad consists of\ntwo pipelines: one collects and filters supplementary data using two encoders\nto build a detailed dataset, while the other fine-tunes a pre-trained model on\nthis dataset to generate enriched descriptions. By addressing brevity and\nimproving content quality, Zad produces more comprehensive and cohesive\nvulnerability descriptions. We evaluate Zad using standard summarization\nmetrics and human assessments, demonstrating its effectiveness in enhancing\nvulnerability information."}
{"id": "2504.20789", "pdf": "https://arxiv.org/pdf/2504.20789", "abs": "https://arxiv.org/abs/2504.20789", "authors": ["Collin Beaudoin", "Swaroop Ghosh"], "title": "Evaluating Effects of Augmented SELFIES for Molecular Understanding Using QK-LSTM", "categories": ["cs.LG"], "comment": null, "summary": "Identifying molecular properties, including side effects, is a critical yet\ntime-consuming step in drug development. Failing to detect these side effects\nbefore regulatory submission can result in significant financial losses and\nproduction delays, and overlooking them during the regulatory review can lead\nto catastrophic consequences. This challenge presents an opportunity for\ninnovative machine learning approaches, particularly hybrid quantum-classical\nmodels like the Quantum Kernel-Based Long Short-Term Memory (QK-LSTM) network.\nThe QK-LSTM integrates quantum kernel functions into the classical LSTM\nframework, enabling the capture of complex, non-linear patterns in sequential\ndata. By mapping input data into a high-dimensional quantum feature space, the\nQK-LSTM model reduces the need for large parameter sets, allowing for model\ncompression without sacrificing accuracy in sequence-based tasks. Recent\nadvancements have been made in the classical domain using augmented variations\nof the Simplified Molecular Line-Entry System (SMILES). However, to the best of\nour knowledge, no research has explored the impact of augmented SMILES in the\nquantum domain, nor the role of augmented Self-Referencing Embedded Strings\n(SELFIES) in either classical or hybrid quantum-classical settings. This study\npresents the first analysis of these approaches, providing novel insights into\ntheir potential for enhancing molecular property prediction and side effect\nidentification. Results reveal that augmenting SELFIES yields in statistically\nsignificant improvements from SMILES by a 5.97% improvement for the classical\ndomain and a 5.91% improvement for the hybrid quantum-classical domain."}
{"id": "2504.07072", "pdf": "https://arxiv.org/pdf/2504.07072", "abs": "https://arxiv.org/abs/2504.07072", "authors": ["Israfel Salazar", "Manuel Fernández Burda", "Shayekh Bin Islam", "Arshia Soltani Moakhar", "Shivalika Singh", "Fabian Farestam", "Angelika Romanou", "Danylo Boiko", "Dipika Khullar", "Mike Zhang", "Dominik Krzemiński", "Jekaterina Novikova", "Luísa Shimabucoro", "Joseph Marvin Imperial", "Rishabh Maheshwary", "Sharad Duwal", "Alfonso Amayuelas", "Swati Rajwal", "Jebish Purbey", "Ahmed Ruby", "Nicholas Popovič", "Marek Suppa", "Azmine Toushik Wasi", "Ram Mohan Rao Kadiyala", "Olga Tsymboi", "Maksim Kostritsya", "Bardia Soltani Moakhar", "Gabriel da Costa Merlin", "Otávio Ferracioli Coletti", "Maral Jabbari Shiviari", "MohammadAmin farahani fard", "Silvia Fernandez", "María Grandury", "Dmitry Abulkhanov", "Drishti Sharma", "Andre Guarnier De Mitri", "Leticia Bossatto Marchezi", "Setayesh Heydari", "Johan Obando-Ceron", "Nazar Kohut", "Beyza Ermis", "Desmond Elliott", "Enzo Ferrante", "Sara Hooker", "Marzieh Fadaee"], "title": "Kaleidoscope: In-language Exams for Massively Multilingual Vision Evaluation", "categories": ["cs.CL", "cs.CV"], "comment": "v2: corrected the author list", "summary": "The evaluation of vision-language models (VLMs) has mainly relied on\nEnglish-language benchmarks, leaving significant gaps in both multilingual and\nmulticultural coverage. While multilingual benchmarks have expanded, both in\nsize and languages, many rely on translations of English datasets, failing to\ncapture cultural nuances. In this work, we propose Kaleidoscope, as the most\ncomprehensive exam benchmark to date for the multilingual evaluation of\nvision-language models. Kaleidoscope is a large-scale, in-language multimodal\nbenchmark designed to evaluate VLMs across diverse languages and visual inputs.\nKaleidoscope covers 18 languages and 14 different subjects, amounting to a\ntotal of 20,911 multiple-choice questions. Built through an open science\ncollaboration with a diverse group of researchers worldwide, Kaleidoscope\nensures linguistic and cultural authenticity. We evaluate top-performing\nmultilingual vision-language models and find that they perform poorly on\nlow-resource languages and in complex multimodal scenarios. Our results\nhighlight the need for progress on culturally inclusive multimodal evaluation\nframeworks."}
{"id": "2504.20584", "pdf": "https://arxiv.org/pdf/2504.20584", "abs": "https://arxiv.org/abs/2504.20584", "authors": ["Martin Huber", "Huanyu Tian", "Christopher E. Mower", "Lucas-Raphael Müller", "Sébastien Ourselin", "Christos Bergeles", "Tom Vercauteren"], "title": "Hydra: Marker-Free RGB-D Hand-Eye Calibration", "categories": ["cs.RO", "cs.CV"], "comment": null, "summary": "This work presents an RGB-D imaging-based approach to marker-free hand-eye\ncalibration using a novel implementation of the iterative closest point (ICP)\nalgorithm with a robust point-to-plane (PTP) objective formulated on a Lie\nalgebra. Its applicability is demonstrated through comprehensive experiments\nusing three well known serial manipulators and two RGB-D cameras. With only\nthree randomly chosen robot configurations, our approach achieves approximately\n90% successful calibrations, demonstrating 2-3x higher convergence rates to the\nglobal optimum compared to both marker-based and marker-free baselines. We also\nreport 2 orders of magnitude faster convergence time (0.8 +/- 0.4 s) for 9\nrobot configurations over other marker-free methods. Our method exhibits\nsignificantly improved accuracy (5 mm in task space) over classical approaches\n(7 mm in task space) whilst being marker-free. The benchmarking dataset and\ncode are open sourced under Apache 2.0 License, and a ROS 2 integration with\nrobot abstraction is provided to facilitate deployment."}
{"id": "2504.20741", "pdf": "https://arxiv.org/pdf/2504.20741", "abs": "https://arxiv.org/abs/2504.20741", "authors": ["Joshua Hatherley", "Lauritz Munch", "Jens Christian Bjerring"], "title": "In defence of post-hoc explanations in medical AI", "categories": ["cs.HC", "cs.AI", "cs.CY", "cs.LG"], "comment": null, "summary": "Since the early days of the Explainable AI movement, post-hoc explanations\nhave been praised for their potential to improve user understanding, promote\ntrust, and reduce patient safety risks in black box medical AI systems.\nRecently, however, critics have argued that the benefits of post-hoc\nexplanations are greatly exaggerated since they merely approximate, rather than\nreplicate, the actual reasoning processes that black box systems take to arrive\nat their outputs. In this article, we aim to defend the value of post-hoc\nexplanations against this recent critique. We argue that even if post-hoc\nexplanations do not replicate the exact reasoning processes of black box\nsystems, they can still improve users' functional understanding of black box\nsystems, increase the accuracy of clinician-AI teams, and assist clinicians in\njustifying their AI-informed decisions. While post-hoc explanations are not a\n\"silver bullet\" solution to the black box problem in medical AI, we conclude\nthat they remain a useful strategy for addressing the black box problem in\nmedical AI."}
{"id": "2504.20794", "pdf": "https://arxiv.org/pdf/2504.20794", "abs": "https://arxiv.org/abs/2504.20794", "authors": ["Collin Beaudoin", "Swaroop Ghosh"], "title": "Q-Fusion: Diffusing Quantum Circuits", "categories": ["cs.LG"], "comment": null, "summary": "Quantum computing holds great potential for solving socially relevant and\ncomputationally complex problems. Furthermore, quantum machine learning (QML)\npromises to rapidly improve our current machine learning capabilities. However,\ncurrent noisy intermediate-scale quantum (NISQ) devices are constrained by\nlimitations in the number of qubits and gate counts, which hinder their full\ncapabilities. Furthermore, the design of quantum algorithms remains a laborious\ntask, requiring significant domain expertise and time. Quantum Architecture\nSearch (QAS) aims to streamline this process by automatically generating novel\nquantum circuits, reducing the need for manual intervention. In this paper, we\npropose a diffusion-based algorithm leveraging the LayerDAG framework to\ngenerate new quantum circuits. This method contrasts with other approaches that\nutilize large language models (LLMs), reinforcement learning (RL), variational\nautoencoders (VAE), and similar techniques. Our results demonstrate that the\nproposed model consistently generates 100% valid quantum circuit outputs."}
{"id": "2504.13914", "pdf": "https://arxiv.org/pdf/2504.13914", "abs": "https://arxiv.org/abs/2504.13914", "authors": ["ByteDance Seed", ":", "Jiaze Chen", "Tiantian Fan", "Xin Liu", "Lingjun Liu", "Zhiqi Lin", "Mingxuan Wang", "Chengyi Wang", "Xiangpeng Wei", "Wenyuan Xu", "Yufeng Yuan", "Yu Yue", "Lin Yan", "Qiying Yu", "Xiaochen Zuo", "Chi Zhang", "Ruofei Zhu", "Zhecheng An", "Zhihao Bai", "Yu Bao", "Xingyan Bin", "Jiangjie Chen", "Feng Chen", "Hongmin Chen", "Riwei Chen", "Liangqiang Chen", "Zixin Chen", "Jinsong Chen", "Siyan Chen", "Kaiyuan Chen", "Zhi Chen", "Jin Chen", "Jiecao Chen", "Jinxin Chi", "Weinan Dai", "Ning Dai", "Jiahui Dai", "Shihan Dou", "Yantao Du", "Zhengyin Du", "Jianhui Duan", "Chen Dun", "Ting-Han Fan", "Jiazhan Feng", "Junda Feng", "Ziyuan Feng", "Yuwei Fu", "Wenqi Fu", "Hanjie Fu", "Hao Ge", "Hongyi Guo", "Mingji Han", "Li Han", "Wenhao Hao", "Xintong Hao", "Qianyu He", "Jerry He", "Feng He", "Wen Heng", "Zehua Hong", "Qi Hou", "Liang Hu", "Shengding Hu", "Nan Hu", "Kai Hua", "Qi Huang", "Ziyue Huang", "Hongzhi Huang", "Zihao Huang", "Ting Huang", "Wenhao Huang", "Wei Jia", "Bin Jia", "Xiaoying Jia", "Yuhua Jiang", "Haobin Jiang", "Ziheng Jiang", "Kaihua Jiang", "Chengquan Jiang", "Jianpeng Jiao", "Xiaoran Jin", "Xing Jin", "Xunhao Lai", "Zheng Li", "Xiang Li", "Liyi Li", "Hongkai Li", "Zheng Li", "Shengxian Wan", "Ya Wang", "Yunshui Li", "Chenggang Li", "Niuniu Li", "Siyu Li", "Xi Li", "Xiao Li", "Aoyan Li", "Yuntao Li", "Nianning Liang", "Xinnian Liang", "Haibin Lin", "Weijian Lin", "Ye Lin", "Zhicheng Liu", "Guanlin Liu", "Guanlin Liu", "Chenxiao Liu", "Yan Liu", "Gaohong Liu", "Juncai Liu", "Chundian Liu", "Deyi Liu", "Kaibo Liu", "Siyao Liu", "Qi Liu", "Yongfei Liu", "Kang Liu", "Gan Liu", "Boyi Liu", "Rui Long", "Weiqiang Lou", "Chenwei Lou", "Xiang Luo", "Yao Luo", "Caiping Lv", "Heyang Lv", "Bole Ma", "Qianli Ma", "Hongzhi Ma", "Yiyuan Ma", "Jin Ma", "Wenchang Ma", "Tingting Ma", "Chen Mao", "Qiyang Min", "Zhe Nan", "Guanghan Ning", "Jinxiang Ou", "Haojie Pan", "Renming Pang", "Yanghua Peng", "Tao Peng", "Lihua Qian", "Lihua Qian", "Mu Qiao", "Meng Qu", "Cheng Ren", "Hongbin Ren", "Yong Shan", "Wei Shen", "Ke Shen", "Kai Shen", "Guangming Sheng", "Jinlong Shi", "Wenlei Shi", "Guang Shi", "Shuai Shuai Cao", "Yuxin Song", "Zuquan Song", "Jing Su", "Yifan Sun", "Tao Sun", "Zewei Sun", "Borui Wan", "Zihan Wang", "Xiaohui Wang", "Xi Wang", "Shuguang Wang", "Jun Wang", "Qinlong Wang", "Chenyuan Wang", "Shuai Wang", "Zihan Wang", "Changbao Wang", "Jiaqiang Wang", "Shihang Wang", "Xuwu Wang", "Zaiyuan Wang", "Yuxuan Wang", "Wenqi Wang", "Taiqing Wang", "Chengzhi Wei", "Houmin Wei", "Ziyun Wei", "Shufa Wei", "Zheng Wu", "Yonghui Wu", "Yangjun Wu", "Bohong Wu", "Shuang Wu", "Jingqiao Wu", "Ning Wu", "Shuangzhi Wu", "Jianmin Wu", "Chenguang Xi", "Fan Xia", "Yuqiao Xian", "Liang Xiang", "Boren Xiang", "Bowen Xiao", "Zhen Xiao", "Xia Xiao", "Yongsheng Xiao", "Chao Xin", "Shulin Xin", "Yuwen Xiong", "Jingjing Xu", "Ziwen Xu", "Chenyin Xu", "Jiayi Xu", "Yifan Xu", "Wei Xu", "Yufei Xu", "Shikun Xu", "Shipeng Yan", "Shen Yan", "Qingping Yang", "Xi Yang", "Tianhao Yang", "Yuehang Yang", "Yuan Yang", "Ximing Yang", "Zeyu Yang", "Guang Yang", "Yifan Yang", "Xuesong Yao", "Bairen Yi", "Fan Yin", "Jianian Yin", "Ziqiang Ying", "Xiangyu Yu", "Hongli Yu", "Song Yu", "Menghan Yu", "Huan Yu", "Siyu Yuan", "Jun Yuan", "Yutao Zeng", "Tianyang Zhan", "Zheng Zhang", "Yun Zhang", "Mofan Zhang", "Wang Zhang", "Ru Zhang", "Zhi Zhang", "Tianqi Zhang", "Xinyi Zhang", "Zhexi Zhang", "Sijun Zhang", "Wenqiang Zhang", "Xiangxiang Zhang", "Yongtao Zhang", "Yuyu Zhang", "Ge Zhang", "He Zhang", "Yue Zhang", "Renjie Zheng", "Ningxin Zheng", "Zhuolin Zheng", "Yaowei Zheng", "Chen Zheng", "Xiaoyun Zhi", "Wanjun Zhong", "Cheng Zhong", "Zheng Zhong", "Baoquan Zhong", "Xun Zhou", "Na Zhou", "Huan Zhou", "Hang Zhu", "Defa Zhu", "Wenjia Zhu", "Lei Zuo"], "title": "Seed1.5-Thinking: Advancing Superb Reasoning Models with Reinforcement Learning", "categories": ["cs.CL"], "comment": null, "summary": "We introduce Seed1.5-Thinking, capable of reasoning through thinking before\nresponding, resulting in improved performance on a wide range of benchmarks.\nSeed1.5-Thinking achieves 86.7 on AIME 2024, 55.0 on Codeforces and 77.3 on\nGPQA, demonstrating excellent reasoning abilities in STEM and coding. Beyond\nreasoning tasks, the method demonstrates notable generalization across diverse\ndomains. For instance, it surpasses DeepSeek R1 by 8% in win rate on\nnon-reasoning tasks, indicating its broader applicability. Compared to other\nstate-of-the-art reasoning models, Seed1.5-Thinking is a Mixture-of-Experts\n(MoE) model with a relatively small size, featuring 20B activated and 200B\ntotal parameters. As part of our effort to assess generalized reasoning, we\ndevelop two internal benchmarks, BeyondAIME and Codeforces, both of which will\nbe publicly released to support future research. Model trial link:\nhttps://www.volcengine.com/experience/ark."}
{"id": "2504.20720", "pdf": "https://arxiv.org/pdf/2504.20720", "abs": "https://arxiv.org/abs/2504.20720", "authors": ["Yiming Liu", "Lijun Han", "Enlin Gu", "Hesheng Wang"], "title": "Learning a General Model: Folding Clothing with Topological Dynamics", "categories": ["cs.RO", "cs.CV"], "comment": null, "summary": "The high degrees of freedom and complex structure of garments present\nsignificant challenges for clothing manipulation. In this paper, we propose a\ngeneral topological dynamics model to fold complex clothing. By utilizing the\nvisible folding structure as the topological skeleton, we design a novel\ntopological graph to represent the clothing state. This topological graph is\nlow-dimensional and applied for complex clothing in various folding states. It\nindicates the constraints of clothing and enables predictions regarding\nclothing movement. To extract graphs from self-occlusion, we apply semantic\nsegmentation to analyze the occlusion relationships and decompose the clothing\nstructure. The decomposed structure is then combined with keypoint detection to\ngenerate the topological graph. To analyze the behavior of the topological\ngraph, we employ an improved Graph Neural Network (GNN) to learn the general\ndynamics. The GNN model can predict the deformation of clothing and is employed\nto calculate the deformation Jacobi matrix for control. Experiments using\njackets validate the algorithm's effectiveness to recognize and fold complex\nclothing with self-occlusion."}
{"id": "2504.20781", "pdf": "https://arxiv.org/pdf/2504.20781", "abs": "https://arxiv.org/abs/2504.20781", "authors": ["Xiyu Zhou", "Ruiyin Li", "Peng Liang", "Beiqi Zhang", "Mojtaba Shahin", "Zengyang Li", "Chen Yang"], "title": "Using LLMs in Generating Design Rationale for Software Architecture Decisions", "categories": ["cs.SE", "cs.AI"], "comment": "28 pages, 5 images, 7 tables, Manuscript submitted to a journal\n  (2025)", "summary": "Design Rationale (DR) for software architecture decisions refers to the\nreasoning underlying architectural choices, which provides valuable insights\ninto the different phases of the architecting process throughout software\ndevelopment. However, in practice, DR is often inadequately documented due to a\nlack of motivation and effort from developers. With the recent advancements in\nLarge Language Models (LLMs), their capabilities in text comprehension,\nreasoning, and generation may enable the generation and recovery of DR for\narchitecture decisions. In this study, we evaluated the performance of LLMs in\ngenerating DR for architecture decisions. First, we collected 50 Stack Overflow\n(SO) posts, 25 GitHub issues, and 25 GitHub discussions related to architecture\ndecisions to construct a dataset of 100 architecture-related problems. Then, we\nselected five LLMs to generate DR for the architecture decisions with three\nprompting strategies, including zero-shot, chain of thought (CoT), and\nLLM-based agents. With the DR provided by human experts as ground truth, the\nPrecision of LLM-generated DR with the three prompting strategies ranges from\n0.267 to 0.278, Recall from 0.627 to 0.715, and F1-score from 0.351 to 0.389.\nAdditionally, 64.45% to 69.42% of the arguments of DR not mentioned by human\nexperts are also helpful, 4.12% to 4.87% of the arguments have uncertain\ncorrectness, and 1.59% to 3.24% of the arguments are potentially misleading.\nBased on the results, we further discussed the pros and cons of the three\nprompting strategies and the strengths and limitations of the DR generated by\nLLMs."}
{"id": "2504.20821", "pdf": "https://arxiv.org/pdf/2504.20821", "abs": "https://arxiv.org/abs/2504.20821", "authors": ["Loren Nuyts", "Jesse Davis"], "title": "The When and How of Target Variable Transformations", "categories": ["cs.LG"], "comment": null, "summary": "The machine learning pipeline typically involves the iterative process of (1)\ncollecting the data, (2) preparing the data, (3) learning a model, and (4)\nevaluating a model. Practitioners recognize the importance of the data\npreparation phase in terms of its impact on the ability to learn accurate\nmodels. In this regard, significant attention is often paid to manipulating the\nfeature set (e.g., selection, transformations, dimensionality reduction). A\npoint that is less well appreciated is that transformations on the target\nvariable can also have a large impact on whether it is possible to learn a\nsuitable model. These transformations may include accounting for\nsubject-specific biases (e.g., in how someone uses a rating scale), contexts\n(e.g., population size effects), and general trends (e.g., inflation). However,\nthis point has received a much more cursory treatment in the existing\nliterature. The goal of this paper is three-fold. First, we aim to highlight\nthe importance of this problem by showing when transforming the target variable\nhas been useful in practice. Second, we will provide a set of generic ``rules\nof thumb'' that indicate situations when transforming the target variable may\nbe needed. Third, we will discuss which transformations should be considered in\na given situation."}
{"id": "2504.15642", "pdf": "https://arxiv.org/pdf/2504.15642", "abs": "https://arxiv.org/abs/2504.15642", "authors": ["Gerhard Jäger"], "title": "Computational Typology", "categories": ["cs.CL", "q-bio.PE"], "comment": "19 pages, s5 figure", "summary": "Typology is a subfield of linguistics that focuses on the study and\nclassification of languages based on their structural features. Unlike\ngenealogical classification, which examines the historical relationships\nbetween languages, typology seeks to understand the diversity of human\nlanguages by identifying common properties and patterns, known as universals.\nIn recent years, computational methods have played an increasingly important\nrole in typological research, enabling the analysis of large-scale linguistic\ndata and the testing of hypotheses about language structure and evolution. This\narticle provides an illustration of the benefits of computational statistical\nmodeling in typology."}
{"id": "2504.20736", "pdf": "https://arxiv.org/pdf/2504.20736", "abs": "https://arxiv.org/abs/2504.20736", "authors": ["Nafiseh Jabbari Tofighi", "Maxime Robic", "Fabio Morbidi", "Pascal Vasseur"], "title": "A Survey on Event-based Optical Marker Systems", "categories": ["cs.RO", "cs.CV"], "comment": "10 pages, 6 figures, 1 table", "summary": "The advent of event-based cameras, with their low latency, high dynamic\nrange, and reduced power consumption, marked a significant change in robotic\nvision and machine perception. In particular, the combination of these\nneuromorphic sensors with widely-available passive or active optical markers\n(e.g. AprilTags, arrays of blinking LEDs), has recently opened up a wide field\nof possibilities. This survey paper provides a comprehensive review on\nEvent-Based Optical Marker Systems (EBOMS). We analyze the basic principles and\ntechnologies on which these systems are based, with a special focus on their\nasynchronous operation and robustness against adverse lighting conditions. We\nalso describe the most relevant applications of EBOMS, including object\ndetection and tracking, pose estimation, and optical communication. The article\nconcludes with a discussion of possible future research directions in this\nrapidly-emerging and multidisciplinary field."}
{"id": "2504.20799", "pdf": "https://arxiv.org/pdf/2504.20799", "abs": "https://arxiv.org/abs/2504.20799", "authors": ["Yunseo Lee", "John Youngeun Song", "Dongsun Kim", "Jindae Kim", "Mijung Kim", "Jaechang Nam"], "title": "Hallucination by Code Generation LLMs: Taxonomy, Benchmarks, Mitigation, and Challenges", "categories": ["cs.SE", "cs.AI"], "comment": "15 pages, 4 figures", "summary": "Recent technical breakthroughs in large language models (LLMs) have enabled\nthem to fluently generate source code. Software developers often leverage both\ngeneral-purpose and code-specialized LLMs to revise existing code or even\ngenerate a whole function from scratch. These capabilities are also beneficial\nin no-code or low-code contexts, in which one can write programs without a\ntechnical background. However, due to their internal design, LLMs are prone to\ngenerating hallucinations, which are incorrect, nonsensical, and not\njustifiable information but difficult to identify its presence. This problem\nalso occurs when generating source code. Once hallucinated code is produced, it\nis often challenging for users to identify and fix it, especially when such\nhallucinations can be identified under specific execution paths. As a result,\nthe hallucinated code may remain unnoticed within the codebase. This survey\ninvestigates recent studies and techniques relevant to hallucinations generated\nby CodeLLMs. We categorize the types of hallucinations in the code generated by\nCodeLLMs, review existing benchmarks and mitigation strategies, and identify\nopen challenges. Based on these findings, this survey outlines further research\ndirections in the detection and removal of hallucinations produced by CodeLLMs."}
{"id": "2504.20822", "pdf": "https://arxiv.org/pdf/2504.20822", "abs": "https://arxiv.org/abs/2504.20822", "authors": ["Gissel Velarde", "Tillman Weyde", "David Meredith"], "title": "An approach to melodic segmentation and classification based on filtering with the Haar-wavelet", "categories": ["cs.LG"], "comment": "39 pages, 12 figures. Version of record published in the Journal of\n  New Music Research:\n  http://www.tandfonline.com/doi/full/10.1080/09298215.2013.841713", "summary": "We present a novel method of classification and segmentation of melodies in\nsymbolic representation. The method is based on filtering pitch as a signal\nover time with the Haar-wavelet, and we evaluate it on two tasks. The filtered\nsignal corresponds to a single-scale signal ws from the continuous Haar wavelet\ntransform. The melodies are first segmented using local maxima or\nzero-crossings of w_s. The segments of w_s are then classified using the\nk-nearest neighbour algorithm with Euclidian and city-block distances. The\nmethod proves more effective than using unfiltered pitch signals and\nGestalt-based segmentation when used to recognize the parent works of segments\nfrom Bach's Two-Part Inventions (BWV 772-786). When used to classify 360 Dutch\nfolk tunes into 26 tune families, the performance of the method is comparable\nto the use of pitch signals, but not as good as that of string-matching methods\nbased on multiple features."}
{"id": "2504.15900", "pdf": "https://arxiv.org/pdf/2504.15900", "abs": "https://arxiv.org/abs/2504.15900", "authors": ["Cheng Wen", "Tingwei Guo", "Shuaijiang Zhao", "Wei Zou", "Xiangang Li"], "title": "SARI: Structured Audio Reasoning via Curriculum-Guided Reinforcement Learning", "categories": ["cs.CL"], "comment": null, "summary": "Recent work shows that reinforcement learning(RL) can markedly sharpen the\nreasoning ability of large language models (LLMs) by prompting them to \"think\nbefore answering.\" Yet whether and how these gains transfer to audio-language\nreasoning remains largely unexplored. We extend the Group-Relative Policy\nOptimization (GRPO) framework from DeepSeek-R1 to a Large Audio-Language Model\n(LALM), and construct a 32k sample multiple-choice corpus. Using a two-stage\nregimen supervised fine-tuning on structured and unstructured\nchains-of-thought, followed by curriculum-guided GRPO, we systematically\ncompare implicit vs. explicit, and structured vs. free form reasoning under\nidentical architectures. Our structured audio reasoning model, SARI (Structured\nAudio Reasoning via Curriculum-Guided Reinforcement Learning), achieves a\n16.35% improvement in average accuracy over the base model\nQwen2-Audio-7B-Instruct. Furthermore, the variant built upon Qwen2.5-Omni\nreaches state-of-the-art performance of 67.08% on the MMAU test-mini benchmark.\nAblation experiments show that on the base model we use: (i) SFT warm-up is\nimportant for stable RL training, (ii) structured chains yield more robust\ngeneralization than unstructured ones, and (iii) easy-to-hard curricula\naccelerate convergence and improve final performance. These findings\ndemonstrate that explicit, structured reasoning and curriculum learning\nsubstantially enhances audio-language understanding."}
{"id": "2303.01903", "pdf": "https://arxiv.org/pdf/2303.01903", "abs": "https://arxiv.org/abs/2303.01903", "authors": ["Zhou Yu", "Xuecheng Ouyang", "Zhenwei Shao", "Meng Wang", "Jun Yu"], "title": "Prophet: Prompting Large Language Models with Complementary Answer Heuristics for Knowledge-based Visual Question Answering", "categories": ["cs.CV", "cs.CL", "cs.LG"], "comment": "An extended journal version of our CVPR 2023 paper, which has been\n  accepted at IEEE T-PAMI 2025. The original conference version can be referred\n  to as the v1 version", "summary": "Knowledge-based visual question answering (VQA) requires external knowledge\nbeyond the image to answer the question. Early studies retrieve required\nknowledge from explicit knowledge bases (KBs), which often introduces\nirrelevant information to the question, hence restricting the performance of\ntheir models. Recent works have resorted to using a powerful large language\nmodel (LLM) as an implicit knowledge engine to acquire the necessary knowledge\nfor answering. Despite the encouraging results achieved by these methods, we\nargue that they have not fully activated the capacity of the \\emph{blind} LLM\nas the provided textual input is insufficient to depict the required visual\ninformation to answer the question. In this paper, we present Prophet -- a\nconceptually simple, flexible, and general framework designed to prompt LLM\nwith answer heuristics for knowledge-based VQA. Specifically, we first train a\nvanilla VQA model on a specific knowledge-based VQA dataset without external\nknowledge. After that, we extract two types of complementary answer heuristics\nfrom the VQA model: answer candidates and answer-aware examples. The two types\nof answer heuristics are jointly encoded into a formatted prompt to facilitate\nthe LLM's understanding of both the image and question, thus generating a more\naccurate answer. By incorporating the state-of-the-art LLM GPT-3, Prophet\nsignificantly outperforms existing state-of-the-art methods on four challenging\nknowledge-based VQA datasets. Prophet is general that can be instantiated with\nthe combinations of different VQA models (i.e., both discriminative and\ngenerative ones) and different LLMs (i.e., both commercial and open-source\nones). Moreover, Prophet can also be integrated with modern large multimodal\nmodels in different stages, which is named Prophet++, to further improve the\ncapabilities on knowledge-based VQA tasks."}
{"id": "2504.20808", "pdf": "https://arxiv.org/pdf/2504.20808", "abs": "https://arxiv.org/abs/2504.20808", "authors": ["Florian Vahl", "Jörn Griepenburg", "Jan Gutsche", "Jasper Güldenstein", "Jianwei Zhang"], "title": "SoccerDiffusion: Toward Learning End-to-End Humanoid Robot Soccer from Gameplay Recordings", "categories": ["cs.RO", "cs.AI", "cs.LG"], "comment": null, "summary": "This paper introduces SoccerDiffusion, a transformer-based diffusion model\ndesigned to learn end-to-end control policies for humanoid robot soccer\ndirectly from real-world gameplay recordings. Using data collected from RoboCup\ncompetitions, the model predicts joint command trajectories from multi-modal\nsensor inputs, including vision, proprioception, and game state. We employ a\ndistillation technique to enable real-time inference on embedded platforms that\nreduces the multi-step diffusion process to a single step. Our results\ndemonstrate the model's ability to replicate complex motion behaviors such as\nwalking, kicking, and fall recovery both in simulation and on physical robots.\nAlthough high-level tactical behavior remains limited, this work provides a\nrobust foundation for subsequent reinforcement learning or preference\noptimization methods. We release the dataset, pretrained models, and code\nunder: https://bit-bots.github.io/SoccerDiffusion"}
{"id": "2504.20823", "pdf": "https://arxiv.org/pdf/2504.20823", "abs": "https://arxiv.org/abs/2504.20823", "authors": ["Olga Tsurkan", "Aleksandra Konstantinova", "Aleksandr Sedykh", "Dmitrii Zhiganov", "Arsenii Senokosov", "Daniil Tarpanov", "Matvei Anoshin", "Leonid Fedichkin"], "title": "Hybrid Quantum Recurrent Neural Network For Remaining Useful Life Prediction", "categories": ["cs.LG", "quant-ph"], "comment": "11 pages", "summary": "Predictive maintenance in aerospace heavily relies on accurate estimation of\nthe remaining useful life of jet engines. In this paper, we introduce a Hybrid\nQuantum Recurrent Neural Network framework, combining Quantum Long Short-Term\nMemory layers with classical dense layers for Remaining Useful Life forecasting\non NASA's Commercial Modular Aero-Propulsion System Simulation dataset. Each\nQuantum Long Short-Term Memory gate replaces conventional linear\ntransformations with Quantum Depth-Infused circuits, allowing the network to\nlearn high-frequency components more effectively. Experimental results\ndemonstrate that, despite having fewer trainable parameters, the Hybrid Quantum\nRecurrent Neural Network achieves up to a 5% improvement over a Recurrent\nNeural Network based on stacked Long Short-Term Memory layers in terms of mean\nroot mean squared error and mean absolute error. Moreover, a thorough\ncomparison of our method with established techniques, including Random Forest,\nConvolutional Neural Network, and Multilayer Perceptron, demonstrates that our\napproach, which achieves a Root Mean Squared Error of 15.46, surpasses these\nbaselines by approximately 13.68%, 16.21%, and 7.87%, respectively.\nNevertheless, it remains outperformed by certain advanced joint architectures.\nOur findings highlight the potential of hybrid quantum-classical approaches for\nrobust time-series forecasting under limited data conditions, offering new\navenues for enhancing reliability in predictive maintenance tasks."}
{"id": "2504.18406", "pdf": "https://arxiv.org/pdf/2504.18406", "abs": "https://arxiv.org/abs/2504.18406", "authors": ["Yusen Zhang", "Wenliang Zheng", "Aashrith Madasu", "Peng Shi", "Ryo Kamoi", "Hao Zhou", "Zhuoyang Zou", "Shu Zhao", "Sarkar Snigdha Sarathi Das", "Vipul Gupta", "Xiaoxin Lu", "Nan Zhang", "Ranran Haoran Zhang", "Avitej Iyer", "Renze Lou", "Wenpeng Yin", "Rui Zhang"], "title": "HRScene: How Far Are VLMs from Effective High-Resolution Image Understanding?", "categories": ["cs.CL"], "comment": "22 pages, 8 figures", "summary": "High-resolution image (HRI) understanding aims to process images with a large\nnumber of pixels, such as pathological images and agricultural aerial images,\nboth of which can exceed 1 million pixels. Vision Large Language Models (VLMs)\ncan allegedly handle HRIs, however, there is a lack of a comprehensive\nbenchmark for VLMs to evaluate HRI understanding. To address this gap, we\nintroduce HRScene, a novel unified benchmark for HRI understanding with rich\nscenes. HRScene incorporates 25 real-world datasets and 2 synthetic diagnostic\ndatasets with resolutions ranging from 1,024 $\\times$ 1,024 to 35,503 $\\times$\n26,627. HRScene is collected and re-annotated by 10 graduate-level annotators,\ncovering 25 scenarios, ranging from microscopic to radiology images, street\nviews, long-range pictures, and telescope images. It includes HRIs of\nreal-world objects, scanned documents, and composite multi-image. The two\ndiagnostic evaluation datasets are synthesized by combining the target image\nwith the gold answer and distracting images in different orders, assessing how\nwell models utilize regions in HRI. We conduct extensive experiments involving\n28 VLMs, including Gemini 2.0 Flash and GPT-4o. Experiments on HRScene show\nthat current VLMs achieve an average accuracy of around 50% on real-world\ntasks, revealing significant gaps in HRI understanding. Results on synthetic\ndatasets reveal that VLMs struggle to effectively utilize HRI regions, showing\nsignificant Regional Divergence and lost-in-middle, shedding light on future\nresearch."}
{"id": "2303.16078", "pdf": "https://arxiv.org/pdf/2303.16078", "abs": "https://arxiv.org/abs/2303.16078", "authors": ["Charalambos Tzamos", "Viktor Kocur", "Yaqing Ding", "Daniel Barath", "Zuzana Berger Haladova", "Torsten Sattler", "Zuzana Kukelova"], "title": "Practical solutions to the relative pose of three calibrated cameras", "categories": ["cs.CV"], "comment": "Paper accepted at CVPR 2025", "summary": "We study the challenging problem of estimating the relative pose of three\ncalibrated cameras from four point correspondences. We propose novel efficient\nsolutions to this problem that are based on the simple idea of using four\ncorrespondences to estimate an approximate geometry of the first two views. We\nmodel this geometry either as an affine or a fully perspective geometry\nestimated using one additional approximate correspondence. We generate such an\napproximate correspondence using a very simple and efficient strategy, where\nthe new point is the mean point of three corresponding input points. The new\nsolvers are efficient and easy to implement, since they are based on existing\nefficient minimal solvers, i.e., the 4-point affine fundamental matrix, the\nwell-known 5-point relative pose solver, and the P3P solver. Extensive\nexperiments on real data show that the proposed solvers, when properly coupled\nwith local optimization, achieve state-of-the-art results, with the novel\nsolver based on approximate mean-point correspondences being more robust and\naccurate than the affine-based solver."}
{"id": "2504.20834", "pdf": "https://arxiv.org/pdf/2504.20834", "abs": "https://arxiv.org/abs/2504.20834", "authors": ["Alan Lee", "Harry Tong"], "title": "Reinforcement Learning for LLM Reasoning Under Memory Constraints", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "We explore reinforcement learning (RL) techniques to enhance reasoning within\ntargeted problem spaces in large language models (LLMs) under memory and\ncompute constraints. Our focus is on critic-free methods compatible with LoRA\nfine-tuning on a single 40GB GPU, a common limitation in academic settings. We\nintroduce S-GRPO, a memory-efficient variant of Group Relative Policy\nOptimization, and T-SPMO, a token-level prefix matching strategy for\nfine-grained credit assignment. Despite limited resources, when used to\nfine-tune Qwen2-1.5B both methods significantly improve SVAMP benchmark\naccuracy from 46% to above 70% using LoRA training. T-SPMO also excels in\nmulti-digit multiplication tasks, underscoring the potential of RL fine-tuning\nunder hardware constraints. Additionally, we find that our full-token GRPO\nbaseline under LoRA fine-tuning did not improve model performance (compared to\nbase model) on either task, suggesting that our memory-efficient methods may\nact as a form of regularization that stabilizes training when only a small\nsubset of parameters are updated."}
{"id": "2504.20848", "pdf": "https://arxiv.org/pdf/2504.20848", "abs": "https://arxiv.org/abs/2504.20848", "authors": ["Junyuan Fang", "Huimin Liu", "Han Yang", "Jiajing Wu", "Zibin Zheng", "Chi K. Tse"], "title": "Mitigating the Structural Bias in Graph Adversarial Defenses", "categories": ["cs.LG", "cs.AI", "cs.CR"], "comment": "Under Review", "summary": "In recent years, graph neural networks (GNNs) have shown great potential in\naddressing various graph structure-related downstream tasks. However, recent\nstudies have found that current GNNs are susceptible to malicious adversarial\nattacks. Given the inevitable presence of adversarial attacks in the real\nworld, a variety of defense methods have been proposed to counter these attacks\nand enhance the robustness of GNNs. Despite the commendable performance of\nthese defense methods, we have observed that they tend to exhibit a structural\nbias in terms of their defense capability on nodes with low degree (i.e., tail\nnodes), which is similar to the structural bias of traditional GNNs on nodes\nwith low degree in the clean graph. Therefore, in this work, we propose a\ndefense strategy by including hetero-homo augmented graph construction, $k$NN\naugmented graph construction, and multi-view node-wise attention modules to\nmitigate the structural bias of GNNs against adversarial attacks. Notably, the\nhetero-homo augmented graph consists of removing heterophilic links (i.e.,\nlinks connecting nodes with dissimilar features) globally and adding homophilic\nlinks (i.e., links connecting nodes with similar features) for nodes with low\ndegree. To further enhance the defense capability, an attention mechanism is\nadopted to adaptively combine the representations from the above two kinds of\ngraph views. We conduct extensive experiments to demonstrate the defense and\ndebiasing effect of the proposed strategy on benchmark datasets."}
{"id": "2504.19333", "pdf": "https://arxiv.org/pdf/2504.19333", "abs": "https://arxiv.org/abs/2504.19333", "authors": ["James O' Neill", "Santhosh Subramanian", "Eric Lin", "Vaikkunth Mugunthan"], "title": "Unified Multi-Task Learning & Model Fusion for Efficient Language Model Guardrailing", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": null, "summary": "The trend towards large language models (LLMs) for guardrailing against\nundesired behaviors is increasing and has shown promise for censoring user\ninputs. However, increased latency, memory consumption, hosting expenses and\nnon-structured outputs can make their use prohibitive.\n  In this work, we show that task-specific data generation can lead to\nfine-tuned classifiers that significantly outperform current state of the art\n(SoTA) while being orders of magnitude smaller. Secondly, we show that using a\nsingle model, \\texttt{MultiTaskGuard}, that is pretrained on a large\nsynthetically generated dataset with unique task instructions further improves\ngeneralization. Thirdly, our most performant models, \\texttt{UniGuard}, are\nfound using our proposed search-based model merging approach that finds an\noptimal set of parameters to combine single-policy models and multi-policy\nguardrail models. % On 7 public datasets and 4 guardrail benchmarks we created,\nour efficient guardrail classifiers improve over the best performing SoTA\npublicly available LLMs and 3$^{\\text{rd}}$ party guardrail APIs in detecting\nunsafe and safe behaviors by an average F1 score improvement of \\textbf{29.92}\npoints over Aegis-LlamaGuard and \\textbf{21.62} over \\texttt{gpt-4o},\nrespectively. Lastly, our guardrail synthetic data generation process that uses\ncustom task-specific guardrail poli"}
{"id": "2306.07520", "pdf": "https://arxiv.org/pdf/2306.07520", "abs": "https://arxiv.org/abs/2306.07520", "authors": ["Weizhen He", "Yiheng Deng", "Shixiang Tang", "Qihao Chen", "Qingsong Xie", "Yizhou Wang", "Lei Bai", "Feng Zhu", "Rui Zhao", "Wanli Ouyang", "Donglian Qi", "Yunfeng Yan"], "title": "Instruct-ReID: A Multi-purpose Person Re-identification Task with Instructions", "categories": ["cs.CV"], "comment": null, "summary": "Human intelligence can retrieve any person according to both visual and\nlanguage descriptions. However, the current computer vision community studies\nspecific person re-identification (ReID) tasks in different scenarios\nseparately, which limits the applications in the real world. This paper strives\nto resolve this problem by proposing a new instruct-ReID task that requires the\nmodel to retrieve images according to the given image or language instructions.\nOur instruct-ReID is a more general ReID setting, where existing 6 ReID tasks\ncan be viewed as special cases by designing different instructions. We propose\na large-scale OmniReID benchmark and an adaptive triplet loss as a baseline\nmethod to facilitate research in this new setting. Experimental results show\nthat the proposed multi-purpose ReID model, trained on our OmniReID benchmark\nwithout fine-tuning, can improve +0.5%, +0.6%, +7.7% mAP on Market1501, MSMT17,\nCUHK03 for traditional ReID, +6.4%, +7.1%, +11.2% mAP on PRCC, VC-Clothes, LTCC\nfor clothes-changing ReID, +11.7% mAP on COCAS+ real2 for clothes template\nbased clothes-changing ReID when using only RGB images, +24.9% mAP on COCAS+\nreal2 for our newly defined language-instructed ReID, +4.3% on LLCM for\nvisible-infrared ReID, +2.6% on CUHK-PEDES for text-to-image ReID. The\ndatasets, the model, and code will be available at\nhttps://github.com/hwz-zju/Instruct-ReID."}
{"id": "2504.20851", "pdf": "https://arxiv.org/pdf/2504.20851", "abs": "https://arxiv.org/abs/2504.20851", "authors": ["Qianrun Mao"], "title": "Fostering Self-Directed Growth with Generative AI: Toward a New Learning Analytics Framework", "categories": ["cs.CY", "cs.AI"], "comment": null, "summary": "In an era increasingly shaped by decentralized knowledge ecosystems and\npervasive AI technologies, fostering sustainable learner agency has become a\ncritical educational imperative. This study introduces a novel conceptual\nframework integrating Generative Artificial Intelligence and Learning Analytics\nto cultivate Self-Directed Growth, a dynamic competency that enables learners\nto iteratively drive their own developmental pathways across diverse\ncontexts.Building upon critical gaps in current research on Self Directed\nLearning and AI-mediated education, the proposed Aspire to Potentials for\nLearners (A2PL) model reconceptualizes the interplay of learner aspirations,\ncomplex thinking, and summative self-assessment within GAI supported\nenvironments.Methodological implications for future intervention design and\nlearning analytics applications are discussed, positioning Self-Directed Growth\nas a pivotal axis for developing equitable, adaptive, and sustainable learning\nsystems in the digital era."}
{"id": "2504.20862", "pdf": "https://arxiv.org/pdf/2504.20862", "abs": "https://arxiv.org/abs/2504.20862", "authors": ["Dayananda Herurkar", "Jörn Hees", "Vesselin Tzvetkov", "Andreas Dengel"], "title": "Tabular Data Adapters: Improving Outlier Detection for Unlabeled Private Data", "categories": ["cs.LG", "cs.AI"], "comment": "outlier detection, tabular data, neural networks, weak annotations,\n  soft labeling, unsupervised approach", "summary": "The remarkable success of Deep Learning approaches is often based and\ndemonstrated on large public datasets. However, when applying such approaches\nto internal, private datasets, one frequently faces challenges arising from\nstructural differences in the datasets, domain shift, and the lack of labels.\nIn this work, we introduce Tabular Data Adapters (TDA), a novel method for\ngenerating soft labels for unlabeled tabular data in outlier detection tasks.\nBy identifying statistically similar public datasets and transforming private\ndata (based on a shared autoencoder) into a format compatible with\nstate-of-the-art public models, our approach enables the generation of weak\nlabels. It thereby can help to mitigate the cold start problem of labeling by\nbasing on existing outlier detection models for public datasets. In experiments\non 50 tabular datasets across different domains, we demonstrate that our method\nis able to provide more accurate annotations than baseline approaches while\nreducing computational time. Our approach offers a scalable, efficient, and\ncost-effective solution, to bridge the gap between public research models and\nreal-world industrial applications."}
{"id": "2504.19406", "pdf": "https://arxiv.org/pdf/2504.19406", "abs": "https://arxiv.org/abs/2504.19406", "authors": ["Mengxia Yu", "Bang Nguyen", "Olivia Zino", "Meng Jiang"], "title": "Context Selection and Rewriting for Video-based Educational Question Generation", "categories": ["cs.CL"], "comment": null, "summary": "Educational question generation (EQG) is a crucial component of intelligent\neducational systems, significantly aiding self-assessment, active learning, and\npersonalized education. While EQG systems have emerged, existing datasets\ntypically rely on predefined, carefully edited texts, failing to represent\nreal-world classroom content, including lecture speech with a set of\ncomplementary slides. To bridge this gap, we collect a dataset of educational\nquestions based on lectures from real-world classrooms. On this realistic\ndataset, we find that current methods for EQG struggle with accurately\ngenerating questions from educational videos, particularly in aligning with\nspecific timestamps and target answers. Common challenges include selecting\ninformative contexts from extensive transcripts and ensuring generated\nquestions meaningfully incorporate the target answer. To address the\nchallenges, we introduce a novel framework utilizing large language models for\ndynamically selecting and rewriting contexts based on target timestamps and\nanswers. First, our framework selects contexts from both lecture transcripts\nand video keyframes based on answer relevance and temporal proximity. Then, we\nintegrate the contexts selected from both modalities and rewrite them into\nanswer-containing knowledge statements, to enhance the logical connection\nbetween the contexts and the desired answer. This approach significantly\nimproves the quality and relevance of the generated questions. Our dataset and\ncode are released in https://github.com/mengxiayu/COSER."}
{"id": "2310.18511", "pdf": "https://arxiv.org/pdf/2310.18511", "abs": "https://arxiv.org/abs/2310.18511", "authors": ["Habib Slim", "Xiang Li", "Yuchen Li", "Mahmoud Ahmed", "Mohamed Ayman", "Ujjwal Upadhyay", "Ahmed Abdelreheem", "Arpit Prajapati", "Suhail Pothigara", "Peter Wonka", "Mohamed Elhoseiny"], "title": "3DCoMPaT$^{++}$: An improved Large-scale 3D Vision Dataset for Compositional Recognition", "categories": ["cs.CV", "cs.AI"], "comment": "https://3dcompat-dataset.org/v2/", "summary": "In this work, we present 3DCoMPaT$^{++}$, a multimodal 2D/3D dataset with 160\nmillion rendered views of more than 10 million stylized 3D shapes carefully\nannotated at the part-instance level, alongside matching RGB point clouds, 3D\ntextured meshes, depth maps, and segmentation masks. 3DCoMPaT$^{++}$ covers 41\nshape categories, 275 fine-grained part categories, and 293 fine-grained\nmaterial classes that can be compositionally applied to parts of 3D objects. We\nrender a subset of one million stylized shapes from four equally spaced views\nas well as four randomized views, leading to a total of 160 million renderings.\nParts are segmented at the instance level, with coarse-grained and fine-grained\nsemantic levels. We introduce a new task, called Grounded CoMPaT Recognition\n(GCR), to collectively recognize and ground compositions of materials on parts\nof 3D objects. Additionally, we report the outcomes of a data challenge\norganized at CVPR2023, showcasing the winning method's utilization of a\nmodified PointNet$^{++}$ model trained on 6D inputs, and exploring alternative\ntechniques for GCR enhancement. We hope our work will help ease future research\non compositional 3D Vision."}
{"id": "2504.20854", "pdf": "https://arxiv.org/pdf/2504.20854", "abs": "https://arxiv.org/abs/2504.20854", "authors": ["Jinsun Yoo", "ChonLam Lao", "Lianjie Cao", "Bob Lantz", "Minlan Yu", "Tushar Krishna", "Puneet Sharma"], "title": "Towards Easy and Realistic Network Infrastructure Testing for Large-scale Machine Learning", "categories": ["cs.NI", "cs.AI", "cs.DC", "cs.SY", "eess.SY"], "comment": "Presented as a poster in NSDI 25", "summary": "This paper lays the foundation for Genie, a testing framework that captures\nthe impact of real hardware network behavior on ML workload performance,\nwithout requiring expensive GPUs. Genie uses CPU-initiated traffic over a\nhardware testbed to emulate GPU to GPU communication, and adapts the ASTRA-sim\nsimulator to model interaction between the network and the ML workload."}
{"id": "2504.20869", "pdf": "https://arxiv.org/pdf/2504.20869", "abs": "https://arxiv.org/abs/2504.20869", "authors": ["Junyuan Fang", "Han Yang", "Haixian Wen", "Jiajing Wu", "Zibin Zheng", "Chi K. Tse"], "title": "Quantifying the Noise of Structural Perturbations on Graph Adversarial Attacks", "categories": ["cs.LG", "cs.AI", "cs.CR"], "comment": "Ubder Review", "summary": "Graph neural networks have been widely utilized to solve graph-related tasks\nbecause of their strong learning power in utilizing the local information of\nneighbors. However, recent studies on graph adversarial attacks have proven\nthat current graph neural networks are not robust against malicious attacks.\nYet much of the existing work has focused on the optimization objective based\non attack performance to obtain (near) optimal perturbations, but paid less\nattention to the strength quantification of each perturbation such as the\ninjection of a particular node/link, which makes the choice of perturbations a\nblack-box model that lacks interpretability. In this work, we propose the\nconcept of noise to quantify the attack strength of each adversarial link.\nFurthermore, we propose three attack strategies based on the defined noise and\nclassification margins in terms of single and multiple steps optimization.\nExtensive experiments conducted on benchmark datasets against three\nrepresentative graph neural networks demonstrate the effectiveness of the\nproposed attack strategies. Particularly, we also investigate the preferred\npatterns of effective adversarial perturbations by analyzing the corresponding\nproperties of the selected perturbation nodes."}
{"id": "2504.20013", "pdf": "https://arxiv.org/pdf/2504.20013", "abs": "https://arxiv.org/abs/2504.20013", "authors": ["Beizhe Hu", "Qiang Sheng", "Juan Cao", "Yang Li", "Danding Wang"], "title": "LLM-Generated Fake News Induces Truth Decay in News Ecosystem: A Case Study on Neural News Recommendation", "categories": ["cs.CL", "cs.CY", "cs.IR"], "comment": "ACM SIGIR 2025 Full Paper", "summary": "Online fake news moderation now faces a new challenge brought by the\nmalicious use of large language models (LLMs) in fake news production. Though\nexisting works have shown LLM-generated fake news is hard to detect from an\nindividual aspect, it remains underexplored how its large-scale release will\nimpact the news ecosystem. In this study, we develop a simulation pipeline and\na dataset with ~56k generated news of diverse types to investigate the effects\nof LLM-generated fake news within neural news recommendation systems. Our\nfindings expose a truth decay phenomenon, where real news is gradually losing\nits advantageous position in news ranking against fake news as LLM-generated\nnews is involved in news recommendation. We further provide an explanation\nabout why truth decay occurs from a familiarity perspective and show the\npositive correlation between perplexity and news ranking. Finally, we discuss\nthe threats of LLM-generated fake news and provide possible countermeasures. We\nurge stakeholders to address this emerging challenge to preserve the integrity\nof news ecosystems."}
{"id": "2403.16184", "pdf": "https://arxiv.org/pdf/2403.16184", "abs": "https://arxiv.org/abs/2403.16184", "authors": ["Yuxuan Wang", "Xiaoyuan Liu"], "title": "Predicate Debiasing in Vision-Language Models Integration for Scene Graph Generation Enhancement", "categories": ["cs.CV"], "comment": null, "summary": "Scene Graph Generation (SGG) provides basic language representation of visual\nscenes, requiring models to grasp complex and diverse semantics between\nobjects. This complexity and diversity in SGG leads to underrepresentation,\nwhere parts of triplet labels are rare or even unseen during training,\nresulting in imprecise predictions. To tackle this, we propose integrating the\npretrained Vision-language Models to enhance representation. However, due to\nthe gap between pretraining and SGG, direct inference of pretrained VLMs on SGG\nleads to severe bias, which stems from the imbalanced predicates distribution\nin the pretraining language set. To alleviate the bias, we introduce a novel LM\nEstimation to approximate the unattainable predicates distribution. Finally, we\nensemble the debiased VLMs with SGG models to enhance the representation, where\nwe design a certainty-aware indicator to score each sample and dynamically\nadjust the ensemble weights. Our training-free method effectively addresses the\npredicates bias in pretrained VLMs, enhances SGG's representation, and\nsignificantly improve the performance."}
{"id": "2504.20887", "pdf": "https://arxiv.org/pdf/2504.20887", "abs": "https://arxiv.org/abs/2504.20887", "authors": ["Harry Mead", "Clarissa Costen", "Bruno Lacerda", "Nick Hawes"], "title": "Return Capping: Sample-Efficient CVaR Policy Gradient Optimisation", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "When optimising for conditional value at risk (CVaR) using policy gradients\n(PG), current methods rely on discarding a large proportion of trajectories,\nresulting in poor sample efficiency. We propose a reformulation of the CVaR\noptimisation problem by capping the total return of trajectories used in\ntraining, rather than simply discarding them, and show that this is equivalent\nto the original problem if the cap is set appropriately. We show, with\nempirical results in an number of environments, that this reformulation of the\nproblem results in consistently improved performance compared to baselines."}
{"id": "2504.20894", "pdf": "https://arxiv.org/pdf/2504.20894", "abs": "https://arxiv.org/abs/2504.20894", "authors": ["Merve Karakas", "Osama Hanna", "Lin F. Yang", "Christina Fragouli"], "title": "Does Feedback Help in Bandits with Arm Erasures?", "categories": ["cs.LG", "stat.ML"], "comment": null, "summary": "We study a distributed multi-armed bandit (MAB) problem over arm erasure\nchannels, motivated by the increasing adoption of MAB algorithms over\ncommunication-constrained networks. In this setup, the learner communicates the\nchosen arm to play to an agent over an erasure channel with probability\n$\\epsilon \\in [0,1)$; if an erasure occurs, the agent continues pulling the\nlast successfully received arm; the learner always observes the reward of the\narm pulled. In past work, we considered the case where the agent cannot convey\nfeedback to the learner, and thus the learner does not know whether the arm\nplayed is the requested or the last successfully received one. In this paper,\nwe instead consider the case where the agent can send feedback to the learner\non whether the arm request was received, and thus the learner exactly knows\nwhich arm was played. Surprisingly, we prove that erasure feedback does not\nimprove the worst-case regret upper bound order over the previously studied\nno-feedback setting. In particular, we prove a regret lower bound of\n$\\Omega(\\sqrt{KT} + K / (1 - \\epsilon))$, where $K$ is the number of arms and\n$T$ the time horizon, that matches no-feedback upper bounds up to logarithmic\nfactors. We note however that the availability of feedback enables simpler\nalgorithm designs that may achieve better constants (albeit not better order)\nregret bounds; we design one such algorithm and evaluate its performance\nnumerically."}
{"id": "2410.13675", "pdf": "https://arxiv.org/pdf/2410.13675", "abs": "https://arxiv.org/abs/2410.13675", "authors": ["Amit Moryossef", "Gerard Sant", "Zifan Jiang"], "title": "Pose-Based Sign Language Appearance Transfer", "categories": ["cs.CV", "cs.CL"], "comment": null, "summary": "We introduce a method for transferring the signer's appearance in sign\nlanguage skeletal poses while preserving the sign content. Using estimated\nposes, we transfer the appearance of one signer to another, maintaining natural\nmovements and transitions. This approach improves pose-based rendering and sign\nstitching while obfuscating identity. Our experiments show that while the\nmethod reduces signer identification accuracy, it slightly harms sign\nrecognition performance, highlighting a tradeoff between privacy and utility.\nOur code is available at\nhttps://github.com/sign-language-processing/pose-anonymization."}
{"id": "2405.17790", "pdf": "https://arxiv.org/pdf/2405.17790", "abs": "https://arxiv.org/abs/2405.17790", "authors": ["Weizhen He", "Yiheng Deng", "Yunfeng Yan", "Feng Zhu", "Yizhou Wang", "Lei Bai", "Qingsong Xie", "Donglian Qi", "Wanli Ouyang", "Shixiang Tang"], "title": "Instruct-ReID++: Towards Universal Purpose Instruction-Guided Person Re-identification", "categories": ["cs.CV"], "comment": "arXiv admin note: substantial text overlap with arXiv:2306.07520", "summary": "Human intelligence can retrieve any person according to both visual and\nlanguage descriptions. However, the current computer vision community studies\nspecific person re-identification (ReID) tasks in different scenarios\nseparately, which limits the applications in the real world. This paper strives\nto resolve this problem by proposing a novel instruct-ReID task that requires\nthe model to retrieve images according to the given image or language\ninstructions. Instruct-ReID is the first exploration of a general ReID setting,\nwhere existing 6 ReID tasks can be viewed as special cases by assigning\ndifferent instructions. To facilitate research in this new instruct-ReID task,\nwe propose a large-scale OmniReID++ benchmark equipped with diverse data and\ncomprehensive evaluation methods e.g., task specific and task-free evaluation\nsettings. In the task-specific evaluation setting, gallery sets are categorized\naccording to specific ReID tasks. We propose a novel baseline model, IRM, with\nan adaptive triplet loss to handle various retrieval tasks within a unified\nframework. For task-free evaluation setting, where target person images are\nretrieved from task-agnostic gallery sets, we further propose a new method\ncalled IRM++ with novel memory bank-assisted learning. Extensive evaluations of\nIRM and IRM++ on OmniReID++ benchmark demonstrate the superiority of our\nproposed methods, achieving state-of-the-art performance on 10 test sets. The\ndatasets, the model, and the code will be available at\nhttps://github.com/hwz-zju/Instruct-ReID"}
{"id": "2504.20910", "pdf": "https://arxiv.org/pdf/2504.20910", "abs": "https://arxiv.org/abs/2504.20910", "authors": ["Sachin R. Pendse", "Darren Gergle", "Rachel Kornfield", "Jonah Meyerhoff", "David Mohr", "Jina Suh", "Annie Wescott", "Casey Williams", "Jessica Schleider"], "title": "When Testing AI Tests Us: Safeguarding Mental Health on the Digital Frontlines", "categories": ["cs.CY", "cs.AI", "cs.HC"], "comment": "Accepted to ACM Conference on Fairness, Accountability, and\n  Transparency (FAccT 2025)", "summary": "Red-teaming is a core part of the infrastructure that ensures that AI models\ndo not produce harmful content. Unlike past technologies, the black box nature\nof generative AI systems necessitates a uniquely interactional mode of testing,\none in which individuals on red teams actively interact with the system,\nleveraging natural language to simulate malicious actors and solicit harmful\noutputs. This interactional labor done by red teams can result in mental health\nharms that are uniquely tied to the adversarial engagement strategies necessary\nto effectively red team. The importance of ensuring that generative AI models\ndo not propagate societal or individual harm is widely recognized -- one less\nvisible foundation of end-to-end AI safety is also the protection of the mental\nhealth and wellbeing of those who work to keep model outputs safe. In this\npaper, we argue that the unmet mental health needs of AI red-teamers is a\ncritical workplace safety concern. Through analyzing the unique mental health\nimpacts associated with the labor done by red teams, we propose potential\nindividual and organizational strategies that could be used to meet these\nneeds, and safeguard the mental health of red-teamers. We develop our proposed\nstrategies through drawing parallels between common red-teaming practices and\ninteractional labor common to other professions (including actors, mental\nhealth professionals, conflict photographers, and content moderators),\ndescribing how individuals and organizations within these professional spaces\nsafeguard their mental health given similar psychological demands. Drawing on\nthese protective practices, we describe how safeguards could be adapted for the\ndistinct mental health challenges experienced by red teaming organizations as\nthey mitigate emerging technological risks on the new digital frontlines."}
{"id": "2504.20900", "pdf": "https://arxiv.org/pdf/2504.20900", "abs": "https://arxiv.org/abs/2504.20900", "authors": ["Dayananda Herurkar", "Ahmad Ali", "Andreas Dengel"], "title": "Evaluating Generative Models for Tabular Data: Novel Metrics and Benchmarking", "categories": ["cs.LG"], "comment": "Tabular Data, Generative Models, Evaluation Metrics, Network\n  Intrusion Detection, Outlier Detection, Anomaly Detection", "summary": "Generative models have revolutionized multiple domains, yet their application\nto tabular data remains underexplored. Evaluating generative models for tabular\ndata presents unique challenges due to structural complexity, large-scale\nvariability, and mixed data types, making it difficult to intuitively capture\nintricate patterns. Existing evaluation metrics offer only partial insights,\nlacking a comprehensive measure of generative performance. To address this\nlimitation, we propose three novel evaluation metrics: FAED, FPCAD, and RFIS.\nOur extensive experimental analysis, conducted on three standard network\nintrusion detection datasets, compares these metrics with established\nevaluation methods such as Fidelity, Utility, TSTR, and TRTS. Our results\ndemonstrate that FAED effectively captures generative modeling issues\noverlooked by existing metrics. While FPCAD exhibits promising performance,\nfurther refinements are necessary to enhance its reliability. Our proposed\nframework provides a robust and practical approach for assessing generative\nmodels in tabular data applications."}
{"id": "2412.02467", "pdf": "https://arxiv.org/pdf/2412.02467", "abs": "https://arxiv.org/abs/2412.02467", "authors": ["Tejumade Afonja", "Hui-Po Wang", "Raouf Kerkouche", "Mario Fritz"], "title": "DP-2Stage: Adapting Language Models as Differentially Private Tabular Data Generators", "categories": ["cs.LG", "cs.CL", "cs.CR", "D.4.6; G.3; I.2.7"], "comment": null, "summary": "Generating tabular data under differential privacy (DP) protection ensures\ntheoretical privacy guarantees but poses challenges for training machine\nlearning models, primarily due to the need to capture complex structures under\nnoisy supervision signals. Recently, pre-trained Large Language Models (LLMs)\n-- even those at the scale of GPT-2 -- have demonstrated great potential in\nsynthesizing tabular data. However, their applications under DP constraints\nremain largely unexplored. In this work, we address this gap by applying DP\ntechniques to the generation of synthetic tabular data. Our findings shows that\nLLMs face difficulties in generating coherent text when fine-tuned with DP, as\nprivacy budgets are inefficiently allocated to non-private elements like table\nstructures. To overcome this, we propose DP-2Stage, a two-stage fine-tuning\nframework for differentially private tabular data generation. The first stage\ninvolves non-private fine-tuning on a pseudo dataset, followed by DP\nfine-tuning on a private dataset. Our empirical results show that this approach\nimproves performance across various settings and metrics compared to directly\nfine-tuned LLMs in DP contexts. We release our code and setup at\nhttps://github.com/tejuafonja/DP-2Stage."}
{"id": "2408.13509", "pdf": "https://arxiv.org/pdf/2408.13509", "abs": "https://arxiv.org/abs/2408.13509", "authors": ["Ying Jin", "Jinlong Peng", "Qingdong He", "Teng Hu", "Jiafu Wu", "Hao Chen", "Haoxuan Wang", "Wenbing Zhu", "Mingmin Chi", "Jun Liu", "Yabiao Wang"], "title": "Dual-Interrelated Diffusion Model for Few-Shot Anomaly Image Generation", "categories": ["cs.CV"], "comment": "Accepted to CVPR 2025", "summary": "The performance of anomaly inspection in industrial manufacturing is\nconstrained by the scarcity of anomaly data. To overcome this challenge,\nresearchers have started employing anomaly generation approaches to augment the\nanomaly dataset. However, existing anomaly generation methods suffer from\nlimited diversity in the generated anomalies and struggle to achieve a seamless\nblending of this anomaly with the original image. Moreover, the generated mask\nis usually not aligned with the generated anomaly. In this paper, we overcome\nthese challenges from a new perspective, simultaneously generating a pair of\nthe overall image and the corresponding anomaly part. We propose DualAnoDiff, a\nnovel diffusion-based few-shot anomaly image generation model, which can\ngenerate diverse and realistic anomaly images by using a dual-interrelated\ndiffusion model, where one of them is employed to generate the whole image\nwhile the other one generates the anomaly part. Moreover, we extract background\nand shape information to mitigate the distortion and blurriness phenomenon in\nfew-shot image generation. Extensive experiments demonstrate the superiority of\nour proposed model over state-of-the-art methods in terms of diversity, realism\nand the accuracy of mask. Overall, our approach significantly improves the\nperformance of downstream anomaly inspection tasks, including anomaly\ndetection, anomaly localization, and anomaly classification tasks."}
{"id": "2504.20988", "pdf": "https://arxiv.org/pdf/2504.20988", "abs": "https://arxiv.org/abs/2504.20988", "authors": ["Atul Sharma", "Kavindu Herath", "Saurabh Bagchi", "Chaoyue Liu", "Somali Chaterji"], "title": "Hubs and Spokes Learning: Efficient and Scalable Collaborative Machine Learning", "categories": ["cs.LG", "cs.AI", "cs.DC"], "comment": null, "summary": "We introduce the Hubs and Spokes Learning (HSL) framework, a novel paradigm\nfor collaborative machine learning that combines the strengths of Federated\nLearning (FL) and Decentralized Learning (P2PL). HSL employs a two-tier\ncommunication structure that avoids the single point of failure inherent in FL\nand outperforms the state-of-the-art P2PL framework, Epidemic Learning Local\n(ELL). At equal communication budgets (total edges), HSL achieves higher\nperformance than ELL, while at significantly lower communication budgets, it\ncan match ELL's performance. For instance, with only 400 edges, HSL reaches the\nsame test accuracy that ELL achieves with 1000 edges for 100 peers (spokes) on\nCIFAR-10, demonstrating its suitability for resource-constrained systems. HSL\nalso achieves stronger consensus among nodes after mixing, resulting in\nimproved performance with fewer training rounds. We substantiate these claims\nthrough rigorous theoretical analyses and extensive experimental results,\nshowcasing HSL's practicality for large-scale collaborative learning."}
{"id": "2504.20908", "pdf": "https://arxiv.org/pdf/2504.20908", "abs": "https://arxiv.org/abs/2504.20908", "authors": ["Wenxin Chen", "Weishen Pan", "Kyra Gan", "Fei Wang"], "title": "MOSIC: Model-Agnostic Optimal Subgroup Identification with Multi-Constraint for Improved Reliability", "categories": ["cs.LG"], "comment": null, "summary": "Identifying subgroups that benefit from specific treatments using\nobservational data is a critical challenge in personalized medicine. Most\nexisting approaches solely focus on identifying a subgroup with an improved\ntreatment effect. However, practical considerations, such as ensuring a minimum\nsubgroup size for representativeness or achieving sufficient confounder balance\nfor reliability, are also important for making findings clinically meaningful\nand actionable. While some studies address these constraints individually, none\noffer a unified approach to handle them simultaneously. To bridge this gap, we\npropose a model-agnostic framework for optimal subgroup identification under\nmultiple constraints. We reformulate this combinatorial problem as an\nunconstrained min-max optimization problem with novel modifications and solve\nit by a gradient descent ascent algorithm. We further prove its convergence to\na feasible and locally optimal solution. Our method is stable and highly\nflexible, supporting various models and techniques for estimating and\noptimizing treatment effectiveness with observational data. Extensive\nexperiments on both synthetic and real-world datasets demonstrate its\neffectiveness in identifying subgroups that satisfy multiple constraints,\nachieving higher treatment effects and better confounder balancing results\nacross different group sizes."}
{"id": "2412.06832", "pdf": "https://arxiv.org/pdf/2412.06832", "abs": "https://arxiv.org/abs/2412.06832", "authors": ["Michael Iannelli", "Sneha Kuchipudi", "Vera Dvorak"], "title": "SLA Management in Reconfigurable Multi-Agent RAG: A Systems Approach to Question Answering", "categories": ["cs.SE", "cs.AI", "cs.CL", "cs.DC", "H.3.4; H.3.3; I.2.7; I.2.11; C.2.4"], "comment": null, "summary": "Retrieval Augmented Generation (RAG) enables Large Language Models (LLMs) to\ngeneralize to new information by decoupling reasoning capabilities from static\nknowledge bases. Traditional RAG enhancements have explored vertical\nscaling-assigning subtasks to specialized modules-and horizontal\nscaling-replicating tasks across multiple agents-to improve performance.\nHowever, real-world applications impose diverse Service Level Agreements (SLAs)\nand Quality of Service (QoS) requirements, involving trade-offs among\nobjectives such as reducing cost, ensuring answer quality, and adhering to\nspecific operational constraints.\n  In this work, we present a systems-oriented approach to multi-agent RAG\ntailored for real-world Question Answering (QA) applications. By integrating\ntask-specific non-functional requirements-such as answer quality, cost, and\nlatency-into the system, we enable dynamic reconfiguration to meet diverse\nSLAs. Our method maps these Service Level Objectives (SLOs) to system-level\nparameters, allowing the generation of optimal results within specified\nresource constraints.\n  We conduct a case study in the QA domain, demonstrating how dynamic\nre-orchestration of a multi-agent RAG system can effectively manage the\ntrade-off between answer quality and cost. By adjusting the system based on\nquery intent and operational conditions, we systematically balance performance\nand resource utilization. This approach allows the system to meet SLOs for\nvarious query types, showcasing its practicality for real-world applications."}
{"id": "2408.16005", "pdf": "https://arxiv.org/pdf/2408.16005", "abs": "https://arxiv.org/abs/2408.16005", "authors": ["Ziyi Zhang", "Nicolas Roussel", "Wenzel Jakob"], "title": "Many-Worlds Inverse Rendering", "categories": ["cs.CV", "cs.GR"], "comment": null, "summary": "Discontinuous visibility changes remain a major bottleneck when optimizing\nsurfaces within a physically-based inverse renderer. Many previous works have\nproposed sophisticated algorithms and data structures to sample visibility\nsilhouettes more efficiently.\n  Our work presents another solution: instead of differentiating a tentative\nsurface locally, we differentiate a volumetric perturbation of a surface. We\nrefer this as a many-worlds representation because it models a non-interacting\nsuperposition of conflicting explanations (worlds) of the input dataset. Each\nworld is optically isolated from others, leading to a new transport law that\ndistinguishes our method from prior work based on exponential random media.\n  The resulting Monte Carlo algorithm is simpler and more efficient than prior\nmethods. We demonstrate that our method promotes rapid convergence, both in\nterms of the total iteration count and the cost per iteration."}
{"id": "2504.20997", "pdf": "https://arxiv.org/pdf/2504.20997", "abs": "https://arxiv.org/abs/2504.20997", "authors": ["Dilip Arumugam", "Thomas L. Griffiths"], "title": "Toward Efficient Exploration by Large Language Model Agents", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "A burgeoning area within reinforcement learning (RL) is the design of\nsequential decision-making agents centered around large language models (LLMs).\nWhile autonomous decision-making agents powered by modern LLMs could facilitate\nnumerous real-world applications, such successes demand agents that are capable\nof data-efficient RL. One key obstacle to achieving data efficiency in RL is\nexploration, a challenge that we demonstrate many recent proposals for LLM\nagent designs struggle to contend with. Meanwhile, classic algorithms from the\nRL literature known to gracefully address exploration require technical\nmachinery that can be challenging to operationalize in purely natural language\nsettings. In this work, rather than relying on finetuning or in-context\nlearning to coax LLMs into implicitly imitating a RL algorithm, we illustrate\nhow LLMs can be used to explicitly implement an existing RL algorithm\n(Posterior Sampling for Reinforcement Learning) whose capacity for\nstatistically-efficient exploration is already well-studied. We offer empirical\nresults demonstrating how our LLM-based implementation of a known,\ndata-efficient RL algorithm can be considerably more effective in natural\nlanguage tasks that demand prudent exploration."}
{"id": "2504.20915", "pdf": "https://arxiv.org/pdf/2504.20915", "abs": "https://arxiv.org/abs/2504.20915", "authors": ["Milad Leyli-abadi", "Jean-Patrick Brunet", "Axel Tahmasebimoradi"], "title": "Statistical and Predictive Analysis to Identify Risk Factors and Effects of Post COVID-19 Syndrome", "categories": ["cs.LG", "68T01", "I.2.1; G.3"], "comment": "8 pages, 9 figures, 2 tables, initially submitted in IJCNN 2025, but\n  rejected because of the high number of contributions (requested to be\n  presented as a poster in the conference without being published in conference\n  proceedings)", "summary": "Based on recent studies, some COVID-19 symptoms can persist for months after\ninfection, leading to what is termed long COVID. Factors such as vaccination\ntiming, patient characteristics, and symptoms during the acute phase of\ninfection may contribute to the prolonged effects and intensity of long COVID.\nEach patient, based on their unique combination of factors, develops a specific\nrisk or intensity of long COVID. In this work, we aim to achieve two\nobjectives: (1) conduct a statistical analysis to identify relationships\nbetween various factors and long COVID, and (2) perform predictive analysis of\nlong COVID intensity using these factors. We benchmark and interpret various\ndata-driven approaches, including linear models, random forests, gradient\nboosting, and neural networks, using data from the Lifelines COVID-19 cohort.\nOur results show that Neural Networks (NN) achieve the best performance in\nterms of MAPE, with predictions averaging 19\\% error. Additionally,\ninterpretability analysis reveals key factors such as loss of smell, headache,\nmuscle pain, and vaccination timing as significant predictors, while chronic\ndisease and gender are critical risk factors. These insights provide valuable\nguidance for understanding long COVID and developing targeted interventions."}
{"id": "2501.12433", "pdf": "https://arxiv.org/pdf/2501.12433", "abs": "https://arxiv.org/abs/2501.12433", "authors": ["Tabinda Aman", "Mohammad Nadeem", "Shahab Saquib Sohail", "Mohammad Anas", "Erik Cambria"], "title": "Owls are wise and foxes are unfaithful: Uncovering animal stereotypes in vision-language models", "categories": ["cs.CV", "cs.AI", "cs.CL"], "comment": null, "summary": "Animal stereotypes are deeply embedded in human culture and language. They\noften shape our perceptions and expectations of various species. Our study\ninvestigates how animal stereotypes manifest in vision-language models during\nthe task of image generation. Through targeted prompts, we explore whether\nDALL-E perpetuates stereotypical representations of animals, such as \"owls as\nwise,\" \"foxes as unfaithful,\" etc. Our findings reveal significant stereotyped\ninstances where the model consistently generates images aligned with cultural\nbiases. The current work is the first of its kind to examine animal\nstereotyping in vision-language models systematically and to highlight a\ncritical yet underexplored dimension of bias in AI-generated visual content."}
{"id": "2408.16690", "pdf": "https://arxiv.org/pdf/2408.16690", "abs": "https://arxiv.org/abs/2408.16690", "authors": ["Zhirui Gao", "Renjiao Yi", "Chenyang Zhu", "Ke Zhuang", "Wei Chen", "Kai Xu"], "title": "Generic Objects as Pose Probes for Few-shot View Synthesis", "categories": ["cs.CV"], "comment": "Accepted by IEEE TCSVT 2025 Project page:\n  https://zhirui-gao.github.io/PoseProbe.github.io/", "summary": "Radiance fields including NeRFs and 3D Gaussians demonstrate great potential\nin high-fidelity rendering and scene reconstruction, while they require a\nsubstantial number of posed images as inputs. COLMAP is frequently employed for\npreprocessing to estimate poses, while it necessitates a large number of\nfeature matches to operate effectively, and it struggles with scenes\ncharacterized by sparse features, large baselines between images, or a limited\nnumber of input images. We aim to tackle few-view NeRF reconstruction using\nonly 3 to 6 unposed scene images. Traditional methods often use calibration\nboards but they are not common in images. We propose a novel idea of utilizing\neveryday objects, commonly found in both images and real life, as \"pose\nprobes\". The probe object is automatically segmented by SAM, whose shape is\ninitialized from a cube. We apply a dual-branch volume rendering optimization\n(object NeRF and scene NeRF) to constrain the pose optimization and jointly\nrefine the geometry. Specifically, object poses of two views are first\nestimated by PnP matching in an SDF representation, which serves as initial\nposes. PnP matching, requiring only a few features, is suitable for\nfeature-sparse scenes. Additional views are incrementally incorporated to\nrefine poses from preceding views. In experiments, PoseProbe achieves\nstate-of-the-art performance in both pose estimation and novel view synthesis\nacross multiple datasets. We demonstrate its effectiveness, particularly in\nfew-view and large-baseline scenes where COLMAP struggles. In ablations, using\ndifferent objects in a scene yields comparable performance. Our project page is\navailable at: \\href{https://zhirui-gao.github.io/PoseProbe.github.io/}{this\nhttps URL}"}
{"id": "2311.18662", "pdf": "https://arxiv.org/pdf/2311.18662", "abs": "https://arxiv.org/abs/2311.18662", "authors": ["Daniel Fuertes", "Carlos R. del-Blanco", "Fernando Jaureguizar", "Narciso García"], "title": "TOP-Former: A Multi-Agent Transformer Approach for the Team Orienteering Problem", "categories": ["cs.AI"], "comment": null, "summary": "Route planning for a fleet of vehicles is an important task in applications\nsuch as package delivery, surveillance, or transportation, often integrated\nwithin larger Intelligent Transportation Systems (ITS). This problem is\ncommonly formulated as a Vehicle Routing Problem (VRP) known as the Team\nOrienteering Problem (TOP). Existing solvers for this problem primarily rely on\neither linear programming, which provides accurate solutions but requires\ncomputation times that grow with the size of the problem, or heuristic methods,\nwhich typically find suboptimal solutions in a shorter time. In this paper, we\nintroduce TOP-Former, a multi-agent route planning neural network designed to\nefficiently and accurately solve the Team Orienteering Problem. The proposed\nalgorithm is based on a centralized Transformer neural network capable of\nlearning to encode the scenario (modeled as a graph) and analyze the complete\ncontext of all agents to deliver fast, precise, and collaborative solutions.\nUnlike other neural network-based approaches that adopt a more local\nperspective, TOP-Former is trained to understand the global situation of the\nvehicle fleet and generate solutions that maximize long-term expected returns.\nExtensive experiments demonstrate that the presented system outperforms most\nstate-of-the-art methods in terms of both accuracy and computation speed."}
{"id": "2504.20932", "pdf": "https://arxiv.org/pdf/2504.20932", "abs": "https://arxiv.org/abs/2504.20932", "authors": ["Taisuke Kobayashi"], "title": "Improvements of Dark Experience Replay and Reservoir Sampling towards Better Balance between Consolidation and Plasticity", "categories": ["cs.LG"], "comment": "29 pages, 8 figures", "summary": "Continual learning is the one of the most essential abilities for autonomous\nagents, which can incrementally learn daily-life skills. For this ultimate\ngoal, a simple but powerful method, dark experience replay (DER), has been\nproposed recently. DER mitigates catastrophic forgetting, in which the skills\nacquired in the past are unintentionally forgotten, by stochastically storing\nthe streaming data in a reservoir sampling (RS) buffer and by relearning them\nor retaining the past outputs for them. However, since DER considers multiple\nobjectives, it will not function properly without appropriate weighting of\nthem. In addition, the ability to retain past outputs inhibits learning if the\npast outputs are incorrect due to distribution shift or other effects. This is\ndue to a tradeoff between memory consolidation and plasticity. The tradeoff is\nhidden even in the RS buffer, which gradually stops storing new data for new\nskills in it as data is continuously passed to it. To alleviate the tradeoff\nand achieve better balance, this paper proposes improvement strategies to each\nof DER and RS. Specifically, DER is improved with automatic adaptation of\nweights, block of replaying erroneous data, and correction of past outputs. RS\nis also improved with generalization of acceptance probability, stratification\nof plural buffers, and intentional omission of unnecessary data. These\nimprovements are verified through multiple benchmarks including regression,\nclassification, and reinforcement learning problems. As a result, the proposed\nmethods achieve steady improvements in learning performance by balancing the\nmemory consolidation and plasticity."}
{"id": "2501.18045", "pdf": "https://arxiv.org/pdf/2501.18045", "abs": "https://arxiv.org/abs/2501.18045", "authors": ["Myra Cheng", "Angela Y. Lee", "Kristina Rapuano", "Kate Niederhoffer", "Alex Liebscher", "Jeffrey Hancock"], "title": "From tools to thieves: Measuring and understanding public perceptions of AI through crowdsourced metaphors", "categories": ["cs.CY", "cs.AI", "cs.CL", "cs.HC"], "comment": "To appear at the ACM Conference on Fairness, Accountability, and\n  Transparency 2025", "summary": "How has the public responded to the increasing prevalence of artificial\nintelligence (AI)-based technologies? We investigate public perceptions of AI\nby collecting over 12,000 responses over 12 months from a nationally\nrepresentative U.S. sample. Participants provided open-ended metaphors\nreflecting their mental models of AI, a methodology that overcomes the\nlimitations of traditional self-reported measures by capturing more nuance.\nUsing a mixed-methods approach combining quantitative clustering and\nqualitative coding, we identify 20 dominant metaphors shaping public\nunderstanding of AI. To analyze these metaphors systematically, we present a\nscalable framework integrating language modeling (LM)-based techniques to\nmeasure key dimensions of public perception: anthropomorphism (attribution of\nhuman-like qualities), warmth, and competence. We find that Americans generally\nview AI as warm and competent, and that over the past year, perceptions of AI's\nhuman-likeness and warmth have significantly increased ($+34\\%, r = 0.80, p <\n0.01; +41\\%, r = 0.62, p < 0.05$). These implicit perceptions, along with the\nidentified dominant metaphors, strongly predict trust in and willingness to\nadopt AI ($r^2 = 0.21, 0.18, p < 0.001$). Moreover, we uncover systematic\ndemographic differences in metaphors and implicit perceptions, such as the\nhigher propensity of women, older individuals, and people of color to\nanthropomorphize AI, which shed light on demographic disparities in trust and\nadoption. In addition to our dataset and framework for tracking evolving public\nattitudes, we provide actionable insights on using metaphors for inclusive and\nresponsible AI development."}
{"id": "2409.16902", "pdf": "https://arxiv.org/pdf/2409.16902", "abs": "https://arxiv.org/abs/2409.16902", "authors": ["Chunhui Zhang", "Li Liu", "Guanjie Huang", "Zhipeng Zhang", "Hao Wen", "Xi Zhou", "Shiming Ge", "Yanfeng Wang"], "title": "Underwater Camouflaged Object Tracking Meets Vision-Language SAM2", "categories": ["cs.CV", "cs.AI"], "comment": "Accepted to CVPR 2025 Workshop on CV4Animals.\n  https://github.com/983632847/Awesome-Multimodal-Object-Tracking", "summary": "Over the past decade, significant progress has been made in visual object\ntracking, largely due to the availability of large-scale datasets. However,\nthese datasets have primarily focused on open-air scenarios and have largely\noverlooked underwater animal tracking-especially the complex challenges posed\nby camouflaged marine animals. To bridge this gap, we take a step forward by\nproposing the first large-scale multi-modal underwater camouflaged object\ntracking dataset, namely UW-COT220. Based on the proposed dataset, this work\nfirst comprehensively evaluates current advanced visual object tracking\nmethods, including SAM- and SAM2-based trackers, in challenging underwater\nenvironments, \\eg, coral reefs. Our findings highlight the improvements of SAM2\nover SAM, demonstrating its enhanced ability to handle the complexities of\nunderwater camouflaged objects. Furthermore, we propose a novel vision-language\ntracking framework called VL-SAM2, based on the video foundation model SAM2.\nExperimental results demonstrate that our VL-SAM2 achieves state-of-the-art\nperformance on the UW-COT220 dataset. The dataset and codes are available\nat~\\href{https://github.com/983632847/Awesome-Multimodal-Object-Tracking}{\\color{magenta}{here}}."}
{"id": "2408.07461", "pdf": "https://arxiv.org/pdf/2408.07461", "abs": "https://arxiv.org/abs/2408.07461", "authors": ["Subhabrata Dutta", "Timo Kaufmann", "Goran Glavaš", "Ivan Habernal", "Kristian Kersting", "Frauke Kreuter", "Mira Mezini", "Iryna Gurevych", "Eyke Hüllermeier", "Hinrich Schuetze"], "title": "Problem Solving Through Human-AI Preference-Based Cooperation", "categories": ["cs.AI", "cs.HC"], "comment": "20 pages", "summary": "While there is a widespread belief that artificial general intelligence (AGI)\n-- or even superhuman AI -- is imminent, complex problems in expert domains are\nfar from being solved. We argue that such problems require human-AI cooperation\nand that the current state of the art in generative AI is unable to play the\nrole of a reliable partner due to a multitude of shortcomings, including\ndifficulty to keep track of a complex solution artifact (e.g., a software\nprogram), limited support for versatile human preference expression and lack of\nadapting to human preference in an interactive setting. To address these\nchallenges, we propose HAICo2, a novel human-AI co-construction framework. We\ntake first steps towards a formalization of HAICo2 and discuss the difficult\nopen research problems that it faces."}
{"id": "2504.20942", "pdf": "https://arxiv.org/pdf/2504.20942", "abs": "https://arxiv.org/abs/2504.20942", "authors": ["Christopher Watson", "Rajeev Alur", "Divya Gopinath", "Ravi Mangal", "Corina S. Pasareanu"], "title": "Scenario-based Compositional Verification of Autonomous Systems with Neural Perception", "categories": ["cs.LG", "cs.RO"], "comment": null, "summary": "Recent advances in deep learning have enabled the development of autonomous\nsystems that use deep neural networks for perception. Formal verification of\nthese systems is challenging due to the size and complexity of the perception\nDNNs as well as hard-to-quantify, changing environment conditions. To address\nthese challenges, we propose a probabilistic verification framework for\nautonomous systems based on the following key concepts: (1) Scenario-based\nModeling: We decompose the task (e.g., car navigation) into a composition of\nscenarios, each representing a different environment condition. (2)\nProbabilistic Abstractions: For each scenario, we build a compact abstraction\nof perception based on the DNN's performance on an offline dataset that\nrepresents the scenario's environment condition. (3) Symbolic Reasoning and\nAcceleration: The abstractions enable efficient compositional verification of\nthe autonomous system via symbolic reasoning and a novel acceleration proof\nrule that bounds the error probability of the system under arbitrary variations\nof environment conditions. We illustrate our approach on two case studies: an\nexperimental autonomous system that guides airplanes on taxiways using\nhigh-dimensional perception DNNs and a simulation model of an F1Tenth\nautonomous car using LiDAR observations."}
{"id": "2502.03629", "pdf": "https://arxiv.org/pdf/2502.03629", "abs": "https://arxiv.org/abs/2502.03629", "authors": ["Peter Sushko", "Ayana Bharadwaj", "Zhi Yang Lim", "Vasily Ilin", "Ben Caffee", "Dongping Chen", "Mohammadreza Salehi", "Cheng-Yu Hsieh", "Ranjay Krishna"], "title": "REALEDIT: Reddit Edits As a Large-scale Empirical Dataset for Image Transformations", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.LG"], "comment": "Published at CVPR 2025", "summary": "Existing image editing models struggle to meet real-world demands. Despite\nexcelling in academic benchmarks, they have yet to be widely adopted for real\nuser needs. Datasets that power these models use artificial edits, lacking the\nscale and ecological validity necessary to address the true diversity of user\nrequests. We introduce REALEDIT, a large-scale image editing dataset with\nauthentic user requests and human-made edits sourced from Reddit. REALEDIT\nincludes a test set of 9300 examples to evaluate models on real user requests.\nOur results show that existing models fall short on these tasks, highlighting\nthe need for realistic training data. To address this, we introduce 48K\ntraining examples and train our REALEDIT model, achieving substantial gains -\noutperforming competitors by up to 165 Elo points in human judgment and 92\npercent relative improvement on the automated VIEScore metric. We deploy our\nmodel on Reddit, testing it on new requests, and receive positive feedback.\nBeyond image editing, we explore REALEDIT's potential in detecting edited\nimages by partnering with a deepfake detection non-profit. Finetuning their\nmodel on REALEDIT data improves its F1-score by 14 percentage points,\nunderscoring the dataset's value for broad applications."}
{"id": "2410.11259", "pdf": "https://arxiv.org/pdf/2410.11259", "abs": "https://arxiv.org/abs/2410.11259", "authors": ["Hyunchul Bae", "Minhee Kang", "Minwoo Song", "Heejin Ahn"], "title": "Rethinking the Role of Infrastructure in Collaborative Perception", "categories": ["cs.CV"], "comment": "Accepted by ECCV 2024 Workshop MAAS, 14 pages", "summary": "Collaborative Perception (CP) is a process in which an ego agent receives and\nfuses sensor information from surrounding vehicles and infrastructure to\nenhance its perception capability. To evaluate the need for infrastructure\nequipped with sensors, extensive and quantitative analysis of the role of\ninfrastructure data in CP is crucial, yet remains underexplored. To address\nthis gap, we first quantitatively assess the importance of infrastructure data\nin existing vehicle-centric CP, where the ego agent is a vehicle. Furthermore,\nwe compare vehicle-centric CP with infra-centric CP, where the ego agent is now\nthe infrastructure, to evaluate the effectiveness of each approach. Our results\ndemonstrate that incorporating infrastructure data improves 3D detection\naccuracy by up to 10.30%, and infra-centric CP shows enhanced noise robustness\nand increases accuracy by up to 46.47% compared with vehicle-centric CP."}
{"id": "2302.00671", "pdf": "https://arxiv.org/pdf/2302.00671", "abs": "https://arxiv.org/abs/2302.00671", "authors": ["Grace Zhang", "Ayush Jain", "Injune Hwang", "Shao-Hua Sun", "Joseph J. Lim"], "title": "QMP: Q-switch Mixture of Policies for Multi-Task Behavior Sharing", "categories": ["cs.LG", "cs.AI", "cs.RO"], "comment": "ICLR 2025", "summary": "Multi-task reinforcement learning (MTRL) aims to learn several tasks\nsimultaneously for better sample efficiency than learning them separately.\nTraditional methods achieve this by sharing parameters or relabeled data\nbetween tasks. In this work, we introduce a new framework for sharing\nbehavioral policies across tasks, which can be used in addition to existing\nMTRL methods. The key idea is to improve each task's off-policy data collection\nby employing behaviors from other task policies. Selectively sharing helpful\nbehaviors acquired in one task to collect training data for another task can\nlead to higher-quality trajectories, leading to more sample-efficient MTRL.\nThus, we introduce a simple and principled framework called Q-switch mixture of\npolicies (QMP) that selectively shares behavior between different task policies\nby using the task's Q-function to evaluate and select useful shareable\nbehaviors. We theoretically analyze how QMP improves the sample efficiency of\nthe underlying RL algorithm. Our experiments show that QMP's behavioral policy\nsharing provides complementary gains over many popular MTRL algorithms and\noutperforms alternative ways to share behaviors in various manipulation,\nlocomotion, and navigation environments. Videos are available at\nhttps://qmp-mtrl.github.io."}
{"id": "2504.20944", "pdf": "https://arxiv.org/pdf/2504.20944", "abs": "https://arxiv.org/abs/2504.20944", "authors": ["Kleanthis Avramidis", "Woojae Jeong", "Aditya Kommineni", "Sudarsana R. Kadiri", "Marcus Ma", "Colin McDaniel", "Myzelle Hughes", "Thomas McGee", "Elsi Kaiser", "Dani Byrd", "Assal Habibi", "B. Rael Cahn", "Idan A. Blank", "Kristina Lerman", "Takfarinas Medani", "Richard M. Leahy", "Shrikanth Narayanan"], "title": "Deep Learning Characterizes Depression and Suicidal Ideation from Eye Movements", "categories": ["cs.LG", "eess.SP"], "comment": "Preprint. 12 pages, 5 figures", "summary": "Identifying physiological and behavioral markers for mental health conditions\nis a longstanding challenge in psychiatry. Depression and suicidal ideation, in\nparticular, lack objective biomarkers, with screening and diagnosis primarily\nrelying on self-reports and clinical interviews. Here, we investigate eye\ntracking as a potential marker modality for screening purposes. Eye movements\nare directly modulated by neuronal networks and have been associated with\nattentional and mood-related patterns; however, their predictive value for\ndepression and suicidality remains unclear. We recorded eye-tracking sequences\nfrom 126 young adults as they read and responded to affective sentences, and\nsubsequently developed a deep learning framework to predict their clinical\nstatus. The proposed model included separate branches for trials of positive\nand negative sentiment, and used 2D time-series representations to account for\nboth intra-trial and inter-trial variations. We were able to identify\ndepression and suicidal ideation with an area under the receiver operating\ncurve (AUC) of 0.793 (95% CI: 0.765-0.819) against healthy controls, and\nsuicidality specifically with 0.826 AUC (95% CI: 0.797-0.852). The model also\nexhibited moderate, yet significant, accuracy in differentiating depressed from\nsuicidal participants, with 0.609 AUC (95% CI 0.571-0.646). Discriminative\npatterns emerge more strongly when assessing the data relative to response\ngeneration than relative to the onset time of the final word of the sentences.\nThe most pronounced effects were observed for negative-sentiment sentences,\nthat are congruent to depressed and suicidal participants. Our findings\nhighlight eye tracking as an objective tool for mental health assessment and\nunderscore the modulatory impact of emotional stimuli on cognitive processes\naffecting oculomotor control."}
{"id": "2503.04606", "pdf": "https://arxiv.org/pdf/2503.04606", "abs": "https://arxiv.org/abs/2503.04606", "authors": ["Aoxiong Yin", "Kai Shen", "Yichong Leng", "Xu Tan", "Xinyu Zhou", "Juncheng Li", "Siliang Tang"], "title": "The Best of Both Worlds: Integrating Language Models and Diffusion Models for Video Generation", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.LG"], "comment": "Our code is available at https://github.com/LanDiff/LanDiff", "summary": "Recent advancements in text-to-video (T2V) generation have been driven by two\ncompeting paradigms: autoregressive language models and diffusion models.\nHowever, each paradigm has intrinsic limitations: language models struggle with\nvisual quality and error accumulation, while diffusion models lack semantic\nunderstanding and causal modeling. In this work, we propose LanDiff, a hybrid\nframework that synergizes the strengths of both paradigms through\ncoarse-to-fine generation. Our architecture introduces three key innovations:\n(1) a semantic tokenizer that compresses 3D visual features into compact 1D\ndiscrete representations through efficient semantic compression, achieving a\n$\\sim$14,000$\\times$ compression ratio; (2) a language model that generates\nsemantic tokens with high-level semantic relationships; (3) a streaming\ndiffusion model that refines coarse semantics into high-fidelity videos.\nExperiments show that LanDiff, a 5B model, achieves a score of 85.43 on the\nVBench T2V benchmark, surpassing the state-of-the-art open-source models\nHunyuan Video (13B) and other commercial models such as Sora, Kling, and\nHailuo. Furthermore, our model also achieves state-of-the-art performance in\nlong video generation, surpassing other open-source models in this field. Our\ndemo can be viewed at https://landiff.github.io/."}
{"id": "2410.11838", "pdf": "https://arxiv.org/pdf/2410.11838", "abs": "https://arxiv.org/abs/2410.11838", "authors": ["Junhwa Hur", "Charles Herrmann", "Saurabh Saxena", "Janne Kontkanen", "Wei-Sheng Lai", "Yichang Shih", "Michael Rubinstein", "David J. Fleet", "Deqing Sun"], "title": "High-Resolution Frame Interpolation with Patch-based Cascaded Diffusion", "categories": ["cs.CV"], "comment": "Project page: https://hifi-diffusion.github.io", "summary": "Despite the recent progress, existing frame interpolation methods still\nstruggle with processing extremely high resolution input and handling\nchallenging cases such as repetitive textures, thin objects, and large motion.\nTo address these issues, we introduce a patch-based cascaded pixel diffusion\nmodel for high resolution frame interpolation, HiFI, that excels in these\nscenarios while achieving competitive performance on standard benchmarks.\nCascades, which generate a series of images from low to high resolution, can\nhelp significantly with large or complex motion that require both global\ncontext for a coarse solution and detailed context for high resolution output.\nHowever, contrary to prior work on cascaded diffusion models which perform\ndiffusion on increasingly large resolutions, we use a single model that always\nperforms diffusion at the same resolution and upsamples by processing patches\nof the inputs and the prior solution. At inference time, this drastically\nreduces memory usage and allows a single model, solving both frame\ninterpolation (base model's task) and spatial up-sampling, saving training cost\nas well. HiFI excels at high-resolution images and complex repeated textures\nthat require global context, achieving comparable or state-of-the-art\nperformance on various benchmarks (Vimeo, Xiph, X-Test, and SEPE-8K). We\nfurther introduce a new dataset, LaMoR, that focuses on particularly\nchallenging cases, and HiFI significantly outperforms other baselines. Please\nvisit our project page for video results: https://hifi-diffusion.github.io"}
{"id": "2401.10266", "pdf": "https://arxiv.org/pdf/2401.10266", "abs": "https://arxiv.org/abs/2401.10266", "authors": ["Maryam Ahang", "Todd Charter", "Oluwaseyi Ogunfowora", "Maziyar Khadivi", "Mostafa Abbasi", "Homayoun Najjaran"], "title": "Intelligent Condition Monitoring of Industrial Plants: An Overview of Methodologies and Uncertainty Management Strategies", "categories": ["cs.LG", "cs.AI", "cs.SY", "eess.SP", "eess.SY"], "comment": null, "summary": "Condition monitoring plays a significant role in the safety and reliability\nof modern industrial systems. Artificial intelligence (AI) approaches are\ngaining attention from academia and industry as a growing subject in industrial\napplications and as a powerful way of identifying faults. This paper provides\nan overview of intelligent condition monitoring and fault detection and\ndiagnosis methods for industrial plants with a focus on the open-source\nbenchmark Tennessee Eastman Process (TEP). In this survey, the most popular and\nstate-of-the-art deep learning (DL) and machine learning (ML) algorithms for\nindustrial plant condition monitoring, fault detection, and diagnosis are\nsummarized and the advantages and disadvantages of each algorithm are studied.\nChallenges like imbalanced data, unlabelled samples and how deep learning\nmodels can handle them are also covered. Finally, a comparison of the\naccuracies and specifications of different algorithms utilizing the Tennessee\nEastman Process (TEP) is conducted. This research will be beneficial for both\nresearchers who are new to the field and experts, as it covers the literature\non condition monitoring and state-of-the-art methods alongside the challenges\nand possible solutions to them."}
{"id": "2504.20965", "pdf": "https://arxiv.org/pdf/2504.20965", "abs": "https://arxiv.org/abs/2504.20965", "authors": ["Zikui Cai", "Shayan Shabihi", "Bang An", "Zora Che", "Brian R. Bartoldson", "Bhavya Kailkhura", "Tom Goldstein", "Furong Huang"], "title": "AegisLLM: Scaling Agentic Systems for Self-Reflective Defense in LLM Security", "categories": ["cs.LG"], "comment": "ICLR 2025 Workshop BuildingTrust", "summary": "We introduce AegisLLM, a cooperative multi-agent defense against adversarial\nattacks and information leakage. In AegisLLM, a structured workflow of\nautonomous agents - orchestrator, deflector, responder, and evaluator -\ncollaborate to ensure safe and compliant LLM outputs, while self-improving over\ntime through prompt optimization. We show that scaling agentic reasoning system\nat test-time - both by incorporating additional agent roles and by leveraging\nautomated prompt optimization (such as DSPy)- substantially enhances robustness\nwithout compromising model utility. This test-time defense enables real-time\nadaptability to evolving attacks, without requiring model retraining.\nComprehensive evaluations across key threat scenarios, including unlearning and\njailbreaking, demonstrate the effectiveness of AegisLLM. On the WMDP unlearning\nbenchmark, AegisLLM achieves near-perfect unlearning with only 20 training\nexamples and fewer than 300 LM calls. For jailbreaking benchmarks, we achieve\n51% improvement compared to the base model on StrongReject, with false refusal\nrates of only 7.9% on PHTest compared to 18-55% for comparable methods. Our\nresults highlight the advantages of adaptive, agentic reasoning over static\ndefenses, establishing AegisLLM as a strong runtime alternative to traditional\napproaches based on model modifications. Code is available at\nhttps://github.com/zikuicai/aegisllm"}
{"id": "2503.04992", "pdf": "https://arxiv.org/pdf/2503.04992", "abs": "https://arxiv.org/abs/2503.04992", "authors": ["Yifan Yang", "Kai Zhen", "Bhavana Ganesh", "Aram Galstyan", "Goeric Huybrechts", "Markus Müller", "Jonas M. Kübler", "Rupak Vignesh Swaminathan", "Athanasios Mouchtaris", "Sravan Babu Bodapati", "Nathan Susanj", "Zheng Zhang", "Jack FitzGerald", "Abhishek Kumar"], "title": "Wanda++: Pruning Large Language Models via Regional Gradients", "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": null, "summary": "Large Language Models (LLMs) pruning seeks to remove unimportant weights for\ninference speedup with minimal performance impact. However, existing methods\noften suffer from performance loss without full-model sparsity-aware\nfine-tuning. This paper presents Wanda++, a novel pruning framework that\noutperforms the state-of-the-art methods by utilizing decoder-block-level\n\\textbf{regional} gradients. Specifically, Wanda++ improves the pruning score\nwith regional gradients for the first time and proposes an efficient regional\noptimization method to minimize pruning-induced output discrepancies between\nthe dense and sparse decoder output. Notably, Wanda++ improves perplexity by up\nto 32\\% over Wanda in the language modeling task and generalizes effectively to\ndownstream tasks. Further experiments indicate our proposed method is\northogonal to sparsity-aware fine-tuning, where Wanda++ can be combined with\nLoRA fine-tuning to achieve a similar perplexity improvement as the Wanda\nmethod. The proposed method is lightweight, pruning a 7B LLaMA model in under\n10 minutes on a single NVIDIA H100 GPU."}
{"id": "2410.20084", "pdf": "https://arxiv.org/pdf/2410.20084", "abs": "https://arxiv.org/abs/2410.20084", "authors": ["Quanjian Song", "Mingbao Lin", "Wengyi Zhan", "Shuicheng Yan", "Liujuan Cao", "Rongrong Ji"], "title": "UniVST: A Unified Framework for Training-free Localized Video Style Transfer", "categories": ["cs.CV"], "comment": "14 pages including reference", "summary": "This paper presents UniVST, a unified framework for localized video style\ntransfer based on diffusion models. It operates without the need for training,\noffering a distinct advantage over existing diffusion methods that transfer\nstyle across entire videos. The endeavors of this paper comprise: (1) A\npoint-matching mask propagation strategy that leverages the feature maps from\nthe DDIM inversion. This streamlines the model's architecture by obviating the\nneed for tracking models. (2) A training-free AdaIN-guided localized video\nstylization mechanism that operates at both the latent and attention levels.\nThis balances content fidelity and style richness, mitigating the loss of\nlocalized details commonly associated with direct video stylization. (3) A\nsliding-window consistent smoothing scheme that harnesses optical flow within\nthe pixel representation and refines predicted noise to update the latent\nspace. This significantly enhances temporal consistency and diminishes\nartifacts in stylized video. Our proposed UniVST has been validated to be\nsuperior to existing methods in quantitative and qualitative metrics. It\nadeptly addresses the challenges of preserving the primary object's style while\nensuring temporal consistency and detail preservation. Our code is available at\nhttps://github.com/QuanjianSong/UniVST."}
{"id": "2403.15509", "pdf": "https://arxiv.org/pdf/2403.15509", "abs": "https://arxiv.org/abs/2403.15509", "authors": ["Phai Vu Dinh", "Quang Uy Nguyen", "Thai Hoang Dinh", "Diep N. Nguyen", "Bao Son Pham", "Eryk Dutkiewicz"], "title": "Twin Auto-Encoder Model for Learning Separable Representation in Cyberattack Detection", "categories": ["cs.CR", "cs.AI", "cs.LG"], "comment": null, "summary": "Representation learning (RL) methods for cyberattack detection face the\ndiversity and sophistication of attack data, leading to the issue of mixed\nrepresentations of different classes, particularly as the number of classes\nincreases. To address this, the paper proposes a novel deep learning\narchitecture/model called the Twin Auto-Encoder (TAE). TAE first maps the input\ndata into latent space and then deterministically shifts data samples of\ndifferent classes further apart to create separable data representations,\nreferred to as representation targets. TAE's decoder then projects the input\ndata into these representation targets. After training, TAE's decoder extracts\ndata representations. TAE's representation target serves as a novel dynamic\ncodeword, which refers to the vector that represents a specific class. This\nvector is updated after each training epoch for every data sample, in contrast\nto the conventional fixed codeword that does not incorporate information from\nthe input data. We conduct extensive experiments on diverse cybersecurity\ndatasets, including seven IoT botnet datasets, two network IDS datasets, three\nmalware datasets, one cloud DDoS dataset, and ten artificial datasets as the\nnumber of classes increases. TAE boosts accuracy and F-score in attack\ndetection by around 2% compared to state-of-the-art models, achieving up to\n96.1% average accuracy in IoT attack detection. Additionally, TAE is\nwell-suited for cybersecurity applications and potentially for IoT systems,\nwith a model size of approximately 1 MB and an average running time of around\n2.6E-07 seconds for extracting a data sample."}
{"id": "2504.20966", "pdf": "https://arxiv.org/pdf/2504.20966", "abs": "https://arxiv.org/abs/2504.20966", "authors": ["Zayd M. K. Zuhri", "Erland Hilman Fuadi", "Alham Fikri Aji"], "title": "Softpick: No Attention Sink, No Massive Activations with Rectified Softmax", "categories": ["cs.LG"], "comment": null, "summary": "We introduce softpick, a rectified, not sum-to-one, drop-in replacement for\nsoftmax in transformer attention mechanisms that eliminates attention sink and\nmassive activations. Our experiments with 340M parameter models demonstrate\nthat softpick maintains performance parity with softmax on standard benchmarks\nwhile achieving 0% sink rate. The softpick transformer produces hidden states\nwith significantly lower kurtosis (340 vs 33,510) and creates sparse attention\nmaps (46.97% sparsity). Models using softpick consistently outperform softmax\nwhen quantized, with particularly pronounced advantages at lower bit\nprecisions. Our analysis and discussion shows how softpick has the potential to\nopen new possibilities for quantization, low-precision training, sparsity\noptimization, pruning, and interpretability. Our code is available at\nhttps://github.com/zaydzuhri/softpick-attention."}
{"id": "2503.09089", "pdf": "https://arxiv.org/pdf/2503.09089", "abs": "https://arxiv.org/abs/2503.09089", "authors": ["Zhaoling Chen", "Xiangru Tang", "Gangda Deng", "Fang Wu", "Jialong Wu", "Zhiwei Jiang", "Viktor Prasanna", "Arman Cohan", "Xingyao Wang"], "title": "LocAgent: Graph-Guided LLM Agents for Code Localization", "categories": ["cs.SE", "cs.AI", "cs.CL"], "comment": null, "summary": "Code localization--identifying precisely where in a codebase changes need to\nbe made--is a fundamental yet challenging task in software maintenance.\nExisting approaches struggle to efficiently navigate complex codebases when\nidentifying relevant code sections. The challenge lies in bridging natural\nlanguage problem descriptions with the appropriate code elements, often\nrequiring reasoning across hierarchical structures and multiple dependencies.\nWe introduce LocAgent, a framework that addresses code localization through\ngraph-based representation. By parsing codebases into directed heterogeneous\ngraphs, LocAgent creates a lightweight representation that captures code\nstructures (files, classes, functions) and their dependencies (imports,\ninvocations, inheritance), enabling LLM agents to effectively search and locate\nrelevant entities through powerful multi-hop reasoning. Experimental results on\nreal-world benchmarks demonstrate that our approach significantly enhances\naccuracy in code localization. Notably, our method with the fine-tuned\nQwen-2.5-Coder-Instruct-32B model achieves comparable results to SOTA\nproprietary models at greatly reduced cost (approximately 86% reduction),\nreaching up to 92.7% accuracy on file-level localization while improving\ndownstream GitHub issue resolution success rates by 12% for multiple attempts\n(Pass@10). Our code is available at https://github.com/gersteinlab/LocAgent."}
{"id": "2411.01263", "pdf": "https://arxiv.org/pdf/2411.01263", "abs": "https://arxiv.org/abs/2411.01263", "authors": ["Xingming Long", "Jie Zhang", "Shiguang Shan"], "title": "Confidence Aware Learning for Reliable Face Anti-spoofing", "categories": ["cs.CV"], "comment": "11 pages", "summary": "Current Face Anti-spoofing (FAS) models tend to make overly confident\npredictions even when encountering unfamiliar scenarios or unknown presentation\nattacks, which leads to serious potential risks. To solve this problem, we\npropose a Confidence Aware Face Anti-spoofing (CA-FAS) model, which is aware of\nits capability boundary, thus achieving reliable liveness detection within this\nboundary. To enable the CA-FAS to \"know what it doesn't know\", we propose to\nestimate its confidence during the prediction of each sample. Specifically, we\nbuild Gaussian distributions for both the live faces and the known attacks. The\nprediction confidence for each sample is subsequently assessed using the\nMahalanobis distance between the sample and the Gaussians for the \"known data\".\nWe further introduce the Mahalanobis distance-based triplet mining to optimize\nthe parameters of both the model and the constructed Gaussians as a whole.\nExtensive experiments show that the proposed CA-FAS can effectively recognize\nsamples with low prediction confidence and thus achieve much more reliable\nperformance than other FAS models by filtering out samples that are beyond its\nreliable range."}
{"id": "2405.00252", "pdf": "https://arxiv.org/pdf/2405.00252", "abs": "https://arxiv.org/abs/2405.00252", "authors": ["Pingzhi Li", "Junyu Liu", "Hanrui Wang", "Tianlong Chen"], "title": "Q-Newton: Hybrid Quantum-Classical Scheduling for Accelerating Neural Network Training with Newton's Gradient Descent", "categories": ["quant-ph", "cs.AI", "cs.LG"], "comment": "Our code is provided at https://github.com/UNITES-Lab/q-newton", "summary": "Optimization techniques in deep learning are predominantly led by first-order\ngradient methodologies, such as SGD. However, neural network training can\ngreatly benefit from the rapid convergence characteristics of second-order\noptimization. Newton's GD stands out in this category, by rescaling the\ngradient using the inverse Hessian. Nevertheless, one of its major bottlenecks\nis matrix inversion, which is notably time-consuming in $O(N^3)$ time with weak\nscalability.\n  Matrix inversion can be translated into solving a series of linear equations.\nGiven that quantum linear solver algorithms (QLSAs), leveraging the principles\nof quantum superposition and entanglement, can operate within a\n$\\text{polylog}(N)$ time frame, they present a promising approach with\nexponential acceleration. Specifically, one of the most recent QLSAs\ndemonstrates a complexity scaling of $O(d\\cdot\\kappa\n\\log(N\\cdot\\kappa/\\epsilon))$, depending on: {size~$N$, condition\nnumber~$\\kappa$, error tolerance~$\\epsilon$, quantum oracle sparsity~$d$} of\nthe matrix. However, this also implies that their potential exponential\nadvantage may be hindered by certain properties (i.e. $\\kappa$ and $d$).\n  We propose Q-Newton, a hybrid quantum-classical scheduler for accelerating\nneural network training with Newton's GD. Q-Newton utilizes a streamlined\nscheduling module that coordinates between quantum and classical linear\nsolvers, by estimating & reducing $\\kappa$ and constructing $d$ for the quantum\nsolver.\n  Our evaluation showcases the potential for Q-Newton to significantly reduce\nthe total training time compared to commonly used optimizers like SGD. We\nhypothesize a future scenario where the gate time of quantum machines is\nreduced, possibly realized by attoseconds physics. Our evaluation establishes\nan ambitious and promising target for the evolution of quantum computing."}
{"id": "2504.20974", "pdf": "https://arxiv.org/pdf/2504.20974", "abs": "https://arxiv.org/abs/2504.20974", "authors": ["Elias Nyholm", "Oscar Carlsson", "Maurice Weiler", "Daniel Persson"], "title": "Equivariant non-linear maps for neural networks on homogeneous spaces", "categories": ["cs.LG", "math.RT", "stat.ML"], "comment": "45 pages,10 figures", "summary": "This paper presents a novel framework for non-linear equivariant neural\nnetwork layers on homogeneous spaces. The seminal work of Cohen et al. on\nequivariant $G$-CNNs on homogeneous spaces characterized the representation\ntheory of such layers in the linear setting, finding that they are given by\nconvolutions with kernels satisfying so-called steerability constraints.\nMotivated by the empirical success of non-linear layers, such as self-attention\nor input dependent kernels, we set out to generalize these insights to the\nnon-linear setting. We derive generalized steerability constraints that any\nsuch layer needs to satisfy and prove the universality of our construction. The\ninsights gained into the symmetry-constrained functional dependence of\nequivariant operators on feature maps and group elements informs the design of\nfuture equivariant neural network layers. We demonstrate how several common\nequivariant network architectures - $G$-CNNs, implicit steerable kernel\nnetworks, conventional and relative position embedded attention based\ntransformers, and LieTransformers - may be derived from our framework."}
{"id": "2504.13120", "pdf": "https://arxiv.org/pdf/2504.13120", "abs": "https://arxiv.org/abs/2504.13120", "authors": ["Yongqian Peng", "Yuxi Ma", "Mengmeng Wang", "Yuxuan Wang", "Yizhou Wang", "Chi Zhang", "Yixin Zhu", "Zilong Zheng"], "title": "Probing and Inducing Combinational Creativity in Vision-Language Models", "categories": ["cs.CV", "cs.AI", "cs.CL"], "comment": "Project page: https://ppyyqq.github.io/aicc/ The first two authors\n  contribute equally", "summary": "The ability to combine existing concepts into novel ideas stands as a\nfundamental hallmark of human intelligence. Recent advances in Vision-Language\nModels (VLMs) like GPT-4V and DALLE-3 have sparked debate about whether their\noutputs reflect combinational creativity--defined by M. A. Boden (1998) as\nsynthesizing novel ideas through combining existing concepts--or sophisticated\npattern matching of training data. Drawing inspiration from cognitive science,\nwe investigate the combinational creativity of VLMs from the lens of concept\nblending. We propose the Identification-Explanation-Implication (IEI)\nframework, which decomposes creative processes into three levels: identifying\ninput spaces, extracting shared attributes, and deriving novel semantic\nimplications. To validate this framework, we curate CreativeMashup, a\nhigh-quality dataset of 666 artist-generated visual mashups annotated according\nto the IEI framework. Through extensive experiments, we demonstrate that in\ncomprehension tasks, best VLMs have surpassed average human performance while\nfalling short of expert-level understanding; in generation tasks, incorporating\nour IEI framework into the generation pipeline significantly enhances the\ncreative quality of VLMs' outputs. Our findings establish both a theoretical\nfoundation for evaluating artificial creativity and practical guidelines for\nimproving creative generation in VLMs."}
{"id": "2411.01411", "pdf": "https://arxiv.org/pdf/2411.01411", "abs": "https://arxiv.org/abs/2411.01411", "authors": ["Amit Misra", "Kevin White", "Simone Fobi Nsutezo", "William Straka", "Juan Lavista"], "title": "Mapping Global Floods with 10 Years of Satellite Radar Data", "categories": ["cs.CV"], "comment": "20 pages, 8 figures, Accepted", "summary": "Floods cause extensive global damage annually, making effective monitoring\nessential. While satellite observations have proven invaluable for flood\ndetection and tracking, comprehensive global flood datasets spanning extended\ntime periods remain scarce. In this study, we introduce a novel deep learning\nflood detection model that leverages the cloud-penetrating capabilities of\nSentinel-1 Synthetic Aperture Radar (SAR) satellite imagery, enabling\nconsistent flood extent mapping in through cloud cover and in both day and\nnight conditions. By applying this model to 10 years of SAR data, we create a\nunique, longitudinal global flood extent dataset with predictions unaffected by\ncloud coverage, offering comprehensive and consistent insights into\nhistorically flood-prone areas over the past decade. We use our model\npredictions to identify historically flood-prone areas in Ethiopia and\ndemonstrate real-time disaster response capabilities during the May 2024 floods\nin Kenya. Additionally, our longitudinal analysis reveals potential increasing\ntrends in global flood extent over time, although further validation is\nrequired to explore links to climate change. To maximize impact, we provide\npublic access to both our model predictions and a code repository, empowering\nresearchers and practitioners worldwide to advance flood monitoring and enhance\ndisaster response strategies."}
{"id": "2408.05231", "pdf": "https://arxiv.org/pdf/2408.05231", "abs": "https://arxiv.org/abs/2408.05231", "authors": ["Bogdan Łobodziński"], "title": "Predictive maintenance solution for industrial systems -- an unsupervised approach based on log periodic power law", "categories": ["stat.AP", "cs.AI", "physics.data-an"], "comment": "14 pages, 4 figures, 1 table", "summary": "A new unsupervised predictive maintenance analysis method based on the\nrenormalization group approach used to discover critical behavior in complex\nsystems has been proposed. The algorithm analyzes univariate time series and\ndetects critical points based on a newly proposed theorem that identifies\ncritical points using a Log Periodic Power Law function fits. Application of a\nnew algorithm for predictive maintenance analysis of industrial data collected\nfrom reciprocating compressor systems is presented. Based on the knowledge of\nthe dynamics of the analyzed compressor system, the proposed algorithm predicts\nvalve and piston rod seal failures well in advance."}
{"id": "2504.20058", "pdf": "https://arxiv.org/pdf/2504.20058", "abs": "https://arxiv.org/abs/2504.20058", "authors": ["Ambedkar Dukkipati", "Kawin Mayilvaghanan", "Naveen Kumar Pallekonda", "Sai Prakash Hadnoor", "Ranga Shaarad Ayyagari"], "title": "Predictive AI with External Knowledge Infusion for Stocks", "categories": ["q-fin.ST", "cs.LG"], "comment": null, "summary": "Fluctuations in stock prices are influenced by a complex interplay of factors\nthat go beyond mere historical data. These factors, themselves influenced by\nexternal forces, encompass inter-stock dynamics, broader economic factors,\nvarious government policy decisions, outbreaks of wars, etc. Furthermore, all\nof these factors are dynamic and exhibit changes over time. In this paper, for\nthe first time, we tackle the forecasting problem under external influence by\nproposing learning mechanisms that not only learn from historical trends but\nalso incorporate external knowledge from temporal knowledge graphs. Since there\nare no such datasets or temporal knowledge graphs available, we study this\nproblem with stock market data, and we construct comprehensive temporal\nknowledge graph datasets. In our proposed approach, we model relations on\nexternal temporal knowledge graphs as events of a Hawkes process on graphs.\nWith extensive experiments, we show that learned dynamic representations\neffectively rank stocks based on returns across multiple holding periods,\noutperforming related baselines on relevant metrics."}
{"id": "2504.17365", "pdf": "https://arxiv.org/pdf/2504.17365", "abs": "https://arxiv.org/abs/2504.17365", "authors": ["Ling You", "Wenxuan Huang", "Xinni Xie", "Xiangyi Wei", "Bangyan Li", "Shaohui Lin", "Yang Li", "Changbo Wang"], "title": "TimeSoccer: An End-to-End Multimodal Large Language Model for Soccer Commentary Generation", "categories": ["cs.CV", "cs.CL"], "comment": null, "summary": "Soccer is a globally popular sporting event, typically characterized by long\nmatches and distinctive highlight moments. Recent advances in Multimodal Large\nLanguage Models (MLLMs) offer promising capabilities in temporal grounding and\nvideo understanding, soccer commentary generation often requires precise\ntemporal localization and semantically rich descriptions over long-form video.\nHowever, existing soccer MLLMs often rely on the temporal a priori for caption\ngeneration, so they cannot process the soccer video end-to-end. While some\ntraditional approaches follow a two-step paradigm that is complex and fails to\ncapture the global context to achieve suboptimal performance. To solve the\nabove issues, we present TimeSoccer, the first end-to-end soccer MLLM for\nSingle-anchor Dense Video Captioning (SDVC) in full-match soccer videos.\nTimeSoccer jointly predicts timestamps and generates captions in a single pass,\nenabling global context modeling across 45-minute matches. To support long\nvideo understanding of soccer matches, we introduce MoFA-Select, a\ntraining-free, motion-aware frame compression module that adaptively selects\nrepresentative frames via a coarse-to-fine strategy, and incorporates\ncomplementary training paradigms to strengthen the model's ability to handle\nlong temporal sequences. Extensive experiments demonstrate that our TimeSoccer\nachieves State-of-The-Art (SoTA) performance on the SDVC task in an end-to-end\nform, generating high-quality commentary with accurate temporal alignment and\nstrong semantic relevance."}
{"id": "2411.09413", "pdf": "https://arxiv.org/pdf/2411.09413", "abs": "https://arxiv.org/abs/2411.09413", "authors": ["Wenxing Liu", "Yueran Pan", "Dong Zhang", "Hongzhu Deng", "Xiaobing Zou", "Ming Li"], "title": "Detecting Children with Autism Spectrum Disorder based on Script-Centric Behavior Understanding with Emotional Enhancement", "categories": ["cs.CV", "cs.AI"], "comment": "15 pages, 12 figures, sumbitted to IEEE transactions on affective\n  computing", "summary": "The early diagnosis of autism spectrum disorder (ASD) is critically dependent\non systematic observation and analysis of children's social behaviors. While\ncurrent methodologies predominantly utilize supervised learning approaches,\ntheir clinical adoption faces two principal limitations: insufficient ASD\ndiagnostic samples and inadequate interpretability of the detection outcomes.\nThis paper presents a novel zero-shot ASD detection framework based on\nscript-centric behavioral understanding with emotional enhancement, which is\ndesigned to overcome the aforementioned clinical constraints. The proposed\npipeline automatically converts audio-visual data into structured behavioral\ntext scripts through computer vision techniques, subsequently capitalizing on\nthe generalization capabilities of large language models (LLMs) for\nzero-shot/few-shot ASD detection. Three core technical contributions are\nintroduced: (1) A multimodal script transcription module transforming\nbehavioral cues into structured textual representations. (2) An emotion\ntextualization module encoding emotional dynamics as the contextual features to\naugment behavioral understanding. (3) A domain-specific prompt engineering\nstrategy enables the injection of clinical knowledge into LLMs. Our method\nachieves an F1-score of 95.24\\% in diagnosing ASD in children with an average\nage of two years while generating interpretable detection rationales. This work\nopens up new avenues for leveraging the power of LLMs in analyzing and\nunderstanding ASD-related human behavior, thereby enhancing the accuracy of\nassisted autism diagnosis."}
{"id": "2408.15268", "pdf": "https://arxiv.org/pdf/2408.15268", "abs": "https://arxiv.org/abs/2408.15268", "authors": ["Dominic Schneider", "Lutz Rapp", "Christoph Ament"], "title": "Anomaly Detection in Time Series of EDFA Pump Currents to Monitor Degeneration Processes using Fuzzy Clustering", "categories": ["eess.SP", "cs.AI", "cs.LG"], "comment": "6 pages, 6 figures", "summary": "This article proposes a novel fuzzy clustering based anomaly detection method\nfor pump current time series of EDFA systems. The proposed change detection\nframework (CDF) strategically combines the advantages of entropy analysis (EA)\nand principle component analysis (PCA) with fuzzy clustering procedures. In the\nframework, EA is applied for dynamic selection of features for reduction of the\nfeature space and increase of computational performance. Furthermore, PCA is\nutilized to extract features from the raw feature space to enable\ngeneralization capability of the subsequent fuzzy clustering procedures. Three\ndifferent fuzzy clustering methods, more precisely the fuzzy clustering\nalgorithm, a probabilistic clustering algorithm and a possibilistic clustering\nalgorithm are evaluated for performance and generalization. Hence, the proposed\nframework has the innovative feature to detect changes in pump current time\nseries at an early stage for arbitrary points of operation, compared to\nstate-of-the-art predefined alarms in commercially used EDFAs. Moreover, the\napproach is implemented and tested using experimental data. In addition, the\nproposed framework enables further approaches of applying decentralized\npredictive maintenance for optical fiber networks."}
{"id": "2504.20068", "pdf": "https://arxiv.org/pdf/2504.20068", "abs": "https://arxiv.org/abs/2504.20068", "authors": ["Wei Zhang", "Zhiyu Wu", "Yi Mu", "Banruo Liu", "Myungjin Lee", "Fan Lai"], "title": "Tempo: Application-aware LLM Serving with Mixed SLO Requirements", "categories": ["cs.DC", "cs.LG", "cs.SY", "eess.SY"], "comment": null, "summary": "The integration of Large Language Models (LLMs) into diverse applications,\nranging from interactive chatbots and cloud AIOps to intelligent agents, has\nintroduced a wide spectrum of Service Level Objectives (SLOs) for\nresponsiveness. These workloads include latency-sensitive requests focused on\nper-token latency in streaming chat, throughput-intensive requests that require\nrapid full responses to invoke tools, and collective requests with dynamic\ndependencies arising from self-reflection or agent-based reasoning. This\nworkload diversity, amplified by unpredictable request information such as\nresponse lengths and runtime dependencies, makes existing schedulers inadequate\neven within their design envelopes.\n  In this paper, we define service gain as the useful service delivered by\ncompleting requests. We observe that as SLO directly reflects the actual\nperformance needs of requests, completing a request much faster than its SLO\n(e.g., deadline) yields limited additional service gain. Based on this insight,\nwe introduce Tempo, the first systematic SLO-aware scheduler designed to\nmaximize service gain across diverse LLM workloads. Tempo allocates just enough\nserving bandwidth to meet each SLO, maximizing residual capacity for others\nbest-effort workloads. Instead of assuming request information or none at all,\nit adopts a hybrid scheduling strategy: using quantile-based response upper\nbounds and dependency-graph matching for conservative initial estimates,\nprioritizing requests by service gain density, and refining decisions online as\ngeneration progresses. Our evaluation across diverse workloads, including chat,\nreasoning, and agentic pipelines, shows that Tempo improves end-to-end service\ngain by up to 8.3$\\times$ and achieves up to 10.3$\\times$ SLO goodput compared\nto state-of-the-art designs"}
{"id": "2411.10013", "pdf": "https://arxiv.org/pdf/2411.10013", "abs": "https://arxiv.org/abs/2411.10013", "authors": ["Yongfan Liu", "Hyoukjun Kwon"], "title": "Efficient Depth Estimation for Unstable Stereo Camera Systems on AR Glasses", "categories": ["cs.CV", "cs.LG"], "comment": "12 pages, 11 figures. Accepted to CVPR 2024", "summary": "Stereo depth estimation is a fundamental component in augmented reality (AR),\nwhich requires low latency for real-time processing. However, preprocessing\nsuch as rectification and non-ML computations such as cost volume require\nsignificant amount of latency exceeding that of an ML model itself, which\nhinders the real-time processing required by AR. Therefore, we develop\nalternative approaches to the rectification and cost volume that consider ML\nacceleration (GPU and NPUs) in recent hardware. For pre-processing, we\neliminate it by introducing homography matrix prediction network with a\nrectification positional encoding (RPE), which delivers both low latency and\nrobustness to unrectified images. For cost volume, we replace it with a\ngroup-pointwise convolution-based operator and approximation of cosine\nsimilarity based on layernorm and dot product. Based on our approaches, we\ndevelop MultiHeadDepth (replacing cost volume) and HomoDepth (MultiHeadDepth +\nremoving pre-processing) models. MultiHeadDepth provides 11.8-30.3%\nimprovements in accuracy and 22.9-25.2% reduction in latency compared to a\nstate-of-the-art depth estimation model for AR glasses from industry.\nHomoDepth, which can directly process unrectified images, reduces the\nend-to-end latency by 44.5%. We also introduce a multi-task learning method to\nhandle misaligned stereo inputs on HomoDepth, which reduces the AbsRel error by\n10.0-24.3%. The overall results demonstrate the efficacy of our approaches,\nwhich not only reduce the inference latency but also improve the model\nperformance. Our code is available at\nhttps://github.com/UCI-ISA-Lab/MultiHeadDepth-HomoDepth"}
{"id": "2409.14634", "pdf": "https://arxiv.org/pdf/2409.14634", "abs": "https://arxiv.org/abs/2409.14634", "authors": ["Marissa Radensky", "Simra Shahid", "Raymond Fok", "Pao Siangliulue", "Tom Hope", "Daniel S. Weld"], "title": "Scideator: Human-LLM Scientific Idea Generation Grounded in Research-Paper Facet Recombination", "categories": ["cs.HC", "cs.AI", "H.5.2, I.2"], "comment": "Updated with new and improved user study", "summary": "The scientific ideation process often involves blending salient aspects of\nexisting papers to create new ideas, and facet-based ideation is an established\nframework for idea generation. To see how large language models (LLMs) might\nassist in this process, we contribute a novel mixed-initiative ideation tool\ncalled Scideator. Starting from a user-provided set of scientific papers,\nScideator extracts key facets -- purposes, mechanisms, and evaluations -- from\nthese and related papers, allowing users to explore the idea space by\ninteractively recombining facets to synthesize inventive ideas. Scideator also\nhelps users gauge idea originality by searching the literature for overlaps,\nassessing idea novelty and providing explanations. To support these tasks,\nScideator introduces three LLM-powered retrieval-augmented generation (RAG)\nmodules: Analogous Paper Facet Finder, Faceted Idea Generator, and Idea Novelty\nChecker. In a within-subjects user study (N=22) with computer-science\nresearchers comparing Scideator to a strong baseline, our tool provided\nsignificantly more creativity support, particularly with respect to\nexploration, which participants considered the most important factor for idea\ngeneration."}
{"id": "2504.20088", "pdf": "https://arxiv.org/pdf/2504.20088", "abs": "https://arxiv.org/abs/2504.20088", "authors": ["Joao Felipe Gueiros", "Hemanth Chandravamsi", "Steven H. Frankel"], "title": "Deep Learning vs. Black-Scholes: Option Pricing Performance on Brazilian Petrobras Stocks", "categories": ["q-fin.ST", "cs.LG"], "comment": "11 pages, 7 figures, 3 tables", "summary": "This paper explores the use of deep residual networks for pricing European\noptions on Petrobras, one of the world's largest oil and gas producers, and\ncompares its performance with the Black-Scholes (BS) model. Using eight years\nof historical data from B3 (Brazilian Stock Exchange) collected via web\nscraping, a deep learning model was trained using a custom built hybrid loss\nfunction that incorporates market data and analytical pricing. The data for\ntraining and testing were drawn between the period spanning November 2016 to\nJanuary 2025, using an 80-20 train-test split. The test set consisted of data\nfrom the final three months: November, December, and January 2025. The deep\nresidual network model achieved a 64.3\\% reduction in the mean absolute error\nfor the 3-19 BRL (Brazilian Real) range when compared to the Black-Scholes\nmodel on the test set. Furthermore, unlike the Black-Scholes solution, which\ntends to decrease its accuracy for longer periods of time, the deep learning\nmodel performed accurately for longer expiration periods. These findings\nhighlight the potential of deep learning in financial modeling, with future\nwork focusing on specialized models for different price ranges."}
{"id": "2411.14280", "pdf": "https://arxiv.org/pdf/2411.14280", "abs": "https://arxiv.org/abs/2411.14280", "authors": ["Yumeng Liu", "Xiaoxiao Long", "Zemin Yang", "Yuan Liu", "Marc Habermann", "Christian Theobalt", "Yuexin Ma", "Wenping Wang"], "title": "EasyHOI: Unleashing the Power of Large Models for Reconstructing Hand-Object Interactions in the Wild", "categories": ["cs.CV"], "comment": "Project page: https://lym29.github.io/EasyHOI-page/", "summary": "Our work aims to reconstruct hand-object interactions from a single-view\nimage, which is a fundamental but ill-posed task. Unlike methods that\nreconstruct from videos, multi-view images, or predefined 3D templates,\nsingle-view reconstruction faces significant challenges due to inherent\nambiguities and occlusions. These challenges are further amplified by the\ndiverse nature of hand poses and the vast variety of object shapes and sizes.\nOur key insight is that current foundational models for segmentation,\ninpainting, and 3D reconstruction robustly generalize to in-the-wild images,\nwhich could provide strong visual and geometric priors for reconstructing\nhand-object interactions. Specifically, given a single image, we first design a\nnovel pipeline to estimate the underlying hand pose and object shape using\noff-the-shelf large models. Furthermore, with the initial reconstruction, we\nemploy a prior-guided optimization scheme, which optimizes hand pose to comply\nwith 3D physical constraints and the 2D input image content. We perform\nexperiments across several datasets and show that our method consistently\noutperforms baselines and faithfully reconstructs a diverse set of hand-object\ninteractions. Here is the link of our project page:\nhttps://lym29.github.io/EasyHOI-page/"}
{"id": "2410.21491", "pdf": "https://arxiv.org/pdf/2410.21491", "abs": "https://arxiv.org/abs/2410.21491", "authors": ["Hongyang Li", "Caesar Wu", "Mohammed Chadli", "Said Mammar", "Pascal Bouvry"], "title": "Trustworthiness of Stochastic Gradient Descent in Distributed Learning", "categories": ["cs.LG", "cs.AI", "cs.DC"], "comment": null, "summary": "Distributed learning (DL) uses multiple nodes to accelerate training,\nenabling efficient optimization of large-scale models. Stochastic Gradient\nDescent (SGD), a key optimization algorithm, plays a central role in this\nprocess. However, communication bottlenecks often limit scalability and\nefficiency, leading to increasing adoption of compressed SGD techniques to\nalleviate these challenges. Despite addressing communication overheads,\ncompressed SGD introduces trustworthiness concerns, as gradient exchanges among\nnodes are vulnerable to attacks like gradient inversion (GradInv) and\nmembership inference attacks (MIA). The trustworthiness of compressed SGD\nremains unexplored, leaving important questions about its reliability\nunanswered.\n  In this paper, we provide a trustworthiness evaluation of compressed versus\nuncompressed SGD. Specifically, we conducted empirical studies using GradInv\nattacks, revealing that compressed SGD demonstrates significantly higher\nresistance to privacy leakage compared to uncompressed SGD. In addition, our\nfindings suggest that MIA may not be a reliable metric for assessing privacy\nrisks in distributed learning."}
{"id": "2504.20126", "pdf": "https://arxiv.org/pdf/2504.20126", "abs": "https://arxiv.org/abs/2504.20126", "authors": ["Matteo Testi", "Luca Clissa", "Matteo Ballabio", "Salvatore Ricciardi", "Federico Baldo", "Emanuele Frontoni", "Sara Moccia", "Gennario Vessio"], "title": "Enhancing Cell Counting through MLOps: A Structured Approach for Automated Cell Analysis", "categories": ["cs.SE", "cs.LG", "physics.app-ph"], "comment": "21 pages, 4 figures, 1 table", "summary": "Machine Learning (ML) models offer significant potential for advancing cell\ncounting applications in neuroscience, medical research, pharmaceutical\ndevelopment, and environmental monitoring. However, implementing these models\neffectively requires robust operational frameworks. This paper introduces Cell\nCounting Machine Learning Operations (CC-MLOps), a comprehensive framework that\nstreamlines the integration of ML in cell counting workflows. CC-MLOps\nencompasses data access and preprocessing, model training, monitoring,\nexplainability features, and sustainability considerations. Through a practical\nuse case, we demonstrate how MLOps principles can enhance model reliability,\nreduce human error, and enable scalable Cell Counting solutions. This work\nprovides actionable guidance for researchers and laboratory professionals\nseeking to implement machine learning (ML)- powered cell counting systems."}
{"id": "2411.14823", "pdf": "https://arxiv.org/pdf/2411.14823", "abs": "https://arxiv.org/abs/2411.14823", "authors": ["Chenfan Qu", "Yiwu Zhong", "Fengjun Guo", "Lianwen Jin"], "title": "Omni-IML: Towards Unified Image Manipulation Localization", "categories": ["cs.CV", "cs.CR", "cs.LG"], "comment": null, "summary": "Existing Image Manipulation Localization (IML) methods mostly rely heavily on\ntask-specific designs, making them perform well only on the target IML task,\nwhile joint training on multiple IML tasks causes significant performance\ndegradation, hindering real applications.\n  To this end, we propose Omni-IML, the first generalist model designed to\nunify IML across diverse tasks.\n  Specifically, Omni-IML achieves generalization through three key components:\n(1) a Modal Gate Encoder, which adaptively selects the optimal encoding\nmodality per sample, (2) a Dynamic Weight Decoder, which dynamically adjusts\ndecoder filters to the task at hand, and (3) an Anomaly Enhancement module that\nleverages box supervision to highlight the tampered regions and facilitate the\nlearning of task-agnostic features.\n  Beyond localization, to support interpretation of the tampered images, we\nconstruct Omni-273k, a large high-quality dataset that includes natural\nlanguage descriptions of tampered artifact. It is annotated through our\nautomatic, chain-of-thoughts annotation technique.\n  We also design a simple-yet-effective interpretation module to better utilize\nthese descriptive annotations.\n  Our extensive experiments show that our single Omni-IML model achieves\nstate-of-the-art performance across all four major IML tasks, providing a\nvaluable solution for practical deployment and a promising direction of\ngeneralist models in image forensics. Our code and dataset will be publicly\navailable."}
{"id": "2411.05982", "pdf": "https://arxiv.org/pdf/2411.05982", "abs": "https://arxiv.org/abs/2411.05982", "authors": ["Haizhou Wang", "Nanqing Luo", "Xusheng Li", "Peng LIu"], "title": "Unmasking the Shadows: Pinpoint the Implementations of Anti-Dynamic Analysis Techniques in Malware Using LLM", "categories": ["cs.CR", "cs.AI"], "comment": null, "summary": "Sandboxes and other dynamic analysis processes are prevalent in malware\ndetection systems nowadays to enhance the capability of detecting 0-day\nmalware. Therefore, techniques of anti-dynamic analysis (TADA) are prevalent in\nmodern malware samples, and sandboxes can suffer from false negatives and\nanalysis failures when analyzing the samples with TADAs. In such cases, human\nreverse engineers will get involved in conducting dynamic analysis manually\n(i.e., debugging, patching), which in turn also gets obstructed by TADAs. In\nthis work, we propose a Large Language Model (LLM) based workflow that can\npinpoint the location of the TADA implementation in the code, to help reverse\nengineers place breakpoints used in debugging. Our evaluation shows that we\nsuccessfully identified the locations of 87.80% known TADA implementations\nadopted from public repositories. In addition, we successfully pinpoint the\nlocations of TADAs in 4 well-known malware samples that are documented in\nonline malware analysis blogs."}
{"id": "2504.20127", "pdf": "https://arxiv.org/pdf/2504.20127", "abs": "https://arxiv.org/abs/2504.20127", "authors": ["Huiyang Hong", "Xinkai Wu", "Hongyu Sun", "Qi Wang", "Yuquan Li"], "title": "Learning Hierarchical Interaction for Accurate Molecular Property Prediction", "categories": ["q-bio.BM", "cs.LG"], "comment": null, "summary": "Discovering molecules with desirable molecular properties, including ADMET\n(Absorption, Distribution, Metabolism, Excretion, and Toxicity) profiles, is of\ngreat importance in drug discovery. Existing approaches typically employ deep\nlearning models, such as Graph Neural Networks (GNNs) and Transformers, to\npredict these molecular properties by learning from diverse chemical\ninformation. However, these models often fail to efficiently capture and\nutilize the hierarchical nature of molecular structures, and lack mechanisms\nfor effective interaction among multi-level features. To address these\nlimitations, we propose a Hierarchical Interaction Message Passing Mechanism,\nwhich serves as the foundation of our novel model, HimNet. Our method enables\ninteraction-aware representation learning across atomic, motif, and molecular\nlevels via hierarchical attention-guided message passing. This design allows\nHimNet to effectively balance global and local information, ensuring rich and\ntask-relevant feature extraction for downstream property prediction tasks, such\nas Blood-Brain Barrier Permeability (BBBP). Extensive experiments on multiple\nbenchmark datasets demonstrate that HimNet achieves the best or near-best\nperformance in most molecular property prediction tasks. Furthermore, our\nmethod exhibits promising hierarchical interpretability, aligning well with\nchemical intuition on representative molecules. We believe that HimNet offers\nan accurate and efficient solution for molecular activity and ADMET property\nprediction, contributing significantly to advanced decision-making in the early\nstages of drug discovery."}
{"id": "2412.04189", "pdf": "https://arxiv.org/pdf/2412.04189", "abs": "https://arxiv.org/abs/2412.04189", "authors": ["Yayuan Li", "Zhi Cao", "Jason J. Corso"], "title": "HANDI: Hand-Centric Text-and-Image Conditioned Video Generation", "categories": ["cs.CV"], "comment": "16 pages, 7 figures and 4 tables", "summary": "Despite the recent strides in video generation, state-of-the-art methods\nstill struggle with elements of visual detail. One particularly challenging\ncase is the class of videos in which the intricate motion of the hand coupled\nwith a mostly stable and otherwise distracting environment is necessary to\nconvey the execution of some complex action and its effects. To address these\nchallenges, we introduce a new method for video generation that focuses on\nhand-centric actions. Our diffusion-based method incorporates two distinct\ninnovations. First, we propose an automatic method to generate the motion area\n-- the region in the video in which the detailed activities occur -- guided by\nboth the visual context and the action text prompt, rather than assuming this\nregion can be provided manually as is now commonplace. Second, we introduce a\ncritical Hand Refinement Loss to guide the diffusion model to focus on smooth\nand consistent hand poses. We evaluate our method on challenging augmented\ndatasets based on EpicKitchens and Ego4D, demonstrating significant\nimprovements over state-of-the-art methods in terms of action clarity,\nespecially of the hand motion in the target region, across diverse environments\nand actions. Video results can be found in\nhttps://zhicaoisexcited.github.io/project_page"}
{"id": "2411.18384", "pdf": "https://arxiv.org/pdf/2411.18384", "abs": "https://arxiv.org/abs/2411.18384", "authors": ["Mattia Giovanni Spina", "Edoardo Scalzo", "Floriano De Rango", "Francesca Guerriero", "Antonio Iera"], "title": "Optimal In-Network Distribution of Learning Functions for a Secure-by-Design Programmable Data Plane of Next-Generation Networks", "categories": ["cs.NI", "cs.AI", "math.OC"], "comment": null, "summary": "The rise of programmable data plane (PDP) and in-network computing (INC)\nparadigms paves the way for the development of network devices (switches,\nnetwork interface cards, etc.) capable of performing advanced processing tasks.\nThis allows running various types of algorithms, including machine learning,\nwithin the network itself to support user and network services. In particular,\nthis paper delves into the deployment of in-network learning models with the\naim of implementing fully distributed intrusion detection systems (IDS) or\nintrusion prevention systems (IPS). Specifically, a model is proposed for the\noptimal distribution of the IDS/IPS workload among data plane devices with the\naim of ensuring complete network security without excessively burdening the\nnormal operations of the devices. Furthermore, a meta-heuristic approach is\nproposed to reduce the long computation time required by the exact solution\nprovided by the mathematical model and its performance is evaluated. The\nanalysis conducted and the results obtained demonstrate the enormous potential\nof the proposed new approach for the creation of intelligent data planes that\nact effectively and autonomously as the first line of defense against cyber\nattacks, with minimal additional workload on the network devices involved."}
{"id": "2504.20129", "pdf": "https://arxiv.org/pdf/2504.20129", "abs": "https://arxiv.org/abs/2504.20129", "authors": ["Arun M. Saranathan", "Mahmoud Saeedimoghaddam", "Brandon Smith", "Deepthi Raghunandan", "Grey Nearing", "Craig Pelissier"], "title": "A Physically Driven Long Short Term Memory Model for Estimating Snow Water Equivalent over the Continental United States", "categories": ["physics.ao-ph", "cs.LG"], "comment": "Preprint of journal paper in preparation. Details: 24 pages, 8\n  figures", "summary": "Snow is an essential input for various land surface models. Seasonal snow\nestimates are available as snow water equivalent (SWE) from process-based\nreanalysis products or locally from in situ measurements. While the reanalysis\nproducts are computationally expensive and available at only fixed spatial and\ntemporal resolutions, the in situ measurements are highly localized and sparse.\nTo address these issues and enable the analysis of the effect of a large suite\nof physical, morphological, and geological conditions on the presence and\namount of snow, we build a Long Short-Term Memory (LSTM) network, which is able\nto estimate the SWE based on time series input of the various\nphysical/meteorological factors as well static spatial/morphological factors.\nSpecifically, this model breaks down the SWE estimation into two separate\ntasks: (i) a classification task that indicates the presence/absence of snow on\na specific day and (ii) a regression task that indicates the height of the SWE\non a specific day in the case of snow presence. The model is trained using\nphysical/in situ SWE measurements from the SNOw TELemetry (SNOTEL) snow pillows\nin the western United States. We will show that trained LSTM models have a\nclassification accuracy of $\\geq 93\\%$ for the presence of snow and a\ncoefficient of correlation of $\\sim 0.9$ concerning their SWE estimates. We\nwill also demonstrate that the models can generalize both spatially and\ntemporally to previously unseen data."}
{"id": "2412.18565", "pdf": "https://arxiv.org/pdf/2412.18565", "abs": "https://arxiv.org/abs/2412.18565", "authors": ["Yihang Luo", "Shangchen Zhou", "Yushi Lan", "Xingang Pan", "Chen Change Loy"], "title": "3DEnhancer: Consistent Multi-View Diffusion for 3D Enhancement", "categories": ["cs.CV"], "comment": "Project page: https://yihangluo.com/projects/3DEnhancer", "summary": "Despite advances in neural rendering, due to the scarcity of high-quality 3D\ndatasets and the inherent limitations of multi-view diffusion models, view\nsynthesis and 3D model generation are restricted to low resolutions with\nsuboptimal multi-view consistency. In this study, we present a novel 3D\nenhancement pipeline, dubbed 3DEnhancer, which employs a multi-view latent\ndiffusion model to enhance coarse 3D inputs while preserving multi-view\nconsistency. Our method includes a pose-aware encoder and a diffusion-based\ndenoiser to refine low-quality multi-view images, along with data augmentation\nand a multi-view attention module with epipolar aggregation to maintain\nconsistent, high-quality 3D outputs across views. Unlike existing video-based\napproaches, our model supports seamless multi-view enhancement with improved\ncoherence across diverse viewing angles. Extensive evaluations show that\n3DEnhancer significantly outperforms existing methods, boosting both multi-view\nenhancement and per-instance 3D optimization tasks."}
{"id": "2501.01950", "pdf": "https://arxiv.org/pdf/2501.01950", "abs": "https://arxiv.org/abs/2501.01950", "authors": ["Yinkai Wang", "Xiaohui Chen", "Liping Liu", "Soha Hassoun"], "title": "MADGEN: Mass-Spec attends to De Novo Molecular generation", "categories": ["cs.LG", "cs.AI"], "comment": "ICLR 2025", "summary": "The annotation (assigning structural chemical identities) of MS/MS spectra\nremains a significant challenge due to the enormous molecular diversity in\nbiological samples and the limited scope of reference databases. Currently, the\nvast majority of spectral measurements remain in the \"dark chemical space\"\nwithout structural annotations. To improve annotation, we propose MADGEN\n(Mass-spec Attends to De Novo Molecular GENeration), a scaffold-based method\nfor de novo molecular structure generation guided by mass spectrometry data.\nMADGEN operates in two stages: scaffold retrieval and spectra-conditioned\nmolecular generation starting with the scaffold. In the first stage, given an\nMS/MS spectrum, we formulate scaffold retrieval as a ranking problem and employ\ncontrastive learning to align mass spectra with candidate molecular scaffolds.\nIn the second stage, starting from the retrieved scaffold, we employ the MS/MS\nspectrum to guide an attention-based generative model to generate the final\nmolecule. Our approach constrains the molecular generation search space,\nreducing its complexity and improving generation accuracy. We evaluate MADGEN\non three datasets (NIST23, CANOPUS, and MassSpecGym) and evaluate MADGEN's\nperformance with a predictive scaffold retriever and with an oracle retriever.\nWe demonstrate the effectiveness of using attention to integrate spectral\ninformation throughout the generation process to achieve strong results with\nthe oracle retriever."}
{"id": "2504.20194", "pdf": "https://arxiv.org/pdf/2504.20194", "abs": "https://arxiv.org/abs/2504.20194", "authors": ["Alex Kokot", "Alex Luedtke"], "title": "Coreset selection for the Sinkhorn divergence and generic smooth divergences", "categories": ["stat.ML", "cs.LG"], "comment": null, "summary": "We introduce CO2, an efficient algorithm to produce convexly-weighted\ncoresets with respect to generic smooth divergences. By employing a functional\nTaylor expansion, we show a local equivalence between sufficiently regular\nlosses and their second order approximations, reducing the coreset selection\nproblem to maximum mean discrepancy minimization. We apply CO2 to the Sinkhorn\ndivergence, providing a novel sampling procedure that requires logarithmically\nmany data points to match the approximation guarantees of random sampling. To\nshow this, we additionally verify several new regularity properties for\nentropically regularized optimal transport of independent interest. Our\napproach leads to a new perspective linking coreset selection and kernel\nquadrature to classical statistical methods such as moment and score matching.\nWe showcase this method with a practical application of subsampling image data,\nand highlight key directions to explore for improved algorithmic efficiency and\ntheoretical guarantees."}
{"id": "2501.09552", "pdf": "https://arxiv.org/pdf/2501.09552", "abs": "https://arxiv.org/abs/2501.09552", "authors": ["Tuan Truong", "Ivo M. Baltruschat", "Mark Klemens", "Grit Werner", "Matthias Lenga"], "title": "Exploring AI-based System Design for Pixel-level Protected Health Information Detection in Medical Images", "categories": ["cs.CV"], "comment": "In progress", "summary": "De-identification of medical images is a critical step to ensure privacy\nduring data sharing in research and clinical settings. The initial step in this\nprocess involves detecting Protected Health Information (PHI), which can be\nfound in image metadata or imprinted within image pixels. Despite the\nimportance of such systems, there has been limited evaluation of existing\nAI-based solutions, creating barriers to the development of reliable and robust\ntools. In this study, we present an AI-based pipeline for PHI detection,\ncomprising three key components: text detection, text extraction, and text\nanalysis. We benchmark three models, YOLOv11, EasyOCR, and GPT-4o, across\ndifferent setups corresponding to these components, evaluating the performance\nbased on precision, recall, F1 score, and accuracy. All setups demonstrate\nexcellent PHI detection, with all metrics exceeding 0.9. The combination of\nYOLOv11 for text localization and GPT-4o for extraction and analysis yields the\nbest results. However, this setup incurs higher costs due to GPT-4o's token\ngeneration. Conversely, an end-to-end pipeline that relies solely on GPT-4o\nshows lower performance but highlights the potential of multimodal models for\ncomplex tasks. We recommend fine-tuning a dedicated object detection model and\nutilizing built-in OCR tools to achieve optimal performance and\ncost-effectiveness. Additionally, leveraging language models such as GPT-4o can\nfacilitate thorough and flexible analysis of text content."}
{"id": "2501.02330", "pdf": "https://arxiv.org/pdf/2501.02330", "abs": "https://arxiv.org/abs/2501.02330", "authors": ["Seyed Mahdi B. Azad", "Zahra Padar", "Gabriel Kalweit", "Joschka Boedecker"], "title": "SR-Reward: Taking The Path More Traveled", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "In this paper, we propose a novel method for learning reward functions\ndirectly from offline demonstrations. Unlike traditional inverse reinforcement\nlearning (IRL), our approach decouples the reward function from the learner's\npolicy, eliminating the adversarial interaction typically required between the\ntwo. This results in a more stable and efficient training process. Our reward\nfunction, called \\textit{SR-Reward}, leverages successor representation (SR) to\nencode a state based on expected future states' visitation under the\ndemonstration policy and transition dynamics. By utilizing the Bellman\nequation, SR-Reward can be learned concurrently with most reinforcement\nlearning (RL) algorithms without altering the existing training pipeline. We\nalso introduce a negative sampling strategy to mitigate overestimation errors\nby reducing rewards for out-of-distribution data, thereby enhancing robustness.\nThis strategy inherently introduces a conservative bias into RL algorithms that\nemploy the learned reward. We evaluate our method on the D4RL benchmark,\nachieving competitive results compared to offline RL algorithms with access to\ntrue rewards and imitation learning (IL) techniques like behavioral cloning.\nMoreover, our ablation studies on data size and quality reveal the advantages\nand limitations of SR-Reward as a proxy for true rewards."}
{"id": "2504.20198", "pdf": "https://arxiv.org/pdf/2504.20198", "abs": "https://arxiv.org/abs/2504.20198", "authors": ["Alireza Furutanpey", "Carmen Walser", "Philipp Raith", "Pantelis A. Frangoudis", "Schahram Dustdar"], "title": "Leveraging Neural Graph Compilers in Machine Learning Research for Edge-Cloud Systems", "categories": ["cs.DC", "cs.LG"], "comment": "10 pages, 12 figures, 8 tables", "summary": "This work presents a comprehensive evaluation of neural network graph\ncompilers across heterogeneous hardware platforms, addressing the critical gap\nbetween theoretical optimization techniques and practical deployment scenarios.\nWe demonstrate how vendor-specific optimizations can invalidate relative\nperformance comparisons between architectural archetypes, with performance\nadvantages sometimes completely reversing after compilation. Our systematic\nanalysis reveals that graph compilers exhibit performance patterns highly\ndependent on both neural architecture and batch sizes. Through fine-grained\nblock-level experimentation, we establish that vendor-specific compilers can\nleverage repeated patterns in simple architectures, yielding disproportionate\nthroughput gains as model depth increases. We introduce novel metrics to\nquantify a compiler's ability to mitigate performance friction as batch size\nincreases. Our methodology bridges the gap between academic research and\npractical deployment by incorporating compiler effects throughout the research\nprocess, providing actionable insights for practitioners navigating complex\noptimization landscapes across heterogeneous hardware environments."}
{"id": "2501.11153", "pdf": "https://arxiv.org/pdf/2501.11153", "abs": "https://arxiv.org/abs/2501.11153", "authors": ["Huu Phong Nguyen", "Shekhar Madhav Khairnar", "Sofia Garces Palacios", "Amr Al-Abbas", "Melissa E. Hogg", "Amer H. Zureikat", "Patricio M. Polanco", "Herbert Zeh III", "Ganesh Sankaranarayanan"], "title": "Efficient Frame Extraction: A Novel Approach Through Frame Similarity and Surgical Tool Tracking for Video Segmentation", "categories": ["cs.CV"], "comment": "18", "summary": "The interest in leveraging Artificial Intelligence (AI) for surgical\nprocedures to automate analysis has witnessed a significant surge in recent\nyears. One of the primary tools for recording surgical procedures and\nconducting subsequent analyses, such as performance assessment, is through\nvideos. However, these operative videos tend to be notably lengthy compared to\nother fields, spanning from thirty minutes to several hours, which poses a\nchallenge for AI models to effectively learn from them. Despite this challenge,\nthe foreseeable increase in the volume of such videos in the near future\nnecessitates the development and implementation of innovative techniques to\ntackle this issue effectively. In this article, we propose a novel technique\ncalled Kinematics Adaptive Frame Recognition (KAFR) that can efficiently\neliminate redundant frames to reduce dataset size and computation time while\nretaining useful frames to improve accuracy. Specifically, we compute the\nsimilarity between consecutive frames by tracking the movement of surgical\ntools. Our approach follows these steps: $i)$ Tracking phase: a YOLOv8 model is\nutilized to detect tools presented in the scene, $ii)$ Similarity phase:\nSimilarities between consecutive frames are computed by estimating variation in\nthe spatial positions and velocities of the tools, $iii$) Classification phase:\nAn X3D CNN is trained to classify segmentation. We evaluate the effectiveness\nof our approach by analyzing datasets obtained through retrospective reviews of\ncases at two referral centers. The newly annotated Gastrojejunostomy (GJ)\ndataset covers procedures performed between 2017 and 2021, while the previously\nannotated Pancreaticojejunostomy (PJ) dataset spans from 2011 to 2022 at the\nsame centers."}
{"id": "2501.10187", "pdf": "https://arxiv.org/pdf/2501.10187", "abs": "https://arxiv.org/abs/2501.10187", "authors": ["Burcu Canakci", "Junyi Liu", "Xingbo Wu", "Nathanaël Cheriere", "Paolo Costa", "Sergey Legtchenko", "Dushyanth Narayanan", "Ant Rowstron"], "title": "Good things come in small packages: Should we build AI clusters with Lite-GPUs?", "categories": ["cs.AR", "cs.AI", "cs.DC"], "comment": "HotOS'25", "summary": "To match the blooming demand of generative AI workloads, GPU designers have\nso far been trying to pack more and more compute and memory into single complex\nand expensive packages. However, there is growing uncertainty about the\nscalability of individual GPUs and thus AI clusters, as state-of-the-art GPUs\nare already displaying packaging, yield, and cooling limitations. We propose to\nrethink the design and scaling of AI clusters through efficiently-connected\nlarge clusters of Lite-GPUs, GPUs with single, small dies and a fraction of the\ncapabilities of larger GPUs. We think recent advances in co-packaged optics can\nenable distributing AI workloads onto many Lite-GPUs through high bandwidth and\nefficient communication. In this paper, we present the key benefits of\nLite-GPUs on manufacturing cost, blast radius, yield, and power efficiency; and\ndiscuss systems opportunities and challenges around resource, workload, memory,\nand network management."}
{"id": "2504.20238", "pdf": "https://arxiv.org/pdf/2504.20238", "abs": "https://arxiv.org/abs/2504.20238", "authors": ["P. Trent Vonich", "Gregory J. Hakim"], "title": "Testing the Limit of Atmospheric Predictability with a Machine Learning Weather Model", "categories": ["physics.ao-ph", "cs.LG"], "comment": null, "summary": "Atmospheric predictability research has long held that the limit of skillful\ndeterministic weather forecasts is about 14 days. We challenge this limit using\nGraphCast, a machine-learning weather model, by optimizing forecast initial\nconditions using gradient-based techniques for twice-daily forecasts spanning\n2020. This approach yields an average error reduction of 86% at 10 days, with\nskill lasting beyond 30 days. Mean optimal initial-condition perturbations\nreveal large-scale, spatially coherent corrections to ERA5, primarily\nreflecting an intensification of the Hadley circulation. Forecasts using\nGraphCast-optimal initial conditions in the Pangu-Weather model achieve a 21%\nerror reduction, peaking at 4 days, indicating that analysis corrections\nreflect a combination of both model bias and a reduction in analysis error.\nThese results demonstrate that, given accurate initial conditions, skillful\ndeterministic forecasts are consistently achievable far beyond two weeks,\nchallenging long-standing assumptions about the limits of atmospheric\npredictability."}
{"id": "2501.19184", "pdf": "https://arxiv.org/pdf/2501.19184", "abs": "https://arxiv.org/abs/2501.19184", "authors": ["Luca Ciampi", "Ali Azmoudeh", "Elif Ecem Akbaba", "Erdi Sarıtaş", "Ziya Ata Yazıcı", "Hazım Kemal Ekenel", "Giuseppe Amato", "Fabrizio Falchi"], "title": "A Survey on Class-Agnostic Counting: Advancements from Reference-Based to Open-World Text-Guided Approaches", "categories": ["cs.CV"], "comment": null, "summary": "Visual object counting has recently shifted towards class-agnostic counting\n(CAC), which addresses the challenge of counting objects across arbitrary\ncategories -- a crucial capability for flexible and generalizable counting\nsystems. Unlike humans, who effortlessly identify and count objects from\ndiverse categories without prior knowledge, most existing counting methods are\nrestricted to enumerating instances of known classes, requiring extensive\nlabeled datasets for training and struggling in open-vocabulary settings. In\ncontrast, CAC aims to count objects belonging to classes never seen during\ntraining, operating in a few-shot setting. In this paper, we present the first\ncomprehensive review of CAC methodologies. We propose a taxonomy to categorize\nCAC approaches into three paradigms based on how target object classes can be\nspecified: reference-based, reference-less, and open-world text-guided.\nReference-based approaches achieve state-of-the-art performance by relying on\nexemplar-guided mechanisms. Reference-less methods eliminate exemplar\ndependency by leveraging inherent image patterns. Finally, open-world\ntext-guided methods use vision-language models, enabling object class\ndescriptions via textual prompts, offering a flexible and promising solution.\nBased on this taxonomy, we provide an overview of the architectures of 29 CAC\napproaches and report their results on gold-standard benchmarks. We compare\ntheir performance and discuss their strengths and limitations. Specifically, we\npresent results on the FSC-147 dataset, setting a leaderboard using\ngold-standard metrics, and on the CARPK dataset to assess generalization\ncapabilities. Finally, we offer a critical discussion of persistent challenges,\nsuch as annotation dependency and generalization, alongside future directions.\nWe believe this survey will be a valuable resource, showcasing CAC advancements\nand guiding future research."}
{"id": "2501.12352", "pdf": "https://arxiv.org/pdf/2501.12352", "abs": "https://arxiv.org/abs/2501.12352", "authors": ["Ke Alexander Wang", "Jiaxin Shi", "Emily B. Fox"], "title": "Test-time regression: a unifying framework for designing sequence models with associative memory", "categories": ["cs.LG", "cs.AI", "cs.NE", "stat.ML"], "comment": null, "summary": "Sequence models lie at the heart of modern deep learning. However, rapid\nadvancements have produced a diversity of seemingly unrelated architectures,\nsuch as Transformers and recurrent alternatives. In this paper, we introduce a\nunifying framework to understand and derive these sequence models, inspired by\nthe empirical importance of associative recall, the capability to retrieve\ncontextually relevant tokens. We formalize associative recall as a two-step\nprocess, memorization and retrieval, casting memorization as a regression\nproblem. Layers that combine these two steps perform associative recall via\n``test-time regression'' over its input tokens. Prominent layers, including\nlinear attention, state-space models, fast-weight programmers, online learners,\nand softmax attention, arise as special cases defined by three design choices:\nthe regression weights, the regressor function class, and the test-time\noptimization algorithm. Our approach clarifies how linear attention fails to\ncapture inter-token correlations and offers a mathematical justification for\nthe empirical effectiveness of query-key normalization in softmax attention.\nFurther, it illuminates unexplored regions within the design space, which we\nuse to derive novel higher-order generalizations of softmax attention. Beyond\nunification, our work bridges sequence modeling with classic regression\nmethods, a field with extensive literature, paving the way for developing more\npowerful and theoretically principled architectures."}
{"id": "2504.20256", "pdf": "https://arxiv.org/pdf/2504.20256", "abs": "https://arxiv.org/abs/2504.20256", "authors": ["Derek W. Jollie", "Scott G. McCalla"], "title": "Optimizing Hard Thresholding for Sparse Model Discovery", "categories": ["math.OC", "cs.LG", "cs.NA", "math.DS", "math.NA"], "comment": null, "summary": "Many model selection algorithms rely on sparse dictionary learning to provide\ninterpretable and physics-based governing equations. The optimization\nalgorithms typically use a hard thresholding process to enforce sparse\nactivations in the model coefficients by removing library elements from\nconsideration. By introducing an annealing scheme that reactivates a fraction\nof the removed terms with a cooling schedule, we are able to improve the\nperformance of these sparse learning algorithms. We concentrate on two\napproaches to the optimization, SINDy, and an alternative using hard\nthresholding pursuit. We see in both cases that annealing can improve model\naccuracy. The effectiveness of annealing is demonstrated through comparisons on\nseveral nonlinear systems pulled from convective flows, excitable systems, and\npopulation dynamics. Finally we apply these algorithms to experimental data for\nprojectile motion."}
{"id": "2502.03370", "pdf": "https://arxiv.org/pdf/2502.03370", "abs": "https://arxiv.org/abs/2502.03370", "authors": ["Muhammad Ahtsam Naeem", "Muhammad Asim Saleem", "Muhammad Imran Sharif", "Shahzad Akber", "Sajjad Saleem", "Zahid Akhtar", "Kamran Siddique"], "title": "Deep Learning-Based Approach for Identification of Potato Leaf Diseases Using Wrapper Feature Selection and Feature Concatenation", "categories": ["cs.CV", "cs.LG"], "comment": null, "summary": "The potato is a widely grown crop in many regions of the world. In recent\ndecades, potato farming has gained incredible traction in the world. Potatoes\nare susceptible to several illnesses that stunt their development. This plant\nseems to have significant leaf disease. Early Blight and Late Blight are two\nprevalent leaf diseases that affect potato plants. The early detection of these\ndiseases would be beneficial for enhancing the yield of this crop. The ideal\nsolution is to use image processing to identify and analyze these disorders.\nHere, we present an autonomous method based on image processing and machine\nlearning to detect late blight disease affecting potato leaves. The proposed\nmethod comprises four different phases: (1) Histogram Equalization is used to\nimprove the quality of the input image; (2) feature extraction is performed\nusing a Deep CNN model, then these extracted features are concatenated; (3)\nfeature selection is performed using wrapper-based feature selection; (4)\nclassification is performed using an SVM classifier and its variants. This\nproposed method achieves the highest accuracy of 99% using SVM by selecting 550\nfeatures."}
{"id": "2502.05857", "pdf": "https://arxiv.org/pdf/2502.05857", "abs": "https://arxiv.org/abs/2502.05857", "authors": ["Lu Chen", "Yizhou Wang", "Shixiang Tang", "Qianhong Ma", "Tong He", "Wanli Ouyang", "Xiaowei Zhou", "Hujun Bao", "Sida Peng"], "title": "EgoAgent: A Joint Predictive Agent Model in Egocentric Worlds", "categories": ["cs.CV", "cs.AI", "cs.LG"], "comment": null, "summary": "This paper addresses the task of learning an agent model behaving like\nhumans, which can jointly perceive, predict, and act in egocentric worlds.\nPrevious methods usually train separate models for these three abilities, which\nprevents them from learning from each other. In this paper, we propose a joint\npredictive agent model, named EgoAgent, that simultaneously learns to represent\nthe world, predict future states, and take reasonable actions within a single\ntransformer. EgoAgent introduces two innovations to learn from the causal and\ntemporally intertwined nature of these abilities: (1) Interleaved sequential\nmodeling of states and actions with the causal attention mechanism, and (2) A\njoint embedding-action-prediction architecture featuring temporal asymmetric\npredictor-observer branches. Integrating these designs based on JEPA, EgoAgent\nunifies these capabilities in a cohesive learning framework. Comprehensive\nevaluations of EgoAgent on representative tasks such as image classification,\negocentric future state prediction, and 3D human motion prediction tasks\ndemonstrate the superiority of our method. The code and trained model will be\nreleased for reproducibility."}
{"id": "2504.20266", "pdf": "https://arxiv.org/pdf/2504.20266", "abs": "https://arxiv.org/abs/2504.20266", "authors": ["Mohammadhossein Homaei", "Agustin Di Bartolo", "Oscar Mogollon-Gutierrez", "Fernando Broncano Morgado", "Pablo Garcia Rodriguez"], "title": "A Virtual Cybersecurity Department for Securing Digital Twins in Water Distribution Systems", "categories": ["cs.CR", "cs.LG", "cs.NI"], "comment": "7 pages, 9 Figures", "summary": "Digital twins (DTs) help improve real-time monitoring and decision-making in\nwater distribution systems. However, their connectivity makes them easy targets\nfor cyberattacks such as scanning, denial-of-service (DoS), and unauthorized\naccess. Small and medium-sized enterprises (SMEs) that manage these systems\noften do not have enough budget or staff to build strong cybersecurity teams.\nTo solve this problem, we present a Virtual Cybersecurity Department (VCD), an\naffordable and automated framework designed for SMEs. The VCD uses open-source\ntools like Zabbix for real-time monitoring, Suricata for network intrusion\ndetection, Fail2Ban to block repeated login attempts, and simple firewall\nsettings. To improve threat detection, we also add a machine-learning-based IDS\ntrained on the OD-IDS2022 dataset using an improved ensemble model. This model\ndetects cyber threats such as brute-force attacks, remote code execution (RCE),\nand network flooding, with 92\\% accuracy and fewer false alarms. Our solution\ngives SMEs a practical and efficient way to secure water systems using low-cost\nand easy-to-manage tools."}
{"id": "2503.00063", "pdf": "https://arxiv.org/pdf/2503.00063", "abs": "https://arxiv.org/abs/2503.00063", "authors": ["Zezeng Li", "Xiaoyu Du", "Na Lei", "Liming Chen", "Weimin Wang"], "title": "NoPain: No-box Point Cloud Attack via Optimal Transport Singular Boundary", "categories": ["cs.CV"], "comment": null, "summary": "Adversarial attacks exploit the vulnerability of deep models against\nadversarial samples. Existing point cloud attackers are tailored to specific\nmodels, iteratively optimizing perturbations based on gradients in either a\nwhite-box or black-box setting. Despite their promising attack performance,\nthey often struggle to produce transferable adversarial samples due to\noverfitting the specific parameters of surrogate models. To overcome this\nissue, we shift our focus to the data distribution itself and introduce a novel\napproach named NoPain, which employs optimal transport (OT) to identify the\ninherent singular boundaries of the data manifold for cross-network point cloud\nattacks. Specifically, we first calculate the OT mapping from noise to the\ntarget feature space, then identify singular boundaries by locating\nnon-differentiable positions. Finally, we sample along singular boundaries to\ngenerate adversarial point clouds. Once the singular boundaries are determined,\nNoPain can efficiently produce adversarial samples without the need of\niterative updates or guidance from the surrogate classifiers. Extensive\nexperiments demonstrate that the proposed end-to-end method outperforms\nbaseline approaches in terms of both transferability and efficiency, while also\nmaintaining notable advantages even against defense strategies. Code and model\nare available at https://github.com/cognaclee/nopain"}
{"id": "2502.06820", "pdf": "https://arxiv.org/pdf/2502.06820", "abs": "https://arxiv.org/abs/2502.06820", "authors": ["Zhekai Du", "Yinjie Min", "Jingjing Li", "Ke Lu", "Changliang Zou", "Liuhua Peng", "Tingjin Chu", "Mingming Gong"], "title": "LoCA: Location-Aware Cosine Adaptation for Parameter-Efficient Fine-Tuning", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Low-rank adaptation (LoRA) has become a prevalent method for adapting\npre-trained large language models to downstream tasks. However, the simple\nlow-rank decomposition form may constrain the hypothesis space. To address this\nlimitation, we introduce Location-aware Cosine Adaptation (LoCA), a novel\nfrequency-domain parameter-efficient fine-tuning method based on inverse\nDiscrete Cosine Transform (iDCT) with selective locations of learnable\ncomponents. We begin with a comprehensive theoretical comparison between\nfrequency-domain and low-rank decompositions for fine-tuning pre-trained large\nmodels. Our analysis reveals that frequency-domain decomposition with carefully\nselected frequency components can surpass the expressivity of traditional\nlow-rank-based methods. Furthermore, we demonstrate that iDCT offers a more\nefficient implementation compared to inverse Discrete Fourier Transform (iDFT),\nallowing for better selection and tuning of frequency components while\nmaintaining equivalent expressivity to the optimal iDFT-based adaptation. By\nemploying finite-difference approximation to estimate gradients for discrete\nlocations of learnable coefficients on the DCT spectrum, LoCA dynamically\nselects the most informative frequency components during training. Experiments\non diverse language and vision fine-tuning tasks demonstrate that LoCA offers\nenhanced parameter efficiency while maintains computational feasibility\ncomparable to low-rank-based methods."}
{"id": "2504.20395", "pdf": "https://arxiv.org/pdf/2504.20395", "abs": "https://arxiv.org/abs/2504.20395", "authors": ["Tiantian", "Zhang"], "title": "Partial Answer of How Transformers Learn Automata", "categories": ["cs.FL", "cs.LG"], "comment": null, "summary": "We introduce a novel framework for simulating finite automata using\nrepresentation-theoretic semidirect products and Fourier modules, achieving\nmore efficient Transformer-based implementations."}
{"id": "2503.02689", "pdf": "https://arxiv.org/pdf/2503.02689", "abs": "https://arxiv.org/abs/2503.02689", "authors": ["Tianqing Zhang", "Kairong Yu", "Xian Zhong", "Hongwei Wang", "Qi Xu", "Qiang Zhang"], "title": "STAA-SNN: Spatial-Temporal Attention Aggregator for Spiking Neural Networks", "categories": ["cs.CV"], "comment": "Accepted by CVPR 2025", "summary": "Spiking Neural Networks (SNNs) have gained significant attention due to their\nbiological plausibility and energy efficiency, making them promising\nalternatives to Artificial Neural Networks (ANNs). However, the performance gap\nbetween SNNs and ANNs remains a substantial challenge hindering the widespread\nadoption of SNNs. In this paper, we propose a Spatial-Temporal Attention\nAggregator SNN (STAA-SNN) framework, which dynamically focuses on and captures\nboth spatial and temporal dependencies. First, we introduce a spike-driven\nself-attention mechanism specifically designed for SNNs. Additionally, we\npioneeringly incorporate position encoding to integrate latent temporal\nrelationships into the incoming features. For spatial-temporal information\naggregation, we employ step attention to selectively amplify relevant features\nat different steps. Finally, we implement a time-step random dropout strategy\nto avoid local optima. As a result, STAA-SNN effectively captures both spatial\nand temporal dependencies, enabling the model to analyze complex patterns and\nmake accurate predictions. The framework demonstrates exceptional performance\nacross diverse datasets and exhibits strong generalization capabilities.\nNotably, STAA-SNN achieves state-of-the-art results on neuromorphic datasets\nCIFAR10-DVS, with remarkable performances of 97.14%, 82.05% and 70.40% on the\nstatic datasets CIFAR-10, CIFAR-100 and ImageNet, respectively. Furthermore,\nour model exhibits improved performance ranging from 0.33\\% to 2.80\\% with\nfewer time steps. The code for the model is available on GitHub."}
{"id": "2502.08365", "pdf": "https://arxiv.org/pdf/2502.08365", "abs": "https://arxiv.org/abs/2502.08365", "authors": ["Riccardo Zamboni", "Mirco Mutti", "Marcello Restelli"], "title": "Towards Principled Multi-Agent Task Agnostic Exploration", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "In reinforcement learning, we typically refer to task-agnostic exploration\nwhen we aim to explore the environment without access to the task specification\na priori. In a single-agent setting the problem has been extensively studied\nand mostly understood. A popular approach cast the task-agnostic objective as\nmaximizing the entropy of the state distribution induced by the agent's policy,\nfrom which principles and methods follows. In contrast, little is known about\ntask-agnostic exploration in multi-agent settings, which are ubiquitous in the\nreal world. How should different agents explore in the presence of others? In\nthis paper, we address this question through a generalization to multiple\nagents of the problem of maximizing the state distribution entropy. First, we\ninvestigate alternative formulations, highlighting respective positives and\nnegatives. Then, we present a scalable, decentralized, trust-region policy\nsearch algorithm to address the problem in practical settings. Finally, we\nprovide proof of concept experiments to both corroborate the theoretical\nfindings and pave the way for task-agnostic exploration in challenging\nmulti-agent settings."}
{"id": "2504.20401", "pdf": "https://arxiv.org/pdf/2504.20401", "abs": "https://arxiv.org/abs/2504.20401", "authors": ["N. Richardson", "C. Bosch", "R. P. Adams"], "title": "Nonlinear Computation with Linear Optics via Source-Position Encoding", "categories": ["physics.optics", "cs.AR", "cs.LG"], "comment": null, "summary": "Optical computing systems provide an alternate hardware model which appears\nto be aligned with the demands of neural network workloads. However, the\nchallenge of implementing energy efficient nonlinearities in optics -- a key\nrequirement for realizing neural networks -- is a conspicuous missing link. In\nthis work we introduce a novel method to achieve nonlinear computation in fully\nlinear media. Our method can operate at low power and requires only the ability\nto drive the optical system at a data-dependent spatial position. Leveraging\nthis positional encoding, we formulate a fully automated,\ntopology-optimization-based hardware design framework for extremely specialized\noptical neural networks, drawing on modern advancements in optimization and\nmachine learning. We evaluate our optical designs on machine learning\nclassification tasks: demonstrating significant improvements over linear\nmethods, and competitive performance when compared to standard artificial\nneural networks."}
{"id": "2503.03144", "pdf": "https://arxiv.org/pdf/2503.03144", "abs": "https://arxiv.org/abs/2503.03144", "authors": ["Kairong Yu", "Chengting Yu", "Tianqing Zhang", "Xiaochen Zhao", "Shu Yang", "Hongwei Wang", "Qiang Zhang", "Qi Xu"], "title": "Temporal Separation with Entropy Regularization for Knowledge Distillation in Spiking Neural Networks", "categories": ["cs.CV"], "comment": "Accepted by CVPR 2025", "summary": "Spiking Neural Networks (SNNs), inspired by the human brain, offer\nsignificant computational efficiency through discrete spike-based information\ntransfer. Despite their potential to reduce inference energy consumption, a\nperformance gap persists between SNNs and Artificial Neural Networks (ANNs),\nprimarily due to current training methods and inherent model limitations. While\nrecent research has aimed to enhance SNN learning by employing knowledge\ndistillation (KD) from ANN teacher networks, traditional distillation\ntechniques often overlook the distinctive spatiotemporal properties of SNNs,\nthus failing to fully leverage their advantages. To overcome these challenge,\nwe propose a novel logit distillation method characterized by temporal\nseparation and entropy regularization. This approach improves existing SNN\ndistillation techniques by performing distillation learning on logits across\ndifferent time steps, rather than merely on aggregated output features.\nFurthermore, the integration of entropy regularization stabilizes model\noptimization and further boosts the performance. Extensive experimental results\nindicate that our method surpasses prior SNN distillation strategies, whether\nbased on logit distillation, feature distillation, or a combination of both.\nThe code will be available on GitHub."}
{"id": "2502.10712", "pdf": "https://arxiv.org/pdf/2502.10712", "abs": "https://arxiv.org/abs/2502.10712", "authors": ["Jinouwen Zhang", "Junjie Ren", "Aobo Yang", "Yan Lu", "Lu Chen", "Hairun Xie", "Jing Wang", "Miao Zhang", "Wanli Ouyang", "Shixiang Tang"], "title": "FuncGenFoil: Airfoil Generation and Editing Model in Function Space", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Aircraft manufacturing is the jewel in the crown of industry, among which\ngenerating high-fidelity airfoil geometries with controllable and editable\nrepresentations remains a fundamental challenge. While existing\ndeep-learning-based methods rely on predefined parametric function families,\ne.g., B\\'ezier curves and discrete point-based representations, they suffer\nfrom inherent trade-offs between expressiveness and resolution flexibility. To\ntackle this challenge, we introduce FuncGenFoil, a novel function-space\ngenerative model that directly learns functional airfoil geometries. Our method\ninherits both the advantages of arbitrary resolution sampling and the\nsmoothness of parametric functions, as well as the strong expressiveness of\ndiscrete point-based functions. Empirical evaluations on the AFBench dataset\ndemonstrate that FuncGenFoil improves upon state-of-the-art methods in airfoil\ngeneration by achieving a relative -74.4 label error reduction and +23.2\ndiversity increase on the AF-200K dataset. Our results highlight the advantages\nof function-space modeling for aerodynamic shape optimization, offering a\npowerful and flexible framework for high-fidelity airfoil design. Our code will\nbe released."}
{"id": "2503.24091", "pdf": "https://arxiv.org/pdf/2503.24091", "abs": "https://arxiv.org/abs/2503.24091", "authors": ["Xiangyuan Peng", "Miao Tang", "Huawei Sun", "Kay Bierzynski", "Lorenzo Servadei", "Robert Wille"], "title": "4D mmWave Radar for Sensing Enhancement in Adverse Environments: Advances and Challenges", "categories": ["cs.CV"], "comment": "8 pages", "summary": "Intelligent transportation systems require accurate and reliable sensing.\nHowever, adverse environments, such as rain, snow, and fog, can significantly\ndegrade the performance of LiDAR and cameras. In contrast, 4D mmWave radar not\nonly provides 3D point clouds and velocity measurements but also maintains\nrobustness in challenging conditions. Recently, research on 4D mmWave radar\nunder adverse environments has been growing, but a comprehensive review is\nstill lacking. To bridge this gap, this work reviews the current research on 4D\nmmWave radar under adverse environments. First, we present an overview of\nexisting 4D mmWave radar datasets encompassing diverse weather and lighting\nscenarios. Subsequently, we analyze existing learning-based methods leveraging\n4D mmWave radar to enhance performance according to different adverse\nconditions. Finally, the challenges and potential future directions are\ndiscussed for advancing 4D mmWave radar applications in harsh environments. To\nthe best of our knowledge, this is the first review specifically concentrating\non 4D mmWave radar in adverse environments. The related studies are listed at:\nhttps://github.com/XiangyPeng/4D-mmWave-Radar-in-Adverse-Environments."}
{"id": "2502.12710", "pdf": "https://arxiv.org/pdf/2502.12710", "abs": "https://arxiv.org/abs/2502.12710", "authors": ["Malte Hellmeier", "Hendrik Norkowski", "Ernst-Christoph Schrewe", "Haydar Qarawlus", "Falk Howar"], "title": "Innamark: A Whitespace Replacement Information-Hiding Method", "categories": ["cs.CR", "cs.AI", "cs.SE"], "comment": null, "summary": "Large language models (LLMs) have gained significant popularity in recent\nyears. Differentiating between a text written by a human and one generated by\nan LLM has become almost impossible. Information-hiding techniques such as\ndigital watermarking or steganography can help by embedding information inside\ntext in a form that is unlikely to be noticed. However, existing techniques,\nsuch as linguistic-based or format-based methods, change the semantics or\ncannot be applied to pure, unformatted text. In this paper, we introduce a\nnovel method for information hiding called Innamark, which can conceal any\nbyte-encoded sequence within a sufficiently long cover text. This method is\nimplemented as a multi-platform library using the Kotlin programming language,\nwhich is accompanied by a command-line tool and a web interface. By\nsubstituting conventional whitespace characters with visually similar Unicode\nwhitespace characters, our proposed scheme preserves the semantics of the cover\ntext without changing the number of characters. Furthermore, we propose a\nspecified structure for secret messages that enables configurable compression,\nencryption, hashing, and error correction. An experimental benchmark comparison\non a dataset of 1000000 Wikipedia articles compares ten algorithms. The results\ndemonstrate the robustness of our proposed Innamark method in various\napplications and the imperceptibility of its watermarks to humans. We discuss\nthe limits to the embedding capacity and robustness of the algorithm and how\nthese could be addressed in future work."}
{"id": "2504.20617", "pdf": "https://arxiv.org/pdf/2504.20617", "abs": "https://arxiv.org/abs/2504.20617", "authors": ["Yunfei Yang"], "title": "Sobolev norm inconsistency of kernel interpolation", "categories": ["stat.ML", "cs.LG"], "comment": null, "summary": "We study the consistency of minimum-norm interpolation in reproducing kernel\nHilbert spaces corresponding to bounded kernels. Our main result give lower\nbounds for the generalization error of the kernel interpolation measured in a\ncontinuous scale of norms that interpolate between $L^2$ and the hypothesis\nspace. These lower bounds imply that kernel interpolation is always\ninconsistent, when the smoothness index of the norm is larger than a constant\nthat depends only on the embedding index of the hypothesis space and the decay\nrate of the eigenvalues."}
{"id": "2504.00816", "pdf": "https://arxiv.org/pdf/2504.00816", "abs": "https://arxiv.org/abs/2504.00816", "authors": ["Yeqi Fang", "Rong Zhou"], "title": "Two-stage deep learning framework for the restoration of incomplete-ring PET images", "categories": ["cs.CV", "physics.med-ph"], "comment": "20 pages, 8 figures", "summary": "Positron Emission Tomography (PET) is an important molecular imaging tool\nwidely used in medicine. Traditional PET systems rely on complete detector\nrings for full angular coverage and reliable data collection. However,\nincomplete-ring PET scanners have emerged due to hardware failures, cost\nconstraints, or specific clinical needs. Standard reconstruction algorithms\noften suffer from performance degradation with these systems because of reduced\ndata completeness and geometric inconsistencies. We present a two-stage\ndeep-learning framework that, without incorporating any time-of-flight (TOF)\ninformation, restores high-quality images from data with about 50% missing\ncoincidences - double the loss levels previously addressed by CNN-based\nmethods. The pipeline operates in two stages: a projection-domain Attention\nU-Net first predicts the missing sections of the sinogram by leveraging spatial\ncontext from neighbouring slices, after which the completed data are\nreconstructed with OSEM algorithm and passed to a U-Net-diffusion module that\nremoves residual artefacts while reinstating high-frequency detail. Using 206\nbrain volumes from a public dataset, the result shows that our model\nsuccessfully preserves most anatomical structures and tracer distribution\nfeatures with PSNR of 30.92 dB and SSIM of 0.9708. We also achieve higher\ninference speed, thus providing an effective solution for incomplete-ring PET\nimaging."}
{"id": "2502.17801", "pdf": "https://arxiv.org/pdf/2502.17801", "abs": "https://arxiv.org/abs/2502.17801", "authors": ["Yuqing Wang", "Xiao Yang"], "title": "Research on Enhancing Cloud Computing Network Security using Artificial Intelligence Algorithms", "categories": ["cs.CR", "cs.AI"], "comment": null, "summary": "Cloud computing environments are increasingly vulnerable to security threats\nsuch as distributed denial-of-service (DDoS) attacks and SQL injection.\nTraditional security mechanisms, based on rule matching and feature\nrecognition, struggle to adapt to evolving attack strategies. This paper\nproposes an adaptive security protection framework leveraging deep learning to\nconstruct a multi-layered defense architecture. The proposed system is\nevaluated in a real-world business environment, achieving a detection accuracy\nof 97.3%, an average response time of 18 ms, and an availability rate of\n99.999%. Experimental results demonstrate that the proposed method\nsignificantly enhances detection accuracy, response efficiency, and resource\nutilization, offering a novel and effective approach to cloud computing\nsecurity."}
{"id": "2504.20620", "pdf": "https://arxiv.org/pdf/2504.20620", "abs": "https://arxiv.org/abs/2504.20620", "authors": ["Abhishek Pasula", "Deepak N. Subramani"], "title": "Data Driven Deep Learning for Correcting Global Climate Model Projections of SST and DSL in the Bay of Bengal", "categories": ["physics.ao-ph", "cs.LG"], "comment": "28 pages, 10 figures, 3 tables of main paper; 24 pages, 17 figures of\n  Supporting Information", "summary": "Climate change alters ocean conditions, notably temperature and sea level. In\nthe Bay of Bengal, these changes influence monsoon precipitation and marine\nproductivity, critical to the Indian economy. In Phase 6 of the Coupled Model\nIntercomparison Project (CMIP6), Global Climate Models (GCMs) use different\nshared socioeconomic pathways (SSPs) to obtain future climate projections.\nHowever, significant discrepancies are observed between these models and the\nreanalysis data in the Bay of Bengal for 2015-2024. Specifically, the root mean\nsquare error (RMSE) between the climate model output and the Ocean Reanalysis\nSystem (ORAS5) is 1.2C for the sea surface temperature (SST) and 1.1 m for the\ndynamic sea level (DSL). We introduce a new data-driven deep learning model to\ncorrect for this bias. The deep neural model for each variable is trained using\npairs of climatology-removed monthly climate projections as input and the\ncorresponding month's ORAS5 as output. This model is trained with historical\ndata (1950 to 2014), validated with future projection data from 2015 to 2020,\nand tested with future projections from 2021 to 2023. Compared to the\nconventional EquiDistant Cumulative Distribution Function (EDCDF) statistical\nmethod for bias correction in climate models, our approach decreases RMSE by\n0.15C for SST and 0.3 m for DSL. The trained model subsequently corrects the\nprojections for 2024-2100. A detailed analysis of the monthly, seasonal, and\ndecadal means and variability is performed to underscore the implications of\nthe novel dynamics uncovered in our corrected projections."}
{"id": "2504.04907", "pdf": "https://arxiv.org/pdf/2504.04907", "abs": "https://arxiv.org/abs/2504.04907", "authors": ["Hui Han", "Siyuan Li", "Jiaqi Chen", "Yiwen Yuan", "Yuling Wu", "Chak Tou Leong", "Hanwen Du", "Junchen Fu", "Youhua Li", "Jie Zhang", "Chi Zhang", "Li-jia Li", "Yongxin Ni"], "title": "Video-Bench: Human-Aligned Video Generation Benchmark", "categories": ["cs.CV", "cs.AI"], "comment": "Accepted by CVPR'25", "summary": "Video generation assessment is essential for ensuring that generative models\nproduce visually realistic, high-quality videos while aligning with human\nexpectations. Current video generation benchmarks fall into two main\ncategories: traditional benchmarks, which use metrics and embeddings to\nevaluate generated video quality across multiple dimensions but often lack\nalignment with human judgments; and large language model (LLM)-based\nbenchmarks, though capable of human-like reasoning, are constrained by a\nlimited understanding of video quality metrics and cross-modal consistency. To\naddress these challenges and establish a benchmark that better aligns with\nhuman preferences, this paper introduces Video-Bench, a comprehensive benchmark\nfeaturing a rich prompt suite and extensive evaluation dimensions. This\nbenchmark represents the first attempt to systematically leverage MLLMs across\nall dimensions relevant to video generation assessment in generative models. By\nincorporating few-shot scoring and chain-of-query techniques, Video-Bench\nprovides a structured, scalable approach to generated video evaluation.\nExperiments on advanced models including Sora demonstrate that Video-Bench\nachieves superior alignment with human preferences across all dimensions.\nMoreover, in instances where our framework's assessments diverge from human\nevaluations, it consistently offers more objective and accurate insights,\nsuggesting an even greater potential advantage over traditional human judgment."}
{"id": "2502.18773", "pdf": "https://arxiv.org/pdf/2502.18773", "abs": "https://arxiv.org/abs/2502.18773", "authors": ["Yuqing Wang", "Xiao Yang"], "title": "Research on Edge Computing and Cloud Collaborative Resource Scheduling Optimization Based on Deep Reinforcement Learning", "categories": ["cs.LG", "cs.AI", "cs.DC"], "comment": null, "summary": "This study addresses the challenge of resource scheduling optimization in\nedge-cloud collaborative computing using deep reinforcement learning (DRL). The\nproposed DRL-based approach improves task processing efficiency, reduces\noverall processing time, enhances resource utilization, and effectively\ncontrols task migrations. Experimental results demonstrate the superiority of\nDRL over traditional scheduling algorithms, particularly in managing complex\ntask allocation, dynamic workloads, and multiple resource constraints. Despite\nits advantages, further improvements are needed to enhance learning efficiency,\nreduce training time, and address convergence issues. Future research should\nfocus on increasing the algorithm's fault tolerance to handle more complex and\nuncertain scheduling scenarios, thereby advancing the intelligence and\nefficiency of edge-cloud computing systems."}
{"id": "2504.20651", "pdf": "https://arxiv.org/pdf/2504.20651", "abs": "https://arxiv.org/abs/2504.20651", "authors": ["Harsh Vardhan", "Avishek Ghosh", "Arya Mazumdar"], "title": "Learning and Generalization with Mixture Data", "categories": ["stat.ML", "cs.LG"], "comment": null, "summary": "In many, if not most, machine learning applications the training data is\nnaturally heterogeneous (e.g. federated learning, adversarial attacks and\ndomain adaptation in neural net training). Data heterogeneity is identified as\none of the major challenges in modern day large-scale learning. A classical way\nto represent heterogeneous data is via a mixture model. In this paper, we study\ngeneralization performance and statistical rates when data is sampled from a\nmixture distribution. We first characterize the heterogeneity of the mixture in\nterms of the pairwise total variation distance of the sub-population\ndistributions. Thereafter, as a central theme of this paper, we characterize\nthe range where the mixture may be treated as a single (homogeneous)\ndistribution for learning. In particular, we study the generalization\nperformance under the classical PAC framework and the statistical error rates\nfor parametric (linear regression, mixture of hyperplanes) as well as\nnon-parametric (Lipschitz, convex and H\\\"older-smooth) regression problems. In\norder to do this, we obtain Rademacher complexity and (local) Gaussian\ncomplexity bounds with mixture data, and apply them to get the generalization\nand convergence rates respectively. We observe that as the (regression)\nfunction classes get more complex, the requirement on the pairwise total\nvariation distance gets stringent, which matches our intuition. We also do a\nfiner analysis for the case of mixed linear regression and provide a tight\nbound on the generalization error in terms of heterogeneity."}
{"id": "2504.13181", "pdf": "https://arxiv.org/pdf/2504.13181", "abs": "https://arxiv.org/abs/2504.13181", "authors": ["Daniel Bolya", "Po-Yao Huang", "Peize Sun", "Jang Hyun Cho", "Andrea Madotto", "Chen Wei", "Tengyu Ma", "Jiale Zhi", "Jathushan Rajasegaran", "Hanoona Rasheed", "Junke Wang", "Marco Monteiro", "Hu Xu", "Shiyu Dong", "Nikhila Ravi", "Daniel Li", "Piotr Dollár", "Christoph Feichtenhofer"], "title": "Perception Encoder: The best visual embeddings are not at the output of the network", "categories": ["cs.CV"], "comment": "Updated refs, fixed typos, and added new COCO SotA: 66.0 val mAP!\n  Code, models, and data at\n  https://github.com/facebookresearch/perception_models", "summary": "We introduce Perception Encoder (PE), a state-of-the-art vision encoder for\nimage and video understanding trained via simple vision-language learning.\nTraditionally, vision encoders have relied on a variety of pretraining\nobjectives, each tailored to specific downstream tasks such as classification,\ncaptioning, or localization. Surprisingly, after scaling our carefully tuned\nimage pretraining recipe and refining with our robust video data engine, we\nfind that contrastive vision-language training alone can produce strong,\ngeneral embeddings for all of these downstream tasks. There is only one caveat:\nthese embeddings are hidden within the intermediate layers of the network. To\ndraw them out, we introduce two alignment methods: language alignment for\nmultimodal language modeling, and spatial alignment for dense prediction.\nTogether, our PE family of models achieves best-in-class results on a wide\nvariety of tasks, including (1) zero-shot image and video classification and\nretrieval, simultaneously obtaining 86.6 average zero-shot ImageNet robustness\nand 76.9 zero-shot Kinetics-400 video classification; (2) document, image, and\nvideo Q&A, enabling 94.6 DocVQA, 80.9 InfographicVQA, and 82.7 PerceptionTest\nwith an 8B LLM; and (3) spatial tasks such as detection, tracking, and depth\nestimation, setting a new COCO state-of-the-art of 66.0 box mAP. To foster\nfurther research, we release our models, code, and novel dataset of\nsynthetically and human-annotated videos:\nhttps://github.com/facebookresearch/perception_models"}
{"id": "2503.04231", "pdf": "https://arxiv.org/pdf/2503.04231", "abs": "https://arxiv.org/abs/2503.04231", "authors": ["Maciej Krzysztof Zuziak", "Roberto Pellungrini", "Salvatore Rinzivillo"], "title": "One-Shot Clustering for Federated Learning", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Federated Learning (FL) is a widespread and well adopted paradigm of\ndecentralized learning that allows training one model from multiple sources\nwithout the need to directly transfer data between participating clients. Since\nits inception in 2015, it has been divided into numerous sub-fields that deal\nwith application-specific issues, be it data heterogeneity or resource\nallocation. One such sub-field, Clustered Federated Learning (CFL), is dealing\nwith the problem of clustering the population of clients into separate cohorts\nto deliver personalized models. Although few remarkable works have been\npublished in this domain, the problem is still largely unexplored, as its basic\nassumption and settings are slightly different from standard FL. In this work,\nwe present One-Shot Clustered Federated Learning (OCFL), a clustering-agnostic\nalgorithm that can automatically detect the earliest suitable moment for\nclustering. Our algorithm is based on the computation of cosine similarity\nbetween gradients of the clients and a temperature measure that detects when\nthe federated model starts to converge. We empirically evaluate our methodology\nby testing various one-shot clustering algorithms for over thirty different\ntasks on three benchmark datasets. Our experiments showcase the good\nperformance of our approach when used to perform CFL in an automated manner\nwithout the need to adjust hyperparameters."}
{"id": "2504.13754", "pdf": "https://arxiv.org/pdf/2504.13754", "abs": "https://arxiv.org/abs/2504.13754", "authors": ["Zhu Zhu", "Shuo Jiang", "Jingyuan Zheng", "Yawen Li", "Yifei Chen", "Manli Zhao", "Weizhong Gu", "Feiwei Qin", "Jinhu Wang", "Gang Yu"], "title": "Towards Accurate and Interpretable Neuroblastoma Diagnosis via Contrastive Multi-scale Pathological Image Analysis", "categories": ["cs.CV", "cs.AI"], "comment": "10pages, 8 figures", "summary": "Neuroblastoma, adrenal-derived, is among the most common pediatric solid\nmalignancies, characterized by significant clinical heterogeneity. Timely and\naccurate pathological diagnosis from hematoxylin and eosin-stained whole slide\nimages is critical for patient prognosis. However, current diagnostic practices\nprimarily rely on subjective manual examination by pathologists, leading to\ninconsistent accuracy. Existing automated whole slide image classification\nmethods encounter challenges such as poor interpretability, limited feature\nextraction capabilities, and high computational costs, restricting their\npractical clinical deployment. To overcome these limitations, we propose\nCMSwinKAN, a contrastive-learning-based multi-scale feature fusion model\ntailored for pathological image classification, which enhances the Swin\nTransformer architecture by integrating a Kernel Activation Network within its\nmultilayer perceptron and classification head modules, significantly improving\nboth interpretability and accuracy. By fusing multi-scale features and\nleveraging contrastive learning strategies, CMSwinKAN mimics clinicians'\ncomprehensive approach, effectively capturing global and local tissue\ncharacteristics. Additionally, we introduce a heuristic soft voting mechanism\nguided by clinical insights to seamlessly bridge patch-level predictions to\nwhole slide image-level classifications. We validate CMSwinKAN on the PpNTs\ndataset, which was collaboratively established with our partner hospital and\nthe publicly accessible BreakHis dataset. Results demonstrate that CMSwinKAN\nperforms better than existing state-of-the-art pathology-specific models\npre-trained on large datasets. Our source code is available at\nhttps://github.com/JSLiam94/CMSwinKAN."}
{"id": "2503.08727", "pdf": "https://arxiv.org/pdf/2503.08727", "abs": "https://arxiv.org/abs/2503.08727", "authors": ["Lucas Caccia", "Alan Ansell", "Edoardo Ponti", "Ivan Vulić", "Alessandro Sordoni"], "title": "Training Plug-n-Play Knowledge Modules with Deep Context Distillation", "categories": ["cs.LG", "cs.AI"], "comment": "Preprint", "summary": "Dynamically integrating new or rapidly evolving information after (Large)\nLanguage Model pre-training remains challenging, particularly in low-data\nscenarios or when dealing with private and specialized documents. In-context\nlearning and retrieval-augmented generation (RAG) face limitations, including\ntheir high inference costs and their inability to capture global document\ninformation. In this paper, we propose a way of modularizing knowledge by\ntraining document-level Knowledge Modules (KMs). KMs are lightweight components\nimplemented as parameter-efficient LoRA modules, which are trained to store\ninformation about new documents and can be easily plugged into models on\ndemand. We show that next-token prediction performs poorly as the training\nobjective for KMs. We instead propose Deep Context Distillation: we learn KMs\nparameters such as to simulate hidden states and logits of a teacher that takes\nthe document in context. Our method outperforms standard next-token prediction\nand pre-instruction training techniques, across two datasets. Finally, we\nhighlight synergies between KMs and RAG."}
{"id": "2504.20877", "pdf": "https://arxiv.org/pdf/2504.20877", "abs": "https://arxiv.org/abs/2504.20877", "authors": ["Meltem Tatlı", "Arpan Mukherjee", "Prashanth L. A.", "Karthikeyan Shanmugam", "Ali Tajer"], "title": "Preference-centric Bandits: Optimality of Mixtures and Regret-efficient Algorithms", "categories": ["stat.ML", "cs.LG"], "comment": null, "summary": "The objective of canonical multi-armed bandits is to identify and repeatedly\nselect an arm with the largest reward, often in the form of the expected value\nof the arm's probability distribution. Such a utilitarian perspective and focus\non the probability models' first moments, however, is agnostic to the\ndistributions' tail behavior and their implications for variability and risks\nin decision-making. This paper introduces a principled framework for shifting\nfrom expectation-based evaluation to an alternative reward formulation, termed\na preference metric (PM). The PMs can place the desired emphasis on different\nreward realization and can encode a richer modeling of preferences that\nincorporate risk aversion, robustness, or other desired attitudes toward\nuncertainty. A fundamentally distinct observation in such a PM-centric\nperspective is that designing bandit algorithms will have a significantly\ndifferent principle: as opposed to the reward-based models in which the optimal\nsampling policy converges to repeatedly sampling from the single best arm, in\nthe PM-centric framework the optimal policy converges to selecting a mix of\narms based on specific mixing weights. Designing such mixture policies departs\nfrom the principles for designing bandit algorithms in significant ways,\nprimarily because of uncountable mixture possibilities. The paper formalizes\nthe PM-centric framework and presents two algorithm classes (horizon-dependent\nand anytime) that learn and track mixtures in a regret-efficient fashion. These\nalgorithms have two distinctions from their canonical counterparts: (i) they\ninvolve an estimation routine to form reliable estimates of optimal mixtures,\nand (ii) they are equipped with tracking mechanisms to navigate arm selection\nfractions to track the optimal mixtures. These algorithms' regret guarantees\nare investigated under various algebraic forms of the PMs."}
{"id": "2504.14582", "pdf": "https://arxiv.org/pdf/2504.14582", "abs": "https://arxiv.org/abs/2504.14582", "authors": ["Zheng Chen", "Kai Liu", "Jue Gong", "Jingkai Wang", "Lei Sun", "Zongwei Wu", "Radu Timofte", "Yulun Zhang", "Xiangyu Kong", "Xiaoxuan Yu", "Hyunhee Park", "Suejin Han", "Hakjae Jeon", "Dafeng Zhang", "Hyung-Ju Chun", "Donghun Ryou", "Inju Ha", "Bohyung Han", "Lu Zhao", "Yuyi Zhang", "Pengyu Yan", "Jiawei Hu", "Pengwei Liu", "Fengjun Guo", "Hongyuan Yu", "Pufan Xu", "Zhijuan Huang", "Shuyuan Cui", "Peng Guo", "Jiahui Liu", "Dongkai Zhang", "Heng Zhang", "Huiyuan Fu", "Huadong Ma", "Yanhui Guo", "Sisi Tian", "Xin Liu", "Jinwen Liang", "Jie Liu", "Jie Tang", "Gangshan Wu", "Zeyu Xiao", "Zhuoyuan Li", "Yinxiang Zhang", "Wenxuan Cai", "Vijayalaxmi Ashok Aralikatti", "Nikhil Akalwadi", "G Gyaneshwar Rao", "Chaitra Desai", "Ramesh Ashok Tabib", "Uma Mudenagudi", "Marcos V. Conde", "Alejandro Merino", "Bruno Longarela", "Javier Abad", "Weijun Yuan", "Zhan Li", "Zhanglu Chen", "Boyang Yao", "Aagam Jain", "Milan Kumar Singh", "Ankit Kumar", "Shubh Kawa", "Divyavardhan Singh", "Anjali Sarvaiya", "Kishor Upla", "Raghavendra Ramachandra", "Chia-Ming Lee", "Yu-Fan Lin", "Chih-Chung Hsu", "Risheek V Hiremath", "Yashaswini Palani", "Yuxuan Jiang", "Qiang Zhu", "Siyue Teng", "Fan Zhang", "Shuyuan Zhu", "Bing Zeng", "David Bull", "Jingwei Liao", "Yuqing Yang", "Wenda Shao", "Junyi Zhao", "Qisheng Xu", "Kele Xu", "Sunder Ali Khowaja", "Ik Hyun Lee", "Snehal Singh Tomar", "Rajarshi Ray", "Klaus Mueller", "Sachin Chaudhary", "Surya Vashisth", "Akshay Dudhane", "Praful Hambarde", "Satya Naryan Tazi", "Prashant Patil", "Santosh Kumar Vipparthi", "Subrahmanyam Murala", "Bilel Benjdira", "Anas M. Ali", "Wadii Boulila", "Zahra Moammeri", "Ahmad Mahmoudi-Aznaveh", "Ali Karbasi", "Hossein Motamednia", "Liangyan Li", "Guanhua Zhao", "Kevin Le", "Yimo Ning", "Haoxuan Huang", "Jun Chen"], "title": "NTIRE 2025 Challenge on Image Super-Resolution ($\\times$4): Methods and Results", "categories": ["cs.CV"], "comment": "NTIRE 2025 webpage: https://www.cvlai.net/ntire/2025. Code:\n  https://github.com/zhengchen1999/NTIRE2025_ImageSR_x4", "summary": "This paper presents the NTIRE 2025 image super-resolution ($\\times$4)\nchallenge, one of the associated competitions of the 10th NTIRE Workshop at\nCVPR 2025. The challenge aims to recover high-resolution (HR) images from\nlow-resolution (LR) counterparts generated through bicubic downsampling with a\n$\\times$4 scaling factor. The objective is to develop effective network designs\nor solutions that achieve state-of-the-art SR performance. To reflect the dual\nobjectives of image SR research, the challenge includes two sub-tracks: (1) a\nrestoration track, emphasizes pixel-wise accuracy and ranks submissions based\non PSNR; (2) a perceptual track, focuses on visual realism and ranks results by\na perceptual score. A total of 286 participants registered for the competition,\nwith 25 teams submitting valid entries. This report summarizes the challenge\ndesign, datasets, evaluation protocol, the main results, and methods of each\nteam. The challenge serves as a benchmark to advance the state of the art and\nfoster progress in image SR."}
{"id": "2503.22809", "pdf": "https://arxiv.org/pdf/2503.22809", "abs": "https://arxiv.org/abs/2503.22809", "authors": ["Uddhav Bhattarai", "Rajkishan Arikapudi", "Steven A. Fennimore", "Frank N Martin", "Stavros G. Vougioukas"], "title": "Data-Driven Worker Activity Recognition and Efficiency Estimation in Manual Fruit Harvesting", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Manual fruit harvesting is common in agriculture, but the amount of time\npickers spend on non-productive activities can make it very inefficient.\nAccurately identifying picking vs. non-picking activity is crucial for\nestimating picker efficiency and optimising labour management and harvest\nprocesses. In this study, a practical system was developed to calculate the\nefficiency of pickers in commercial strawberry harvesting. Instrumented picking\ncarts were developed to record the harvested fruit weight, geolocation, and\ncart movement in real time. These carts were deployed during the commercial\nstrawberry harvest season in Santa Maria, CA. The collected data was then used\nto train a CNN-LSTM-based deep neural network to classify a picker's activity\ninto \"Pick\" and \"NoPick\" classes. Experimental evaluations showed that the\nCNN-LSTM model showed promising activity recognition performance with an F1\nscore accuracy of over 0.97. The recognition results were then used to compute\npicker efficiency and the time required to fill a tray. Analysis of the\nseason-long harvest data showed that the average picker efficiency was 75.07%\nwith an estimation accuracy of 95.22%. Furthermore, the average tray fill time\nwas 6.79 minutes with an estimation accuracy of 96.43%. When integrated into\ncommercial harvesting, the proposed technology can aid growers in monitoring\nautomated worker activity and optimising harvests to reduce non-productive time\nand enhance overall harvest efficiency."}
{"id": "2504.20883", "pdf": "https://arxiv.org/pdf/2504.20883", "abs": "https://arxiv.org/abs/2504.20883", "authors": ["Aditya Bhaskara", "Sepideh Mahabadi", "Madhusudhan Reddy Pittu", "Ali Vakilian", "David P. Woodruff"], "title": "Guessing Efficiently for Constrained Subspace Approximation", "categories": ["cs.DS", "cs.LG"], "comment": null, "summary": "In this paper we study constrained subspace approximation problem. Given a\nset of $n$ points $\\{a_1,\\ldots,a_n\\}$ in $\\mathbb{R}^d$, the goal of the {\\em\nsubspace approximation} problem is to find a $k$ dimensional subspace that best\napproximates the input points. More precisely, for a given $p\\geq 1$, we aim to\nminimize the $p$th power of the $\\ell_p$ norm of the error vector\n$(\\|a_1-\\bm{P}a_1\\|,\\ldots,\\|a_n-\\bm{P}a_n\\|)$, where $\\bm{P}$ denotes the\nprojection matrix onto the subspace and the norms are Euclidean. In\n\\emph{constrained} subspace approximation (CSA), we additionally have\nconstraints on the projection matrix $\\bm{P}$. In its most general form, we\nrequire $\\bm{P}$ to belong to a given subset $\\mathcal{S}$ that is described\nexplicitly or implicitly.\n  We introduce a general framework for constrained subspace approximation. Our\napproach, that we term coreset-guess-solve, yields either\n$(1+\\varepsilon)$-multiplicative or $\\varepsilon$-additive approximations for a\nvariety of constraints. We show that it provides new algorithms for\npartition-constrained subspace approximation with applications to {\\it fair}\nsubspace approximation, $k$-means clustering, and projected non-negative matrix\nfactorization, among others. Specifically, while we reconstruct the best known\nbounds for $k$-means clustering in Euclidean spaces, we improve the known\nresults for the remainder of the problems."}
{"id": "2504.18317", "pdf": "https://arxiv.org/pdf/2504.18317", "abs": "https://arxiv.org/abs/2504.18317", "authors": ["Zhengru Fang", "Zhenghao Liu", "Jingjing Wang", "Senkang Hu", "Yu Guo", "Yiqin Deng", "Yuguang Fang"], "title": "Task-Oriented Communications for Visual Navigation with Edge-Aerial Collaboration in Low Altitude Economy", "categories": ["cs.CV", "cs.NI"], "comment": "Code and dataset will be made publicly available:\n  https://github.com/fangzr/TOC-Edge-Aerial", "summary": "To support the Low Altitude Economy (LAE), precise unmanned aerial vehicles\n(UAVs) localization in urban areas where global positioning system (GPS)\nsignals are unavailable. Vision-based methods offer a viable alternative but\nface severe bandwidth, memory and processing constraints on lightweight UAVs.\nInspired by mammalian spatial cognition, we propose a task-oriented\ncommunication framework, where UAVs equipped with multi-camera systems extract\ncompact multi-view features and offload localization tasks to edge servers. We\nintroduce the Orthogonally-constrained Variational Information Bottleneck\nencoder (O-VIB), which incorporates automatic relevance determination (ARD) to\nprune non-informative features while enforcing orthogonality to minimize\nredundancy. This enables efficient and accurate localization with minimal\ntransmission cost. Extensive evaluation on a dedicated LAE UAV dataset shows\nthat O-VIB achieves high-precision localization under stringent bandwidth\nbudgets. Code and dataset will be made publicly available:\ngithub.com/fangzr/TOC-Edge-Aerial."}
{"id": "2504.03665", "pdf": "https://arxiv.org/pdf/2504.03665", "abs": "https://arxiv.org/abs/2504.03665", "authors": ["Noujoud Nader", "Patrick Diehl", "Steve Brandt", "Hartmut Kaiser"], "title": "LLM & HPC:Benchmarking DeepSeek's Performance in High-Performance Computing Tasks", "categories": ["cs.DC", "cs.AI"], "comment": "9 pages, 2 figures, 3 tables, conference", "summary": "Large Language Models (LLMs), such as GPT-4 and DeepSeek, have been applied\nto a wide range of domains in software engineering. However, their potential in\nthe context of High-Performance Computing (HPC) much remains to be explored.\nThis paper evaluates how well DeepSeek, a recent LLM, performs in generating a\nset of HPC benchmark codes: a conjugate gradient solver, the parallel heat\nequation, parallel matrix multiplication, DGEMM, and the STREAM triad\noperation. We analyze DeepSeek's code generation capabilities for traditional\nHPC languages like Cpp, Fortran, Julia and Python. The evaluation includes\ntesting for code correctness, performance, and scaling across different\nconfigurations and matrix sizes. We also provide a detailed comparison between\nDeepSeek and another widely used tool: GPT-4. Our results demonstrate that\nwhile DeepSeek generates functional code for HPC tasks, it lags behind GPT-4,\nin terms of scalability and execution efficiency of the generated code."}
{"id": "2504.20904", "pdf": "https://arxiv.org/pdf/2504.20904", "abs": "https://arxiv.org/abs/2504.20904", "authors": ["Hossein Shokouhinejad", "Roozbeh Razavi-Far", "Griffin Higgins", "Ali A. Ghorbani"], "title": "Dual Explanations via Subgraph Matching for Malware Detection", "categories": ["cs.CR", "cs.LG"], "comment": null, "summary": "Interpretable malware detection is crucial for understanding harmful\nbehaviors and building trust in automated security systems. Traditional\nexplainable methods for Graph Neural Networks (GNNs) often highlight important\nregions within a graph but fail to associate them with known benign or\nmalicious behavioral patterns. This limitation reduces their utility in\nsecurity contexts, where alignment with verified prototypes is essential. In\nthis work, we introduce a novel dual prototype-driven explainable framework\nthat interprets GNN-based malware detection decisions. This dual explainable\nframework integrates a base explainer (a state-of-the-art explainer) with a\nnovel second-level explainer which is designed by subgraph matching technique,\ncalled SubMatch explainer. The proposed explainer assigns interpretable scores\nto nodes based on their association with matched subgraphs, offering a\nfine-grained distinction between benign and malicious regions. This\nprototype-guided scoring mechanism enables more interpretable, behavior-aligned\nexplanations. Experimental results demonstrate that our method preserves high\ndetection performance while significantly improving interpretability in malware\nanalysis."}
{"id": "2504.18589", "pdf": "https://arxiv.org/pdf/2504.18589", "abs": "https://arxiv.org/abs/2504.18589", "authors": ["Zhikai Wang", "Jiashuo Sun", "Wenqi Zhang", "Zhiqiang Hu", "Xin Li", "Fan Wang", "Deli Zhao"], "title": "Benchmarking Multimodal Mathematical Reasoning with Explicit Visual Dependency", "categories": ["cs.CV"], "comment": "Home page: https://alibaba-damo-academy.github.io/VCBench/", "summary": "Recent advancements in Large Vision-Language Models (LVLMs) have\nsignificantly enhanced their ability to integrate visual and linguistic\ninformation, achieving near-human proficiency in tasks like object recognition,\ncaptioning, and visual question answering. However, current benchmarks\ntypically focus on knowledge-centric evaluations that assess domain-specific\nexpertise, often neglecting the core ability to reason about fundamental\nmathematical elements and visual concepts. We identify a gap in evaluating\nelementary-level math problems, which rely on explicit visual\ndependencies-requiring models to discern, integrate, and reason across multiple\nimages while incorporating commonsense knowledge, all of which are crucial for\nadvancing toward broader AGI capabilities. To address this gap, we introduce\nVCBENCH, a comprehensive benchmark for multimodal mathematical reasoning with\nexplicit visual dependencies. VCBENCH includes 1,720 problems across six\ncognitive domains, featuring 6,697 images (averaging 3.9 per question) to\nensure multi-image reasoning. We evaluate 26 state-of-the-art LVLMs on VCBENCH,\nrevealing substantial performance disparities, with even the top models unable\nto exceed 50% accuracy. Our findings highlight the ongoing challenges in\nvisual-mathematical integration and suggest avenues for future LVLM\nadvancements."}
{"id": "2504.03799", "pdf": "https://arxiv.org/pdf/2504.03799", "abs": "https://arxiv.org/abs/2504.03799", "authors": ["Hengyu Lin"], "title": "Experimental Study on Time Series Analysis of Lower Limb Rehabilitation Exercise Data Driven by Novel Model Architecture and Large Models", "categories": ["eess.SP", "cs.AI"], "comment": null, "summary": "This study investigates the application of novel model architectures and\nlarge-scale foundational models in temporal series analysis of lower limb\nrehabilitation motion data, aiming to leverage advancements in machine learning\nand artificial intelligence to empower active rehabilitation guidance\nstrategies for post-stroke patients in limb motor function recovery. Utilizing\nthe SIAT-LLMD dataset of lower limb movement data proposed by the Shenzhen\nInstitute of Advanced Technology, Chinese Academy of Sciences, we\nsystematically elucidate the implementation and analytical outcomes of the\ninnovative xLSTM architecture and the foundational model Lag-Llama in\nshort-term temporal prediction tasks involving joint kinematics and dynamics\nparameters. The research provides novel insights for AI-enabled medical\nrehabilitation applications, demonstrating the potential of cutting-edge model\narchitectures and large-scale models in rehabilitation medicine temporal\nprediction. These findings establish theoretical foundations for future\napplications of personalized rehabilitation regimens, offering significant\nimplications for the development of customized therapeutic interventions in\nclinical practice."}
{"id": "2504.20906", "pdf": "https://arxiv.org/pdf/2504.20906", "abs": "https://arxiv.org/abs/2504.20906", "authors": ["Sarad Venugopalan", "Sridhar Adepu"], "title": "GiBy: A Giant-Step Baby-Step Classifier For Anomaly Detection In Industrial Control Systems", "categories": ["cs.CR", "cs.LG"], "comment": null, "summary": "The continuous monitoring of the interactions between cyber-physical\ncomponents of any industrial control system (ICS) is required to secure\nautomation of the system controls, and to guarantee plant processes are\nfail-safe and remain in an acceptably safe state. Safety is achieved by\nmanaging actuation (where electric signals are used to trigger physical\nmovement), dependent on corresponding sensor readings; used as ground truth in\ndecision making. Timely detection of anomalies (attacks, faults and\nunascertained states) in ICSs is crucial for the safe running of a plant, the\nsafety of its personnel, and for the safe provision of any services provided.\nWe propose an anomaly detection method that involves accurate linearization of\nthe non-linear forms arising from sensor-actuator(s) relationships, primarily\nbecause solving linear models is easier and well understood. Further, the time\ncomplexity of the anomaly detection scenario/problem at hand is lowered using\ndimensionality reduction of the actuator(s) in relationship with a sensor. We\naccomplish this by using a well-known water treatment testbed as a use case.\nOur experiments show millisecond time response to detect anomalies and provide\nexplainability; that are not simultaneously achieved by other state of the art\nAI/ML models with eXplainable AI (XAI) used for the same purpose. Further, we\npin-point the sensor(s) and its actuation state for which anomaly was detected."}
{"id": "2504.19165", "pdf": "https://arxiv.org/pdf/2504.19165", "abs": "https://arxiv.org/abs/2504.19165", "authors": ["Yuan Li", "Ziqian Bai", "Feitong Tan", "Zhaopeng Cui", "Sean Fanello", "Yinda Zhang"], "title": "IM-Portrait: Learning 3D-aware Video Diffusion for Photorealistic Talking Heads from Monocular Videos", "categories": ["cs.CV"], "comment": "CVPR2025; project page:\n  https://y-u-a-n-l-i.github.io/projects/IM-Portrait/", "summary": "We propose a novel 3D-aware diffusion-based method for generating\nphotorealistic talking head videos directly from a single identity image and\nexplicit control signals (e.g., expressions). Our method generates Multiplane\nImages (MPIs) that ensure geometric consistency, making them ideal for\nimmersive viewing experiences like binocular videos for VR headsets. Unlike\nexisting methods that often require a separate stage or joint optimization to\nreconstruct a 3D representation (such as NeRF or 3D Gaussians), our approach\ndirectly generates the final output through a single denoising process,\neliminating the need for post-processing steps to render novel views\nefficiently. To effectively learn from monocular videos, we introduce a\ntraining mechanism that reconstructs the output MPI randomly in either the\ntarget or the reference camera space. This approach enables the model to\nsimultaneously learn sharp image details and underlying 3D information.\nExtensive experiments demonstrate the effectiveness of our method, which\nachieves competitive avatar quality and novel-view rendering capabilities, even\nwithout explicit 3D reconstruction or high-quality multi-view training data."}
{"id": "2504.09546", "pdf": "https://arxiv.org/pdf/2504.09546", "abs": "https://arxiv.org/abs/2504.09546", "authors": ["Shiqian Li", "Yuxi Ma", "Jiajun Yan", "Bo Dai", "Yujia Peng", "Chi Zhang", "Yixin Zhu"], "title": "A simulation-heuristics dual-process model for intuitive physics", "categories": ["physics.ed-ph", "cs.AI"], "comment": "8 pages, CogSci 2025", "summary": "The role of mental simulation in human physical reasoning is widely\nacknowledged, but whether it is employed across scenarios with varying\nsimulation costs and where its boundary lies remains unclear. Using a\npouring-marble task, our human study revealed two distinct error patterns when\npredicting pouring angles, differentiated by simulation time. While mental\nsimulation accurately captured human judgments in simpler scenarios, a linear\nheuristic model better matched human predictions when simulation time exceeded\na certain boundary. Motivated by these observations, we propose a dual-process\nframework, Simulation-Heuristics Model (SHM), where intuitive physics employs\nsimulation for short-time simulation but switches to heuristics when simulation\nbecomes costly. By integrating computational methods previously viewed as\nseparate into a unified model, SHM quantitatively captures their switching\nmechanism. The SHM aligns more precisely with human behavior and demonstrates\nconsistent predictive performance across diverse scenarios, advancing our\nunderstanding of the adaptive nature of intuitive physical reasoning."}
{"id": "2504.20940", "pdf": "https://arxiv.org/pdf/2504.20940", "abs": "https://arxiv.org/abs/2504.20940", "authors": ["Maximilian Stupp", "P. S. Koutsourelakis"], "title": "Energy-Based Coarse-Graining in Molecular Dynamics: A Flow-Based Framework Without Data", "categories": ["physics.chem-ph", "cs.LG", "physics.comp-ph"], "comment": null, "summary": "Coarse-grained (CG) models offer an effective route to reducing the\ncomplexity of molecular simulations, yet conventional approaches depend heavily\non long all-atom molecular dynamics (MD) trajectories to adequately sample\nconfigurational space. This data-driven dependence limits their accuracy and\ngeneralizability, as unvisited configurations remain excluded from the\nresulting CG model. We introduce a data-free generative framework for\ncoarse-graining that directly targets the all-atom Boltzmann distribution. Our\nmodel defines a structured latent space comprising slow collective variables,\nwhich are statistically associated with multimodal marginal densities capturing\nmetastable states, and fast variables, which represent the remaining degrees of\nfreedom with simple, unimodal conditional distributions. A potentially\nlearnable, bijective map from the full latent space to the all-atom\nconfiguration space enables automatic and accurate reconstruction of molecular\nstructures. The model is trained using an energy-based objective that minimizes\nthe reverse Kullback-Leibler divergence, relying solely on the interatomic\npotential rather than sampled trajectories. A tempering scheme is used to\nstabilize training and promote exploration of diverse configurations. Once\ntrained, the model can generate unbiased, one-shot equilibrium all-atom\nsamples. We validate the method on two synthetic systems-a double-well\npotential and a Gaussian mixture-as well as on the benchmark alanine dipeptide.\nThe model captures all relevant modes of the Boltzmann distribution, accurately\nreconstructs atomic configurations, and learns physically meaningful\ncoarse-grained representations, all without any simulation data."}
{"id": "2411.17982", "pdf": "https://arxiv.org/pdf/2411.17982", "abs": "https://arxiv.org/abs/2411.17982", "authors": ["Wei Zhang", "Qing Cheng", "David Skuddis", "Niclas Zeller", "Daniel Cremers", "Norbert Haala"], "title": "HI-SLAM2: Geometry-Aware Gaussian SLAM for Fast Monocular Scene Reconstruction", "categories": ["cs.RO", "cs.CV"], "comment": "Under review process", "summary": "We present HI-SLAM2, a geometry-aware Gaussian SLAM system that achieves fast\nand accurate monocular scene reconstruction using only RGB input. Existing\nNeural SLAM or 3DGS-based SLAM methods often trade off between rendering\nquality and geometry accuracy, our research demonstrates that both can be\nachieved simultaneously with RGB input alone. The key idea of our approach is\nto enhance the ability for geometry estimation by combining easy-to-obtain\nmonocular priors with learning-based dense SLAM, and then using 3D Gaussian\nsplatting as our core map representation to efficiently model the scene. Upon\nloop closure, our method ensures on-the-fly global consistency through\nefficient pose graph bundle adjustment and instant map updates by explicitly\ndeforming the 3D Gaussian units based on anchored keyframe updates.\nFurthermore, we introduce a grid-based scale alignment strategy to maintain\nimproved scale consistency in prior depths for finer depth details. Through\nextensive experiments on Replica, ScanNet, and ScanNet++, we demonstrate\nsignificant improvements over existing Neural SLAM methods and even surpass\nRGB-D-based methods in both reconstruction and rendering quality. The project\npage and source code will be made available at https://hi-slam2.github.io/."}
{"id": "2504.10498", "pdf": "https://arxiv.org/pdf/2504.10498", "abs": "https://arxiv.org/abs/2504.10498", "authors": ["Jianling Lu", "Mingqi Lv", "Tieming Chen"], "title": "CCSK:Cognitive Convection of Self-Knowledge Based Retrieval Augmentation for Large Language Models", "categories": ["cs.IR", "cs.AI"], "comment": null, "summary": "The performance of large language models (LLMs) in Q&A task increased\nsubstantially through Retrieval-Augmented Generation (RAG) which brings in\nexternal knowledge. However, the main difficulty lies in balancing the inherent\nself-knowledge of LLMs with external information retrieval (IR). The current\nthreshold-based methods apply one-dimensional static mechanisms with single\ncriterion. As a result, their IR decisions might be irrelevant to the LLMs'\nresponse under difficult queries. To alleviate this problem, we propose\nCognitive Convection of Self-Knowledge (CCSK). Different from traditional\nmethods that maintain single fixed IR activation criteria, CCSK implements a\ndynamic joint decision process via a Siamese Network module and a Response\nQuality Model. The Siamese Network calculates the cosine similarity between the\ncurrent query and the historical queries. The Response Quality Model evaluates\nthe responses of LLMs through LightGBM. The final decision of the CCSK is\nderived from the outputs of the two modules, as well as text features fused\nusing a multi-head attention mechanism. Extensive experiments on real-world\ndatasets show that CCSK significantly enhances the model's effectiveness in\ninformation retrieval."}
{"id": "2504.20969", "pdf": "https://arxiv.org/pdf/2504.20969", "abs": "https://arxiv.org/abs/2504.20969", "authors": ["Yiting Zhang", "Shichen Li", "Elena Shrestha"], "title": "XPG-RL: Reinforcement Learning with Explainable Priority Guidance for Efficiency-Boosted Mechanical Search", "categories": ["cs.RO", "cs.LG"], "comment": "13 pages, 5 figures", "summary": "Mechanical search (MS) in cluttered environments remains a significant\nchallenge for autonomous manipulators, requiring long-horizon planning and\nrobust state estimation under occlusions and partial observability. In this\nwork, we introduce XPG-RL, a reinforcement learning framework that enables\nagents to efficiently perform MS tasks through explainable, priority-guided\ndecision-making based on raw sensory inputs. XPG-RL integrates a task-driven\naction prioritization mechanism with a learned context-aware switching strategy\nthat dynamically selects from a discrete set of action primitives such as\ntarget grasping, occlusion removal, and viewpoint adjustment. Within this\nstrategy, a policy is optimized to output adaptive threshold values that govern\nthe discrete selection among action primitives. The perception module fuses\nRGB-D inputs with semantic and geometric features to produce a structured scene\nrepresentation for downstream decision-making. Extensive experiments in both\nsimulation and real-world settings demonstrate that XPG-RL consistently\noutperforms baseline methods in task success rates and motion efficiency,\nachieving up to 4.5$\\times$ higher efficiency in long-horizon tasks. These\nresults underscore the benefits of integrating domain knowledge with learnable\ndecision-making policies for robust and efficient robotic manipulation."}
{"id": "2412.07775", "pdf": "https://arxiv.org/pdf/2412.07775", "abs": "https://arxiv.org/abs/2412.07775", "authors": ["Zhen Liu", "Tim Z. Xiao", "Weiyang Liu", "Yoshua Bengio", "Dinghuai Zhang"], "title": "Efficient Diversity-Preserving Diffusion Alignment via Gradient-Informed GFlowNets", "categories": ["cs.LG", "cs.CV"], "comment": "Technical Report (37 pages, 31 figures), Accepted at ICLR 2025", "summary": "While one commonly trains large diffusion models by collecting datasets on\ntarget downstream tasks, it is often desired to align and finetune pretrained\ndiffusion models with some reward functions that are either designed by experts\nor learned from small-scale datasets. Existing post-training methods for reward\nfinetuning of diffusion models typically suffer from lack of diversity in\ngenerated samples, lack of prior preservation, and/or slow convergence in\nfinetuning. In response to this challenge, we take inspiration from recent\nsuccesses in generative flow networks (GFlowNets) and propose a reinforcement\nlearning method for diffusion model finetuning, dubbed Nabla-GFlowNet\n(abbreviated as $\\nabla$-GFlowNet), that leverages the rich signal in reward\ngradients for probabilistic diffusion finetuning. We show that our proposed\nmethod achieves fast yet diversity- and prior-preserving finetuning of Stable\nDiffusion, a large-scale text-conditioned image diffusion model, on different\nrealistic reward functions."}
{"id": "2504.12397", "pdf": "https://arxiv.org/pdf/2504.12397", "abs": "https://arxiv.org/abs/2504.12397", "authors": ["Kristjan Greenewald", "Luis Lastras", "Thomas Parnell", "Vraj Shah", "Lucian Popa", "Giulio Zizzo", "Chulaka Gunasekara", "Ambrish Rawat", "David Cox"], "title": "Activated LoRA: Fine-tuned LLMs for Intrinsics", "categories": ["cs.LG", "cs.AI"], "comment": "arXiv admin note: text overlap with arXiv:2504.11704", "summary": "Low-Rank Adaptation (LoRA) has emerged as a highly efficient framework for\nfinetuning the weights of large foundation models, and has become the go-to\nmethod for data-driven customization of LLMs. Despite the promise of highly\ncustomized behaviors and capabilities, switching between relevant LoRAs in a\nmultiturn setting is highly inefficient, as the key-value (KV) cache of the\nentire turn history must be recomputed with the LoRA weights before generation\ncan begin. To address this problem, we propose Activated LoRA (aLoRA), which\nmodifies the LoRA framework to only adapt weights for the tokens in the\nsequence \\emph{after} the aLoRA is invoked. This change crucially allows aLoRA\nto accept the base model's KV cache of the input string, meaning that aLoRA can\nbe instantly activated whenever needed in a chain without recomputing the\ncache. This enables building what we call \\emph{intrinsics}, i.e. highly\nspecialized models invoked to perform well-defined operations on portions of an\ninput chain or conversation that otherwise uses the base model by default. We\nuse aLoRA to train a set of intrinsics models, demonstrating competitive\naccuracy with standard LoRA while achieving significant inference benefits."}
{"id": "2504.20982", "pdf": "https://arxiv.org/pdf/2504.20982", "abs": "https://arxiv.org/abs/2504.20982", "authors": ["Tyler Chen", "Archan Ray", "Akshay Seshadri", "Dylan Herman", "Bao Bach", "Pranav Deshpande", "Abhishek Som", "Niraj Kumar", "Marco Pistoia"], "title": "Provably faster randomized and quantum algorithms for k-means clustering via uniform sampling", "categories": ["quant-ph", "cs.DS", "cs.LG"], "comment": null, "summary": "The $k$-means algorithm (Lloyd's algorithm) is a widely used method for\nclustering unlabeled data. A key bottleneck of the $k$-means algorithm is that\neach iteration requires time linear in the number of data points, which can be\nexpensive in big data applications. This was improved in recent works proposing\nquantum and quantum-inspired classical algorithms to approximate the $k$-means\nalgorithm locally, in time depending only logarithmically on the number of data\npoints (along with data dependent parameters) [$q$-means: A quantum algorithm\nfor unsupervised machine learning; Kerenidis, Landman, Luongo, and Prakash,\nNeurIPS 2019; Do you know what $q$-means?, Doriguello, Luongo, Tang]. In this\nwork, we describe a simple randomized mini-batch $k$-means algorithm and a\nquantum algorithm inspired by the classical algorithm. We prove worse-case\nguarantees that significantly improve upon the bounds for previous algorithms.\nOur improvements are due to a careful use of uniform sampling, which preserves\ncertain symmetries of the $k$-means problem that are not preserved in previous\nalgorithms that use data norm-based sampling."}
{"id": "2501.18627", "pdf": "https://arxiv.org/pdf/2501.18627", "abs": "https://arxiv.org/abs/2501.18627", "authors": ["Ziyi Zhang", "Nicolas Roussel", "Thomas Müller", "Tizian Zeltner", "Merlin Nimier-David", "Fabrice Rousselle", "Wenzel Jakob"], "title": "Radiance Surfaces: Optimizing Surface Representations with a 5D Radiance Field Loss", "categories": ["cs.GR", "cs.CV"], "comment": null, "summary": "We present a fast and simple technique to convert images into a radiance\nsurface-based scene representation. Building on existing radiance volume\nreconstruction algorithms, we introduce a subtle yet impactful modification of\nthe loss function requiring changes to only a few lines of code: instead of\nintegrating the radiance field along rays and supervising the resulting images,\nwe project the training images into the scene to directly supervise the\nspatio-directional radiance field.\n  The primary outcome of this change is the complete removal of alpha blending\nand ray marching from the image formation model, instead moving these steps\ninto the loss computation. In addition to promoting convergence to surfaces,\nthis formulation assigns explicit semantic meaning to 2D subsets of the\nradiance field, turning them into well-defined radiance surfaces. We finally\nextract a level set from this representation, which results in a high-quality\nradiance surface model.\n  Our method retains much of the speed and quality of the baseline algorithm.\nFor instance, a suitably modified variant of Instant NGP maintains comparable\ncomputational efficiency, while achieving an average PSNR that is only 0.1 dB\nlower. Most importantly, our method generates explicit surfaces in place of an\nexponential volume, doing so with a level of simplicity not seen in prior work."}
{"id": "2504.18598", "pdf": "https://arxiv.org/pdf/2504.18598", "abs": "https://arxiv.org/abs/2504.18598", "authors": ["Qingyue Wang", "Qi Pang", "Xixun Lin", "Shuai Wang", "Daoyuan Wu"], "title": "BadMoE: Backdooring Mixture-of-Experts LLMs via Optimizing Routing Triggers and Infecting Dormant Experts", "categories": ["cs.CR", "cs.AI"], "comment": null, "summary": "Mixture-of-Experts (MoE) have emerged as a powerful architecture for large\nlanguage models (LLMs), enabling efficient scaling of model capacity while\nmaintaining manageable computational costs. The key advantage lies in their\nability to route different tokens to different ``expert'' networks within the\nmodel, enabling specialization and efficient handling of diverse input.\nHowever, the vulnerabilities of MoE-based LLMs still have barely been studied,\nand the potential for backdoor attacks in this context remains largely\nunexplored. This paper presents the first backdoor attack against MoE-based\nLLMs where the attackers poison ``dormant experts'' (i.e., underutilized\nexperts) and activate them by optimizing routing triggers, thereby gaining\ncontrol over the model's output. We first rigorously prove the existence of a\nfew ``dominating experts'' in MoE models, whose outputs can determine the\noverall MoE's output. We also show that dormant experts can serve as dominating\nexperts to manipulate model predictions. Accordingly, our attack, namely\nBadMoE, exploits the unique architecture of MoE models by 1) identifying\ndormant experts unrelated to the target task, 2) constructing a routing-aware\nloss to optimize the activation triggers of these experts, and 3) promoting\ndormant experts to dominating roles via poisoned training data. Extensive\nexperiments show that BadMoE successfully enforces malicious prediction on\nattackers' target tasks while preserving overall model utility, making it a\nmore potent and stealthy attack than existing methods."}
{"id": "2504.20984", "pdf": "https://arxiv.org/pdf/2504.20984", "abs": "https://arxiv.org/abs/2504.20984", "authors": ["Evan Li", "Tushin Mallick", "Evan Rose", "William Robertson", "Alina Oprea", "Cristina Nita-Rotaru"], "title": "ACE: A Security Architecture for LLM-Integrated App Systems", "categories": ["cs.CR", "cs.LG"], "comment": "21 pages, 13 figures", "summary": "LLM-integrated app systems extend the utility of Large Language Models (LLMs)\nwith third-party apps that are invoked by a system LLM using interleaved\nplanning and execution phases to answer user queries. These systems introduce\nnew attack vectors where malicious apps can cause integrity violation of\nplanning or execution, availability breakdown, or privacy compromise during\nexecution.\n  In this work, we identify new attacks impacting the integrity of planning, as\nwell as the integrity and availability of execution in LLM-integrated apps, and\ndemonstrate them against IsolateGPT, a recent solution designed to mitigate\nattacks from malicious apps. We propose Abstract-Concrete-Execute (ACE), a new\nsecure architecture for LLM-integrated app systems that provides security\nguarantees for system planning and execution. Specifically, ACE decouples\nplanning into two phases by first creating an abstract execution plan using\nonly trusted information, and then mapping the abstract plan to a concrete plan\nusing installed system apps. We verify that the plans generated by our system\nsatisfy user-specified secure information flow constraints via static analysis\non the structured plan output. During execution, ACE enforces data and\ncapability barriers between apps, and ensures that the execution is conducted\naccording to the trusted abstract plan. We show experimentally that our system\nis secure against attacks from the INJECAGENT benchmark, a standard benchmark\nfor control flow integrity in the face of indirect prompt injection attacks,\nand our newly introduced attacks. Our architecture represents a significant\nadvancement towards hardening LLM-based systems containing system facilities of\nvarying levels of trustworthiness."}
{"id": "2502.00114", "pdf": "https://arxiv.org/pdf/2502.00114", "abs": "https://arxiv.org/abs/2502.00114", "authors": ["Aaron Hao Tan", "Angus Fung", "Haitong Wang", "Goldie Nejat"], "title": "Mobile Robot Navigation Using Hand-Drawn Maps: A Vision Language Model Approach", "categories": ["cs.RO", "cs.CV"], "comment": "8 pages, 8 figures", "summary": "Hand-drawn maps can be used to convey navigation instructions between humans\nand robots in a natural and efficient manner. However, these maps can often\ncontain inaccuracies such as scale distortions and missing landmarks which\npresent challenges for mobile robot navigation. This paper introduces a novel\nHand-drawn Map Navigation (HAM-Nav) architecture that leverages pre-trained\nvision language models (VLMs) for robot navigation across diverse environments,\nhand-drawing styles, and robot embodiments, even in the presence of map\ninaccuracies. HAM-Nav integrates a unique Selective Visual Association\nPrompting approach for topological map-based position estimation and navigation\nplanning as well as a Predictive Navigation Plan Parser to infer missing\nlandmarks. Extensive experiments were conducted in photorealistic simulated\nenvironments, using both wheeled and legged robots, demonstrating the\neffectiveness of HAM-Nav in terms of navigation success rates and Success\nweighted by Path Length. Furthermore, a user study in real-world environments\nhighlighted the practical utility of hand-drawn maps for robot navigation as\nwell as successful navigation outcomes compared against a non-hand-drawn map\napproach."}
{"id": "2504.19013", "pdf": "https://arxiv.org/pdf/2504.19013", "abs": "https://arxiv.org/abs/2504.19013", "authors": ["Júlia Vicens Figueres", "Juliette Vanderhaeghen", "Federica Bragone", "Kateryna Morozovska", "Khemraj Shukla"], "title": "\\$PINN -- a Domain Decomposition Method for Bayesian Physics-Informed Neural Networks", "categories": ["cs.LG", "cs.AI", "math.AP"], "comment": "37 pages, 22 figures", "summary": "Physics-Informed Neural Networks (PINNs) are a novel computational approach\nfor solving partial differential equations (PDEs) with noisy and sparse initial\nand boundary data. Although, efficient quantification of epistemic and\naleatoric uncertainties in big multi-scale problems remains challenging. We\npropose \\$PINN a novel method of computing global uncertainty in PDEs using a\nBayesian framework, by combining local Bayesian Physics-Informed Neural\nNetworks (BPINN) with domain decomposition. The solution continuity across\nsubdomains is obtained by imposing the flux continuity across the interface of\nneighboring subdomains. To demonstrate the effectiveness of \\$PINN, we conduct\na series of computational experiments on PDEs in 1D and 2D spatial domains.\nAlthough we have adopted conservative PINNs (cPINNs), the method can be\nseamlessly extended to other domain decomposition techniques. The results infer\nthat the proposed method recovers the global uncertainty by computing the local\nuncertainty exactly more efficiently as the uncertainty in each subdomain can\nbe computed concurrently. The robustness of \\$PINN is verified by adding\nuncorrelated random noise to the training data up to 15% and testing for\ndifferent domain sizes."}
{"id": "2205.12379", "pdf": "https://arxiv.org/pdf/2205.12379", "abs": "https://arxiv.org/abs/2205.12379", "authors": ["Pierre Wolinski", "Julyan Arbel"], "title": "Gaussian Pre-Activations in Neural Networks: Myth or Reality?", "categories": ["cs.LG", "stat.ML"], "comment": "Published version. OpenReview URL:\n  https://openreview.net/forum?id=goe6fv6iSh", "summary": "The study of feature propagation at initialization in neural networks lies at\nthe root of numerous initialization designs. An assumption very commonly made\nin the field states that the pre-activations are Gaussian. Although this\nconvenient Gaussian hypothesis can be justified when the number of neurons per\nlayer tends to infinity, it is challenged by both theoretical and experimental\nworks for finite-width neural networks. Our major contribution is to construct\na family of pairs of activation functions and initialization distributions that\nensure that the pre-activations remain Gaussian throughout the network's depth,\neven in narrow neural networks. In the process, we discover a set of\nconstraints that a neural network should fulfill to ensure Gaussian\npre-activations. Additionally, we provide a critical review of the claims of\nthe Edge of Chaos line of works and build an exact Edge of Chaos analysis. We\nalso propose a unified view on pre-activations propagation, encompassing the\nframework of several well-known initialization procedures. Finally, our work\nprovides a principled framework for answering the much-debated question: is it\ndesirable to initialize the training of a neural network whose pre-activations\nare ensured to be Gaussian? Our code is available on GitHub:\nhttps://github.com/p-wol/gaussian-preact/ ."}
{"id": "2503.06698", "pdf": "https://arxiv.org/pdf/2503.06698", "abs": "https://arxiv.org/abs/2503.06698", "authors": ["Xavier Thomas", "Deepti Ghadiyaram"], "title": "What's in a Latent? Leveraging Diffusion Latent Space for Domain Generalization", "categories": ["cs.LG", "cs.CV"], "comment": null, "summary": "Domain Generalization aims to develop models that can generalize to novel and\nunseen data distributions. In this work, we study how model architectures and\npre-training objectives impact feature richness and propose a method to\neffectively leverage them for domain generalization. Specifically, given a\npre-trained feature space, we first discover latent domain structures, referred\nto as pseudo-domains, that capture domain-specific variations in an\nunsupervised manner. Next, we augment existing classifiers with these\ncomplementary pseudo-domain representations making them more amenable to\ndiverse unseen test domains. We analyze how different pre-training feature\nspaces differ in the domain-specific variances they capture. Our empirical\nstudies reveal that features from diffusion models excel at separating domains\nin the absence of explicit domain labels and capture nuanced domain-specific\ninformation. On 5 datasets, we show that our very simple framework improves\ngeneralization to unseen domains by a maximum test accuracy improvement of over\n4% compared to the standard baseline Empirical Risk Minimization (ERM).\nCrucially, our method outperforms most algorithms that access domain labels\nduring training."}
{"id": "2504.19323", "pdf": "https://arxiv.org/pdf/2504.19323", "abs": "https://arxiv.org/abs/2504.19323", "authors": ["Hanchen Yang", "Zishen Wan", "Ritik Raj", "Joongun Park", "Ziwei Li", "Ananda Samajdar", "Arijit Raychowdhury", "Tushar Krishna"], "title": "NSFlow: An End-to-End FPGA Framework with Scalable Dataflow Architecture for Neuro-Symbolic AI", "categories": ["cs.AR", "cs.AI", "cs.LG", "cs.PF"], "comment": "2025 IEEE/ACM Design Automation Conference (DAC)", "summary": "Neuro-Symbolic AI (NSAI) is an emerging paradigm that integrates neural\nnetworks with symbolic reasoning to enhance the transparency, reasoning\ncapabilities, and data efficiency of AI systems. Recent NSAI systems have\ngained traction due to their exceptional performance in reasoning tasks and\nhuman-AI collaborative scenarios. Despite these algorithmic advancements,\nexecuting NSAI tasks on existing hardware (e.g., CPUs, GPUs, TPUs) remains\nchallenging, due to their heterogeneous computing kernels, high memory\nintensity, and unique memory access patterns. Moreover, current NSAI algorithms\nexhibit significant variation in operation types and scales, making them\nincompatible with existing ML accelerators. These challenges highlight the need\nfor a versatile and flexible acceleration framework tailored to NSAI workloads.\nIn this paper, we propose NSFlow, an FPGA-based acceleration framework designed\nto achieve high efficiency, scalability, and versatility across NSAI systems.\nNSFlow features a design architecture generator that identifies workload data\ndependencies and creates optimized dataflow architectures, as well as a\nreconfigurable array with flexible compute units, re-organizable memory, and\nmixed-precision capabilities. Evaluating across NSAI workloads, NSFlow achieves\n31x speedup over Jetson TX2, more than 2x over GPU, 8x speedup over TPU-like\nsystolic array, and more than 3x over Xilinx DPU. NSFlow also demonstrates\nenhanced scalability, with only 4x runtime increase when symbolic workloads\nscale by 150x. To the best of our knowledge, NSFlow is the first framework to\nenable real-time generalizable NSAI algorithms acceleration, demonstrating a\npromising solution for next-generation cognitive systems."}
{"id": "2307.13586", "pdf": "https://arxiv.org/pdf/2307.13586", "abs": "https://arxiv.org/abs/2307.13586", "authors": ["Zihan Zhang", "Yuxin Chen", "Jason D. Lee", "Simon S. Du"], "title": "Settling the Sample Complexity of Online Reinforcement Learning", "categories": ["cs.LG"], "comment": "accepted to Journal of the ACM (also appeared in COLT 2024 as an\n  extended abstract)", "summary": "A central issue lying at the heart of online reinforcement learning (RL) is\ndata efficiency. While a number of recent works achieved asymptotically minimal\nregret in online RL, the optimality of these results is only guaranteed in a\n``large-sample'' regime, imposing enormous burn-in cost in order for their\nalgorithms to operate optimally. How to achieve minimax-optimal regret without\nincurring any burn-in cost has been an open problem in RL theory.\n  We settle this problem for the context of finite-horizon inhomogeneous Markov\ndecision processes. Specifically, we prove that a modified version of Monotonic\nValue Propagation (MVP), a model-based algorithm proposed by\n\\cite{zhang2020reinforcement}, achieves a regret on the order of (modulo log\nfactors) \\begin{equation*}\n  \\min\\big\\{ \\sqrt{SAH^3K}, \\,HK \\big\\}, \\end{equation*} where $S$ is the\nnumber of states, $A$ is the number of actions, $H$ is the planning horizon,\nand $K$ is the total number of episodes. This regret matches the minimax lower\nbound for the entire range of sample size $K\\geq 1$, essentially eliminating\nany burn-in requirement. It also translates to a PAC sample complexity (i.e.,\nthe number of episodes needed to yield $\\varepsilon$-accuracy) of\n$\\frac{SAH^3}{\\varepsilon^2}$ up to log factor, which is minimax-optimal for\nthe full $\\varepsilon$-range.\n  Further, we extend our theory to unveil the influences of problem-dependent\nquantities like the optimal value/cost and certain variances. The key technical\ninnovation lies in the development of a new regret decomposition strategy and a\nnovel analysis paradigm to decouple complicated statistical dependency -- a\nlong-standing challenge facing the analysis of online RL in the sample-hungry\nregime."}
{"id": "2504.01790", "pdf": "https://arxiv.org/pdf/2504.01790", "abs": "https://arxiv.org/abs/2504.01790", "authors": ["Sveinung Johan Ohrem", "Bent Haugaløkken", "Eleni Kelasidi"], "title": "SOLAQUA: SINTEF Ocean Large Aquaculture Robotics Dataset", "categories": ["cs.RO", "cs.CV"], "comment": null, "summary": "This paper presents a dataset gathered with an underwater robot in a\nsea-based aquaculture setting. Data was gathered from an operational fish farm\nand includes data from sensors such as the Waterlinked A50 DVL, the Nortek\nNucleus 1000 DVL, Sonardyne Micro Ranger 2 USBL, Sonoptix Mulitbeam Sonar, mono\nand stereo cameras, and vehicle sensor data such as power usage, IMU, pressure,\ntemperature, and more. Data acquisition is performed during both manual and\nautonomous traversal of the net pen structure. The collected vision data is of\nundamaged nets with some fish and marine growth presence, and it is expected\nthat both the research community and the aquaculture industry will benefit\ngreatly from the utilization of the proposed SOLAQUA dataset."}
{"id": "2504.19373", "pdf": "https://arxiv.org/pdf/2504.19373", "abs": "https://arxiv.org/abs/2504.19373", "authors": ["Weidi Luo", "Qiming Zhang", "Tianyu Lu", "Xiaogeng Liu", "Yue Zhao", "Zhen Xiang", "Chaowei Xiao"], "title": "Doxing via the Lens: Revealing Privacy Leakage in Image Geolocation for Agentic Multi-Modal Large Reasoning Model", "categories": ["cs.CR", "cs.AI"], "comment": null, "summary": "The increasing capabilities of agentic multi-modal large reasoning models,\nsuch as ChatGPT o3, have raised critical concerns regarding privacy leakage\nthrough inadvertent image geolocation. In this paper, we conduct the first\nsystematic and controlled study on the potential privacy risks associated with\nvisual reasoning abilities of ChatGPT o3. We manually collect and construct a\ndataset comprising 50 real-world images that feature individuals alongside\nprivacy-relevant environmental elements, capturing realistic and sensitive\nscenarios for analysis. Our experimental evaluation reveals that ChatGPT o3 can\npredict user locations with high precision, achieving street-level accuracy\n(within one mile) in 60% of cases. Through analysis, we identify key visual\ncues, including street layout and front yard design, that significantly\ncontribute to the model inference success. Additionally, targeted occlusion\nexperiments demonstrate that masking critical features effectively mitigates\ngeolocation accuracy, providing insights into potential defense mechanisms. Our\nfindings highlight an urgent need for privacy-aware development for agentic\nmulti-modal large reasoning models, particularly in applications involving\nprivate imagery."}
{"id": "2405.01480", "pdf": "https://arxiv.org/pdf/2405.01480", "abs": "https://arxiv.org/abs/2405.01480", "authors": ["Junaid Akhter", "Paul David Fährmann", "Konstantin Sonntag", "Sebastian Peitz", "Daniel Schwietert"], "title": "Common pitfalls to avoid while using multiobjective optimization in machine learning", "categories": ["cs.LG", "math.OC"], "comment": null, "summary": "Recently, there has been an increasing interest in the application of\nmultiobjective optimization (MOO) in machine learning (ML). This interest is\ndriven by the numerous real-life situations where multiple objectives must be\noptimized simultaneously. A key aspect of MOO is the existence of a Pareto set,\nrather than a single optimal solution, which represents the optimal trade-offs\nbetween different objectives. Despite its potential, there is a noticeable lack\nof satisfactory literature serving as an entry-level guide for ML practitioners\naiming to apply MOO effectively. In this paper, our goal is to provide such a\nresource and highlight pitfalls to avoid. We begin by establishing the\ngroundwork for MOO, focusing on well-known approaches such as the weighted sum\n(WS) method, alongside more advanced techniques like the multiobjective\ngradient descent algorithm (MGDA). We critically review existing studies across\nvarious ML fields where MOO has been applied and identify challenges that can\nlead to incorrect interpretations. One of these fields is physics informed\nneural networks (PINNs), which we use as a guiding example to carefully\nconstruct experiments illustrating these pitfalls. By comparing WS and MGDA\nwith one of the most common evolutionary algorithms, NSGA-II, we demonstrate\nthat difficulties can arise regardless of the specific MOO method used. We\nemphasize the importance of understanding the specific problem, the objective\nspace, and the selected MOO method, while also noting that neglecting factors\nsuch as convergence criteria can result in misleading experiments."}
{"id": "2504.10143", "pdf": "https://arxiv.org/pdf/2504.10143", "abs": "https://arxiv.org/abs/2504.10143", "authors": ["Yichao Cai", "Yuhang Liu", "Erdun Gao", "Tianjiao Jiang", "Zhen Zhang", "Anton van den Hengel", "Javen Qinfeng Shi"], "title": "Negate or Embrace: On How Misalignment Shapes Multimodal Representation Learning", "categories": ["cs.LG", "cs.CV"], "comment": null, "summary": "Multimodal representation learning, exemplified by multimodal contrastive\nlearning (MMCL) using image-text pairs, aims to learn powerful representations\nby aligning cues across modalities. This approach relies on the core assumption\nthat the exemplar image-text pairs constitute two representations of an\nidentical concept. However, recent research has revealed that real-world\ndatasets often exhibit misalignment. There are two distinct viewpoints on how\nto address this issue: one suggests mitigating the misalignment, and the other\nleveraging it. We seek here to reconcile these seemingly opposing perspectives,\nand to provide a practical guide for practitioners. Using latent variable\nmodels we thus formalize misalignment by introducing two specific mechanisms:\nselection bias, where some semantic variables are missing, and perturbation\nbias, where semantic variables are distorted -- both affecting latent variables\nshared across modalities. Our theoretical analysis demonstrates that, under\nmild assumptions, the representations learned by MMCL capture exactly the\ninformation related to the subset of the semantic variables invariant to\nselection and perturbation biases. This provides a unified perspective for\nunderstanding misalignment. Based on this, we further offer actionable insights\ninto how misalignment should inform the design of real-world ML systems. We\nvalidate our theoretical findings through extensive empirical studies on both\nsynthetic data and real image-text datasets, shedding light on the nuanced\nimpact of misalignment on multimodal representation learning."}
{"id": "2405.18554", "pdf": "https://arxiv.org/pdf/2405.18554", "abs": "https://arxiv.org/abs/2405.18554", "authors": ["Feiyang Cai", "Chuchu Fan", "Stanley Bak"], "title": "Scalable Surrogate Verification of Image-based Neural Network Control Systems using Composition and Unrolling", "categories": ["cs.LG", "cs.RO", "cs.SY", "eess.SY"], "comment": "Accepted by AAAI-25", "summary": "Verifying safety of neural network control systems that use images as input\nis a difficult problem because, from a given system state, there is no known\nway to mathematically model what images are possible in the real-world. We\nbuild on recent work that considers a surrogate verification approach, training\na conditional generative adversarial network (cGAN) as an image generator in\nplace of the real world. This enables set-based formal analysis of the\nclosed-loop system, providing analysis beyond simulation and testing. While\nexisting work is effective on small examples, excessive overapproximation both\nwithin a single control period and across multiple control periods limits its\nscalability. We propose approaches to overcome these two sources of error.\nFirst, we overcome one-step error by composing the system's dynamics along with\nthe cGAN and neural network controller, without losing the dependencies between\ninput states and the control outputs as in the monotonic analysis of the system\ndynamics. Second, we reduce multi-step error by repeating the single-step\ncomposition, essentially unrolling multiple steps of the control loop into a\nlarge neural network. We then leverage existing network verification tools to\ncompute accurate reachable sets for multiple steps, avoiding the accumulation\nof abstraction error at each step. We demonstrate the effectiveness of our\napproach in terms of both accuracy and scalability using two case studies: an\nautonomous aircraft taxiing system and an advanced emergency braking system. On\nthe aircraft taxiing system, the converged reachable set is 175% larger using\nthe prior baseline method compared with our proposed approach. On the emergency\nbraking system, with 24x the number of image output variables from the cGAN,\nthe baseline method fails to prove any states are safe, whereas our\nimprovements enable set-based safety analysis."}
{"id": "2504.16062", "pdf": "https://arxiv.org/pdf/2504.16062", "abs": "https://arxiv.org/abs/2504.16062", "authors": ["Hardik Shah", "Jiaxu Xing", "Nico Messikommer", "Boyang Sun", "Marc Pollefeys", "Davide Scaramuzza"], "title": "ForesightNav: Learning Scene Imagination for Efficient Exploration", "categories": ["cs.RO", "cs.CV"], "comment": null, "summary": "Understanding how humans leverage prior knowledge to navigate unseen\nenvironments while making exploratory decisions is essential for developing\nautonomous robots with similar abilities. In this work, we propose\nForesightNav, a novel exploration strategy inspired by human imagination and\nreasoning. Our approach equips robotic agents with the capability to predict\ncontextual information, such as occupancy and semantic details, for unexplored\nregions. These predictions enable the robot to efficiently select meaningful\nlong-term navigation goals, significantly enhancing exploration in unseen\nenvironments. We validate our imagination-based approach using the Structured3D\ndataset, demonstrating accurate occupancy prediction and superior performance\nin anticipating unseen scene geometry. Our experiments show that the\nimagination module improves exploration efficiency in unseen environments,\nachieving a 100% completion rate for PointNav and an SPL of 67% for ObjectNav\non the Structured3D Validation split. These contributions demonstrate the power\nof imagination-driven reasoning for autonomous systems to enhance generalizable\nand efficient exploration."}
{"id": "2407.03821", "pdf": "https://arxiv.org/pdf/2407.03821", "abs": "https://arxiv.org/abs/2407.03821", "authors": ["Davide Gabrielli", "Bardh Prenkaj", "Paola Velardi"], "title": "Seamless Monitoring of Stress Levels Leveraging a Universal Model for Time Sequences", "categories": ["cs.LG"], "comment": "38 pages, 10 figures", "summary": "Monitoring the stress level in patients with neurodegenerative diseases can\nhelp manage symptoms, improve patient's quality of life, and provide insight\ninto disease progression. In the literature, ECG, actigraphy, speech, voice,\nand facial analysis have proven effective at detecting patients' emotions. On\nthe other hand, these tools are invasive and do not integrate smoothly into the\npatient's daily life. HRV has also been proven to effectively indicate stress\nconditions, especially in combination with other signals. However, when HRV is\nderived from less invasive devices than the ECG, like wristbands and\nsmartwatches, the quality of measurements significantly degrades. This paper\npresents a methodology for stress detection from a wristband based on a\nuniversal model for time series, UniTS, which we finetuned for the task and\nequipped with explainability features. We cast the problem as anomaly detection\nrather than classification to favor model adaptation to individual patients and\nallow the clinician to maintain greater control over the system's predictions.\nWe demonstrate that our proposed model considerably surpasses 12 top-performing\nmethods on three benchmark datasets. Furthermore, unlike other state-of-the-art\nsystems, UniTS enables seamless monitoring, as it shows comparable performance\nwhen using signals from invasive or lightweight devices."}
{"id": "2407.08010", "pdf": "https://arxiv.org/pdf/2407.08010", "abs": "https://arxiv.org/abs/2407.08010", "authors": ["Fulong Yao", "Wanqing Zhao", "Matthew Forshaw", "Yang Song"], "title": "A Self-organizing Interval Type-2 Fuzzy Neural Network for Multi-Step Time Series Prediction", "categories": ["cs.LG", "cs.NE"], "comment": null, "summary": "Data uncertainty is inherent in many real-world applications and poses\nsignificant challenges for accurate time series predictions. The interval type\n2 fuzzy neural network (IT2FNN) has shown exceptional performance in\nuncertainty modelling for single-step prediction tasks. However, extending it\nfor multi-step ahead predictions introduces further issues in uncertainty\nhandling as well as model interpretability and accuracy. To address these\nissues, this paper proposes a new selforganizing interval type-2 fuzzy neural\nnetwork with multiple outputs (SOIT2FNN-MO). Differing from the traditional\nsix-layer IT2FNN, a nine-layer network architecture is developed. First, a new\nco-antecedent layer and a modified consequent layer are devised to improve the\ninterpretability of the fuzzy model for multi-step time series prediction\nproblems. Second, a new link layer is created to improve the accuracy by\nbuilding temporal connections between multi-step predictions. Third, a new\ntransformation layer is designed to address the problem of the vanishing rule\nstrength caused by high-dimensional inputs. Furthermore, a two-stage,\nself-organizing learning mechanism is developed to automatically extract fuzzy\nrules from data and optimize network parameters. Experimental results on\nchaotic and microgrid prediction problems demonstrate that SOIT2FNN-MO\noutperforms state-of-the-art methods, by achieving a better accuracy ranging\nfrom 1.6% to 30% depending on the level of noises in data. Additionally, the\nproposed model is more interpretable, offering deeper insights into the\nprediction process."}
{"id": "2407.20152", "pdf": "https://arxiv.org/pdf/2407.20152", "abs": "https://arxiv.org/abs/2407.20152", "authors": ["Rahul Ghosh", "Arvind Renganathan", "Zac McEachran", "Kelly Lindsay", "Somya Sharma", "Michael Steinbach", "John Nieber", "Christopher Duffy", "Vipin Kumar"], "title": "Hierarchically Disentangled Recurrent Network for Factorizing System Dynamics of Multi-scale Systems: An application on Hydrological Systems", "categories": ["cs.LG"], "comment": null, "summary": "We present a framework for modeling multi-scale processes, and study its\nperformance in the context of streamflow forecasting in hydrology.\nSpecifically, we propose a novel hierarchical recurrent neural architecture\nthat factorizes the system dynamics at multiple temporal scales and captures\ntheir interactions. This framework consists of an inverse and a forward model.\nThe inverse model is used to empirically resolve the system's temporal modes\nfrom data (physical model simulations, observed data, or a combination of them\nfrom the past), and these states are then used in the forward model to predict\nstreamflow. Experiments on several catchments from the National Weather Service\nNorth Central River Forecast Center show that FHNN outperforms standard\nbaselines, including physics-based models and transformer-based approaches. The\nmodel demonstrates particular effectiveness in catchments with low runoff\nratios and colder climates. We further validate FHNN on the CAMELS (Catchment\nAttributes and MEteorology for Large-sample Studies), which is a widely used\ncontinental-scale hydrology benchmark dataset, confirming consistent\nperformance improvements for 1-7 day streamflow forecasts across diverse\nhydrological conditions. Additionally, we show that FHNN can maintain accuracy\neven with limited training data through effective pre-training strategies and\ntraining global models."}
{"id": "2410.03883", "pdf": "https://arxiv.org/pdf/2410.03883", "abs": "https://arxiv.org/abs/2410.03883", "authors": ["Xinwei Zhang", "Zhiqi Bu", "Borja Balle", "Mingyi Hong", "Meisam Razaviyayn", "Vahab Mirrokni"], "title": "DiSK: Differentially Private Optimizer with Simplified Kalman Filter for Noise Reduction", "categories": ["cs.LG", "cs.CR", "stat.ML"], "comment": null, "summary": "Differential privacy (DP) offers a robust framework for safeguarding\nindividual data privacy. To utilize DP in training modern machine learning\nmodels, differentially private optimizers have been widely used in recent\nyears. A popular approach to privatize an optimizer is to clip the individual\ngradients and add sufficiently large noise to the clipped gradient. This\napproach led to the development of DP optimizers that have comparable\nperformance with their non-private counterparts in fine-tuning tasks or in\ntasks with a small number of training parameters. However, a significant\nperformance drop is observed when these optimizers are applied to large-scale\ntraining. This degradation stems from the substantial noise injection required\nto maintain DP, which disrupts the optimizer's dynamics. This paper introduces\nDiSK, a novel framework designed to significantly enhance the performance of DP\noptimizers. DiSK employs Kalman filtering, a technique drawn from control and\nsignal processing, to effectively denoise privatized gradients and generate\nprogressively refined gradient estimations. To ensure practicality for\nlarge-scale training, we simplify the Kalman filtering process, minimizing its\nmemory and computational demands. We establish theoretical privacy-utility\ntrade-off guarantees for DiSK, and demonstrate provable improvements over\nstandard DP optimizers like DPSGD in terms of iteration complexity upper-bound.\nExtensive experiments across diverse tasks, including vision tasks such as\nCIFAR-100 and ImageNet-1k and language fine-tuning tasks such as GLUE, E2E, and\nDART, validate the effectiveness of DiSK. The results showcase its ability to\nsignificantly improve the performance of DP optimizers, surpassing\nstate-of-the-art results under the same privacy constraints on several\nbenchmarks."}
{"id": "2410.05452", "pdf": "https://arxiv.org/pdf/2410.05452", "abs": "https://arxiv.org/abs/2410.05452", "authors": ["Barak Gahtan", "Shany Funk", "Einat Kodesh", "Itay Ketko", "Tsvi Kuflik", "Alex M. Bronstein"], "title": "WearableMil: An End-to-End Framework for Military Activity Recognition and Performance Monitoring", "categories": ["cs.LG", "cs.HC"], "comment": null, "summary": "Musculoskeletal injuries during military training significantly impact\nreadiness, making prevention through activity monitoring crucial. While Human\nActivity Recognition (HAR) using wearable devices offers promising solutions,\nit faces challenges in processing continuous data streams and recognizing\ndiverse activities without predefined sessions. This paper introduces an\nend-to-end framework for preprocessing, analyzing, and recognizing activities\nfrom wearable data in military training contexts. Using data from 135 soldiers\nwearing \\textit{Garmin--55} smartwatches over six months with over 15 million\nminutes. We develop a hierarchical deep learning approach that achieves 93.8%\naccuracy in temporal splits and 83.8% in cross-user evaluation. Our framework\naddresses missing data through physiologically-informed methods, reducing\nunknown sleep states from 40.38% to 3.66%. We demonstrate that while longer\ntime windows (45-60 minutes) improve basic state classification, they present\ntrade-offs in detecting fine-grained activities. Additionally, we introduce an\nintuitive visualization system that enables real-time comparison of individual\nperformance against group metrics across multiple physiological indicators.\nThis approach to activity recognition and performance monitoring provides\nmilitary trainers with actionable insights for optimizing training programs and\npreventing injuries."}
{"id": "2410.10572", "pdf": "https://arxiv.org/pdf/2410.10572", "abs": "https://arxiv.org/abs/2410.10572", "authors": ["Avrim Blum", "Donya Saless"], "title": "Regularized Robustly Reliable Learners and Instance Targeted Attacks", "categories": ["cs.LG", "cs.CR", "cs.DS", "stat.ML"], "comment": null, "summary": "Instance-targeted data poisoning attacks, where an adversary corrupts a\ntraining set to induce errors on specific test points, have raised significant\nconcerns. Balcan et al (2022) proposed an approach to addressing this challenge\nby defining a notion of robustly-reliable learners that provide per-instance\nguarantees of correctness under well-defined assumptions, even in the presence\nof data poisoning attacks. They then give a generic optimal (but\ncomputationally inefficient) robustly reliable learner as well as a\ncomputationally efficient algorithm for the case of linear separators over\nlog-concave distributions.\n  In this work, we address two challenges left open by Balcan et al (2022). The\nfirst is that the definition of robustly-reliable learners in Balcan et al\n(2022) becomes vacuous for highly-flexible hypothesis classes: if there are two\nclassifiers h_0, h_1 \\in H both with zero error on the training set such that\nh_0(x) \\neq h_1(x), then a robustly-reliable learner must abstain on x. We\naddress this problem by defining a modified notion of regularized\nrobustly-reliable learners that allows for nontrivial statements in this case.\nThe second is that the generic algorithm of Balcan et al (2022) requires\nre-running an ERM oracle (essentially, retraining the classifier) on each test\npoint x, which is generally impractical even if ERM can be implemented\nefficiently. To tackle this problem, we show that at least in certain\ninteresting cases we can design algorithms that can produce their outputs in\ntime sublinear in training time, by using techniques from dynamic algorithm\ndesign."}
{"id": "2411.07729", "pdf": "https://arxiv.org/pdf/2411.07729", "abs": "https://arxiv.org/abs/2411.07729", "authors": ["Sungyoon Kim", "Aaron Mishkin", "Mert Pilanci"], "title": "Exploring the loss landscape of regularized neural networks via convex duality", "categories": ["cs.LG"], "comment": "Updated accepted version and authorship", "summary": "We discuss several aspects of the loss landscape of regularized neural\nnetworks: the structure of stationary points, connectivity of optimal\nsolutions, path with nonincreasing loss to arbitrary global optimum, and the\nnonuniqueness of optimal solutions, by casting the problem into an equivalent\nconvex problem and considering its dual. Starting from two-layer neural\nnetworks with scalar output, we first characterize the solution set of the\nconvex problem using its dual and further characterize all stationary points.\nWith the characterization, we show that the topology of the global optima goes\nthrough a phase transition as the width of the network changes, and construct\ncounterexamples where the problem may have a continuum of optimal solutions.\nFinally, we show that the solution set characterization and connectivity\nresults can be extended to different architectures, including two-layer\nvector-valued neural networks and parallel three-layer neural networks."}
{"id": "2411.12636", "pdf": "https://arxiv.org/pdf/2411.12636", "abs": "https://arxiv.org/abs/2411.12636", "authors": ["Pascal Tribel", "Gianluca Bontempi"], "title": "PyAWD: A Library for Generating Large Synthetic Datasets of Acoustic Wave Propagation", "categories": ["cs.LG"], "comment": null, "summary": "Seismic data is often sparse and unevenly distributed due to the high costs\nand logistical challenges associated with deploying physical seismometers,\nlimiting the application of Machine Learning (ML) in earthquake analysis. While\nsimulation methods exist, no tool allows the generation of large datasets\ncontaining simulated measurements of the ground motion. To address this gap, we\nintroduce PyAWD, a Python library designed to generate high-resolution\nsynthetic datasets simulating spatio-temporal acoustic wave propagation in both\ntwo-dimensional and three-dimensional heterogeneous media. By allowing fine\ncontrol over parameters such as the wave speed, external forces, spatial and\ntemporal discretization, and media composition, PyAWD enables the creation of\nML-scale datasets that capture the complexity of seismic wave behavior. We\nillustrate the library's potential with an epicenter retrieval task, showcasing\nits suitability for designing complex, accurate seismic problems that require\nadvanced ML approaches in the absence or lack of dense real-world data. We also\nshow the usefulness of our tool to tackle the problem of data budgeting in the\nframework of epicenter retrieval."}
{"id": "2412.07062", "pdf": "https://arxiv.org/pdf/2412.07062", "abs": "https://arxiv.org/abs/2412.07062", "authors": ["Weihang Chen", "Cheng Yang", "Jie Ren", "Zhiqiang Li", "Zheng Wang"], "title": "Optimizing Personalized Federated Learning through Adaptive Layer-Wise Learning", "categories": ["cs.LG"], "comment": null, "summary": "Real-life deployment of federated Learning (FL) often faces non-IID data,\nwhich leads to poor accuracy and slow convergence. Personalized FL (pFL)\ntackles these issues by tailoring local models to individual data sources and\nusing weighted aggregation methods for client-specific learning. However,\nexisting pFL methods often fail to provide each local model with global\nknowledge on demand while maintaining low computational overhead. Additionally,\nlocal models tend to over-personalize their data during the training process,\npotentially dropping previously acquired global information. We propose FLAYER,\na novel layer-wise learning method for pFL that optimizes local model\npersonalization performance. FLAYER considers the different roles and learning\nabilities of neural network layers of individual local models. It incorporates\nglobal information for each local model as needed to initialize the local model\ncost-effectively. It then dynamically adjusts learning rates for each layer\nduring local training, optimizing the personalized learning process for each\nlocal model while preserving global knowledge. Additionally, to enhance global\nrepresentation in pFL, FLAYER selectively uploads parameters for global\naggregation in a layer-wise manner. We evaluate FLAYER on four representative\ndatasets in computer vision and natural language processing domains. Compared\nto six state-of-the-art pFL methods, FLAYER improves the inference accuracy, on\naverage, by 5.40\\% (up to 14.29\\%)."}
{"id": "2412.09009", "pdf": "https://arxiv.org/pdf/2412.09009", "abs": "https://arxiv.org/abs/2412.09009", "authors": ["Sumanth Kumar Boya", "Deepak Subramani"], "title": "A physics-informed transformer neural operator for learning generalized solutions of initial boundary value problems", "categories": ["cs.LG", "physics.comp-ph", "35C05"], "comment": "30 pages, 14 figures, 9 tables", "summary": "Initial boundary value problems arise commonly in applications with\nengineering and natural systems governed by nonlinear partial differential\nequations (PDEs). Operator learning is an emerging field for solving these\nequations by using a neural network to learn a map between infinite dimensional\ninput and output function spaces. These neural operators are trained using a\ncombination of data (observations or simulations) and PDE-residuals\n(physics-loss). A major drawback of existing neural approaches is the\nrequirement to retrain with new initial/boundary conditions, and the necessity\nfor a large amount of simulation data for training. We develop a\nphysics-informed transformer neural operator (named PINTO) that efficiently\ngeneralizes to unseen initial and boundary conditions, trained in a\nsimulation-free setting using only physics loss. The main innovation lies in\nour new iterative kernel integral operator units, implemented using\ncross-attention, to transform the PDE solution's domain points into an\ninitial/boundary condition-aware representation vector, enabling efficient\nlearning of the solution function for new scenarios. The PINTO architecture is\napplied to simulate the solutions of important equations used in engineering\napplications: advection, Burgers, and steady and unsteady Navier-Stokes\nequations (three flow scenarios). For these five test cases, we show that the\nrelative errors during testing under challenging conditions of unseen\ninitial/boundary conditions are only one-fifth to one-third of other leading\nphysics informed operator learning methods. Moreover, our PINTO model is able\nto accurately solve the advection and Burgers equations at time steps that are\nnot included in the training collocation points. The code is available at\nhttps://github.com/quest-lab-iisc/PINTO"}
{"id": "2501.09298", "pdf": "https://arxiv.org/pdf/2501.09298", "abs": "https://arxiv.org/abs/2501.09298", "authors": ["Ying Qian", "Kui Zhang", "Éric Marty", "Avranil Basu", "Eamon B. O'Dea", "Xianqiao Wang", "Spencer Fox", "Pejman Rohani", "John M. Drake", "He Li"], "title": "Physics-informed deep learning for infectious disease forecasting", "categories": ["cs.LG", "q-bio.QM"], "comment": null, "summary": "Accurate forecasting of contagious diseases is critical for public health\npolicymaking and pandemic preparedness. We propose a new infectious disease\nforecasting model based on physics-informed neural networks (PINNs), an\nemerging scientific machine learning approach. By embedding a compartmental\nmodel into the loss function, our method integrates epidemiological theory with\ndata, helping to prevent model overfitting. We further enhance the model with a\nsub-network that accounts for covariates such as mobility and cumulative\nvaccine doses, which influence the transmission rate. Using state-level\nCOVID-19 data from California, we demonstrate that the PINN model accurately\npredicts cases, deaths, and hospitalizations, aligning well with existing\nbenchmarks. Notably, the PINN model outperforms naive baseline forecasts and\nseveral sequence deep learning models, including Recurrent Neural Networks\n(RNNs), Long Short-Term Memory (LSTM) networks, Gated Recurrent Units (GRUs),\nand Transformers. It also achieves performance comparable to a sophisticated\nGaussian infection state forecasting model that combines compartmental\ndynamics, a data observation model, and parameter regression. However, the PINN\nmodel features a simpler structure and is easier to implement. In summary, we\nsystematically evaluate the PINN model's ability to forecast infectious disease\ndynamics, demonstrating its potential as an efficient computational tool to\nstrengthen forecasting capabilities."}
{"id": "2501.10221", "pdf": "https://arxiv.org/pdf/2501.10221", "abs": "https://arxiv.org/abs/2501.10221", "authors": ["Fred Shone", "Tim Hillel"], "title": "Synthesising Activity Participations and Scheduling with Deep Generative Machine Learning", "categories": ["cs.LG"], "comment": null, "summary": "Using a deep generative machine learning approach, we synthesise human\nactivity participations and scheduling (the choices of what activities to\nparticipate in and when). Activity schedules, which represent what people do\nand when, are a core component of many applied transport, energy, and\nepidemiology models. Our data-driven approach learns the distributions\nresulting from human preferences and scheduling logic without the need for\ncomplex interacting combinations of sub-models and custom rules, This makes our\napproach significantly faster and simpler to operate than existing approaches\nto synthesise or anonymise schedule data. We additionally contribute a novel\nschedule representation and a comprehensive evaluation framework. We evaluate a\nrange of schedule encoding and deep model architecture combinations. The\nevaluation shows our approach can rapidly generate large, diverse, novel, and\nrealistic synthetic samples of activity schedules."}
{"id": "2502.00607", "pdf": "https://arxiv.org/pdf/2502.00607", "abs": "https://arxiv.org/abs/2502.00607", "authors": ["Shaddin Dughmi"], "title": "PAC Learning is just Bipartite Matching (Sort of)", "categories": ["cs.LG", "cs.DS", "stat.ML", "F.0"], "comment": "Position paper", "summary": "The main goal of this article is to convince you, the reader, that supervised\nlearning in the Probably Approximately Correct (PAC) model is closely related\nto -- of all things -- bipartite matching! En-route from PAC learning to\nbipartite matching, I will overview a particular transductive model of\nlearning, and associated one-inclusion graphs, which can be viewed as a\ngeneralization of some of the hat puzzles that are popular in recreational\nmathematics. Whereas this transductive model is far from new, it has recently\nseen a resurgence of interest as a tool for tackling deep questions in learning\ntheory. A secondary purpose of this article could be as a (biased) tutorial on\nthe connections between the PAC and transductive models of learning."}
{"id": "2502.01070", "pdf": "https://arxiv.org/pdf/2502.01070", "abs": "https://arxiv.org/abs/2502.01070", "authors": ["Jiwoo Kim", "Joonhyung Lee", "Gunho Park", "Byeongwook Kim", "Se Jung Kwon", "Dongsoo Lee", "Youngjoo Lee"], "title": "An Inquiry into Datacenter TCO for LLM Inference with FP8", "categories": ["cs.LG", "cs.PF"], "comment": null, "summary": "As large language models (LLMs) continue to scale, their inference demands\npresent significant challenges, particularly due to the high power consumption\nof AI accelerators in datacenters. These facilities require specialized cooling\nand power management systems, substantially increasing the total cost of\nownership (TCO) for cloud service providers (CSPs). In this work, we analyze\nthe computational characteristics and constraints of LLM inference from a TCO\nperspective, focusing on two representative accelerators: the Gaudi 2 and\nNVIDIA H100. We present a generalizable framework that enables CSPs to compare\nand select AI accelerators according to diverse operational requirements. Using\nthis model, we analyze the impact of FP8 precision and LLM inference workload\ncharacteristics as key factors influencing TCO. We investigate FP8\nquantization, which is gaining adoption in LLM training, as a technique to\nimprove inference throughput while maintaining cost efficiency. Furthermore,\nour analysis of LLM inference workloads reveals that performance on thin GEMMs,\nwhich dominate the decode phase, can have a greater impact than theoretical\nhardware peak performance. By studying the interaction between power\nconsumption, quantization strategies, and hardware architecture, we offer\ninsights that support informed deployment decisions and guide future\naccelerator designs to improve the TCO of LLM inference."}
{"id": "2502.07319", "pdf": "https://arxiv.org/pdf/2502.07319", "abs": "https://arxiv.org/abs/2502.07319", "authors": ["Mingkai Xu", "Yongpeng Wu", "Yuxuan Shi", "Xiang-Gen Xia", "Wenjun Zhang", "Ping Zhang"], "title": "Learnable Residual-based Latent Denoising in Semantic Communication", "categories": ["cs.LG", "cs.IT", "math.IT"], "comment": "This paper has been accepted by IEEE Wireless Communications Letters", "summary": "A latent denoising semantic communication (SemCom) framework is proposed for\nrobust image transmission over noisy channels. By incorporating a learnable\nlatent denoiser into the receiver, the received signals are preprocessed to\neffectively remove the channel noise and recover the semantic information,\nthereby enhancing the quality of the decoded images. Specifically, a latent\ndenoising mapping is established by an iterative residual learning approach to\nimprove the denoising efficiency while ensuring stable performance. Moreover,\nchannel signal-to-noise ratio (SNR) is utilized to estimate and predict the\nlatent similarity score (SS) for conditional denoising, where the number of\ndenoising steps is adapted based on the predicted SS sequence, further reducing\nthe communication latency. Finally, simulations demonstrate that the proposed\nframework can effectively and efficiently remove the channel noise at various\nlevels and reconstruct visual-appealing images."}
{"id": "2502.07608", "pdf": "https://arxiv.org/pdf/2502.07608", "abs": "https://arxiv.org/abs/2502.07608", "authors": ["Arvind Pillai", "Dimitris Spathis", "Subigya Nepal", "Amanda C Collins", "Daniel M Mackin", "Michael V Heinz", "Tess Z Griffin", "Nicholas C Jacobson", "Andrew Campbell"], "title": "Time2Lang: Bridging Time-Series Foundation Models and Large Language Models for Health Sensing Beyond Prompting", "categories": ["cs.LG", "cs.HC"], "comment": "Accepted to CHIL 2025. Code and models:\n  https://github.com/arvind1609/time2lang", "summary": "Large language models (LLMs) show promise for health applications when\ncombined with behavioral sensing data. Traditional approaches convert sensor\ndata into text prompts, but this process is prone to errors, computationally\nexpensive, and requires domain expertise. These challenges are particularly\nacute when processing extended time series data. While time series foundation\nmodels (TFMs) have recently emerged as powerful tools for learning\nrepresentations from temporal data, bridging TFMs and LLMs remains challenging.\nHere, we present Time2Lang, a framework that directly maps TFM outputs to LLM\nrepresentations without intermediate text conversion. Our approach first trains\non synthetic data using periodicity prediction as a pretext task, followed by\nevaluation on mental health classification tasks. We validate Time2Lang on two\nlongitudinal wearable and mobile sensing datasets: daily depression prediction\nusing step count data (17,251 days from 256 participants) and flourishing\nclassification based on conversation duration (46 participants over 10 weeks).\nTime2Lang maintains near constant inference times regardless of input length,\nunlike traditional prompting methods. The generated embeddings preserve\nessential time-series characteristics such as auto-correlation. Our results\ndemonstrate that TFMs and LLMs can be effectively integrated while minimizing\ninformation loss and enabling performance transfer across these distinct\nmodeling paradigms. To our knowledge, we are the first to integrate a TFM and\nan LLM for health, thus establishing a foundation for future research combining\ngeneral-purpose large models for complex healthcare tasks."}
{"id": "2503.04088", "pdf": "https://arxiv.org/pdf/2503.04088", "abs": "https://arxiv.org/abs/2503.04088", "authors": ["Yuqing Wang", "Xiao Yang"], "title": "Cloud Computing Energy Consumption Prediction Based on Kernel Extreme Learning Machine Algorithm Improved by Vector Weighted Average Algorithm", "categories": ["cs.LG", "cs.PF"], "comment": null, "summary": "With the rapid expansion of cloud computing infrastructure, energy\nconsumption has become a critical challenge, driving the need for accurate and\nefficient prediction models. This study proposes a novel Vector Weighted\nAverage Kernel Extreme Learning Machine (VWAA-KELM) model to enhance energy\nconsumption prediction in cloud computing environments. By integrating a vector\nweighted average algorithm (VWAA) with kernel extreme learning machine (KELM),\nthe proposed model dynamically adjusts feature weights and optimizes kernel\nfunctions, significantly improving prediction accuracy and generalization.\nExperimental results demonstrate the superior performance of VWAA-KELM: 94.7%\nof test set prediction errors fall within [0, 50] units, with only three cases\nexceeding 100 units, indicating strong stability. The model achieves a\ncoefficient of determination (R2) of 0.987 in the training set (RMSE = 28.108,\nRPD = 8.872) and maintains excellent generalization with R2 = 0.973 in the test\nset (RMSE = 43.227, RPD = 6.202). Visual analysis confirms that predicted\nvalues closely align with actual energy consumption trends, avoiding\noverfitting while capturing nonlinear dependencies. A key innovation of this\nstudy is the introduction of adaptive feature weighting, allowing the model to\ndynamically assign importance to different input parameters, thereby enhancing\nhigh-dimensional data processing. This advancement provides a scalable and\nefficient approach for optimizing cloud data center energy consumption. Beyond\ncloud computing, the proposed hybrid framework has broader applications in\nInternet of Things (IoT) and edge computing, supporting real-time energy\nmanagement and intelligent resource allocation."}
{"id": "2503.21971", "pdf": "https://arxiv.org/pdf/2503.21971", "abs": "https://arxiv.org/abs/2503.21971", "authors": ["Armin Abdollahi", "Mehdi Kamal", "Massoud Pedram"], "title": "RocketPPA: Ultra-Fast LLM-Based PPA Estimator at Code-Level Abstraction", "categories": ["cs.LG", "cs.SE"], "comment": null, "summary": "Large language models have recently transformed hardware design, yet bridging\nthe gap between code synthesis and PPA (power, performance, and area)\nestimation remains a challenge. In this work, we introduce a novel framework\nthat leverages a 21k dataset of thoroughly cleaned and synthesizable Verilog\nmodules, each annotated with detailed power, delay, and area metrics. By\nemploying chain-of-thought techniques, we automatically debug and curate this\ndataset to ensure high fidelity in downstream applications. We then fine-tune\nCodeLlama using LoRA-based parameter-efficient methods, framing the task as a\nregression problem to accurately predict PPA metrics from Verilog code.\nFurthermore, we augment our approach with a mixture-of-experts\narchitecture-integrating both LoRA and an additional MLP expert layer-to\nfurther refine predictions. Experimental results demonstrate significant\nimprovements: power estimation accuracy is enhanced by 5.9% at a 20% error\nthreshold and by 7.2% at a 10% threshold, delay estimation improves by 5.1% and\n3.9%, and area estimation sees gains of 4% and 7.9% for the 20% and 10%\nthresholds, respectively. Notably, the incorporation of the mixture-of-experts\nmodule contributes an additional 3--4% improvement across these tasks. Our\nresults establish a new benchmark for PPA-aware Verilog generation,\nhighlighting the effectiveness of our integrated dataset and modeling\nstrategies for next-generation EDA workflows."}
{"id": "2503.22480", "pdf": "https://arxiv.org/pdf/2503.22480", "abs": "https://arxiv.org/abs/2503.22480", "authors": ["Wangtao Sun", "Xiang Cheng", "Xing Yu", "Haotian Xu", "Zhao Yang", "Shizhu He", "Jun Zhao", "Kang Liu"], "title": "Probabilistic Uncertain Reward Model: A Natural Generalization of Bradley-Terry Reward Model", "categories": ["cs.LG"], "comment": null, "summary": "Reinforcement Learning from Human Feedback (RLHF) has emerged as a critical\ntechnique for training large language models. However, reward hacking-a\nphenomenon where models exploit flaws in the reward model-remains a significant\nbarrier to achieving robust and scalable intelligence through long-term\ntraining. Existing studies have proposed the uncertain reward models to address\nreward hacking, however, they often lack systematic or theoretical foundations,\nfailing to model the uncertainty intrinsically emerging from preference data,\nand thus cannot sufficiently mitigate reward hacking to sustain prolonged RLHF\ntraining and exploration. In this paper, we propose a Probabilistic Uncertain\nReward Model (PURM), a natural generalization of the classical Bradley-Terry\nreward model, that can directly learn the reward distribution emerged from the\npreference data. We theoretically derived PURM's loss function and the reward\ndistribution uncertainty calculation based on Bhattacharyya Coefficient. To\nmitigate reward hacking with PURM, we further introduce an uncertainty-aware\npenalty into Proximal Policy Optimization (PPO), which leverages the learned\nuncertainty to dynamically balance reward optimization and exploration. We\npropose a lightweight and easy-to-use implementation of PURM. Experiments\ndemonstrate that PURM effectively models the rewards and uncertainties, and\nsignificantly delays the onset of reward hacking while improving final reward\nperformance compared with existing methods."}
{"id": "2503.23236", "pdf": "https://arxiv.org/pdf/2503.23236", "abs": "https://arxiv.org/abs/2503.23236", "authors": ["Ismaël Zighed", "Nicolas Thome", "Patrick Gallinari", "Taraneh Sayadi"], "title": "UP-dROM : Uncertainty-Aware and Parametrised dynamic Reduced-Order Model, application to unsteady flows", "categories": ["cs.LG", "physics.flu-dyn"], "comment": null, "summary": "Reduced order models (ROMs) play a critical role in fluid mechanics by\nproviding low-cost predictions, making them an attractive tool for engineering\napplications. However, for ROMs to be widely applicable, they must not only\ngeneralise well across different regimes, but also provide a measure of\nconfidence in their predictions. While recent data-driven approaches have begun\nto address nonlinear reduction techniques to improve predictions in transient\nenvironments, challenges remain in terms of robustness and parametrisation. In\nthis work, we present a nonlinear reduction strategy specifically designed for\ntransient flows that incorporates parametrisation and uncertainty\nquantification. Our reduction strategy features a variational auto-encoder\n(VAE) that uses variational inference for confidence measurement. We use a\nlatent space transformer that incorporates recent advances in attention\nmechanisms to predict dynamical systems. Attention's versatility in learning\nsequences and capturing their dependence on external parameters enhances\ngeneralisation across a wide range of dynamics. Prediction, coupled with\nconfidence, enables more informed decision making and addresses the need for\nmore robust models. In addition, this confidence is used to cost-effectively\nsample the parameter space, improving model performance a priori across the\nentire parameter space without requiring evaluation data for the entire domain."}
{"id": "2504.14268", "pdf": "https://arxiv.org/pdf/2504.14268", "abs": "https://arxiv.org/abs/2504.14268", "authors": ["Xinye Chen"], "title": "Mixed-Precision Conjugate Gradient Solvers with RL-Driven Precision Tuning", "categories": ["cs.LG"], "comment": null, "summary": "This paper presents a novel reinforcement learning (RL) framework for\ndynamically optimizing numerical precision in the preconditioned conjugate\ngradient (CG) method. By modeling precision selection as a Markov Decision\nProcess (MDP), we employ Q-learning to adaptively assign precision levels to\nkey operations, striking an optimal balance between computational efficiency\nand numerical accuracy, while ensuring stability through double-precision\nscalar computations and residual computing. In practice, the algorithm is\ntrained on a set of data and subsequently performs inference for precision\nselection on out-of-sample data, without requiring re-analysis or retraining\nfor new datasets. This enables the method to adapt seamlessly to new problem\ninstances without the computational overhead of recalibration. Our results\ndemonstrate the effectiveness of RL in enhancing solver's performance, marking\nthe first application of RL to mixed-precision numerical methods. The findings\nhighlight the approach's practical advantages, robustness, and scalability,\nproviding valuable insights into its integration with iterative solvers and\npaving the way for AI-driven advancements in scientific computing."}
{"id": "2504.17857", "pdf": "https://arxiv.org/pdf/2504.17857", "abs": "https://arxiv.org/abs/2504.17857", "authors": ["AJ Miller", "Fangzhou Yu", "Michael Brauckmann", "Farbod Farshidian"], "title": "High-Performance Reinforcement Learning on Spot: Optimizing Simulation Parameters with Distributional Measures", "categories": ["cs.LG", "cs.RO"], "comment": null, "summary": "This work presents an overview of the technical details behind a high\nperformance reinforcement learning policy deployment with the Spot RL\nResearcher Development Kit for low level motor access on Boston Dynamics Spot.\nThis represents the first public demonstration of an end to end end\nreinforcement learning policy deployed on Spot hardware with training code\npublicly available through Nvidia IsaacLab and deployment code available\nthrough Boston Dynamics. We utilize Wasserstein Distance and Maximum Mean\nDiscrepancy to quantify the distributional dissimilarity of data collected on\nhardware and in simulation to measure our sim2real gap. We use these measures\nas a scoring function for the Covariance Matrix Adaptation Evolution Strategy\nto optimize simulated parameters that are unknown or difficult to measure from\nSpot. Our procedure for modeling and training produces high quality\nreinforcement learning policies capable of multiple gaits, including a flight\nphase. We deploy policies capable of over 5.2ms locomotion, more than triple\nSpots default controller maximum speed, robustness to slippery surfaces,\ndisturbance rejection, and overall agility previously unseen on Spot. We detail\nour method and release our code to support future work on Spot with the low\nlevel API."}
{"id": "2504.18583", "pdf": "https://arxiv.org/pdf/2504.18583", "abs": "https://arxiv.org/abs/2504.18583", "authors": ["Zihao An", "Huajun Bai", "Ziqiong Liu", "Dong Li", "Emad Barsoum"], "title": "PARD: Accelerating LLM Inference with Low-Cost PARallel Draft Model Adaptation", "categories": ["cs.LG", "cs.PF"], "comment": "15 pages, 6 figures", "summary": "The autoregressive nature of large language models (LLMs) limits inference\nspeed. Each forward pass generates only a single token and is often\nbottlenecked by memory bandwidth. Speculative decoding alleviates this issue\nusing a draft-then-verify approach to accelerate token generation. However, the\noverhead introduced during the draft phase and the training cost of the draft\nmodel limit the efficiency and adaptability of speculative decoding. In this\nwork, we introduce PARallel Draft (PARD), a novel speculative decoding method\nthat enables low-cost adaptation of autoregressive draft models into parallel\ndraft models. PARD enhances inference efficiency by predicting multiple future\ntokens in a single forward pass of the draft phase, and incorporates a\nconditional drop token method to accelerate training. Its target-independence\nproperty allows a single draft model to be applied to an entire family of\ndifferent models, minimizing the adaptation cost. Our proposed conditional drop\ntoken method can improves draft model training efficiency by 3x. On our\noptimized inference framework, PARD accelerates LLaMA3.1-8B inference by 4.08x,\nachieving 311.5 tokens per second."}
{"id": "2504.19452", "pdf": "https://arxiv.org/pdf/2504.19452", "abs": "https://arxiv.org/abs/2504.19452", "authors": ["Qibang Liu", "Vincient Zhong", "Hadi Meidani", "Diab Abueidda", "Seid Koric", "Philippe Geubelle"], "title": "Geometry-Informed Neural Operator Transformer", "categories": ["cs.LG", "physics.comp-ph"], "comment": null, "summary": "Machine-learning-based surrogate models offer significant computational\nefficiency and faster simulations compared to traditional numerical methods,\nespecially for problems requiring repeated evaluations of partial differential\nequations. This work introduces the Geometry-Informed Neural Operator\nTransformer (GINOT), which integrates the transformer architecture with the\nneural operator framework to enable forward predictions for arbitrary\ngeometries. GINOT encodes the surface points cloud of a geometry using a\nsampling and grouping mechanism combined with an attention mechanism, ensuring\ninvariance to point order and padding while maintaining robustness to\nvariations in point density. The geometry information is seamlessly integrated\nwith query points in the solution decoder through the attention mechanism. The\nperformance of GINOT is validated on multiple challenging datasets, showcasing\nits high accuracy and strong generalization capabilities for complex and\narbitrary 2D and 3D geometries."}
{"id": "2504.19774", "pdf": "https://arxiv.org/pdf/2504.19774", "abs": "https://arxiv.org/abs/2504.19774", "authors": ["Nicola Debole", "Pietro Barbiero", "Francesco Giannini", "Andrea Passerini", "Stefano Teso", "Emanuele Marconato"], "title": "If Concept Bottlenecks are the Question, are Foundation Models the Answer?", "categories": ["cs.LG"], "comment": null, "summary": "Concept Bottleneck Models (CBMs) are neural networks designed to conjoin high\nperformance with ante-hoc interpretability. CBMs work by first mapping inputs\n(e.g., images) to high-level concepts (e.g., visible objects and their\nproperties) and then use these to solve a downstream task (e.g., tagging or\nscoring an image) in an interpretable manner. Their performance and\ninterpretability, however, hinge on the quality of the concepts they learn. The\ngo-to strategy for ensuring good quality concepts is to leverage expert\nannotations, which are expensive to collect and seldom available in\napplications. Researchers have recently addressed this issue by introducing\n\"VLM-CBM\" architectures that replace manual annotations with weak supervision\nfrom foundation models. It is however unclear what is the impact of doing so on\nthe quality of the learned concepts. To answer this question, we put\nstate-of-the-art VLM-CBMs to the test, analyzing their learned concepts\nempirically using a selection of significant metrics. Our results show that,\ndepending on the task, VLM supervision can sensibly differ from expert\nannotations, and that concept accuracy and quality are not strongly correlated.\nOur code is available at https://github.com/debryu/CQA."}
{"id": "2504.19979", "pdf": "https://arxiv.org/pdf/2504.19979", "abs": "https://arxiv.org/abs/2504.19979", "authors": ["Liyuan Wang", "Jiachen Chen", "Kathryn L. Lunetta", "Danyang Huang", "Huimin Cheng", "Debarghya Mukherjee"], "title": "Transfer Learning Under High-Dimensional Network Convolutional Regression Model", "categories": ["cs.LG", "stat.ME"], "comment": null, "summary": "Transfer learning enhances model performance by utilizing knowledge from\nrelated domains, particularly when labeled data is scarce. While existing\nresearch addresses transfer learning under various distribution shifts in\nindependent settings, handling dependencies in networked data remains\nchallenging. To address this challenge, we propose a high-dimensional transfer\nlearning framework based on network convolutional regression (NCR), inspired by\nthe success of graph convolutional networks (GCNs). The NCR model incorporates\nrandom network structure by allowing each node's response to depend on its\nfeatures and the aggregated features of its neighbors, capturing local\ndependencies effectively. Our methodology includes a two-step transfer learning\nalgorithm that addresses domain shift between source and target networks, along\nwith a source detection mechanism to identify informative domains.\nTheoretically, we analyze the lasso estimator in the context of a random graph\nbased on the Erdos-Renyi model assumption, demonstrating that transfer learning\nimproves convergence rates when informative sources are present. Empirical\nevaluations, including simulations and a real-world application using Sina\nWeibo data, demonstrate substantial improvements in prediction accuracy,\nparticularly when labeled data in the target domain is limited."}
{"id": "1910.04295", "pdf": "https://arxiv.org/pdf/1910.04295", "abs": "https://arxiv.org/abs/1910.04295", "authors": ["René Carmona", "Mathieu Laurière", "Zongjun Tan"], "title": "Linear-Quadratic Mean-Field Reinforcement Learning: Convergence of Policy Gradient Methods", "categories": ["math.OC", "cs.LG"], "comment": null, "summary": "We investigate reinforcement learning in the setting of Markov decision\nprocesses for a large number of exchangeable agents interacting in a mean field\nmanner. Applications include, for example, the control of a large number of\nrobots communicating through a central unit dispatching the optimal policy\ncomputed by maximizing an aggregate reward. An approximate solution is obtained\nby learning the optimal policy of a generic agent interacting with the\nstatistical distribution of the states and actions of the other agents. We\nfirst provide a full analysis this discrete-time mean field control problem. We\nthen rigorously prove the convergence of exact and model-free policy gradient\nmethods in a mean-field linear-quadratic setting and establish bounds on the\nrates of convergence. We also provide graphical evidence of the convergence\nbased on implementations of our algorithms."}
{"id": "2206.02702", "pdf": "https://arxiv.org/pdf/2206.02702", "abs": "https://arxiv.org/abs/2206.02702", "authors": ["Michał Dereziński"], "title": "Stochastic Variance-Reduced Newton: Accelerating Finite-Sum Minimization with Large Batches", "categories": ["math.OC", "cs.LG", "stat.ML"], "comment": null, "summary": "Stochastic variance reduction has proven effective at accelerating\nfirst-order algorithms for solving convex finite-sum optimization tasks such as\nempirical risk minimization. Incorporating second-order information has proven\nhelpful in further improving the performance of these first-order methods. Yet,\ncomparatively little is known about the benefits of using variance reduction to\naccelerate popular stochastic second-order methods such as Subsampled Newton.\nTo address this, we propose Stochastic Variance-Reduced Newton (SVRN), a\nfinite-sum minimization algorithm that provably accelerates existing stochastic\nNewton methods from $O(\\alpha\\log(1/\\epsilon))$ to\n$O\\big(\\frac{\\log(1/\\epsilon)}{\\log(n)}\\big)$ passes over the data, i.e., by a\nfactor of $O(\\alpha\\log(n))$, where $n$ is the number of sum components and\n$\\alpha$ is the approximation factor in the Hessian estimate. Surprisingly,\nthis acceleration gets more significant the larger the data size $n$, which is\na unique property of SVRN. Our algorithm retains the key advantages of\nNewton-type methods, such as easily parallelizable large-batch operations and a\nsimple unit step size. We use SVRN to accelerate Subsampled Newton and\nIterative Hessian Sketch algorithms, and show that it compares favorably to\npopular first-order methods with variance~reduction."}
{"id": "2304.09310", "pdf": "https://arxiv.org/pdf/2304.09310", "abs": "https://arxiv.org/abs/2304.09310", "authors": ["Emadaldin Mozafari-Majd", "Visa Koivunen"], "title": "The Adaptive $τ$-Lasso: Robustness and Oracle Properties", "categories": ["stat.ML", "cs.LG", "eess.SP"], "comment": null, "summary": "This paper introduces a new regularized version of the robust\n$\\tau$-regression estimator for analyzing high-dimensional datasets subject to\ngross contamination in the response variables and covariates. The resulting\nestimator, termed adaptive $\\tau$-Lasso, is robust to outliers and\nhigh-leverage points. It also incorporates an adaptive $\\ell_1$-norm penalty\nterm, which enables the selection of relevant variables and reduces the bias\nassociated with large true regression coefficients. More specifically, this\nadaptive $\\ell_1$-norm penalty term assigns a weight to each regression\ncoefficient. For a fixed number of predictors $p$, we show that the adaptive\n$\\tau$-Lasso has the oracle property, ensuring both variable-selection\nconsistency and asymptotic normality. Asymptotic normality applies only to the\nentries of the regression vector corresponding to the true support, assuming\nknowledge of the true regression vector support. We characterize its robustness\nby establishing the finite-sample breakdown point and the influence function.\nWe carry out extensive simulations and observe that the class of $\\tau$-Lasso\nestimators exhibits robustness and reliable performance in both contaminated\nand uncontaminated data settings. We also validate our theoretical findings on\nrobustness properties through simulations. In the face of outliers and\nhigh-leverage points, the adaptive $\\tau$-Lasso and $\\tau$-Lasso estimators\nachieve the best performance or match the best performances of competing\nregularized estimators, with minimal or no loss in terms of prediction and\nvariable selection accuracy for almost all scenarios considered in this study.\nTherefore, the adaptive $\\tau$-Lasso and $\\tau$-Lasso estimators provide\nattractive tools for a variety of sparse linear regression problems,\nparticularly in high-dimensional settings and when the data is contaminated by\noutliers and high-leverage points."}
{"id": "2305.00241", "pdf": "https://arxiv.org/pdf/2305.00241", "abs": "https://arxiv.org/abs/2305.00241", "authors": ["Joey Huchette", "Gonzalo Muñoz", "Thiago Serra", "Calvin Tsay"], "title": "When Deep Learning Meets Polyhedral Theory: A Survey", "categories": ["math.OC", "cs.LG"], "comment": null, "summary": "In the past decade, deep learning became the prevalent methodology for\npredictive modeling thanks to the remarkable accuracy of deep neural networks\nin tasks such as computer vision and natural language processing. Meanwhile,\nthe structure of neural networks converged back to simpler representations\nbased on piecewise constant and piecewise linear functions such as the\nRectified Linear Unit (ReLU), which became the most commonly used type of\nactivation function in neural networks. That made certain types of network\nstructure $\\unicode{x2014}$such as the typical fully-connected feedforward\nneural network$\\unicode{x2014}$ amenable to analysis through polyhedral theory\nand to the application of methodologies such as Linear Programming (LP) and\nMixed-Integer Linear Programming (MILP) for a variety of purposes. In this\npaper, we survey the main topics emerging from this fast-paced area of work,\nwhich bring a fresh perspective to understanding neural networks in more detail\nas well as to applying linear optimization techniques to train, verify, and\nreduce the size of such networks."}
{"id": "2408.05854", "pdf": "https://arxiv.org/pdf/2408.05854", "abs": "https://arxiv.org/abs/2408.05854", "authors": ["Xing Liu", "François-Xavier Briol"], "title": "On the Robustness of Kernel Goodness-of-Fit Tests", "categories": ["stat.ML", "cs.LG", "math.ST", "stat.ME", "stat.TH"], "comment": "50 pages, 15 figures", "summary": "Goodness-of-fit testing is often criticized for its lack of practical\nrelevance: since ``all models are wrong'', the null hypothesis that the data\nconform to our model is ultimately always rejected as sample size grows.\nDespite this, probabilistic models are still used extensively, raising the more\npertinent question of whether the model is \\emph{good enough} for the task at\nhand. This question can be formalized as a robust goodness-of-fit testing\nproblem by asking whether the data were generated from a distribution that is a\nmild perturbation of the model. In this paper, we show that existing kernel\ngoodness-of-fit tests are not robust under common notions of robustness\nincluding both qualitative and quantitative robustness. We further show that\nrobustification techniques using tilted kernels, while effective in the\nparameter estimation literature, are not sufficient to ensure both types of\nrobustness in the testing setting. To address this, we propose the first robust\nkernel goodness-of-fit test, which resolves this open problem by using kernel\nStein discrepancy (KSD) balls. This framework encompasses many well-known\nperturbation models, such as Huber's contamination and density-band models."}
{"id": "2408.15253", "pdf": "https://arxiv.org/pdf/2408.15253", "abs": "https://arxiv.org/abs/2408.15253", "authors": ["Hans van Gorp", "Merel M. van Gilst", "Pedro Fonseca", "Fokke B. van Meulen", "Johannes P. van Dijk", "Sebastiaan Overeem", "Ruud J. G. van Sloun"], "title": "A Deep Generative Model for Five-Class Sleep Staging with Arbitrary Sensor Input", "categories": ["eess.SP", "cs.LG"], "comment": null, "summary": "Gold-standard sleep scoring is based on epoch-based assignment of sleep\nstages based on a combination of EEG, EOG and EMG signals. However, a\npolysomnographic recording consists of many other signals that could be used\nfor sleep staging, including cardio-respiratory modalities. Leveraging this\nsignal variety would offer important advantages, for example increasing\nreliability, resilience to signal loss, and application to long-term\nnon-obtrusive recordings. We developed a deep generative model for automatic\nsleep staging from a plurality of sensors and any -- arbitrary -- combination\nthereof. We trained a score-based diffusion model using a dataset of 1947\nexpert-labelled overnight recordings with 36 different signals, and achieved\nzero-shot inference on any sensor set by leveraging a novel Bayesian\nfactorization of the score function across the sensors. On single-channel EEG,\nthe model reaches the performance limit in terms of polysomnography inter-rater\nagreement (5-class accuracy 85.6%, Cohen's kappa 0.791). Moreover, the method\noffers full flexibility to use any sensor set, for example finger\nphotoplethysmography, nasal flow and thoracic respiratory movements, (5-class\naccuracy 79.0%, Cohen's kappa of 0.697), or even derivations very\nunconventional for sleep staging, such as tibialis and sternocleidomastoid EMG\n(5-class accuracy 71.0%, kappa 0.575). Additionally, we propose a novel\ninterpretability metric in terms of information gain per sensor and show this\nis linearly correlated with classification performance. Finally, our model\nallows for post-hoc addition of entirely new sensor modalities by merely\ntraining a score estimator on the novel input instead of having to retrain from\nscratch on all inputs."}
{"id": "2409.08295", "pdf": "https://arxiv.org/pdf/2409.08295", "abs": "https://arxiv.org/abs/2409.08295", "authors": ["Jakub Kořenek", "Pavel Sanda", "Jaroslav Hlinka"], "title": "Higher order definition of causality by optimally conditioned transfer entropy", "categories": ["stat.ML", "cs.IT", "cs.LG", "math.IT", "physics.data-an"], "comment": null, "summary": "The description of the dynamics of complex systems, in particular the capture\nof the interaction structure and causal relationships between elements of the\nsystem, is one of the central questions of interdisciplinary research. While\nthe characterization of pairwise causal interactions is a relatively ripe field\nwith established theoretical concepts and the current focus is on technical\nissues of their efficient estimation, it turns out that the standard concepts\nsuch as Granger causality or transfer entropy may not faithfully reflect\npossible synergies or interactions of higher orders, phenomena highly relevant\nfor many real-world complex systems. In this paper, we propose a generalization\nand refinement of the information-theoretic approach to causal inference,\nenabling the description of truly multivariate, rather than multiple pairwise,\ncausal interactions, and moving thus from causal networks to causal\nhypernetworks. In particular, while keeping the ability to control for\nmediating variables or common causes, in case of purely synergetic interactions\nsuch as the exclusive disjunction, it ascribes the causal role to the\nmultivariate causal set but \\emph{not} to individual inputs, distinguishing it\nthus from the case of e.g. two additive univariate causes. We demonstrate this\nconcept by application to illustrative theoretical examples as well as a\nbiophysically realistic simulation of biological neuronal dynamics recently\nreported to employ synergetic computations."}
{"id": "2410.14760", "pdf": "https://arxiv.org/pdf/2410.14760", "abs": "https://arxiv.org/abs/2410.14760", "authors": ["Vasileios Vatellis"], "title": "Advancing Physics Data Analysis through Machine Learning and Physics-Informed Neural Networks", "categories": ["hep-ph", "cs.LG"], "comment": "7 page", "summary": "In an era increasingly focused on green computing and explainable AI,\nrevisiting traditional approaches in theoretical and phenomenological particle\nphysics is paramount. This project evaluates various machine learning (ML)\nalgorithms-including Nearest Neighbors, Decision Trees, Random Forest,\nAdaBoost, Naive Bayes, Quadratic Discriminant Analysis (QDA), and\nXGBoost-alongside standard neural networks and a novel Physics-Informed Neural\nNetwork (PINN) for physics data analysis. We apply these techniques to a binary\nclassification task that distinguishes the experimental viability of simulated\nscenarios based on Higgs observables and essential parameters. Through this\ncomprehensive analysis, we aim to showcase the capabilities and computational\nefficiency of each model in binary classification tasks, thereby contributing\nto the ongoing discourse on integrating ML and Deep Neural Networks (DNNs) into\nphysics research. In this study, XGBoost emerged as the preferred choice among\nthe evaluated machine learning algorithms for its speed and effectiveness,\nespecially in the initial stages of computation with limited datasets. However,\nwhile standard Neural Networks and Physics-Informed Neural Networks (PINNs)\ndemonstrated superior performance in terms of accuracy and adherence to\nphysical laws, they require more computational time. These findings underscore\nthe trade-offs between computational efficiency and model sophistication."}
{"id": "2410.21081", "pdf": "https://arxiv.org/pdf/2410.21081", "abs": "https://arxiv.org/abs/2410.21081", "authors": ["Benjamin Schiffer", "Lucas Janson"], "title": "Foundations of Safe Online Reinforcement Learning in the Linear Quadratic Regulator: Generalized Baselines", "categories": ["stat.ML", "cs.LG", "cs.SY", "eess.SY"], "comment": null, "summary": "Many practical applications of online reinforcement learning require the\nsatisfaction of safety constraints while learning about the unknown\nenvironment. In this work, we establish theoretical foundations for\nreinforcement learning with safety constraints by studying the canonical\nproblem of Linear Quadratic Regulator learning with unknown dynamics, but with\nthe additional constraint that the position must stay within a safe region for\nthe entire trajectory with high probability. Our primary contribution is a\ngeneral framework for studying stronger baselines of nonlinear controllers that\nare better suited for constrained problems than linear controllers. Due to the\ndifficulty of analyzing non-linear controllers in a constrained problem, we\nfocus on 1-dimensional state- and action- spaces, however we also discuss how\nwe expect the high-level takeaways can generalize to higher dimensions. Using\nour framework, we show that for \\emph{any} non-linear baseline satisfying\nnatural assumptions, $\\tilde{O}_T(\\sqrt{T})$-regret is possible when the noise\ndistribution has sufficiently large support, and $\\tilde{O}_T(T^{2/3})$-regret\nis possible for \\emph{any} subgaussian noise distribution. In proving these\nresults, we introduce a new uncertainty estimation bound for nonlinear controls\nwhich shows that enforcing safety in the presence of sufficient noise can\nprovide ``free exploration'' that compensates for the added cost of uncertainty\nin safety-constrained control."}
{"id": "2411.00405", "pdf": "https://arxiv.org/pdf/2411.00405", "abs": "https://arxiv.org/abs/2411.00405", "authors": ["Tuan Ngo Nguyen", "Jay Barrett", "Kwang-Sung Jun"], "title": "HAVER: Instance-Dependent Error Bounds for Maximum Mean Estimation and Applications to Q-Learning and Monte Carlo Tree Search", "categories": ["stat.ML", "cs.LG"], "comment": "In Proceedings of the Artificial Intelligence and Statistics\n  (AISTATS) 2025", "summary": "We study the problem of estimating the \\emph{value} of the largest mean among\nK distributions via samples from them (rather than estimating \\emph{which}\ndistribution has the largest mean), which arises from various machine learning\ntasks including Q-learning and Monte Carlo Tree Search (MCTS). While there have\nbeen a few proposed algorithms, their performance analyses have been limited to\ntheir biases rather than a precise error metric. In this paper, we propose a\nnovel algorithm called HAVER (Head AVERaging) and analyze its mean squared\nerror. Our analysis reveals that HAVER has a compelling performance in two\nrespects. First, HAVER estimates the maximum mean as well as the oracle who\nknows the identity of the best distribution and reports its sample mean.\nSecond, perhaps surprisingly, HAVER exhibits even better rates than this oracle\nwhen there are many distributions near the best one. Both of these improvements\nare the first of their kind in the literature, and we also prove that the naive\nalgorithm that reports the largest empirical mean does not achieve these\nbounds. Finally, we confirm our theoretical findings via numerical experiments\nwhere we implement HAVER in bandit, Q-learning, and MCTS algorithms. In these\nexperiments, HAVER consistently outperforms the baseline methods, demonstrating\nits effectiveness across different applications."}
{"id": "2411.02549", "pdf": "https://arxiv.org/pdf/2411.02549", "abs": "https://arxiv.org/abs/2411.02549", "authors": ["Daniel Kuhn", "Soroosh Shafiee", "Wolfram Wiesemann"], "title": "Distributionally Robust Optimization", "categories": ["math.OC", "cs.LG", "stat.ML"], "comment": null, "summary": "Distributionally robust optimization (DRO) studies decision problems under\nuncertainty where the probability distribution governing the uncertain problem\nparameters is itself uncertain. A key component of any DRO model is its\nambiguity set, that is, a family of probability distributions consistent with\nany available structural or statistical information. DRO seeks decisions that\nperform best under the worst distribution in the ambiguity set. This worst case\ncriterion is supported by findings in psychology and neuroscience, which\nindicate that many decision-makers have a low tolerance for distributional\nambiguity. DRO is rooted in statistics, operations research and control theory,\nand recent research has uncovered its deep connections to regularization\ntechniques and adversarial training in machine learning. This survey presents\nthe key findings of the field in a unified and self-contained manner."}
{"id": "2411.08773", "pdf": "https://arxiv.org/pdf/2411.08773", "abs": "https://arxiv.org/abs/2411.08773", "authors": ["Shabarish Chenakkod", "Michał Dereziński", "Xiaoyu Dong"], "title": "Optimal Oblivious Subspace Embeddings with Near-optimal Sparsity", "categories": ["cs.DS", "cs.LG", "cs.NA", "math.NA", "math.PR", "stat.ML"], "comment": "ICALP 2025", "summary": "An oblivious subspace embedding is a random $m\\times n$ matrix $\\Pi$ such\nthat, for any $d$-dimensional subspace, with high probability $\\Pi$ preserves\nthe norms of all vectors in that subspace within a $1\\pm\\epsilon$ factor. In\nthis work, we give an oblivious subspace embedding with the optimal dimension\n$m=\\Theta(d/\\epsilon^2)$ that has a near-optimal sparsity of $\\tilde\nO(1/\\epsilon)$ non-zero entries per column of $\\Pi$. This is the first result\nto nearly match the conjecture of Nelson and Nguyen [FOCS 2013] in terms of the\nbest sparsity attainable by an optimal oblivious subspace embedding, improving\non a prior bound of $\\tilde O(1/\\epsilon^6)$ non-zeros per column [Chenakkod et\nal., STOC 2024]. We further extend our approach to the non-oblivious setting,\nproposing a new family of Leverage Score Sparsified embeddings with Independent\nColumns, which yield faster runtimes for matrix approximation and regression\ntasks.\n  In our analysis, we develop a new method which uses a decoupling argument\ntogether with the cumulant method for bounding the edge universality error of\nisotropic random matrices. To achieve near-optimal sparsity, we combine this\ngeneral-purpose approach with new traces inequalities that leverage the\nspecific structure of our subspace embedding construction."}
{"id": "2412.01858", "pdf": "https://arxiv.org/pdf/2412.01858", "abs": "https://arxiv.org/abs/2412.01858", "authors": ["Siddhant Dutta", "Nouhaila Innan", "Sadok Ben Yahia", "Muhammad Shafique", "David Esteban Bernal Neira"], "title": "MQFL-FHE: Multimodal Quantum Federated Learning Framework with Fully Homomorphic Encryption", "categories": ["quant-ph", "cs.CR", "cs.DC", "cs.ET", "cs.LG"], "comment": "10 pages, 6 figures, 6 Tables. Accepted at IJCNN 2025", "summary": "The integration of fully homomorphic encryption (FHE) in federated learning\n(FL) has led to significant advances in data privacy. However, during the\naggregation phase, it often results in performance degradation of the\naggregated model, hindering the development of robust representational\ngeneralization. In this work, we propose a novel multimodal quantum federated\nlearning framework that utilizes quantum computing to counteract the\nperformance drop resulting from FHE. For the first time in FL, our framework\ncombines a multimodal quantum mixture of experts (MQMoE) model with FHE,\nincorporating multimodal datasets for enriched representation and task-specific\nlearning. Our MQMoE framework enhances performance on multimodal datasets and\ncombined genomics and brain MRI scans, especially for underrepresented\ncategories. Our results also demonstrate that the quantum-enhanced approach\nmitigates the performance degradation associated with FHE and improves\nclassification accuracy across diverse datasets, validating the potential of\nquantum interventions in enhancing privacy in FL."}
{"id": "2501.11280", "pdf": "https://arxiv.org/pdf/2501.11280", "abs": "https://arxiv.org/abs/2501.11280", "authors": ["Tsukasa Yoshida", "Kazuho Watanabe"], "title": "Empirical Bayes Estimation for Lasso-Type Regularizers: Analysis of Automatic Relevance Determination", "categories": ["math.ST", "cs.IT", "cs.LG", "math.IT", "stat.TH"], "comment": "8 pages, 1 figure", "summary": "This paper focuses on linear regression models with non-conjugate\nsparsity-inducing regularizers such as lasso and group lasso. Although the\nempirical Bayes approach enables us to estimate the regularization parameter,\nlittle is known on the properties of the estimators. In particular, many\naspects regarding the specific conditions under which the mechanism of\nautomatic relevance determination (ARD) occurs remain unexplained. In this\npaper, we derive the empirical Bayes estimators for the group lasso regularized\nlinear regression models with limited parameters. It is shown that the\nestimators diverge under a specific condition, giving rise to the ARD\nmechanism. We also prove that empirical Bayes methods can produce the ARD\nmechanism in general regularized linear regression models and clarify the\nconditions under which models such as ridge, lasso, and group lasso can do so."}
{"id": "2501.14246", "pdf": "https://arxiv.org/pdf/2501.14246", "abs": "https://arxiv.org/abs/2501.14246", "authors": ["Tianzhi Feng", "Chennan Wu", "Yi Niu", "Fu Li", "Yang Li", "Boxun Fu", "Zhifu Zhao", "Xiaotian Wang"], "title": "Adaptive Progressive Attention Graph Neural Network for EEG Emotion Recognition", "categories": ["eess.SP", "cs.LG"], "comment": null, "summary": "In recent years, numerous neuroscientific studies demonstrate that specific\nareas of the brain are connected to human emotional responses, with these\nregions exhibiting variability across individuals and emotional states. To\nfully leverage these neural patterns, we propose an Adaptive Progressive\nAttention Graph Neural Network (APAGNN), which dynamically captures the spatial\nrelationships among brain regions during emotional processing. The APAGNN\nemploys three specialized experts that progressively analyze brain topology.\nThe first expert captures global brain patterns, the second focuses on\nregion-specific features, and the third examines emotion-related channels. This\nhierarchical approach enables increasingly refined analysis of neural activity.\nAdditionally, a weight generator integrates the outputs of all three experts,\nbalancing their contributions to produce the final predictive label. Extensive\nexperiments conducted on SEED, SEED-IV and MPED datasets indicate that our\nmethod enhances EEG emotion recognition performance, achieving superior results\ncompared to baseline methods."}
{"id": "2502.05074", "pdf": "https://arxiv.org/pdf/2502.05074", "abs": "https://arxiv.org/abs/2502.05074", "authors": ["Alexander Atanasov", "Blake Bordelon", "Jacob A. Zavatone-Veth", "Courtney Paquette", "Cengiz Pehlevan"], "title": "Two-Point Deterministic Equivalence for Stochastic Gradient Dynamics in Linear Models", "categories": ["cond-mat.dis-nn", "cs.LG", "stat.ML"], "comment": "Fixing typo in equation 2 (model definition)", "summary": "We derive a novel deterministic equivalence for the two-point function of a\nrandom matrix resolvent. Using this result, we give a unified derivation of the\nperformance of a wide variety of high-dimensional linear models trained with\nstochastic gradient descent. This includes high-dimensional linear regression,\nkernel regression, and random feature models. Our results include previously\nknown asymptotics as well as novel ones."}
{"id": "2503.14453", "pdf": "https://arxiv.org/pdf/2503.14453", "abs": "https://arxiv.org/abs/2503.14453", "authors": ["Qiushuo Hou", "Sangwoo Park", "Matteo Zecchin", "Yunlong Cai", "Guanding Yu", "Osvaldo Simeone"], "title": "Online Conformal Probabilistic Numerics via Adaptive Edge-Cloud Offloading", "categories": ["stat.ML", "cs.LG"], "comment": "This paper has been submitted to a conference", "summary": "Consider an edge computing setting in which a user submits queries for the\nsolution of a linear system to an edge processor, which is subject to\ntime-varying computing availability. The edge processor applies a probabilistic\nlinear solver (PLS) so as to be able to respond to the user's query within the\nallotted time and computing budget. Feedback to the user is in the form of a\nset of plausible solutions. Due to model misspecification, the\nhighest-probability-density (HPD) set obtained via a direct application of PLS\ndoes not come with coverage guarantees with respect to the true solution of the\nlinear system. This work introduces a new method to calibrate the HPD sets\nproduced by PLS with the aim of guaranteeing long-term coverage requirements.\nThe proposed method, referred to as online conformal prediction-PLS (OCP-PLS),\nassumes sporadic feedback from cloud to edge. This enables the online\ncalibration of uncertainty thresholds via online conformal prediction (OCP), an\nonline optimization method previously studied in the context of prediction\nmodels. The validity of OCP-PLS is verified via experiments that bring insights\ninto trade-offs between coverage, prediction set size, and cloud usage."}
{"id": "2503.17427", "pdf": "https://arxiv.org/pdf/2503.17427", "abs": "https://arxiv.org/abs/2503.17427", "authors": ["Michael D. White", "Michael D. Atkinson", "Adam J. Plowman", "Pratheek Shanthraj"], "title": "3D variational autoencoder for fingerprinting microstructure volume elements", "categories": ["cond-mat.mtrl-sci", "cs.LG"], "comment": "24 pages, 9 figures", "summary": "Microstructure quantification is an important step towards establishing\nstructure-property relationships in materials. Machine learning-based image\nprocessing methods have been shown to outperform conventional image processing\ntechniques and are increasingly applied to microstructure quantification tasks.\nIn this work, we present a 3D variational autoencoder (VAE) for encoding\nmicrostructure volume elements (VEs) comprising voxelated crystallographic\norientation data. Crystal symmetries in the orientation space are accounted for\nby mapping to the crystallographic fundamental zone as a preprocessing step,\nwhich allows for a continuous loss function to be used and improves the\ntraining convergence rate. The VAE is then used to encode a training set of VEs\nwith an equiaxed polycrystalline microstructure with random texture. Accurate\nreconstructions are achieved with a relative average misorientation error of\n9x10-3 on the test dataset, for a continuous latent space with dimension 256.\nWe show that the model generalises well to microstructures with textures, grain\nsizes and aspect ratios outside the training distribution. Structure-property\nrelationships are explored through using the training set of VEs as initial\nconfigurations in various crystal plasticity (CP) simulations. Microstructural\nfingerprints extracted from the VAE, which parameterise the VEs in a\nlow-dimensional latent space, are stored alongside the volume-averaged stress\nresponse, at each strain increment, to uniaxial tensile deformation from CP\nsimulations. This is then used to train a fully connected neural network\nmapping the input fingerprint to the resulting stress response, which acts as a\nsurrogate model for the CP simulation. The fingerprint-based surrogate model is\nshown to accurately predict the microstructural dependence in the CP stress\nresponse, with a relative mean-squared error of 8.9x10-4 on unseen test data."}
{"id": "2504.04421", "pdf": "https://arxiv.org/pdf/2504.04421", "abs": "https://arxiv.org/abs/2504.04421", "authors": ["Hang Zhao", "Juzhan Xu", "Kexiong Yu", "Ruizhen Hu", "Chenyang Zhu", "Kai Xu"], "title": "Deliberate Planning of 3D Bin Packing on Packing Configuration Trees", "categories": ["cs.RO", "cs.LG"], "comment": null, "summary": "Online 3D Bin Packing Problem (3D-BPP) has widespread applications in\nindustrial automation. Existing methods usually solve the problem with limited\nresolution of spatial discretization, and/or cannot deal with complex practical\nconstraints well. We propose to enhance the practical applicability of online\n3D-BPP via learning on a novel hierarchical representation, packing\nconfiguration tree (PCT). PCT is a full-fledged description of the state and\naction space of bin packing which can support packing policy learning based on\ndeep reinforcement learning (DRL). The size of the packing action space is\nproportional to the number of leaf nodes, making the DRL model easy to train\nand well-performing even with continuous solution space. We further discover\nthe potential of PCT as tree-based planners in deliberately solving packing\nproblems of industrial significance, including large-scale packing and\ndifferent variations of BPP setting. A recursive packing method is proposed to\ndecompose large-scale packing into smaller sub-trees while a spatial ensemble\nmechanism integrates local solutions into global. For different BPP variations\nwith additional decision variables, such as lookahead, buffering, and offline\npacking, we propose a unified planning framework enabling out-of-the-box\nproblem solving. Extensive evaluations demonstrate that our method outperforms\nexisting online BPP baselines and is versatile in incorporating various\npractical constraints. The planning process excels across large-scale problems\nand diverse problem variations. We develop a real-world packing robot for\nindustrial warehousing, with careful designs accounting for constrained\nplacement and transportation stability. Our packing robot operates reliably and\nefficiently on unprotected pallets at 10 seconds per box. It achieves averagely\n19 boxes per pallet with 57.4% space utilization for relatively large-size\nboxes."}
{"id": "2504.10560", "pdf": "https://arxiv.org/pdf/2504.10560", "abs": "https://arxiv.org/abs/2504.10560", "authors": ["Yaroslav Gusev", "Vitaly Vanchurin"], "title": "Molecular Learning Dynamics", "categories": ["physics.chem-ph", "cs.LG", "physics.comp-ph"], "comment": "16 pages, 7 figures, 1 table", "summary": "We apply the physics-learning duality to molecular systems by complementing\nthe physical description of interacting particles with a dual learning\ndescription, where each particle is modeled as an agent minimizing a loss\nfunction. In the traditional physics framework, the equations of motion are\nderived from the Lagrangian function, while in the learning framework, the same\nequations emerge from learning dynamics driven by the agent loss function. The\nloss function depends on scalar quantities that describe invariant properties\nof all other agents or particles. To demonstrate this approach, we first infer\nthe loss functions of oxygen and hydrogen directly from a dataset generated by\nthe CP2K physics-based simulation of water molecules. We then employ the loss\nfunctions to develop a learning-based simulation of water molecules, which\nachieves comparable accuracy while being significantly more computationally\nefficient than standard physics-based simulations."}
{"id": "2504.13962", "pdf": "https://arxiv.org/pdf/2504.13962", "abs": "https://arxiv.org/abs/2504.13962", "authors": ["Jose Manuel Aroca-Fernandez", "Jose Francisco Diez-Pastor", "Pedro Latorre-Carmona", "Victor Elvira", "Gustau Camps-Valls", "Rodrigo Pascual", "Cesar Garcia-Osorio"], "title": "A Collaborative Platform for Soil Organic Carbon Inference Based on Spatiotemporal Remote Sensing Data", "categories": ["cs.CY", "cs.LG"], "comment": "28 pages, 11 figures. Submitted for review to \"Environmental\n  Modelling & Software\"", "summary": "Soil organic carbon (SOC) is a key indicator of soil health, fertility, and\ncarbon sequestration, making it essential for sustainable land management and\nclimate change mitigation. However, large-scale SOC monitoring remains\nchallenging due to spatial variability, temporal dynamics, and multiple\ninfluencing factors. We present WALGREEN, a platform that enhances SOC\ninference by overcoming limitations of current applications. Leveraging machine\nlearning and diverse soil samples, WALGREEN generates predictive models using\nhistorical public and private data. Built on cloud-based technologies, it\noffers a user-friendly interface for researchers, policymakers, and land\nmanagers to access carbon data, analyze trends, and support evidence-based\ndecision-making. Implemented in Python, Java, and JavaScript, WALGREEN\nintegrates Google Earth Engine and Sentinel Copernicus via scripting,\nOpenLayers, and Thymeleaf in a Model-View-Controller framework. This paper aims\nto advance soil science, promote sustainable agriculture, and drive critical\necosystem responses to climate change."}
{"id": "2504.15388", "pdf": "https://arxiv.org/pdf/2504.15388", "abs": "https://arxiv.org/abs/2504.15388", "authors": ["Tianyi Ma", "Tengyao Wang", "Richard J. Samworth"], "title": "Deep learning with missing data", "categories": ["stat.ME", "cs.LG", "math.ST", "stat.ML", "stat.TH", "62C20, 62D10, 62G08"], "comment": "49 pages, 9 figures", "summary": "In the context of multivariate nonparametric regression with missing\ncovariates, we propose Pattern Embedded Neural Networks (PENNs), which can be\napplied in conjunction with any existing imputation technique. In addition to a\nneural network trained on the imputed data, PENNs pass the vectors of\nobservation indicators through a second neural network to provide a compact\nrepresentation. The outputs are then combined in a third neural network to\nproduce final predictions. Our main theoretical result exploits an assumption\nthat the observation patterns can be partitioned into cells on which the Bayes\nregression function behaves similarly, and belongs to a compositional H\\\"older\nclass. It provides a finite-sample excess risk bound that holds for an\narbitrary missingness mechanism, and in combination with a complementary\nminimax lower bound, demonstrates that our PENN estimator attains in typical\ncases the minimax rate of convergence as if the cells of the partition were\nknown in advance, up to a poly-logarithmic factor in the sample size. Numerical\nexperiments on simulated, semi-synthetic and real data confirm that the PENN\nestimator consistently improves, often dramatically, on standard neural\nnetworks without pattern embedding. Code to reproduce our experiments, as well\nas a tutorial on how to apply our method, is publicly available."}
{"id": "2504.16137", "pdf": "https://arxiv.org/pdf/2504.16137", "abs": "https://arxiv.org/abs/2504.16137", "authors": ["Jasper Götting", "Pedro Medeiros", "Jon G Sanders", "Nathaniel Li", "Long Phan", "Karam Elabd", "Lennart Justen", "Dan Hendrycks", "Seth Donoughe"], "title": "Virology Capabilities Test (VCT): A Multimodal Virology Q&A Benchmark", "categories": ["cs.CY", "cs.LG"], "comment": "31 pages", "summary": "We present the Virology Capabilities Test (VCT), a large language model (LLM)\nbenchmark that measures the capability to troubleshoot complex virology\nlaboratory protocols. Constructed from the inputs of dozens of PhD-level expert\nvirologists, VCT consists of $322$ multimodal questions covering fundamental,\ntacit, and visual knowledge that is essential for practical work in virology\nlaboratories. VCT is difficult: expert virologists with access to the internet\nscore an average of $22.1\\%$ on questions specifically in their sub-areas of\nexpertise. However, the most performant LLM, OpenAI's o3, reaches $43.8\\%$\naccuracy, outperforming $94\\%$ of expert virologists even within their\nsub-areas of specialization. The ability to provide expert-level virology\ntroubleshooting is inherently dual-use: it is useful for beneficial research,\nbut it can also be misused. Therefore, the fact that publicly available models\noutperform virologists on VCT raises pressing governance considerations. We\npropose that the capability of LLMs to provide expert-level troubleshooting of\ndual-use virology work should be integrated into existing frameworks for\nhandling dual-use technologies in the life sciences."}
