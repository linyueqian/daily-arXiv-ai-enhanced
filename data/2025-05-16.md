<div id=toc></div>

# Table of Contents

- [cs.CL](#cs.CL) [Total: 83]
- [cs.CV](#cs.CV) [Total: 96]
- [cs.AI](#cs.AI) [Total: 28]
- [cs.SD](#cs.SD) [Total: 10]
- [cs.LG](#cs.LG) [Total: 134]
- [cs.MA](#cs.MA) [Total: 3]
- [cs.MM](#cs.MM) [Total: 1]
- [eess.AS](#eess.AS) [Total: 6]
- [eess.IV](#eess.IV) [Total: 14]


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [1] [Next Word Suggestion using Graph Neural Network](https://arxiv.org/pdf/2505.09649)
*Abisha Thapa Magar, Anup Shakya*

Main category: cs.CL

TL;DR: Proposes a resource-efficient method for context embedding in language modeling using Graph Convolution and LSTMs.


<details>
  <summary>Details</summary>
Motivation: Address the high resource demands of current language models by focusing on context embedding.

Method: Combines Graph Convolution (GNNs) with LSTMs to encode context and predict the next word.

Result: Works well on a custom Wikipedia corpus with limited resources.

Conclusion: Demonstrates a viable, resource-efficient alternative for context embedding in language modeling.

Abstract: Language Modeling is a prevalent task in Natural Language Processing. The
currently existing most recent and most successful language models often tend
to build a massive model with billions of parameters, feed in a tremendous
amount of text data, and train with enormous computation resources which
require millions of dollars. In this project, we aim to address an important
sub-task in language modeling, i.e., context embedding. We propose an approach
to exploit the Graph Convolution operation in GNNs to encode the context and
use it in coalition with LSTMs to predict the next word given a local context
of preceding words. We test this on the custom Wikipedia text corpus using a
very limited amount of resources and show that this approach works fairly well
to predict the next word.

</details>


### [2] [DRA-GRPO: Exploring Diversity-Aware Reward Adjustment for R1-Zero-Like Training of Large Language Models](https://arxiv.org/pdf/2505.09655)
*Xiwen Chen, Wenhui Zhu, Peijie Qiu, Xuanzhao Dong, Hao Wang, Haiyu Wu, Huayu Li, Aristeidis Sotiras, Yalin Wang, Abolfazl Razi*

Main category: cs.CL

TL;DR: Proposes Diversity-aware Reward Adjustment (DRA) to address diversity-quality inconsistency in GRPO, improving performance in low-resource settings.


<details>
  <summary>Details</summary>
Motivation: GRPO's scalar rewards fail to capture semantic diversity, leading to indistinguishable rewards for diverse reasoning paths.

Method: DRA uses Submodular Mutual Information (SMI) to adjust rewards, downweighting redundant completions and amplifying diverse ones.

Result: Achieves state-of-the-art 58.2% accuracy on five benchmarks with only 7,000 samples and $55 training cost.

Conclusion: DRA enhances GRPO by incorporating semantic diversity, improving exploration and maintaining high-quality exploitation.

Abstract: Recent advances in reinforcement learning for language model post-training,
such as Group Relative Policy Optimization (GRPO), have shown promise in
low-resource settings. However, GRPO typically relies on solution-level and
scalar reward signals that fail to capture the semantic diversity among sampled
completions. This leads to what we identify as a diversity-quality
inconsistency, where distinct reasoning paths may receive indistinguishable
rewards. To address this limitation, we propose $\textit{Diversity-aware Reward
Adjustment}$ (DRA), a method that explicitly incorporates semantic diversity
into the reward computation. DRA uses Submodular Mutual Information (SMI) to
downweight redundant completions and amplify rewards for diverse ones. This
encourages better exploration during learning, while maintaining stable
exploitation of high-quality samples. Our method integrates seamlessly with
both GRPO and its variant DR.~GRPO, resulting in $\textit{DRA-GRPO}$ and
$\textit{DGA-DR.~GRPO}$. We evaluate our method on five mathematical reasoning
benchmarks and find that it outperforms recent strong baselines. It achieves
state-of-the-art performance with an average accuracy of 58.2%, using only
7,000 fine-tuning samples and a total training cost of approximately $55. The
code is available at https://github.com/xiwenc1/DRA-GRPO.

</details>


### [3] [Large Language Models Are More Persuasive Than Incentivized Human Persuaders](https://arxiv.org/pdf/2505.09662)
*Philipp Schoenegger, Francesco Salvi, Jiacheng Liu, Xiaoli Nan, Ramit Debnath, Barbara Fasolo, Evelina Leivada, Gabriel Recchia, Fritz Günther, Ali Zarifhonarvar, Joe Kwon, Zahoor Ul Islam, Marco Dehnert, Daryl Y. H. Lee, Madeline G. Reinecke, David G. Kamper, Mert Kobaş, Adam Sandford, Jonas Kgomo, Luke Hewitt, Shreya Kapoor, Kerem Oktar, Eyup Engin Kucuk, Bo Feng, Cameron R. Jones, Izzy Gainsburg, Sebastian Olschewski, Nora Heinzelmann, Francisco Cruz, Ben M. Tappin, Tao Ma, Peter S. Park, Rayan Onyonka, Arthur Hjorth, Peter Slattery, Qingcheng Zeng, Lennart Finke, Igor Grossmann, Alessandro Salatiello, Ezra Karger*

Main category: cs.CL

TL;DR: LLMs outperform humans in persuasive tasks, increasing accuracy when truthful and decreasing it when deceptive, highlighting the need for AI governance.


<details>
  <summary>Details</summary>
Motivation: To compare the persuasion capabilities of LLMs (Claude Sonnet 3.5) against incentivized humans in real-time conversations.

Method: A preregistered, large-scale experiment where LLMs and humans attempted to persuade quiz takers toward correct or incorrect answers.

Result: LLMs achieved higher compliance than humans, improving accuracy for truthful persuasion and reducing it for deceptive persuasion.

Conclusion: AI's persuasion already surpasses incentivized humans, emphasizing the urgency of alignment and governance frameworks.

Abstract: We directly compare the persuasion capabilities of a frontier large language
model (LLM; Claude Sonnet 3.5) against incentivized human persuaders in an
interactive, real-time conversational quiz setting. In this preregistered,
large-scale incentivized experiment, participants (quiz takers) completed an
online quiz where persuaders (either humans or LLMs) attempted to persuade quiz
takers toward correct or incorrect answers. We find that LLM persuaders
achieved significantly higher compliance with their directional persuasion
attempts than incentivized human persuaders, demonstrating superior persuasive
capabilities in both truthful (toward correct answers) and deceptive (toward
incorrect answers) contexts. We also find that LLM persuaders significantly
increased quiz takers' accuracy, leading to higher earnings, when steering quiz
takers toward correct answers, and significantly decreased their accuracy,
leading to lower earnings, when steering them toward incorrect answers.
Overall, our findings suggest that AI's persuasion capabilities already exceed
those of humans that have real-money bonuses tied to performance. Our findings
of increasingly capable AI persuaders thus underscore the urgency of emerging
alignment and governance frameworks.

</details>


### [4] [System Prompt Optimization with Meta-Learning](https://arxiv.org/pdf/2505.09666)
*Yumin Choi, Jinheon Baek, Sung Ju Hwang*

Main category: cs.CL

TL;DR: The paper introduces bilevel system prompt optimization for LLMs, focusing on robust and transferable system prompts across tasks, using a meta-learning framework.


<details>
  <summary>Details</summary>
Motivation: Existing prompt optimization focuses on task-specific user prompts, neglecting system prompts that can generalize across tasks.

Method: A meta-learning framework optimizes system prompts over diverse user prompts and datasets, iteratively updating both for synergy.

Result: Experiments on 14 datasets show the optimized system prompts generalize well and enable rapid adaptation to unseen tasks with fewer steps.

Conclusion: The approach effectively improves LLM performance by optimizing system prompts for robustness and transferability.

Abstract: Large Language Models (LLMs) have shown remarkable capabilities, with
optimizing their input prompts playing a pivotal role in maximizing their
performance. However, while LLM prompts consist of both the task-agnostic
system prompts and task-specific user prompts, existing work on prompt
optimization has focused on user prompts specific to individual queries or
tasks, and largely overlooked the system prompt that is, once optimized,
applicable across different tasks and domains. Motivated by this, we introduce
the novel problem of bilevel system prompt optimization, whose objective is to
design system prompts that are robust to diverse user prompts and transferable
to unseen tasks. To tackle this problem, we then propose a meta-learning
framework, which meta-learns the system prompt by optimizing it over various
user prompts across multiple datasets, while simultaneously updating the user
prompts in an iterative manner to ensure synergy between them. We conduct
experiments on 14 unseen datasets spanning 5 different domains, on which we
show that our approach produces system prompts that generalize effectively to
diverse user prompts. Also, our findings reveal that the optimized system
prompt enables rapid adaptation even to unseen tasks, requiring fewer
optimization steps for test-time user prompts while achieving improved
performance.

</details>


### [5] [VeriFact: Enhancing Long-Form Factuality Evaluation with Refined Fact Extraction and Reference Facts](https://arxiv.org/pdf/2505.09701)
*Xin Liu, Lechen Zhang, Sheza Munir, Yiyang Gu, Lu Wang*

Main category: cs.CL

TL;DR: VeriFact is a framework improving factuality evaluation in LLMs by addressing incomplete facts, while FactRBench benchmarks precision and recall in long-form responses.


<details>
  <summary>Details</summary>
Motivation: Existing methods for evaluating LLM factuality often miss context and relational facts, necessitating a better framework.

Method: Introduces VeriFact for enhanced fact extraction and verification, and FactRBench for evaluating precision and recall.

Result: VeriFact improves fact completeness and preserves relational facts, while FactRBench shows larger models enhance precision and recall.

Conclusion: Comprehensive factuality assessment is crucial, as high precision doesn't always mean high recall.

Abstract: Large language models (LLMs) excel at generating long-form responses, but
evaluating their factuality remains challenging due to complex inter-sentence
dependencies within the generated facts. Prior solutions predominantly follow a
decompose-decontextualize-verify pipeline but often fail to capture essential
context and miss key relational facts. In this paper, we introduce VeriFact, a
factuality evaluation framework designed to enhance fact extraction by
identifying and resolving incomplete and missing facts to support more accurate
verification results. Moreover, we introduce FactRBench , a benchmark that
evaluates both precision and recall in long-form model responses, whereas prior
work primarily focuses on precision. FactRBench provides reference fact sets
from advanced LLMs and human-written answers, enabling recall assessment.
Empirical evaluations show that VeriFact significantly enhances fact
completeness and preserves complex facts with critical relational information,
resulting in more accurate factuality evaluation. Benchmarking various open-
and close-weight LLMs on FactRBench indicate that larger models within same
model family improve precision and recall, but high precision does not always
correlate with high recall, underscoring the importance of comprehensive
factuality assessment.

</details>


### [6] [An AI-Powered Research Assistant in the Lab: A Practical Guide for Text Analysis Through Iterative Collaboration with LLMs](https://arxiv.org/pdf/2505.09724)
*Gino Carmona-Díaz, William Jiménez-Leal, María Alejandra Grisales, Chandra Sripada, Santiago Amaya, Michael Inzlicht, Juan Pablo Bermúdez*

Main category: cs.CL

TL;DR: A tutorial on using LLMs for efficient, iterative text analysis with predefined or data-driven taxonomies, demonstrated with personal goals data.


<details>
  <summary>Details</summary>
Motivation: Text analysis is often biased and labor-intensive; LLMs offer a promising solution without quality loss.

Method: Step-by-step tutorial involving iterative collaboration between researchers and LLMs, including prompt writing, taxonomy evaluation, and intercoder reliability testing.

Result: High intercoder reliability achieved when applying the taxonomy to categorize datasets.

Conclusion: LLMs are effective for text analysis but have limitations; the method is practical and collaborative.

Abstract: Analyzing texts such as open-ended responses, headlines, or social media
posts is a time- and labor-intensive process highly susceptible to bias. LLMs
are promising tools for text analysis, using either a predefined (top-down) or
a data-driven (bottom-up) taxonomy, without sacrificing quality. Here we
present a step-by-step tutorial to efficiently develop, test, and apply
taxonomies for analyzing unstructured data through an iterative and
collaborative process between researchers and LLMs. Using personal goals
provided by participants as an example, we demonstrate how to write prompts to
review datasets and generate a taxonomy of life domains, evaluate and refine
the taxonomy through prompt and direct modifications, test the taxonomy and
assess intercoder agreements, and apply the taxonomy to categorize an entire
dataset with high intercoder reliability. We discuss the possibilities and
limitations of using LLMs for text analysis.

</details>


### [7] [Achieving Tokenizer Flexibility in Language Models through Heuristic Adaptation and Supertoken Learning](https://arxiv.org/pdf/2505.09738)
*Shaurya Sharthak, Vinayak Pahalwan, Adithya Kamath, Adarsh Shirawalmath*

Main category: cs.CL

TL;DR: The paper introduces TokenAdapt, a model-agnostic tokenizer transplantation method, and pre-tokenization learning for multi-word Supertokens to address inefficiencies in fixed tokenization schemes of LLMs.


<details>
  <summary>Details</summary>
Motivation: Fixed tokenization in LLMs causes inefficiencies and performance issues, especially for multilingual or specialized tasks. Existing solutions are resource-intensive or fail to preserve semantics.

Method: TokenAdapt combines local subword decomposition and global semantic similarity to initialize new tokens. Supertokens enhance compression and reduce fragmentation.

Result: TokenAdapt outperforms baselines like ReTok and TransTokenizer, achieving lower perplexity ratios and significant compression gains.

Conclusion: TokenAdapt effectively addresses tokenizer lock-in with minimal retraining, preserving semantics and improving performance.

Abstract: Pretrained language models (LLMs) are often constrained by their fixed
tokenization schemes, leading to inefficiencies and performance limitations,
particularly for multilingual or specialized applications. This tokenizer
lock-in presents significant challenges. standard methods to overcome this
often require prohibitive computational resources. Although tokenizer
replacement with heuristic initialization aims to reduce this burden, existing
methods often require exhaustive residual fine-tuning and still may not fully
preserve semantic nuances or adequately address the underlying compression
inefficiencies. Our framework introduces two innovations: first, Tokenadapt, a
model-agnostic tokenizer transplantation method, and second, novel
pre-tokenization learning for multi-word Supertokens to enhance compression and
reduce fragmentation. Tokenadapt initializes new unique token embeddings via a
hybrid heuristic that combines two methods: a local estimate based on subword
decomposition using the old tokenizer, and a global estimate utilizing the
top-k semantically similar tokens from the original vocabulary. This
methodology aims to preserve semantics while significantly minimizing
retraining requirements. Empirical investigations validate both contributions:
the transplantation heuristic successfully initializes unique tokens, markedly
outperforming conventional baselines and sophisticated methods including
Transtokenizer and ReTok, while our Supertokens achieve notable compression
gains. Our zero-shot perplexity results demonstrate that the TokenAdapt hybrid
initialization consistently yields lower perplexity ratios compared to both
ReTok and TransTokenizer baselines across different base models and newly
trained target tokenizers. TokenAdapt typically reduced the overall perplexity
ratio significantly compared to ReTok, yielding at least a 2-fold improvement
in these aggregate scores.

</details>


### [8] [Automated Detection of Clinical Entities in Lung and Breast Cancer Reports Using NLP Techniques](https://arxiv.org/pdf/2505.09794)
*J. Moreno-Casanova, J. M. Auñón, A. Mártinez-Pérez, M. E. Pérez-Martínez, M. E. Gas-López*

Main category: cs.CL

TL;DR: The paper proposes using NLP, specifically NER, to automate data extraction from EHRs for lung and breast cancer, improving efficiency and accuracy compared to manual methods.


<details>
  <summary>Details</summary>
Motivation: Manual extraction of clinical data is time-consuming and error-prone, hindering healthcare research. NLP offers a solution to automate this process, especially for high-impact cancers like lung and breast cancer.

Method: The study used GMV's NLP tool uQuery and fine-tuned the bsc-bio-ehr-en3 model (RoBERTa-based) for NER on annotated EHRs from IIS La Fe, focusing on eight clinical entities.

Result: The approach showed strong performance in identifying key entities (e.g., MET, PAT) but faced challenges with less frequent ones (e.g., EVOL).

Conclusion: NLP, particularly NER, is effective for automating clinical data extraction, though further improvements are needed for rare entities.

Abstract: Research projects, including those focused on cancer, rely on the manual
extraction of information from clinical reports. This process is time-consuming
and prone to errors, limiting the efficiency of data-driven approaches in
healthcare. To address these challenges, Natural Language Processing (NLP)
offers an alternative for automating the extraction of relevant data from
electronic health records (EHRs). In this study, we focus on lung and breast
cancer due to their high incidence and the significant impact they have on
public health. Early detection and effective data management in both types of
cancer are crucial for improving patient outcomes. To enhance the accuracy and
efficiency of data extraction, we utilized GMV's NLP tool uQuery, which excels
at identifying relevant entities in clinical texts and converting them into
standardized formats such as SNOMED and OMOP. uQuery not only detects and
classifies entities but also associates them with contextual information,
including negated entities, temporal aspects, and patient-related details. In
this work, we explore the use of NLP techniques, specifically Named Entity
Recognition (NER), to automatically identify and extract key clinical
information from EHRs related to these two cancers. A dataset from Health
Research Institute Hospital La Fe (IIS La Fe), comprising 200 annotated breast
cancer and 400 lung cancer reports, was used, with eight clinical entities
manually labeled using the Doccano platform. To perform NER, we fine-tuned the
bsc-bio-ehr-en3 model, a RoBERTa-based biomedical linguistic model pre-trained
in Spanish. Fine-tuning was performed using the Transformers architecture,
enabling accurate recognition of clinical entities in these cancer types. Our
results demonstrate strong overall performance, particularly in identifying
entities like MET and PAT, although challenges remain with less frequent
entities like EVOL.

</details>


### [9] [Exploring the generalization of LLM truth directions on conversational formats](https://arxiv.org/pdf/2505.09807)
*Timour Ichmoukhamedov, David Martens*

Main category: cs.CL

TL;DR: LLMs have a universal truth direction in their activation space, but generalization of this direction varies across conversational formats. A solution using fixed key phrases improves generalization, though challenges remain for reliable lie detection.


<details>
  <summary>Details</summary>
Motivation: To explore how the universal truth direction in LLMs generalizes across different conversational formats and improve its reliability for lie detection.

Method: Investigating generalization of the truth direction in short vs. long conversations and proposing a solution using fixed key phrases at the end of conversations.

Result: Good generalization in short conversations ending with lies, poor in longer ones with earlier lies. The key phrase solution significantly improves generalization.

Conclusion: While progress is made, reliable LLM lie detectors face challenges in generalizing to new settings.

Abstract: Several recent works argue that LLMs have a universal truth direction where
true and false statements are linearly separable in the activation space of the
model. It has been demonstrated that linear probes trained on a single hidden
state of the model already generalize across a range of topics and might even
be used for lie detection in LLM conversations. In this work we explore how
this truth direction generalizes between various conversational formats. We
find good generalization between short conversations that end on a lie, but
poor generalization to longer formats where the lie appears earlier in the
input prompt. We propose a solution that significantly improves this type of
generalization by adding a fixed key phrase at the end of each conversation.
Our results highlight the challenges towards reliable LLM lie detectors that
generalize to new settings.

</details>


### [10] [KRISTEVA: Close Reading as a Novel Task for Benchmarking Interpretive Reasoning](https://arxiv.org/pdf/2505.09825)
*Peiqi Sui, Juan Diego Rodriguez, Philippe Laban, Dean Murphy, Joseph P. Dexter, Richard Jean So, Samuel Baker, Pramit Chaudhuri*

Main category: cs.CL

TL;DR: KRISTEVA is the first benchmark for evaluating close reading in LLMs, testing interpretive reasoning with 1331 questions. LLMs show some competency but lag behind humans.


<details>
  <summary>Details</summary>
Motivation: Close reading is foundational in education but untested in LLMs. KRISTEVA fills this gap by evaluating LLMs' interpretive reasoning.

Method: Adapted 1331 classroom questions into three task sets: stylistic feature extraction, contextual retrieval, and multi-hop reasoning.

Result: LLMs achieved 49.7%-69.7% accuracy, trailing humans in 10 of 11 tasks.

Conclusion: LLMs have emerging close reading skills but need improvement to match human performance.

Abstract: Each year, tens of millions of essays are written and graded in college-level
English courses. Students are asked to analyze literary and cultural texts
through a process known as close reading, in which they gather textual details
to formulate evidence-based arguments. Despite being viewed as a basis for
critical thinking and widely adopted as a required element of university
coursework, close reading has never been evaluated on large language models
(LLMs), and multi-discipline benchmarks like MMLU do not include literature as
a subject. To fill this gap, we present KRISTEVA, the first close reading
benchmark for evaluating interpretive reasoning, consisting of 1331
multiple-choice questions adapted from classroom data. With KRISTEVA, we
propose three progressively more difficult sets of tasks to approximate
different elements of the close reading process, which we use to test how well
LLMs may seem to understand and reason about literary works: 1) extracting
stylistic features, 2) retrieving relevant contextual information from
parametric knowledge, and 3) multi-hop reasoning between style and external
contexts. Our baseline results find that, while state-of-the-art LLMs possess
some college-level close reading competency (accuracy 49.7% - 69.7%), their
performances still trail those of experienced human evaluators on 10 out of our
11 tasks.

</details>


### [11] [Do Large Language Models Know Conflict? Investigating Parametric vs. Non-Parametric Knowledge of LLMs for Conflict Forecasting](https://arxiv.org/pdf/2505.09852)
*Apollinaire Poli Nemkova, Sarath Chandra Lingareddy, Sagnik Ray Choudhury, Mark V. Albert*

Main category: cs.CL

TL;DR: LLMs are tested for conflict forecasting, comparing parametric (pretrained) and non-parametric (external data) methods, showing benefits of external knowledge.


<details>
  <summary>Details</summary>
Motivation: To explore LLMs' potential in predicting violent conflict, aiding early warning systems and policy-making.

Method: Two-part evaluation: parametric (pretrained weights) vs. non-parametric (external data via RAG) on conflict trends and fatalities.

Result: LLMs show promise but benefit from external data for accurate conflict forecasting.

Conclusion: Augmenting LLMs with external knowledge enhances their conflict prediction capabilities.

Abstract: Large Language Models (LLMs) have shown impressive performance across natural
language tasks, but their ability to forecast violent conflict remains
underexplored. We investigate whether LLMs possess meaningful parametric
knowledge-encoded in their pretrained weights-to predict conflict escalation
and fatalities without external data. This is critical for early warning
systems, humanitarian planning, and policy-making. We compare this parametric
knowledge with non-parametric capabilities, where LLMs access structured and
unstructured context from conflict datasets (e.g., ACLED, GDELT) and recent
news reports via Retrieval-Augmented Generation (RAG). Incorporating external
information could enhance model performance by providing up-to-date context
otherwise missing from pretrained weights. Our two-part evaluation framework
spans 2020-2024 across conflict-prone regions in the Horn of Africa and the
Middle East. In the parametric setting, LLMs predict conflict trends and
fatalities relying only on pretrained knowledge. In the non-parametric setting,
models receive summaries of recent conflict events, indicators, and
geopolitical developments. We compare predicted conflict trend labels (e.g.,
Escalate, Stable Conflict, De-escalate, Peace) and fatalities against
historical data. Our findings highlight the strengths and limitations of LLMs
for conflict forecasting and the benefits of augmenting them with structured
external knowledge.

</details>


### [12] [Crossing Borders Without Crossing Boundaries: How Sociolinguistic Awareness Can Optimize User Engagement with Localized Spanish AI Models Across Hispanophone Countries](https://arxiv.org/pdf/2505.09902)
*Martin Capdevila, Esteban Villa Turek, Ellen Karina Chumbe Fernandez, Luis Felipe Polo Galvez, Luis Cadavid, Andrea Marroquin, Rebeca Vargas Quesada, Johanna Crew, Nicole Vallejo Galarraga, Christopher Rodriguez, Diego Gutierrez, Radhi Datla*

Main category: cs.CL

TL;DR: The paper highlights the need for localized Spanish language models in AI to address sociolinguistic differences across regions, improving inclusivity and user trust.


<details>
  <summary>Details</summary>
Motivation: To address the significant gaps in Spanish language use across Latin America and Spain, emphasizing the need for locale-sensitive AI models.

Method: Examines sociocultural and linguistic differences in Spanish variants, proposing five sub-variants for localization.

Result: Locale-sensitive AI models can bridge sociolinguistic divides, enhance inclusivity, and boost user growth.

Conclusion: Implementing localized Spanish variants fosters trust, cultural awareness, and supports internationalization strategies.

Abstract: Large language models are, by definition, based on language. In an effort to
underscore the critical need for regional localized models, this paper examines
primary differences between variants of written Spanish across Latin America
and Spain, with an in-depth sociocultural and linguistic contextualization
therein. We argue that these differences effectively constitute significant
gaps in the quotidian use of Spanish among dialectal groups by creating
sociolinguistic dissonances, to the extent that locale-sensitive AI models
would play a pivotal role in bridging these divides. In doing so, this approach
informs better and more efficient localization strategies that also serve to
more adequately meet inclusivity goals, while securing sustainable active daily
user growth in a major low-risk investment geographic area. Therefore,
implementing at least the proposed five sub variants of Spanish addresses two
lines of action: to foment user trust and reliance on AI language models while
also demonstrating a level of cultural, historical, and sociolinguistic
awareness that reflects positively on any internationalization strategy.

</details>


### [13] [uDistil-Whisper: Label-Free Data Filtering for Knowledge Distillation in Low-Data Regimes](https://arxiv.org/pdf/2407.01257)
*Abdul Waheed, Karima Kadaoui, Bhiksha Raj, Muhammad Abdul-Mageed*

Main category: cs.CL

TL;DR: A distillation framework for Whisper models eliminates the need for labeled data, outperforming the teacher model by 5-7 WER points and achieving efficiency gains of 25-50%.


<details>
  <summary>Details</summary>
Motivation: Current distillation methods rely on human labels and large datasets, limiting applicability in low-resource settings.

Method: Proposes a label-free distillation framework, filtering high-quality pseudo-labels without ground truth.

Result: Best-distilled models outperform the teacher and supervised setups, especially with scaled data.

Conclusion: The framework enables efficient, high-performance models without labeled data, broadening applicability.

Abstract: Recent work on distilling Whisper's knowledge into small models using
pseudo-labels shows promising performance while reducing the size by up to 50%.
This results in small, efficient, and dedicated models. However, a critical
step of distillation using pseudo-labels involves filtering high-quality
predictions and using only those during training. This step requires ground
truth labels to compare with and filter low-quality examples, making the
process dependent on human labels. Additionally, the distillation process
requires a large amount of data thereby limiting its applicability in
low-resource settings. To address this, we propose a distillation framework
that does not require any labeled data. Through experimentation, we show that
our best-distilled models outperform the teacher model by 5-7 WER points and
are on par with or outperform similar supervised data filtering setups. When
scaling the data, our models significantly outperform all zero-shot and
supervised models. Our models are also 25-50% more compute- and
memory-efficient while maintaining performance equal to or better than that of
the teacher model. For more details about our models, dataset, and other
resources, please visit our GitHub page:
https://github.com/UBC-NLP/uDistilWhisper.

</details>


### [14] [MultiMed: Multilingual Medical Speech Recognition via Attention Encoder Decoder](https://arxiv.org/pdf/2409.14074)
*Khai Le-Duc, Phuc Phan, Tan-Hanh Pham, Bach Phan Tat, Minh-Huong Ngo, Chris Ngo, Thanh Nguyen-Tang, Truong-Son Hy*

Main category: cs.CL

TL;DR: MultiMed introduces the first multilingual medical ASR dataset and models, covering five languages, and provides extensive analysis and training schemes.


<details>
  <summary>Details</summary>
Motivation: To improve patient care by enabling multilingual communication in healthcare, addressing workforce shortages, and enhancing diagnosis/treatment.

Method: Creation of the MultiMed dataset, development of end-to-end ASR models, and conducting multilinguality studies with empirical baselines and comparative analyses.

Result: MultiMed is the largest medical ASR dataset, with models and training schemes optimized for industry use.

Conclusion: The work advances multilingual medical ASR, offering practical tools and insights for real-world applications.

Abstract: Multilingual automatic speech recognition (ASR) in the medical domain serves
as a foundational task for various downstream applications such as speech
translation, spoken language understanding, and voice-activated assistants.
This technology improves patient care by enabling efficient communication
across language barriers, alleviating specialized workforce shortages, and
facilitating improved diagnosis and treatment, particularly during pandemics.
In this work, we introduce MultiMed, the first multilingual medical ASR
dataset, along with the first collection of small-to-large end-to-end medical
ASR models, spanning five languages: Vietnamese, English, German, French, and
Mandarin Chinese. To our best knowledge, MultiMed stands as the world's largest
medical ASR dataset across all major benchmarks: total duration, number of
recording conditions, number of accents, and number of speaking roles.
Furthermore, we present the first multilinguality study for medical ASR, which
includes reproducible empirical baselines, a monolinguality-multilinguality
analysis, Attention Encoder Decoder (AED) vs Hybrid comparative study and a
linguistic analysis. We present practical ASR end-to-end training schemes
optimized for a fixed number of trainable parameters that are common in
industry settings. All code, data, and models are available online:
https://github.com/leduckhai/MultiMed/tree/master/MultiMed.

</details>


### [15] [From Trade-off to Synergy: A Versatile Symbiotic Watermarking Framework for Large Language Models](https://arxiv.org/pdf/2505.09924)
*Yidan Wang, Yubing Ren, Yanan Cao, Binxing Fang*

Main category: cs.CL

TL;DR: A hybrid watermarking framework for LLMs combines logits-based and sampling-based methods, optimizing robustness, text quality, and security, achieving SOTA performance.


<details>
  <summary>Details</summary>
Motivation: Addressing trade-offs in existing LLM watermarking schemes by integrating logits-based and sampling-based approaches for better performance.

Method: Proposes a versatile symbiotic watermarking framework with serial, parallel, and hybrid strategies, using token and semantic entropy for adaptive embedding.

Result: Outperforms existing baselines, achieving state-of-the-art performance in detectability, robustness, text quality, and security.

Conclusion: The framework offers novel insights into watermarking paradigms and is validated through comprehensive experiments.

Abstract: The rise of Large Language Models (LLMs) has heightened concerns about the
misuse of AI-generated text, making watermarking a promising solution.
Mainstream watermarking schemes for LLMs fall into two categories: logits-based
and sampling-based. However, current schemes entail trade-offs among
robustness, text quality, and security. To mitigate this, we integrate
logits-based and sampling-based schemes, harnessing their respective strengths
to achieve synergy. In this paper, we propose a versatile symbiotic
watermarking framework with three strategies: serial, parallel, and hybrid. The
hybrid framework adaptively embeds watermarks using token entropy and semantic
entropy, optimizing the balance between detectability, robustness, text
quality, and security. Furthermore, we validate our approach through
comprehensive experiments on various datasets and models. Experimental results
indicate that our method outperforms existing baselines and achieves
state-of-the-art (SOTA) performance. We believe this framework provides novel
insights into diverse watermarking paradigms. Our code is available at
\href{https://github.com/redwyd/SymMark}{https://github.com/redwyd/SymMark}.

</details>


### [16] [Interim Report on Human-Guided Adaptive Hyperparameter Optimization with Multi-Fidelity Sprints](https://arxiv.org/pdf/2505.09792)
*Michael Kamfonas*

Main category: cs.CL

TL;DR: A phased hyperparameter optimization process compares multitask NLP model variants using multiphase learning rate scheduling and optimizer grouping, with Bayesian optimization and human guidance.


<details>
  <summary>Details</summary>
Motivation: To efficiently optimize hyperparameters for multitask NLP models, leveraging advanced techniques like Bayesian optimization and human guidance.

Method: Uses Bayesian optimization with Optuna TPE sampler, Hyperband pruner, and Scikit-Learn Gaussian process. Includes low-fidelity sprints for pruning and meta-learner for threshold tuning.

Result: Demonstrated on variants of the 2021 Joint Entity and Relation Extraction model.

Conclusion: The approach efficiently optimizes hyperparameters and resolves classification probabilities, validated on a specific NLP model.

Abstract: This case study applies a phased hyperparameter optimization process to
compare multitask natural language model variants that utilize multiphase
learning rate scheduling and optimizer parameter grouping. We employ short,
Bayesian optimization sessions that leverage multi-fidelity, hyperparameter
space pruning, progressive halving, and a degree of human guidance. We utilize
the Optuna TPE sampler and Hyperband pruner, as well as the Scikit-Learn
Gaussian process minimization. Initially, we use efficient low-fidelity sprints
to prune the hyperparameter space. Subsequent sprints progressively increase
their model fidelity and employ hyperband pruning for efficiency. A second
aspect of our approach is using a meta-learner to tune threshold values to
resolve classification probabilities during inference. We demonstrate our
method on a collection of variants of the 2021 Joint Entity and Relation
Extraction model proposed by Eberts and Ulges.

</details>


### [17] [Rethinking Prompt Optimizers: From Prompt Merits to Optimization](https://arxiv.org/pdf/2505.09930)
*Zixiao Zhu, Hanzhang Zhou, Zijian Feng, Tianjiao Li, Chua Jia Jim Deryl, Mak Lee Onn, Gee Wah Ng, Kezhi Mao*

Main category: cs.CL

TL;DR: MePO introduces a merit-guided, lightweight prompt optimizer for LLMs, improving performance without relying on large-scale models or online optimization.


<details>
  <summary>Details</summary>
Motivation: Existing prompt optimization methods rely on advanced LLMs, which can degrade performance in lightweight models due to incompatibility. MePO addresses this by focusing on interpretable, model-agnostic prompt merits.

Method: MePO is trained on a preference dataset of merit-aligned prompts from a lightweight LLM, avoiding reliance on large-scale models or online optimization.

Result: MePO achieves better results across diverse tasks and model types, offering a scalable and robust solution.

Conclusion: MePO provides a cost-effective, privacy-preserving, and generalizable approach to prompt optimization for both large and lightweight LLMs.

Abstract: Prompt optimization (PO) offers a practical alternative to fine-tuning large
language models (LLMs), enabling performance improvements without altering
model weights. Existing methods typically rely on advanced, large-scale LLMs
like GPT-4 to generate optimized prompts. However, due to limited downward
compatibility, verbose, instruction-heavy prompts from advanced LLMs can
overwhelm lightweight inference models and degrade response quality. In this
work, we rethink prompt optimization through the lens of interpretable design.
We first identify a set of model-agnostic prompt quality merits and empirically
validate their effectiveness in enhancing prompt and response quality. We then
introduce MePO, a merit-guided, lightweight, and locally deployable prompt
optimizer trained on our preference dataset built from merit-aligned prompts
generated by a lightweight LLM. Unlike prior work, MePO avoids online
optimization reliance, reduces cost and privacy concerns, and, by learning
clear, interpretable merits, generalizes effectively to both large-scale and
lightweight inference models. Experiments demonstrate that MePO achieves better
results across diverse tasks and model types, offering a scalable and robust
solution for real-world deployment. Our model and dataset are available at:
https://github.com/MidiyaZhu/MePO

</details>


### [18] [Personalizing Large Language Models using Retrieval Augmented Generation and Knowledge Graph](https://arxiv.org/pdf/2505.09945)
*Deeksha Prahlad, Chanhee Lee, Dongha Kim, Hokeun Kim*

Main category: cs.CL

TL;DR: The paper proposes retrieval augmented generation (RAG) with knowledge graphs (KGs) to reduce hallucinations in LLMs by providing timely, factual, and personalized data, focusing on calendar data.


<details>
  <summary>Details</summary>
Motivation: LLMs often generate incorrect or extra data (hallucinations) due to lack of timely and personalized information.

Method: Introduces RAG using KGs to supply structured, updated data (e.g., calendar info) to LLMs for personalized responses.

Result: The approach outperforms baseline LLMs in accuracy for personal data responses, with a slight reduction in response time.

Conclusion: Using KGs with RAG improves LLM response accuracy for personalized queries, especially with dynamic data like calendars.

Abstract: The advent of large language models (LLMs) has allowed numerous applications,
including the generation of queried responses, to be leveraged in chatbots and
other conversational assistants. Being trained on a plethora of data, LLMs
often undergo high levels of over-fitting, resulting in the generation of extra
and incorrect data, thus causing hallucinations in output generation. One of
the root causes of such problems is the lack of timely, factual, and
personalized information fed to the LLM. In this paper, we propose an approach
to address these problems by introducing retrieval augmented generation (RAG)
using knowledge graphs (KGs) to assist the LLM in personalized response
generation tailored to the users. KGs have the advantage of storing
continuously updated factual information in a structured way. While our KGs can
be used for a variety of frequently updated personal data, such as calendar,
contact, and location data, we focus on calendar data in this paper. Our
experimental results show that our approach works significantly better in
understanding personal information and generating accurate responses compared
to the baseline LLMs using personal data as text inputs, with a moderate
reduction in response time.

</details>


### [19] [DIF: A Framework for Benchmarking and Verifying Implicit Bias in LLMs](https://arxiv.org/pdf/2505.10013)
*Lake Yin, Fan Huang*

Main category: cs.CL

TL;DR: The paper introduces DIF (Demographic Implicit Fairness), a method to benchmark implicit bias in LLMs, revealing an inverse trend between bias and accuracy.


<details>
  <summary>Details</summary>
Motivation: Addressing the lack of standard methods to measure implicit bias in LLMs, which is both an ethical and technical issue.

Method: Developed DIF by evaluating LLM logic/math problem datasets with sociodemographic personas.

Result: Statistically validated implicit bias in LLMs and found an inverse relationship between bias and accuracy.

Conclusion: DIF provides a standardized benchmark for implicit bias, highlighting its impact on LLM performance.

Abstract: As Large Language Models (LLMs) have risen in prominence over the past few
years, there has been concern over the potential biases in LLMs inherited from
the training data. Previous studies have examined how LLMs exhibit implicit
bias, such as when response generation changes when different social contexts
are introduced. We argue that this implicit bias is not only an ethical, but
also a technical issue, as it reveals an inability of LLMs to accommodate
extraneous information. However, unlike other measures of LLM intelligence,
there are no standard methods to benchmark this specific subset of LLM bias. To
bridge this gap, we developed a method for calculating an easily interpretable
benchmark, DIF (Demographic Implicit Fairness), by evaluating preexisting LLM
logic and math problem datasets with sociodemographic personas. We demonstrate
that this method can statistically validate the presence of implicit bias in
LLM behavior and find an inverse trend between question answering accuracy and
implicit bias, supporting our argument.

</details>


### [20] [CAFE: Retrieval Head-based Coarse-to-Fine Information Seeking to Enhance Multi-Document QA Capability](https://arxiv.org/pdf/2505.10063)
*Han Peng, Jinhao Jiang, Zican Dong, Wayne Xin Zhao, Lei Fang*

Main category: cs.CL

TL;DR: CAFE is a two-stage method improving LLMs' question-answering in long-context inputs by coarse-to-fine document filtering and attention steering.


<details>
  <summary>Details</summary>
Motivation: LLMs struggle with retrieval and reasoning in long-context inputs, needing better precision-recall balance.

Method: Coarse-grained filtering ranks documents; fine-grained steering focuses attention on relevant content.

Result: CAFE outperforms baselines, achieving up to 22.1% and 13.7% SubEM improvement over SFT and RAG methods.

Conclusion: CAFE effectively enhances multi-document QA by reducing noise and improving evidence reliance.

Abstract: Advancements in Large Language Models (LLMs) have extended their input
context length, yet they still struggle with retrieval and reasoning in
long-context inputs. Existing methods propose to utilize the prompt strategy
and retrieval head to alleviate this limitation. However, they still face
challenges in balancing retrieval precision and recall, impacting their
efficacy in answering questions. To address this, we introduce $\textbf{CAFE}$,
a two-stage coarse-to-fine method to enhance multi-document question-answering
capacities. By gradually eliminating the negative impacts of background and
distracting documents, CAFE makes the responses more reliant on the evidence
documents. Initially, a coarse-grained filtering method leverages retrieval
heads to identify and rank relevant documents. Then, a fine-grained steering
method guides attention to the most relevant content. Experiments across
benchmarks show CAFE outperforms baselines, achieving up to 22.1% and 13.7%
SubEM improvement over SFT and RAG methods on the Mistral model, respectively.

</details>


### [21] [Dark LLMs: The Growing Threat of Unaligned AI Models](https://arxiv.org/pdf/2505.10066)
*Michael Fire, Yitzhak Elbazis, Adi Wasenstein, Lior Rokach*

Main category: cs.CL

TL;DR: LLMs are vulnerable to jailbreaking due to unfiltered training data, and a universal attack method was found to compromise multiple models, revealing gaps in AI safety practices.


<details>
  <summary>Details</summary>
Motivation: The study aims to highlight the risks of LLMs being exploited through jailbreak attacks, emphasizing the need for better safety measures.

Method: The research identified a universal jailbreak attack that bypasses safety controls in multiple state-of-the-art LLMs.

Result: Many tested LLMs remained vulnerable to the attack, and industry responses to disclosures were inadequate.

Conclusion: Without intervention, LLMs could democratize access to harmful knowledge, posing significant risks.

Abstract: Large Language Models (LLMs) rapidly reshape modern life, advancing fields
from healthcare to education and beyond. However, alongside their remarkable
capabilities lies a significant threat: the susceptibility of these models to
jailbreaking. The fundamental vulnerability of LLMs to jailbreak attacks stems
from the very data they learn from. As long as this training data includes
unfiltered, problematic, or 'dark' content, the models can inherently learn
undesirable patterns or weaknesses that allow users to circumvent their
intended safety controls. Our research identifies the growing threat posed by
dark LLMs models deliberately designed without ethical guardrails or modified
through jailbreak techniques. In our research, we uncovered a universal
jailbreak attack that effectively compromises multiple state-of-the-art models,
enabling them to answer almost any question and produce harmful outputs upon
request. The main idea of our attack was published online over seven months
ago. However, many of the tested LLMs were still vulnerable to this attack.
Despite our responsible disclosure efforts, responses from major LLM providers
were often inadequate, highlighting a concerning gap in industry practices
regarding AI safety. As model training becomes more accessible and cheaper, and
as open-source LLMs proliferate, the risk of widespread misuse escalates.
Without decisive intervention, LLMs may continue democratizing access to
dangerous knowledge, posing greater risks than anticipated.

</details>


### [22] [Designing and Contextualising Probes for African Languages](https://arxiv.org/pdf/2505.10081)
*Wisdom Aduah, Francois Meyer*

Main category: cs.CL

TL;DR: The paper investigates how linguistic knowledge is encoded in pretrained language models (PLMs) for African languages, finding that adapted PLMs outperform multilingual ones and that syntactic and semantic information are distributed differently across layers.


<details>
  <summary>Details</summary>
Motivation: To systematically study how linguistic features are encoded in PLMs for African languages, which is unclear despite their improving performance.

Method: Layer-wise probes for six African languages and control tasks for the MasakhaPOS dataset to analyze feature distribution and interpret probe performance.

Result: Adapted PLMs encode more linguistic knowledge than multilingual PLMs; syntactic info concentrates in middle-to-last layers, while semantic info is distributed across all layers.

Conclusion: The study clarifies the success of strategies like active learning and multilingual adaptation by applying interpretability techniques to African-language PLMs.

Abstract: Pretrained language models (PLMs) for African languages are continually
improving, but the reasons behind these advances remain unclear. This paper
presents the first systematic investigation into probing PLMs for linguistic
knowledge about African languages. We train layer-wise probes for six
typologically diverse African languages to analyse how linguistic features are
distributed. We also design control tasks, a way to interpret probe
performance, for the MasakhaPOS dataset. We find PLMs adapted for African
languages to encode more linguistic information about target languages than
massively multilingual PLMs. Our results reaffirm previous findings that
token-level syntactic information concentrates in middle-to-last layers, while
sentence-level semantic information is distributed across all layers. Through
control tasks and probing baselines, we confirm that performance reflects the
internal knowledge of PLMs rather than probe memorisation. Our study applies
established interpretability techniques to African-language PLMs. In doing so,
we highlight the internal mechanisms underlying the success of strategies like
active learning and multilingual adaptation.

</details>


### [23] [XRAG: Cross-lingual Retrieval-Augmented Generation](https://arxiv.org/pdf/2505.10089)
*Wei Liu, Sony Trenous, Leonardo F. R. Ribeiro, Bill Byrne, Felix Hieber*

Main category: cs.CL

TL;DR: XRAG is a new benchmark for evaluating LLMs in cross-lingual RAG settings, highlighting challenges in response language correctness and multilingual reasoning.


<details>
  <summary>Details</summary>
Motivation: To address the gap in evaluating LLMs' generation abilities in cross-lingual RAG scenarios where user language and retrieval results differ.

Method: Constructed from recent news articles, XRAG includes monolingual and multilingual retrieval scenarios with relevancy annotations.

Result: Five LLMs tested revealed challenges in response language correctness (monolingual) and cross-lingual reasoning (multilingual).

Conclusion: XRAG is a valuable benchmark for studying LLM reasoning, especially in cross-lingual contexts.

Abstract: We propose XRAG, a novel benchmark designed to evaluate the generation
abilities of LLMs in cross-lingual Retrieval-Augmented Generation (RAG)
settings where the user language does not match the retrieval results. XRAG is
constructed from recent news articles to ensure that its questions require
external knowledge to be answered. It covers the real-world scenarios of
monolingual and multilingual retrieval, and provides relevancy annotations for
each retrieved document. Our novel dataset construction pipeline results in
questions that require complex reasoning, as evidenced by the significant gap
between human and LLM performance. Consequently, XRAG serves as a valuable
benchmark for studying LLM reasoning abilities, even before considering the
additional cross-lingual complexity. Experimental results on five LLMs uncover
two previously unreported challenges in cross-lingual RAG: 1) in the
monolingual retrieval setting, all evaluated models struggle with response
language correctness; 2) in the multilingual retrieval setting, the main
challenge lies in reasoning over retrieved information across languages rather
than generation of non-English text.

</details>


### [24] [What Does Neuro Mean to Cardio? Investigating the Role of Clinical Specialty Data in Medical LLMs](https://arxiv.org/pdf/2505.10113)
*Xinlan Yan, Di Wu, Yibin Lei, Christof Monz, Iacer Calixto*

Main category: cs.CL

TL;DR: S-MedQA is an English medical QA dataset for benchmarking LLMs in clinical specialties. Findings show specialty training doesn't guarantee best performance, and gains come from domain shifting, not knowledge injection.


<details>
  <summary>Details</summary>
Motivation: To benchmark LLMs in medical QA and test the hypothesis about knowledge injection in specialized clinical scenarios.

Method: Created S-MedQA dataset, fine-tuned models on specialty data, and analyzed token probabilities of clinically relevant terms.

Result: Specialty training doesn't ensure best performance; gains are from domain shifting, not knowledge injection.

Conclusion: Rethink the role of fine-tuning data in medical QA, emphasizing domain shifting over knowledge injection.

Abstract: In this paper, we introduce S-MedQA, an English medical question-answering
(QA) dataset for benchmarking large language models in fine-grained clinical
specialties. We use S-MedQA to check the applicability of a popular hypothesis
related to knowledge injection in the knowledge-intense scenario of medical QA,
and show that: 1) training on data from a speciality does not necessarily lead
to best performance on that specialty and 2) regardless of the specialty
fine-tuned on, token probabilities of clinically relevant terms for all
specialties increase consistently. Thus, we believe improvement gains come
mostly from domain shifting (e.g., general to medical) rather than knowledge
injection and suggest rethinking the role of fine-tuning data in the medical
domain. We release S-MedQA and all code needed to reproduce all our experiments
to the research community.

</details>


### [25] [GE-Chat: A Graph Enhanced RAG Framework for Evidential Response Generation of LLMs](https://arxiv.org/pdf/2505.10143)
*Longchao Da, Parth Mitesh Shah, Kuan-Ru Liou, Jiaxing Zhang, Hua Wei*

Main category: cs.CL

TL;DR: GE-Chat enhances LLM responses by integrating a knowledge graph and retrieval-augmented generation, improving evidence-based accuracy and trustworthiness.


<details>
  <summary>Details</summary>
Motivation: Address LLM unreliability and hallucinations by providing evidence-backed responses.

Method: Uses knowledge graphs, retrieval-augmented generation, CoT logic, n-hop sub-graph searching, and entailment-based sentence generation.

Result: Improves evidence retrieval and response reliability in free-form contexts.

Conclusion: GE-Chat offers a trustworthy framework for LLM-assisted decision-making.

Abstract: Large Language Models are now key assistants in human decision-making
processes. However, a common note always seems to follow: "LLMs can make
mistakes. Be careful with important info." This points to the reality that not
all outputs from LLMs are dependable, and users must evaluate them manually.
The challenge deepens as hallucinated responses, often presented with seemingly
plausible explanations, create complications and raise trust issues among
users. To tackle such issue, this paper proposes GE-Chat, a knowledge Graph
enhanced retrieval-augmented generation framework to provide Evidence-based
response generation. Specifically, when the user uploads a material document, a
knowledge graph will be created, which helps construct a retrieval-augmented
agent, enhancing the agent's responses with additional knowledge beyond its
training corpus. Then we leverage Chain-of-Thought (CoT) logic generation,
n-hop sub-graph searching, and entailment-based sentence generation to realize
accurate evidence retrieval. We demonstrate that our method improves the
existing models' performance in terms of identifying the exact evidence in a
free-form context, providing a reliable way to examine the resources of LLM's
conclusion and help with the judgment of the trustworthiness.

</details>


### [26] [Mining Hidden Thoughts from Texts: Evaluating Continual Pretraining with Synthetic Data for LLM Reasoning](https://arxiv.org/pdf/2505.10182)
*Yoichi Ishibashi, Taro Yano, Masafumi Oyamada*

Main category: cs.CL

TL;DR: Reasoning CPT, using synthetic data to model hidden thought processes, improves LLM performance across domains, especially on harder problems, with up to 8-point gains.


<details>
  <summary>Details</summary>
Motivation: Current LLM training methods (supervised fine-tuning, reinforcement learning) are domain-specific, limiting scalability. Reasoning CPT avoids this by using synthetic data to generalize reasoning skills.

Method: Applied Reasoning CPT to Gemma2-9B using synthetic data with hidden thoughts from STEM and Law corpora, compared to standard CPT on MMLU benchmark.

Result: Reasoning CPT consistently outperforms standard CPT, with domain transferability and greater gains (up to 8 points) on harder problems. Models also adapt reasoning depth to problem difficulty.

Conclusion: Reasoning CPT is a scalable, domain-agnostic approach that enhances LLM reasoning, with potential for broader applications.

Abstract: Large Language Models (LLMs) have demonstrated significant improvements in
reasoning capabilities through supervised fine-tuning and reinforcement
learning. However, when training reasoning models, these approaches are
primarily applicable to specific domains such as mathematics and programming,
which imposes fundamental constraints on the breadth and scalability of
training data. In contrast, continual pretraining (CPT) offers the advantage of
not requiring task-specific signals. Nevertheless, how to effectively
synthesize training data for reasoning and how such data affect a wide range of
domains remain largely unexplored. This study provides a detailed evaluation of
Reasoning CPT, a form of CPT that uses synthetic data to reconstruct the hidden
thought processes underlying texts, based on the premise that texts are the
result of the author's thinking process. Specifically, we apply Reasoning CPT
to Gemma2-9B using synthetic data with hidden thoughts derived from STEM and
Law corpora, and compare it to standard CPT on the MMLU benchmark. Our analysis
reveals that Reasoning CPT consistently improves performance across all
evaluated domains. Notably, reasoning skills acquired in one domain transfer
effectively to others; the performance gap with conventional methods widens as
problem difficulty increases, with gains of up to 8 points on the most
challenging problems. Furthermore, models trained with hidden thoughts learn to
adjust the depth of their reasoning according to problem difficulty.

</details>


### [27] [The CoT Encyclopedia: Analyzing, Predicting, and Controlling how a Reasoning Model will Think](https://arxiv.org/pdf/2505.10185)
*Seongyun Lee, Seungone Kim, Minju Seo, Yongrae Jo, Dongyoung Go, Hyeonbin Hwang, Jinho Park, Xiang Yue, Sean Welleck, Graham Neubig, Moontae Lee, Minjoon Seo*

Main category: cs.CL

TL;DR: The paper introduces the CoT Encyclopedia, a framework for analyzing and steering model reasoning by extracting and clustering diverse reasoning criteria from model-generated CoTs, improving interpretability and performance.


<details>
  <summary>Details</summary>
Motivation: Limited understanding of reasoning strategies in large language models and the constraints of predefined categorization methods.

Method: Automatically extracts reasoning criteria, embeds them into a semantic space, clusters them, and derives contrastive rubrics for interpretation.

Result: Produces more interpretable analyses, enables performance gains by predicting and guiding model strategies, and reveals training data format's impact on reasoning.

Conclusion: The CoT Encyclopedia offers a comprehensive and practical framework for understanding and improving model reasoning, emphasizing the importance of data format in model design.

Abstract: Long chain-of-thought (CoT) is an essential ingredient in effective usage of
modern large language models, but our understanding of the reasoning strategies
underlying these capabilities remains limited. While some prior works have
attempted to categorize CoTs using predefined strategy types, such approaches
are constrained by human intuition and fail to capture the full diversity of
model behaviors. In this work, we introduce the CoT Encyclopedia, a bottom-up
framework for analyzing and steering model reasoning. Our method automatically
extracts diverse reasoning criteria from model-generated CoTs, embeds them into
a semantic space, clusters them into representative categories, and derives
contrastive rubrics to interpret reasoning behavior. Human evaluations show
that this framework produces more interpretable and comprehensive analyses than
existing methods. Moreover, we demonstrate that this understanding enables
performance gains: we can predict which strategy a model is likely to use and
guide it toward more effective alternatives. Finally, we provide practical
insights, such as that training data format (e.g., free-form vs.
multiple-choice) has a far greater impact on reasoning behavior than data
domain, underscoring the importance of format-aware model design.

</details>


### [28] [VQ-Logits: Compressing the Output Bottleneck of Large Language Models via Vector Quantized Logits](https://arxiv.org/pdf/2505.10202)
*Jintian Shao, Hongyi Huang, Jiayi Wu, YiMing Cheng, ZhiYu Wu, You Shan, MingKai Zheng*

Main category: cs.CL

TL;DR: VQ-Logits reduces LLM output layer parameters and computational costs using Vector Quantization, achieving 99% parameter reduction and 6x speedup with minimal perplexity increase.


<details>
  <summary>Details</summary>
Motivation: Address computational and memory challenges in LLMs caused by large output vocabularies and the costly final linear projection layer.

Method: Proposes VQ-Logits, replacing the large output embedding matrix with a small shared codebook of embedding vectors, predicting logits over the codebook and scattering them to the full vocabulary.

Result: Achieves 99% parameter reduction, 6x speedup in logit computation, and only a 4% perplexity increase on benchmarks like WikiText-103 and C4.

Conclusion: VQ-Logits is a robust and effective method for reducing LLM output layer complexity with minimal performance trade-offs.

Abstract: Large Language Models (LLMs) have achieved remarkable success but face
significant computational and memory challenges, particularly due to their
extensive output vocabularies. The final linear projection layer, mapping
hidden states to vocabulary-sized logits, often constitutes a substantial
portion of the model's parameters and computational cost during inference.
Existing methods like adaptive softmax or hierarchical softmax introduce
structural complexities. In this paper, we propose VQ-Logits, a novel approach
that leverages Vector Quantization (VQ) to drastically reduce the parameter
count and computational load of the LLM output layer. VQ-Logits replaces the
large V * dmodel output embedding matrix with a small, shared codebook of K
embedding vectors (K << V ). Each token in the vocabulary is mapped to one of
these K codebook vectors. The LLM predicts logits over this compact codebook,
which are then efficiently "scattered" to the full vocabulary space using the
learned or preassigned mapping. We demonstrate through extensive experiments on
standard language modeling benchmarks (e.g., WikiText-103, C4) that VQ-Logits
can achieve up to 99% parameter reduction in the output layer and 6x speedup in
logit computation, with only a marginal 4% increase in perplexity compared to
full softmax baselines. We further provide detailed ablation studies on
codebook size, initialization, and learning strategies, showcasing the
robustness and effectiveness of our approach.

</details>


### [29] [RAIDEN-R1: Improving Role-awareness of LLMs via GRPO with Verifiable Reward](https://arxiv.org/pdf/2505.10218)
*Zongsheng Wang, Kaili Sun, Bowen Wu, Qun Yu, Ying Li, Baoxun Wang*

Main category: cs.CL

TL;DR: RAIDEN-R1, a reinforcement learning framework with Verifiable Role-Awareness Reward (VRAR), improves role consistency in RPCAs by using singular and multi-term mining strategies. It achieves high accuracy on benchmarks and enhances reasoning coherence.


<details>
  <summary>Details</summary>
Motivation: Addressing the challenge of maintaining role consistency in role-playing conversational agents (RPCAs).

Method: Proposes RAIDEN-R1, integrating VRAR for quantifiable rewards, and constructs a role-aware Chain-of-Thought dataset via multi-LLM collaboration.

Result: The 14B-GRPO model achieves 88.04% and 88.65% accuracy on Script-Based Knowledge and Conversation Memory metrics, outperforming baselines.

Conclusion: RAIDEN-R1 bridges the non-quantifiability gap in RPCA training and advances role-aware reasoning, improving RPCA development.

Abstract: Role-playing conversational agents (RPCAs) face persistent challenges in
maintaining role consistency. To address this, we propose RAIDEN-R1, a novel
reinforcement learning framework that integrates Verifiable Role-Awareness
Reward (VRAR). The method introduces both singular and multi-term mining
strategies to generate quantifiable rewards by assessing role-specific keys.
Additionally, we construct a high-quality, role-aware Chain-of-Thought dataset
through multi-LLM collaboration, and implement experiments to enhance reasoning
coherence. Experiments on the RAIDEN benchmark demonstrate RAIDEN-R1's
superiority: our 14B-GRPO model achieves 88.04% and 88.65% accuracy on
Script-Based Knowledge and Conversation Memory metrics, respectively,
outperforming baseline models while maintaining robustness. Case analyses
further reveal the model's enhanced ability to resolve conflicting contextual
cues and sustain first-person narrative consistency. This work bridges the
non-quantifiability gap in RPCA training and provides insights into role-aware
reasoning patterns, advancing the development of RPCAs.

</details>


### [30] [Comparing LLM Text Annotation Skills: A Study on Human Rights Violations in Social Media Data](https://arxiv.org/pdf/2505.10260)
*Poli Apollinaire Nemkova, Solomon Ubani, Mark V. Albert*

Main category: cs.CL

TL;DR: The study evaluates GPT-3.5, GPT-4, LLAMA3, Mistral 7B, and Claude-2 for zero-shot and few-shot annotation of human rights violations in Russian and Ukrainian social media posts, comparing their performance to human annotations.


<details>
  <summary>Details</summary>
Motivation: To assess the reliability and applicability of LLMs for sensitive, domain-specific tasks in multilingual contexts, focusing on nuanced textual understanding.

Method: Comparison of LLM annotations (under varying prompting conditions and languages) against a gold standard of human double-annotated labels for 1000 samples.

Result: Analysis reveals model-specific error patterns, strengths, limitations, and cross-linguistic adaptability.

Conclusion: The study highlights the potential and challenges of LLMs for subjective, context-dependent tasks, informing real-world deployment.

Abstract: In the era of increasingly sophisticated natural language processing (NLP)
systems, large language models (LLMs) have demonstrated remarkable potential
for diverse applications, including tasks requiring nuanced textual
understanding and contextual reasoning. This study investigates the
capabilities of multiple state-of-the-art LLMs - GPT-3.5, GPT-4, LLAMA3,
Mistral 7B, and Claude-2 - for zero-shot and few-shot annotation of a complex
textual dataset comprising social media posts in Russian and Ukrainian.
Specifically, the focus is on the binary classification task of identifying
references to human rights violations within the dataset.
  To evaluate the effectiveness of these models, their annotations are compared
against a gold standard set of human double-annotated labels across 1000
samples. The analysis includes assessing annotation performance under different
prompting conditions, with prompts provided in both English and Russian.
Additionally, the study explores the unique patterns of errors and
disagreements exhibited by each model, offering insights into their strengths,
limitations, and cross-linguistic adaptability.
  By juxtaposing LLM outputs with human annotations, this research contributes
to understanding the reliability and applicability of LLMs for sensitive,
domain-specific tasks in multilingual contexts. It also sheds light on how
language models handle inherently subjective and context-dependent judgments, a
critical consideration for their deployment in real-world scenarios.

</details>


### [31] [The Evolving Landscape of Generative Large Language Models and Traditional Natural Language Processing in Medicine](https://arxiv.org/pdf/2505.10261)
*Rui Yang, Huitao Li, Matthew Yu Heng Wong, Yuhe Ke, Xin Li, Kunyu Yu, Jingchi Liao, Jonathan Chong Kai Liew, Sabarinath Vinod Nair, Jasmine Chiat Ling Ong, Irene Li, Douglas Teodoro, Chuan Hong, Daniel Shu Wei Ting, Nan Liu*

Main category: cs.CL

TL;DR: Generative LLMs excel in open-ended medical tasks, while traditional NLP leads in information extraction and analysis. Ethical use is crucial as these technologies evolve.


<details>
  <summary>Details</summary>
Motivation: To explore the differences between generative LLMs and traditional NLP in medical tasks, given their growing prominence.

Method: Analysis of 19,123 studies comparing the performance of generative LLMs and traditional NLP across various medical tasks.

Result: Generative LLMs outperform in open-ended tasks; traditional NLP is better for information extraction and analysis.

Conclusion: Ethical considerations are vital as these technologies advance to maximize their potential in medicine.

Abstract: Natural language processing (NLP) has been traditionally applied to medicine,
and generative large language models (LLMs) have become prominent recently.
However, the differences between them across different medical tasks remain
underexplored. We analyzed 19,123 studies, finding that generative LLMs
demonstrate advantages in open-ended tasks, while traditional NLP dominates in
information extraction and analysis tasks. As these technologies advance,
ethical use of them is essential to ensure their potential in medical
applications.

</details>


### [32] [From Questions to Clinical Recommendations: Large Language Models Driving Evidence-Based Clinical Decision Making](https://arxiv.org/pdf/2505.10282)
*Dubai Li, Nan Jiang, Kangping Huang, Ruiqi Tu, Shuyu Ouyang, Huayu Yu, Lin Qiao, Chen Yu, Tianshu Zhou, Danyang Tong, Qian Wang, Mengtao Li, Xiaofeng Zeng, Yu Tian, Xinping Tian, Jingsong Li*

Main category: cs.CL

TL;DR: Quicker, an LLM-powered clinical decision support system, automates evidence synthesis and generates recommendations, reducing decision-making time and improving accuracy.


<details>
  <summary>Details</summary>
Motivation: Challenges in integrating clinical evidence into practice due to workload and time constraints necessitate automated tools for efficient decision-making.

Method: Quicker automates the entire evidence synthesis process, from question decomposition to recommendation generation, using LLMs and interactive tools.

Result: Quicker outperformed human experts in retrieval sensitivity and literature screening, reducing recommendation time to 20-40 minutes with human collaboration.

Conclusion: Quicker demonstrates potential to enhance evidence-based clinical decisions by improving efficiency and reliability.

Abstract: Clinical evidence, derived from rigorous research and data analysis, provides
healthcare professionals with reliable scientific foundations for informed
decision-making. Integrating clinical evidence into real-time practice is
challenging due to the enormous workload, complex professional processes, and
time constraints. This highlights the need for tools that automate evidence
synthesis to support more efficient and accurate decision making in clinical
settings. This study introduces Quicker, an evidence-based clinical decision
support system powered by large language models (LLMs), designed to automate
evidence synthesis and generate clinical recommendations modeled after standard
clinical guideline development processes. Quicker implements a fully automated
chain that covers all phases, from questions to clinical recommendations, and
further enables customized decision-making through integrated tools and
interactive user interfaces. To evaluate Quicker's capabilities, we developed
the Q2CRBench-3 benchmark dataset, based on clinical guideline development
records for three different diseases. Experimental results highlighted
Quicker's strong performance, with fine-grained question decomposition tailored
to user preferences, retrieval sensitivities comparable to human experts, and
literature screening performance approaching comprehensive inclusion of
relevant studies. In addition, Quicker-assisted evidence assessment effectively
supported human reviewers, while Quicker's recommendations were more
comprehensive and logically coherent than those of clinicians. In system-level
testing, collaboration between a single reviewer and Quicker reduced the time
required for recommendation development to 20-40 minutes. In general, our
findings affirm the potential of Quicker to help physicians make quicker and
more reliable evidence-based clinical decisions.

</details>


### [33] [J1: Incentivizing Thinking in LLM-as-a-Judge via Reinforcement Learning](https://arxiv.org/pdf/2505.10320)
*Chenxi Whitehouse, Tianlu Wang, Ping Yu, Xian Li, Jason Weston, Ilia Kulikov, Swarnadeep Saha*

Main category: cs.CL

TL;DR: J1, a reinforcement learning approach, improves LLM-as-a-Judge models by incentivizing reasoning and reducing bias, outperforming larger models like DeepSeek-R1 and o1-mini.


<details>
  <summary>Details</summary>
Motivation: AI progress is limited by evaluation quality; stronger reasoning in LLM-as-a-Judge models is needed.

Method: J1 converts prompts to judgment tasks with verifiable rewards, training models via reinforcement learning.

Result: J1 outperforms existing 8B and 70B models, including DeepSeek-R1 and o1-mini, on benchmarks.

Conclusion: J1 enhances judgment quality by learning evaluation criteria, self-comparison, and re-evaluation.

Abstract: The progress of AI is bottlenecked by the quality of evaluation, and powerful
LLM-as-a-Judge models have proved to be a core solution. Improved judgment
ability is enabled by stronger chain-of-thought reasoning, motivating the need
to find the best recipes for training such models to think. In this work we
introduce J1, a reinforcement learning approach to training such models. Our
method converts both verifiable and non-verifiable prompts to judgment tasks
with verifiable rewards that incentivize thinking and mitigate judgment bias.
In particular, our approach outperforms all other existing 8B or 70B models
when trained at those sizes, including models distilled from DeepSeek-R1. J1
also outperforms o1-mini, and even R1 on some benchmarks, despite training a
smaller model. We provide analysis and ablations comparing Pairwise-J1 vs
Pointwise-J1 models, offline vs online training recipes, reward strategies,
seed prompts, and variations in thought length and content. We find that our
models make better judgments by learning to outline evaluation criteria,
comparing against self-generated reference answers, and re-evaluating the
correctness of model responses.

</details>


### [34] [LDIR: Low-Dimensional Dense and Interpretable Text Embeddings with Relative Representations](https://arxiv.org/pdf/2505.10354)
*Yile Wang, Zhanyu Shen, Hui Huang*

Main category: cs.CL

TL;DR: LDIR proposes low-dimensional, dense, and interpretable text embeddings using relative representations, balancing performance and interpretability.


<details>
  <summary>Details</summary>
Motivation: Existing text embeddings lack interpretability (e.g., SimCSE) or perform poorly (e.g., bag-of-words). Recent work (Benara et al., 2024) offers interpretability but with high dimensionality.

Method: LDIR uses farthest point sampling to create low-dimensional (under 500) dense embeddings where values indicate semantic relatedness to anchor texts.

Result: LDIR performs close to black-box models and outperforms interpretable baselines with fewer dimensions.

Conclusion: LDIR successfully combines performance and interpretability in text embeddings, validated on multiple NLP tasks.

Abstract: Semantic text representation is a fundamental task in the field of natural
language processing. Existing text embedding (e.g., SimCSE and LLM2Vec) have
demonstrated excellent performance, but the values of each dimension are
difficult to trace and interpret. Bag-of-words, as classic sparse interpretable
embeddings, suffers from poor performance. Recently, Benara et al. (2024)
propose interpretable text embeddings using large language models, which forms
"0/1" embeddings based on responses to a series of questions. These
interpretable text embeddings are typically high-dimensional (larger than
10,000). In this work, we propose Low-dimensional (lower than 500) Dense and
Interpretable text embeddings with Relative representations (LDIR). The
numerical values of its dimensions indicate semantic relatedness to different
anchor texts through farthest point sampling, offering both semantic
representation as well as a certain level of traceability and interpretability.
We validate LDIR on multiple semantic textual similarity, retrieval, and
clustering tasks. Extensive experimental results show that LDIR performs close
to the black-box baseline models and outperforms the interpretable embeddings
baselines with much fewer dimensions. Code is available at
https://github.com/szu-tera/LDIR.

</details>


### [35] [Coherent Language Reconstruction from Brain Recordings with Flexible Multi-Modal Input Stimuli](https://arxiv.org/pdf/2505.10356)
*Chunyu Ye, Shaonan Wang*

Main category: cs.CL

TL;DR: A unified framework for reconstructing language from brain activity across visual, auditory, and textual inputs using visual-language models.


<details>
  <summary>Details</summary>
Motivation: Human thought is multimodal, but prior studies focused on single-modality inputs. This work aims to bridge the gap for more ecologically valid mind decoding.

Method: Leverages visual-language models (VLMs) with modality-specific experts to jointly interpret multimodal brain recordings.

Result: Achieves performance comparable to state-of-the-art systems while being adaptable and extensible.

Conclusion: Advances toward more generalizable and ecologically valid mind decoding.

Abstract: Decoding thoughts from brain activity offers valuable insights into human
cognition and enables promising applications in brain-computer interaction.
While prior studies have explored language reconstruction from fMRI data, they
are typically limited to single-modality inputs such as images or audio. In
contrast, human thought is inherently multimodal. To bridge this gap, we
propose a unified and flexible framework for reconstructing coherent language
from brain recordings elicited by diverse input modalities-visual, auditory,
and textual. Our approach leverages visual-language models (VLMs), using
modality-specific experts to jointly interpret information across modalities.
Experiments demonstrate that our method achieves performance comparable to
state-of-the-art systems while remaining adaptable and extensible. This work
advances toward more ecologically valid and generalizable mind decoding.

</details>


### [36] [Multi-domain Multilingual Sentiment Analysis in Industry: Predicting Aspect-based Opinion Quadruples](https://arxiv.org/pdf/2505.10389)
*Benjamin White, Anastasia Shimorina*

Main category: cs.CL

TL;DR: Aspect-based sentiment analysis system using LLMs for quadruple opinion extraction across domains and languages, showing multi-domain models match single-domain performance while simplifying operations.


<details>
  <summary>Details</summary>
Motivation: To design a practical system for aspect-based sentiment analysis using LLMs, focusing on extracting quadruple opinions (aspect categories, sentiment, targets, opinions) across diverse domains and languages.

Method: Fine-tuned a single LLM model on internal datasets to handle multiple domain-specific taxonomies simultaneously, comparing its performance to specialized single-domain models.

Result: The multi-domain model achieved performance comparable to single-domain models while reducing operational complexity. Insights were shared on handling non-extractive predictions and evaluating failure modes.

Conclusion: A single fine-tuned LLM can effectively perform quadruple opinion extraction across multiple domains, offering a balance between performance and simplicity.

Abstract: This paper explores the design of an aspect-based sentiment analysis system
using large language models (LLMs) for real-world use. We focus on quadruple
opinion extraction -- identifying aspect categories, sentiment polarity,
targets, and opinion expressions from text data across different domains and
languages. Using internal datasets, we investigate whether a single fine-tuned
model can effectively handle multiple domain-specific taxonomies
simultaneously. We demonstrate that a combined multi-domain model achieves
performance comparable to specialized single-domain models while reducing
operational complexity. We also share lessons learned for handling
non-extractive predictions and evaluating various failure modes when developing
LLM-based systems for structured prediction tasks.

</details>


### [37] [Rethinking Repetition Problems of LLMs in Code Generation](https://arxiv.org/pdf/2505.10402)
*Yihong Dong, Yuchen Liu, Xue Jiang, Zhi Jin, Ge Li*

Main category: cs.CL

TL;DR: The paper addresses structural repetition in code generation, proposing RPG, a grammar-based decoding approach to mitigate it, and introduces the CodeRepetEval dataset for evaluation.


<details>
  <summary>Details</summary>
Motivation: Existing work focuses on content repetition, but structural repetition is more prevalent and challenging, requiring a formal solution.

Method: Proposes RPG, which uses grammar rules to identify and penalize repetition-causing tokens during decoding.

Result: RPG outperforms baselines on CodeRepetEval, HumanEval, and MBPP, reducing repetitions and improving code quality.

Conclusion: RPG effectively mitigates structural repetition in code generation, enhancing performance and code quality.

Abstract: With the advent of neural language models, the performance of code generation
has been significantly boosted. However, the problem of repetitions during the
generation process continues to linger. Previous work has primarily focused on
content repetition, which is merely a fraction of the broader repetition
problem in code generation. A more prevalent and challenging problem is
structural repetition. In structural repetition, the repeated code appears in
various patterns but possesses a fixed structure, which can be inherently
reflected in grammar. In this paper, we formally define structural repetition
and propose an efficient decoding approach called RPG, which stands for
Repetition Penalization based on Grammar, to alleviate the repetition problems
in code generation for LLMs. Specifically, RPG first leverages grammar rules to
identify repetition problems during code generation, and then strategically
decays the likelihood of critical tokens that contribute to repetitions,
thereby mitigating them in code generation. To facilitate this study, we
construct a new dataset CodeRepetEval to comprehensively evaluate approaches
for mitigating the repetition problems in code generation. Extensive
experimental results demonstrate that RPG substantially outperforms the
best-performing baselines on CodeRepetEval dataset as well as HumanEval and
MBPP benchmarks, effectively reducing repetitions and enhancing the quality of
generated code.

</details>


### [38] [Are LLM-generated plain language summaries truly understandable? A large-scale crowdsourced evaluation](https://arxiv.org/pdf/2505.10409)
*Yue Guo, Jae Ho Sohn, Gondy Leroy, Trevor Cohen*

Main category: cs.CL

TL;DR: LLM-generated plain language summaries (PLS) appear similar to human-written ones in subjective evaluations but perform worse in comprehension. Automated metrics don't align with human judgment.


<details>
  <summary>Details</summary>
Motivation: To evaluate the effectiveness of LLMs in generating PLS for health communication, addressing gaps in prior evaluations.

Method: Large-scale crowdsourced evaluation with 150 participants, combining subjective ratings and objective comprehension tests, and comparing automated metrics to human judgments.

Result: Human-written PLS outperform LLM-generated ones in comprehension, and automated metrics fail to match human evaluations.

Conclusion: Better evaluation frameworks and generation methods are needed to optimize PLS for layperson comprehension.

Abstract: Plain language summaries (PLSs) are essential for facilitating effective
communication between clinicians and patients by making complex medical
information easier for laypeople to understand and act upon. Large language
models (LLMs) have recently shown promise in automating PLS generation, but
their effectiveness in supporting health information comprehension remains
unclear. Prior evaluations have generally relied on automated scores that do
not measure understandability directly, or subjective Likert-scale ratings from
convenience samples with limited generalizability. To address these gaps, we
conducted a large-scale crowdsourced evaluation of LLM-generated PLSs using
Amazon Mechanical Turk with 150 participants. We assessed PLS quality through
subjective Likert-scale ratings focusing on simplicity, informativeness,
coherence, and faithfulness; and objective multiple-choice comprehension and
recall measures of reader understanding. Additionally, we examined the
alignment between 10 automated evaluation metrics and human judgments. Our
findings indicate that while LLMs can generate PLSs that appear
indistinguishable from human-written ones in subjective evaluations,
human-written PLSs lead to significantly better comprehension. Furthermore,
automated evaluation metrics fail to reflect human judgment, calling into
question their suitability for evaluating PLSs. This is the first study to
systematically evaluate LLM-generated PLSs based on both reader preferences and
comprehension outcomes. Our findings highlight the need for evaluation
frameworks that move beyond surface-level quality and for generation methods
that explicitly optimize for layperson comprehension.

</details>


### [39] [Hierarchical Document Refinement for Long-context Retrieval-augmented Generation](https://arxiv.org/pdf/2505.10413)
*Jiajie Jin, Xiaoxi Li, Guanting Dong, Yuyao Zhang, Yutao Zhu, Yongkang Wu, Zhonghua Li, Qi Ye, Zhicheng Dou*

Main category: cs.CL

TL;DR: LongRefiner is a plug-and-play refiner for long-context RAG applications, reducing computational costs and improving performance through dual-level query analysis and hierarchical structuring.


<details>
  <summary>Details</summary>
Motivation: Addressing the challenges of redundant information and noise in long-context RAG applications, which increase costs and reduce performance.

Method: Uses dual-level query analysis, hierarchical document structuring, and adaptive refinement via multi-task learning on a single foundation model.

Result: Achieves competitive performance on seven QA datasets with 10x fewer computational costs and latency than baselines.

Conclusion: LongRefiner is scalable, efficient, and effective, offering practical insights for real-world long-text RAG applications.

Abstract: Real-world RAG applications often encounter long-context input scenarios,
where redundant information and noise results in higher inference costs and
reduced performance. To address these challenges, we propose LongRefiner, an
efficient plug-and-play refiner that leverages the inherent structural
characteristics of long documents. LongRefiner employs dual-level query
analysis, hierarchical document structuring, and adaptive refinement through
multi-task learning on a single foundation model. Experiments on seven QA
datasets demonstrate that LongRefiner achieves competitive performance in
various scenarios while using 10x fewer computational costs and latency
compared to the best baseline. Further analysis validates that LongRefiner is
scalable, efficient, and effective, providing practical insights for real-world
long-text RAG applications. Our code is available at
https://github.com/ignorejjj/LongRefiner.

</details>


### [40] [Reinforcing the Diffusion Chain of Lateral Thought with Diffusion Language Models](https://arxiv.org/pdf/2505.10446)
*Zemin Huang, Zhiyang Chen, Zijun Wang, Tiancheng Li, Guo-Jun Qi*

Main category: cs.CL

TL;DR: DCoLT is a reasoning framework for diffusion language models that optimizes reasoning trajectories using RL, outperforming traditional methods on math and code tasks.


<details>
  <summary>Details</summary>
Motivation: To enhance reasoning in diffusion language models by allowing non-linear, bidirectional thinking and optimizing intermediate steps for correctness.

Method: Uses outcome-based RL to optimize reasoning trajectories in diffusion models (SEDD and LLaDA), with a focus on probabilistic policies and token unmasking order.

Result: DCoLT-reinforced models outperform others, with LLaDA showing significant accuracy boosts on GSM8K, MATH, MBPP, and HumanEval.

Conclusion: DCoLT effectively improves reasoning in diffusion models, demonstrating superior performance on complex tasks.

Abstract: We introduce the \emph{Diffusion Chain of Lateral Thought (DCoLT)}, a
reasoning framework for diffusion language models. DCoLT treats each
intermediate step in the reverse diffusion process as a latent "thinking"
action and optimizes the entire reasoning trajectory to maximize the reward on
the correctness of the final answer with outcome-based Reinforcement Learning
(RL). Unlike traditional Chain-of-Thought (CoT) methods that follow a causal,
linear thinking process, DCoLT allows bidirectional, non-linear reasoning with
no strict rule on grammatical correctness amid its intermediate steps of
thought. We implement DCoLT on two representative Diffusion Language Models
(DLMs). First, we choose SEDD as a representative continuous-time discrete
diffusion model, where its concrete score derives a probabilistic policy to
maximize the RL reward over the entire sequence of intermediate diffusion
steps. We further consider the discrete-time masked diffusion language model --
LLaDA, and find that the order to predict and unmask tokens plays an essential
role to optimize its RL action resulting from the ranking-based Unmasking
Policy Module (UPM) defined by the Plackett-Luce model. Experiments on both
math and code generation tasks show that using only public data and 16 H800
GPUs, DCoLT-reinforced DLMs outperform other DLMs trained by SFT or RL or even
both. Notably, DCoLT-reinforced LLaDA boosts its reasoning accuracy by +9.8%,
+5.7%, +11.4%, +19.5% on GSM8K, MATH, MBPP, and HumanEval.

</details>


### [41] [CL-RAG: Bridging the Gap in Retrieval-Augmented Generation with Curriculum Learning](https://arxiv.org/pdf/2505.10493)
*Shaohan Wang, Licheng Zhang, Zheren Fu, Zhendong Mao*

Main category: cs.CL

TL;DR: The paper introduces CL-RAG, a multi-stage curriculum learning framework for training RAG systems, improving performance by 2-4% over existing methods.


<details>
  <summary>Details</summary>
Motivation: Existing RAG systems struggle with varying document effectiveness across queries, hindering retriever and generator adaptation. Inspired by human cognitive learning, curriculum learning is integrated to address this.

Method: CL-RAG constructs multi-difficulty training data for retriever and generator via sample evolution, then trains them in stages using curriculum learning.

Result: CL-RAG achieves 2-4% performance gains over advanced methods on four open-domain QA datasets.

Conclusion: CL-RAG effectively enhances RAG system performance and generalization through curriculum learning.

Abstract: Retrieval-Augmented Generation (RAG) is an effective method to enhance the
capabilities of large language models (LLMs). Existing methods focus on
optimizing the retriever or generator in the RAG system by directly utilizing
the top-k retrieved documents. However, the documents effectiveness are various
significantly across user queries, i.e. some documents provide valuable
knowledge while others totally lack critical information. It hinders the
retriever and generator's adaptation during training. Inspired by human
cognitive learning, curriculum learning trains models using samples progressing
from easy to difficult, thus enhancing their generalization ability, and we
integrate this effective paradigm to the training of the RAG system. In this
paper, we propose a multi-stage Curriculum Learning based RAG system training
framework, named CL-RAG. We first construct training data with multiple
difficulty levels for the retriever and generator separately through sample
evolution. Then, we train the model in stages based on the curriculum learning
approach, thereby optimizing the overall performance and generalization of the
RAG system more effectively. Our CL-RAG framework demonstrates consistent
effectiveness across four open-domain QA datasets, achieving performance gains
of 2% to 4% over multiple advanced methods.

</details>


### [42] [Can You Really Trust Code Copilots? Evaluating Large Language Models from a Code Security Perspective](https://arxiv.org/pdf/2505.10494)
*Yutao Mou, Xiao Deng, Yuxiao Luo, Shikun Zhang, Wei Ye*

Main category: cs.CL

TL;DR: CoV-Eval is a multi-task benchmark for evaluating LLM code security, and VC-Judge improves vulnerability assessment. Most LLMs identify vulnerabilities well but struggle with generating secure code and repairs.


<details>
  <summary>Details</summary>
Motivation: Current benchmarks lack comprehensive evaluation of LLM code security across tasks like secure code generation and vulnerability repair.

Method: Proposed CoV-Eval for multi-task evaluation and VC-Judge for improved vulnerability assessment. Evaluated 20 LLMs.

Result: LLMs identify vulnerabilities well but generate insecure code and struggle with repairs.

Conclusion: Key challenges and optimization directions are identified for future LLM code security research.

Abstract: Code security and usability are both essential for various coding assistant
applications driven by large language models (LLMs). Current code security
benchmarks focus solely on single evaluation task and paradigm, such as code
completion and generation, lacking comprehensive assessment across dimensions
like secure code generation, vulnerability repair and discrimination. In this
paper, we first propose CoV-Eval, a multi-task benchmark covering various tasks
such as code completion, vulnerability repair, vulnerability detection and
classification, for comprehensive evaluation of LLM code security. Besides, we
developed VC-Judge, an improved judgment model that aligns closely with human
experts and can review LLM-generated programs for vulnerabilities in a more
efficient and reliable way. We conduct a comprehensive evaluation of 20
proprietary and open-source LLMs. Overall, while most LLMs identify vulnerable
codes well, they still tend to generate insecure codes and struggle with
recognizing specific vulnerability types and performing repairs. Extensive
experiments and qualitative analyses reveal key challenges and optimization
directions, offering insights for future research in LLM code security.

</details>


### [43] [The Devil Is in the Word Alignment Details: On Translation-Based Cross-Lingual Transfer for Token Classification Tasks](https://arxiv.org/pdf/2505.10507)
*Benedikt Ebing, Goran Glavaš*

Main category: cs.CL

TL;DR: The paper revisits word aligners (WAs) for label projection in cross-lingual transfer (XLT) for token classification, optimizing design choices and introducing an ensembling strategy that outperforms marker-based methods.


<details>
  <summary>Details</summary>
Motivation: To systematically investigate the impact of low-level design decisions in word aligners for label projection in XLT and improve performance over existing methods.

Method: Revisits WAs for label projection, examining (i) label projection algorithms, (ii) filtering strategies, and (iii) pre-tokenization. Introduces an ensembling strategy combining translate-train and translate-test predictions.

Result: Optimized WAs perform comparably to marker-based methods. The proposed ensembling strategy outperforms marker-based projection and reduces sensitivity to WA design choices.

Conclusion: With careful design, WAs are competitive for XLT, and ensembling enhances robustness in token classification tasks.

Abstract: Translation-based strategies for cross-lingual transfer XLT such as
translate-train -- training on noisy target language data translated from the
source language -- and translate-test -- evaluating on noisy source language
data translated from the target language -- are competitive XLT baselines. In
XLT for token classification tasks, however, these strategies include label
projection, the challenging step of mapping the labels from each token in the
original sentence to its counterpart(s) in the translation. Although word
aligners (WAs) are commonly used for label projection, the low-level design
decisions for applying them to translation-based XLT have not been
systematically investigated. Moreover, recent marker-based methods, which
project labeled spans by inserting tags around them before (or after)
translation, claim to outperform WAs in label projection for XLT. In this work,
we revisit WAs for label projection, systematically investigating the effects
of low-level design decisions on token-level XLT: (i) the algorithm for
projecting labels between (multi-)token spans, (ii) filtering strategies to
reduce the number of noisily mapped labels, and (iii) the pre-tokenization of
the translated sentences. We find that all of these substantially impact
translation-based XLT performance and show that, with optimized choices, XLT
with WA offers performance at least comparable to that of marker-based methods.
We then introduce a new projection strategy that ensembles translate-train and
translate-test predictions and demonstrate that it substantially outperforms
the marker-based projection. Crucially, we show that our proposed ensembling
also reduces sensitivity to low-level WA design choices, resulting in more
robust XLT for token classification tasks.

</details>


### [44] [Multi-Token Prediction Needs Registers](https://arxiv.org/pdf/2505.10518)
*Anastasios Gerontopoulos, Spyros Gidaris, Nikos Komodakis*

Main category: cs.CL

TL;DR: MuToR is a multi-token prediction method using learnable register tokens, offering compatibility with pretrained models and scalability, effective in fine-tuning and pretraining.


<details>
  <summary>Details</summary>
Motivation: Improve multi-token prediction for language models, ensuring compatibility and scalability without architectural changes.

Method: Interleaves learnable register tokens into input sequences for predicting future targets, adding minimal parameters.

Result: Effective in supervised fine-tuning, PEFT, and pretraining across language and vision tasks.

Conclusion: MuToR is a versatile and efficient approach for multi-token prediction, suitable for diverse applications.

Abstract: Multi-token prediction has emerged as a promising objective for improving
language model pretraining, but its benefits have not consistently generalized
to other settings such as fine-tuning. In this paper, we propose MuToR, a
simple and effective approach to multi-token prediction that interleaves
learnable register tokens into the input sequence, each tasked with predicting
future targets. Compared to existing methods, MuToR offers several key
advantages: it introduces only a negligible number of additional parameters,
requires no architectural changes--ensuring compatibility with off-the-shelf
pretrained language models--and remains aligned with the next-token pretraining
objective, making it especially well-suited for supervised fine-tuning.
Moreover, it naturally supports scalable prediction horizons. We demonstrate
the effectiveness and versatility of MuToR across a range of use cases,
including supervised fine-tuning, parameter-efficient fine-tuning (PEFT), and
pretraining, on challenging generative tasks in both language and vision
domains. Our code will be available at: https://github.com/nasosger/MuToR.

</details>


### [45] [WorldPM: Scaling Human Preference Modeling](https://arxiv.org/pdf/2505.10527)
*Binghai Wang, Runji Lin, Keming Lu, Le Yu, Zhenru Zhang, Fei Huang, Chujie Zheng, Kai Dang, Yang Fan, Xingzhang Ren, An Yang, Binyuan Hui, Dayiheng Liu, Tao Gui, Qi Zhang, Xuanjing Huang, Yu-Gang Jiang, Bowen Yu, Jingren Zhou, Junyang Lin*

Main category: cs.CL

TL;DR: The paper explores scaling laws in preference modeling, proposing WorldPM to unify human preferences. It shows distinct scaling patterns across adversarial, objective, and subjective metrics, with WorldPM improving generalization in preference datasets and RLHF pipelines.


<details>
  <summary>Details</summary>
Motivation: To investigate if scaling laws in language modeling apply to preference modeling, and to propose WorldPM as a unified representation of human preferences.

Method: Collects preference data from public forums, trains models (1.5B to 72B parameters) on 15M-scale data, and evaluates using adversarial, objective, and subjective metrics.

Result: Adversarial and objective metrics scale with data and model size, while subjective metrics do not. WorldPM improves generalization by over 5% on key subtasks and enhances RLHF pipelines by 4-8%.

Conclusion: WorldPM demonstrates scalability and effectiveness in preference modeling, offering a foundation for fine-tuning and improving RLHF performance.

Abstract: Motivated by scaling laws in language modeling that demonstrate how test loss
scales as a power law with model and dataset sizes, we find that similar laws
exist in preference modeling. We propose World Preference Modeling$ (WorldPM)
to emphasize this scaling potential, where World Preference embodies a unified
representation of human preferences. In this paper, we collect preference data
from public forums covering diverse user communities, and conduct extensive
training using 15M-scale data across models ranging from 1.5B to 72B
parameters. We observe distinct patterns across different evaluation metrics:
(1) Adversarial metrics (ability to identify deceptive features) consistently
scale up with increased training data and base model size; (2) Objective
metrics (objective knowledge with well-defined answers) show emergent behavior
in larger language models, highlighting WorldPM's scalability potential; (3)
Subjective metrics (subjective preferences from a limited number of humans or
AI) do not demonstrate scaling trends. Further experiments validate the
effectiveness of WorldPM as a foundation for preference fine-tuning. Through
evaluations on 7 benchmarks with 20 subtasks, we find that WorldPM broadly
improves the generalization performance across human preference datasets of
varying sizes (7K, 100K and 800K samples), with performance gains exceeding 5%
on many key subtasks. Integrating WorldPM into our internal RLHF pipeline, we
observe significant improvements on both in-house and public evaluation sets,
with notable gains of 4% to 8% in our in-house evaluations.

</details>


### [46] [Beyond 'Aha!': Toward Systematic Meta-Abilities Alignment in Large Reasoning Models](https://arxiv.org/pdf/2505.10554)
*Zhiyuan Hu, Yibo Wang, Hanze Dong, Yuhui Xu, Amrita Saha, Caiming Xiong, Bryan Hooi, Junnan Li*

Main category: cs.CL

TL;DR: The paper proposes a method to explicitly align large reasoning models (LRMs) with meta-abilities (deduction, induction, abduction) using self-verifiable tasks, improving reasoning reliability and scalability.


<details>
  <summary>Details</summary>
Motivation: Current LRMs exhibit unpredictable emergent reasoning behaviors, limiting their reliability and scalability.

Method: A three-stage pipeline: individual alignment, parameter-space merging, and domain-specific reinforcement learning.

Result: Performance improved by over 10% relative to baselines, with an additional 2% gain from domain-specific RL.

Conclusion: Explicit meta-ability alignment provides a scalable and dependable foundation for reasoning.

Abstract: Large reasoning models (LRMs) already possess a latent capacity for long
chain-of-thought reasoning. Prior work has shown that outcome-based
reinforcement learning (RL) can incidentally elicit advanced reasoning
behaviors such as self-correction, backtracking, and verification phenomena
often referred to as the model's "aha moment". However, the timing and
consistency of these emergent behaviors remain unpredictable and
uncontrollable, limiting the scalability and reliability of LRMs' reasoning
capabilities. To address these limitations, we move beyond reliance on prompts
and coincidental "aha moments". Instead, we explicitly align models with three
meta-abilities: deduction, induction, and abduction, using automatically
generated, self-verifiable tasks. Our three stage-pipeline individual
alignment, parameter-space merging, and domain-specific reinforcement learning,
boosting performance by over 10\% relative to instruction-tuned baselines.
Furthermore, domain-specific RL from the aligned checkpoint yields an
additional 2\% average gain in the performance ceiling across math, coding, and
science benchmarks, demonstrating that explicit meta-ability alignment offers a
scalable and dependable foundation for reasoning. Code is available at:
https://github.com/zhiyuanhubj/Meta-Ability-Alignment

</details>


### [47] [Construction and Application of Materials Knowledge Graph in Multidisciplinary Materials Science via Large Language Model](https://arxiv.org/pdf/2404.03080)
*Yanpeng Ye, Jie Ren, Shaozhou Wang, Yuwei Wan, Imran Razzak, Bram Hoex, Haofen Wang, Tong Xie, Wenjie Zhang*

Main category: cs.CL

TL;DR: The paper introduces the Materials Knowledge Graph (MKG) to address challenges in materials science by using AI and NLP to organize research data into structured triples, improving efficiency and reducing reliance on traditional methods.


<details>
  <summary>Details</summary>
Motivation: The dispersion of knowledge in materials science and inefficiencies in traditional experimental methods hinder rapid innovation.

Method: MKG employs advanced NLP and large language models to extract and structure research data into 162,605 nodes and 731,772 edges, categorized by a designed ontology.

Result: MKG enhances data usability, facilitates link prediction, and reduces dependence on costly experiments.

Conclusion: MKG streamlines materials research and paves the way for advanced science knowledge graphs.

Abstract: Knowledge in materials science is widely dispersed across extensive
scientific literature, posing significant challenges to the efficient discovery
and integration of new materials. Traditional methods, often reliant on costly
and time-consuming experimental approaches, further complicate rapid
innovation. Addressing these challenges, the integration of artificial
intelligence with materials science has opened avenues for accelerating the
discovery process, though it also demands precise annotation, data extraction,
and traceability of information. To tackle these issues, this article
introduces the Materials Knowledge Graph (MKG), which utilizes advanced natural
language processing techniques integrated with large language models to extract
and systematically organize a decade's worth of high-quality research into
structured triples, contains 162,605 nodes and 731,772 edges. MKG categorizes
information into comprehensive labels such as Name, Formula, and Application,
structured around a meticulously designed ontology, thus enhancing data
usability and integration. By implementing network-based algorithms, MKG not
only facilitates efficient link prediction but also significantly reduces
reliance on traditional experimental methods. This structured approach not only
streamlines materials research but also lays the groundwork for more
sophisticated science knowledge graphs.

</details>


### [48] [Temporal Scaling Law for Large Language Models](https://arxiv.org/pdf/2404.17785)
*Yizhe Xiong, Xiansheng Chen, Xin Ye, Hui Chen, Zijia Lin, Haoran Lian, Zhenpeng Su, Wei Huang, Jianwei Niu, Jungong Han, Guiguang Ding*

Main category: cs.CL

TL;DR: The paper introduces Temporal Scaling Law, a novel concept to study how the test loss of LLMs evolves with training steps, enabling better hyperparameter selection and deeper insights into LLM pre-training.


<details>
  <summary>Details</summary>
Motivation: Existing Scaling Laws focus on final test loss but ignore its temporal evolution during pre-training, which is crucial for tasks like hyperparameter tuning.

Method: The authors break down test loss by token position, develop a dynamic hyperbolic-law, and derive precise temporal scaling laws by analyzing parameter patterns.

Result: The temporal scaling law accurately predicts test loss across training steps on both ID and OOD datasets.

Conclusion: The law aids hyperparameter selection and provides granular insights into LLM pre-training dynamics.

Abstract: Recently, Large Language Models (LLMs) have been widely adopted in a wide
range of tasks, leading to increasing attention towards the research on how
scaling LLMs affects their performance. Existing works, termed Scaling Laws,
have discovered that the final test loss of LLMs scales as power-laws with
model size, computational budget, and dataset size. However, the temporal
change of the test loss of an LLM throughout its pre-training process remains
unexplored, though it is valuable in many aspects, such as selecting better
hyperparameters \textit{directly} on the target LLM. In this paper, we propose
the novel concept of Temporal Scaling Law, studying how the test loss of an LLM
evolves as the training steps scale up. In contrast to modeling the test loss
as a whole in a coarse-grained manner, we break it down and dive into the
fine-grained test loss of each token position, and further develop a dynamic
hyperbolic-law. Afterwards, we derive the much more precise temporal scaling
law by studying the temporal patterns of the parameters in the dynamic
hyperbolic-law. Results on both in-distribution (ID) and out-of-distribution
(OOD) validation datasets demonstrate that our temporal scaling law accurately
predicts the test loss of LLMs across training steps. Our temporal scaling law
has broad practical applications. First, it enables direct and efficient
hyperparameter selection on the target LLM, such as data mixture proportions.
Secondly, viewing the LLM pre-training dynamics from the token position
granularity provides some insights to enhance the understanding of LLM
pre-training.

</details>


### [49] [The Mosaic Memory of Large Language Models](https://arxiv.org/pdf/2405.15523)
*Igor Shilov, Matthieu Meeus, Yves-Alexandre de Montjoye*

Main category: cs.CL

TL;DR: LLMs memorize by assembling information from similar sequences (mosaic memory), not just exact duplicates, challenging common assumptions.


<details>
  <summary>Details</summary>
Motivation: To understand how LLMs learn and memorize training data, especially given privacy and confidentiality concerns.

Method: Analyzed memorization in major LLMs, measuring contributions of exact and fuzzy duplicates to memorization.

Result: LLMs exhibit mosaic memory, with fuzzy duplicates contributing significantly. Memorization is syntactic, not semantic, and fuzzy duplicates are common in real-world data.

Conclusion: Memorization in LLMs is a complex, mosaic process with implications for privacy, confidentiality, and model evaluation.

Abstract: As Large Language Models (LLMs) become widely adopted, understanding how they
learn from, and memorize, training data becomes crucial. Memorization in LLMs
is widely assumed to only occur as a result of sequences being repeated in the
training data. Instead, we show that LLMs memorize by assembling information
from similar sequences, a phenomena we call mosaic memory. We show major LLMs
to exhibit mosaic memory, with fuzzy duplicates contributing to memorization as
much as 0.8 of an exact duplicate and even heavily modified sequences
contributing substantially to memorization. Despite models display reasoning
capabilities, we somewhat surprisingly show memorization to be predominantly
syntactic rather than semantic. We finally show fuzzy duplicates to be
ubiquitous in real-world data, untouched by deduplication techniques. Taken
together, our results challenge widely held beliefs and show memorization to be
a more complex, mosaic process, with real-world implications for privacy,
confidentiality, model utility and evaluation.

</details>


### [50] [Tokenization Matters! Degrading Large Language Models through Challenging Their Tokenization](https://arxiv.org/pdf/2405.17067)
*Dixuan Wang, Yanda Li, Junyuan Jiang, Zepeng Ding, Ziqin Luo, Guochao Jiang, Jiaqing Liang, Deqing Yang*

Main category: cs.CL

TL;DR: The paper identifies tokenization as a key flaw in LLMs, especially for Chinese, and introduces ADT, an adversarial dataset, to challenge and expose this weakness.


<details>
  <summary>Details</summary>
Motivation: To highlight and address the tokenization limitations in LLMs, which cause inaccurate responses, particularly in Chinese contexts.

Method: Constructed ADT (Adversarial Dataset for Tokenizer) with manual (ADT-Human) and automatic (ADT-Auto) subsets to test LLMs' tokenization.

Result: ADT effectively exposed vulnerabilities in leading LLMs like GPT-4o and Llama-3, degrading their performance.

Conclusion: The study underscores the need for improved tokenization methods to enhance LLMs' accuracy and robustness.

Abstract: Large Language Models (LLMs) have shown remarkable capabilities in language
understanding and generation. Nonetheless, it was also witnessed that LLMs tend
to produce inaccurate responses to specific queries. This deficiency can be
traced to the tokenization step LLMs must undergo, which is an inevitable
limitation inherent to all LLMs. In fact, incorrect tokenization is the
critical point that hinders LLMs in understanding the input precisely, thus
leading to unsatisfactory output. This defect is more obvious in Chinese
scenarios. To demonstrate this flaw of LLMs, we construct an adversarial
dataset, named as $\textbf{ADT (Adversarial Dataset for Tokenizer)}$, which
draws upon the vocabularies of various open-source LLMs to challenge LLMs'
tokenization. ADT consists of two subsets: the manually constructed ADT-Human
and the automatically generated ADT-Auto. Our empirical results reveal that our
ADT is highly effective on challenging the tokenization of leading LLMs,
including GPT-4o, Llama-3, Deepseek-R1 and so on, thus degrading these LLMs'
capabilities. Moreover, our method of automatic data generation has been proven
efficient and robust, which can be applied to any open-source LLMs. In this
paper, we substantially investigate LLMs' vulnerability in terms of challenging
their token segmentation, which will shed light on the subsequent research of
improving LLMs' capabilities through optimizing their tokenization process and
algorithms.

</details>


### [51] [RoBERTa-BiLSTM: A Context-Aware Hybrid Model for Sentiment Analysis](https://arxiv.org/pdf/2406.00367)
*Md. Mostafizer Rahman, Ariful Islam Shiplu, Yutaka Watanobe, Md. Ashad Alam*

Main category: cs.CL

TL;DR: The paper introduces RoBERTa-BiLSTM, a hybrid model combining RoBERTa and BiLSTM for sentiment analysis, outperforming existing methods in accuracy and F1-scores on multiple datasets.


<details>
  <summary>Details</summary>
Motivation: Challenges in sentiment analysis like lexical diversity, long dependencies, and imbalanced datasets motivate the need for a more efficient and accurate model.

Method: The hybrid RoBERTa-BiLSTM model uses RoBERTa for word embeddings and BiLSTM to capture contextual semantics in long-dependent texts.

Result: The model achieves accuracies of 80.74%, 92.36%, and 82.25% on Twitter US Airline, IMDb, and Sentiment140 datasets, with matching F1-scores.

Conclusion: RoBERTa-BiLSTM outperforms baseline models, proving its effectiveness in sentiment analysis by leveraging both sequential and Transformer strengths.

Abstract: Effectively analyzing the comments to uncover latent intentions holds immense
value in making strategic decisions across various domains. However, several
challenges hinder the process of sentiment analysis including the lexical
diversity exhibited in comments, the presence of long dependencies within the
text, encountering unknown symbols and words, and dealing with imbalanced
datasets. Moreover, existing sentiment analysis tasks mostly leveraged
sequential models to encode the long dependent texts and it requires longer
execution time as it processes the text sequentially. In contrast, the
Transformer requires less execution time due to its parallel processing nature.
In this work, we introduce a novel hybrid deep learning model, RoBERTa-BiLSTM,
which combines the Robustly Optimized BERT Pretraining Approach (RoBERTa) with
Bidirectional Long Short-Term Memory (BiLSTM) networks. RoBERTa is utilized to
generate meaningful word embedding vectors, while BiLSTM effectively captures
the contextual semantics of long-dependent texts. The RoBERTa-BiLSTM hybrid
model leverages the strengths of both sequential and Transformer models to
enhance performance in sentiment analysis. We conducted experiments using
datasets from IMDb, Twitter US Airline, and Sentiment140 to evaluate the
proposed model against existing state-of-the-art methods. Our experimental
findings demonstrate that the RoBERTa-BiLSTM model surpasses baseline models
(e.g., BERT, RoBERTa-base, RoBERTa-GRU, and RoBERTa-LSTM), achieving accuracies
of 80.74%, 92.36%, and 82.25% on the Twitter US Airline, IMDb, and Sentiment140
datasets, respectively. Additionally, the model achieves F1-scores of 80.73%,
92.35%, and 82.25% on the same datasets, respectively.

</details>


### [52] [PyramidKV: Dynamic KV Cache Compression based on Pyramidal Information Funneling](https://arxiv.org/pdf/2406.02069)
*Zefan Cai, Yichi Zhang, Bofei Gao, Yuliang Liu, Yucheng Li, Tianyu Liu, Keming Lu, Wayne Xiong, Yue Dong, Junjie Hu, Wen Xiao*

Main category: cs.CL

TL;DR: PyramidKV, a novel KV cache compression method, dynamically adjusts cache size across layers, reducing memory usage while maintaining performance.


<details>
  <summary>Details</summary>
Motivation: To address inefficient memory usage in LLMs by leveraging observed pyramidal information flow patterns.

Method: Developed PyramidKV, adjusting KV cache size per layer (more in lower, less in higher layers).

Result: Achieves full KV cache performance with 12% cache, and outperforms others with 0.7% cache (20.5% accuracy boost).

Conclusion: PyramidKV efficiently compresses KV cache, enhancing memory efficiency without sacrificing performance.

Abstract: In this study, we investigate whether attention-based information flow inside
large language models (LLMs) is aggregated through noticeable patterns for long
context processing. Our observations reveal that LLMs aggregate information
through Pyramidal Information Funneling where attention is scattering widely in
lower layers, progressively consolidating within specific contexts, and
ultimately focusing on critical tokens (a.k.a massive activation or attention
sink) in higher layers. Motivated by these insights, we developed PyramidKV, a
novel and effective KV cache compression method. This approach dynamically
adjusts the KV cache size across different layers, allocating more cache in
lower layers and less in higher ones, diverging from traditional methods that
maintain a uniform KV cache size. Our experimental evaluations, utilizing the
LongBench benchmark, show that PyramidKV matches the performance of models with
a full KV cache while retaining only 12% of the KV cache, thus significantly
reducing memory usage. In scenarios emphasizing memory efficiency, where only
0.7% of the KV cache is maintained, PyramidKV surpasses other KV cache
compression techniques, achieving up to a 20.5 absolute accuracy improvement on
TREC dataset. In the Needle-in-a-Haystack experiment, PyramidKV outperforms
competing methods in maintaining long-context comprehension in LLMs; notably,
retaining just 128 KV cache entries enables the LLAMA-3-70B model to achieve
100.0 Acc. performance.

</details>


### [53] [Conversational Query Reformulation with the Guidance of Retrieved Documents](https://arxiv.org/pdf/2407.12363)
*Jeonghyun Park, Hwanhee Lee*

Main category: cs.CL

TL;DR: GuideCQR improves conversational search by refining queries using key information from initially retrieved documents, outperforming previous methods.


<details>
  <summary>Details</summary>
Motivation: Previous CQR methods imitate human queries, which may not always yield meaningful search results. GuideCQR aims to enhance retrieval by leveraging document-derived information.

Method: GuideCQR extracts keywords and generates expected answers from retrieved documents, then filters and unifies them with queries to add useful information.

Result: GuideCQR achieves state-of-the-art performance across multiple datasets and works well with various query types, including human-written ones.

Conclusion: GuideCQR effectively enhances conversational search by refining queries with document-derived insights, demonstrating superior performance.

Abstract: Conversational search seeks to retrieve relevant passages for the given
questions in conversational question answering. Conversational Query
Reformulation (CQR) improves conversational search by refining the original
queries into de-contextualized forms to resolve the issues in the original
queries, such as omissions and coreferences. Previous CQR methods focus on
imitating human written queries which may not always yield meaningful search
results for the retriever. In this paper, we introduce GuideCQR, a framework
that refines queries for CQR by leveraging key information from the initially
retrieved documents. Specifically, GuideCQR extracts keywords and generates
expected answers from the retrieved documents, then unifies them with the
queries after filtering to add useful information that enhances the search
process. Experimental results demonstrate that our proposed method achieves
state-of-the-art performance across multiple datasets, outperforming previous
CQR methods. Additionally, we show that GuideCQR can get additional performance
gains in conversational search using various types of queries, even for queries
written by humans.

</details>


### [54] [PersLLM: A Personified Training Approach for Large Language Models](https://arxiv.org/pdf/2407.12393)
*Zheni Zeng, Jiayi Chen, Huimin Chen, Yukun Yan, Yuxuan Chen, Zhenghao Liu, Zhiyuan Liu, Maosong Sun*

Main category: cs.CL

TL;DR: PersLLM is a framework to enhance LLM personification by improving data construction and model tuning, addressing challenges like insufficient data usage and rigid behavior patterns.


<details>
  <summary>Details</summary>
Motivation: Current personified LLMs struggle with capturing personalized knowledge and expressing persistent opinions due to poor data usage and inflexible behavior.

Method: The framework uses Chain-of-Thought prompting, anti-induction, and automated DPO for better data quality and dynamic personality tuning.

Result: PersLLM improves personality specificity and dynamism, validated by automated metrics and human evaluations.

Conclusion: The approach shows promise for applications in human-machine interactions and multi-agent systems, suggesting future directions for LLM personification.

Abstract: Large language models (LLMs) exhibit human-like intelligence, enabling them
to simulate human behavior and support various applications that require both
humanized communication and extensive knowledge reserves. Efforts are made to
personify LLMs with special training data or hand-crafted prompts, while
correspondingly faced with challenges such as insufficient data usage or rigid
behavior patterns. Consequently, personified LLMs fail to capture personified
knowledge or express persistent opinion. To fully unlock the potential of LLM
personification, we propose PersLLM, a framework for better data construction
and model tuning. For insufficient data usage, we incorporate strategies such
as Chain-of-Thought prompting and anti-induction, improving the quality of data
construction and capturing the personality experiences, knowledge, and thoughts
more comprehensively. For rigid behavior patterns, we design the tuning process
and introduce automated DPO to enhance the specificity and dynamism of the
models' personalities, which leads to a more natural opinion communication.
Both automated metrics and expert human evaluations demonstrate the
effectiveness of our approach. Case studies in human-machine interactions and
multi-agent systems further suggest potential application scenarios and future
directions for LLM personification.

</details>


### [55] [Beyond Next Token Prediction: Patch-Level Training for Large Language Models](https://arxiv.org/pdf/2407.12665)
*Chenze Shao, Fandong Meng, Jie Zhou*

Main category: cs.CL

TL;DR: Patch-level training reduces LLM training costs by 50% without performance loss by aggregating tokens into patches for initial training.


<details>
  <summary>Details</summary>
Motivation: High training costs of LLMs hinder development; this paper aims to reduce costs while maintaining performance.

Method: Introduces patch-level training where tokens are grouped into patches for initial training, followed by token-level training.

Result: Experiments show 50% cost reduction with no performance drop across models (370M-2.7B parameters).

Conclusion: Patch-level training is an effective method to cut LLM training costs without sacrificing quality.

Abstract: The prohibitive training costs of Large Language Models (LLMs) have emerged
as a significant bottleneck in the development of next-generation LLMs. In this
paper, we show that it is possible to significantly reduce the training costs
of LLMs without sacrificing their performance. Specifically, we introduce
patch-level training for LLMs, in which multiple tokens are aggregated into a
unit of higher information density, referred to as a `patch', to serve as the
fundamental text unit for training LLMs. During patch-level training, we feed
the language model shorter sequences of patches and train it to predict the
next patch, thereby processing the majority of the training data at a
significantly reduced cost. Following this, the model continues token-level
training on the remaining training data to align with the inference mode.
Experiments on a diverse range of models (370M-2.7B parameters) demonstrate
that patch-level training can reduce the overall training costs to 0.5$\times$,
without compromising the model performance compared to token-level training.
Source code: https://github.com/shaochenze/PatchTrain.

</details>


### [56] [Compensate Quantization Errors+: Quantized Models Are Inquisitive Learners](https://arxiv.org/pdf/2407.15508)
*Yifei Gao, Jie Ou, Lei Wang, Jun Cheng, Mengchu Zhou*

Main category: cs.CL

TL;DR: The paper introduces Singular-value Diagonal Expansion and Cross-layer Learning to improve weight quantization in large language models, outperforming existing methods.


<details>
  <summary>Details</summary>
Motivation: Existing quantization techniques for LLMs either introduce computational overhead or fail to achieve globally optimal results, prompting the need for better weight distribution refinement.

Method: Proposes Singular-value Diagonal Expansion for refined weight distribution and Cross-layer Learning for even error distribution across layers.

Result: The methods outperform state-of-the-art approaches like OmniQuant, DuQuant, and PrefixQuant.

Conclusion: The proposed techniques offer a plug-and-play solution for better weight quantization in LLMs, enhancing performance without disrupting original distributions.

Abstract: The quantization of large language models (LLMs) has been a prominent
research area aimed at enabling their lightweight deployment in practice.
Existing research about LLM's quantization has mainly explored the interplay
between weights and activations, or employing auxiliary components while
neglecting the necessity of adjusting weights during quantization.
Consequently, original weight distributions frequently fail to yield desired
results after round-to-nearest (RTN) quantization. Even though incorporating
techniques such as mixed precision and low-rank error approximation in LLM's
quantization can yield improved results, they inevitably introduce additional
computational overhead. On the other hand, traditional techniques for weight
quantization, such as Generative Post-Training Quantization, rely on manually
tweaking weight distributions to minimize local errors, but they fall short of
achieving globally optimal outcomes. Although the recently proposed Learnable
Singular-value Increment improves global weight quantization by modifying
weight distributions, it disrupts the original distribution considerably. This
introduces pronounced bias toward the training data and can degrade downstream
task performance. In this paper, we introduce Singular-value Diagonal
Expansion, a more nuanced approach to refining weight distributions to achieve
better quantization alignment. Furthermore, we introduce Cross-layer Learning
that improves overall quantization outcomes by distributing errors more evenly
across layers. Our plug-and-play weight-quantization methods demonstrate
substantial performance improvements over state-of-the-art approaches,
including OmniQuant, DuQuant, and PrefixQuant.

</details>


### [57] [Time Awareness in Large Language Models: Benchmarking Fact Recall Across Time](https://arxiv.org/pdf/2409.13338)
*David Herel, Vojtech Bartek, Jiri Jirak, Tomas Mikolov*

Main category: cs.CL

TL;DR: The paper introduces a framework and dataset for evaluating LLMs on temporal reasoning, revealing their limitations in handling time-sensitive and paraphrased facts.


<details>
  <summary>Details</summary>
Motivation: To address the gap in LLMs' ability to handle temporal context in real-world scenarios, where answer correctness often depends on time.

Method: A novel framework and dataset with 8,000+ events (2018-2024) annotated at day-level granularity, evaluated using the TimeShift method.

Result: Base models outperform instruction-tuned and synthetic-trained models on time-sensitive recall, but all exhibit brittleness with paraphrased facts.

Conclusion: The work advances time-aware LLMs by highlighting unresolved challenges in temporal consistency and dynamic knowledge adaptation.

Abstract: Who is the US President? The answer changes depending on when the question is
asked. While large language models (LLMs) are evaluated on various reasoning
tasks, they often miss a crucial dimension: time. In real-world scenarios, the
correctness of answers is frequently tied to temporal context. To address this
gap, we present a novel framework and dataset spanning over 8,000 events from
2018 to 2024, annotated with day-level granularity and sourced globally across
domains such as politics, science, and business. Our TimeShift evaluation
method systematically probes LLMs for temporal reasoning, revealing that base
models often outperform instruction-tuned and synthetic-trained counterparts on
time-sensitive recall. Additionally, we find that even large-scale models
exhibit brittleness in handling paraphrased facts, highlighting unresolved
challenges in temporal consistency. By identifying these limitations, our work
provides a significant step toward advancing time-aware language models capable
of adapting to the dynamic nature of real-world knowledge.

</details>


### [58] [FAMMA: A Benchmark for Financial Domain Multilingual Multimodal Question Answering](https://arxiv.org/pdf/2410.04526)
*Siqiao Xue, Xiaojing Li, Fan Zhou, Qingyang Dai, Zhixuan Chu, Hongyuan Mei*

Main category: cs.CL

TL;DR: FAMMA is an open-source benchmark for financial multilingual multimodal QA, challenging LLMs with complex reasoning questions. It includes two versions: FAMMA-Basic (1,945 textbook/extracted questions) and FAMMA-LivePro (103 expert-created questions). Experiments show LLMs struggle, but fine-tuning with reasoning data improves performance.


<details>
  <summary>Details</summary>
Motivation: To evaluate LLMs' abilities in complex financial reasoning across languages and modalities, addressing gaps in existing benchmarks.

Method: Created FAMMA-Basic (textbook/extracted questions) and FAMMA-LivePro (expert-created questions) with multilingual, multimodal content. Evaluated LLMs like GPT-o1 and DeepSeek-R1, and fine-tuned Qwen models using reasoning trajectories.

Result: FAMMA challenges LLMs, with fine-tuned models showing significant improvement on FAMMA-LivePro.

Conclusion: FAMMA provides a robust benchmark for financial QA, highlighting LLMs' limitations and the value of reasoning data for improvement.

Abstract: In this paper, we introduce FAMMA, an open-source benchmark for
\underline{f}in\underline{a}ncial \underline{m}ultilingual
\underline{m}ultimodal question \underline{a}nswering (QA). Our benchmark aims
to evaluate the abilities of large language models (LLMs) in answering complex
reasoning questions that require advanced financial knowledge. The benchmark
has two versions: FAMMA-Basic consists of 1,945 questions extracted from
university textbooks and exams, along with human-annotated answers and
rationales; FAMMA-LivePro consists of 103 novel questions created by human
domain experts, with answers and rationales held out from the public for a
contamination-free evaluation. These questions cover advanced knowledge of 8
major subfields in finance (e.g., corporate finance, derivatives, and portfolio
management). Some are in Chinese or French, while a majority of them are in
English. Each question has some non-text data such as charts, diagrams, or
tables. Our experiments reveal that FAMMA poses a significant challenge on
LLMs, including reasoning models such as GPT-o1 and DeepSeek-R1. Additionally,
we curated 1,270 reasoning trajectories of DeepSeek-R1 on the FAMMA-Basic data,
and fine-tuned a series of open-source Qwen models using this reasoning data.
We found that training a model on these reasoning trajectories can
significantly improve its performance on FAMMA-LivePro. We released our
leaderboard, data, code, and trained models at
https://famma-bench.github.io/famma/.

</details>


### [59] [TopoLM: brain-like spatio-functional organization in a topographic language model](https://arxiv.org/pdf/2410.11516)
*Neil Rathi, Johannes Mehrer, Badr AlKhamissi, Taha Binhuraib, Nicholas M. Blauch, Martin Schrimpf*

Main category: cs.CL

TL;DR: TopoLM, a transformer language model with spatial representation, predicts brain-like functional clusters in language processing by combining next-token prediction and spatial smoothness.


<details>
  <summary>Details</summary>
Motivation: To understand the mechanisms behind the functional organization of neurons in the brain's language system, which exhibits clusters for syntactic and semantic categories.

Method: Developed TopoLM, a transformer model with a 2D spatial representation, combining next-token prediction and spatial smoothness loss to form interpretable clusters.

Result: TopoLM predicts brain-like spatio-functional organization and matches empirical observations of linguistic feature clusters in human cortex.

Conclusion: The functional organization of the human language system is driven by a unified spatial objective, and TopoLM provides a spatially aligned model of brain language processing.

Abstract: Neurons in the brain are spatially organized such that neighbors on tissue
often exhibit similar response profiles. In the human language system,
experimental studies have observed clusters for syntactic and semantic
categories, but the mechanisms underlying this functional organization remain
unclear. Here, building on work from the vision literature, we develop TopoLM,
a transformer language model with an explicit two-dimensional spatial
representation of model units. By combining a next-token prediction objective
with a spatial smoothness loss, representations in this model assemble into
clusters that correspond to semantically interpretable groupings of text and
closely match the functional organization in the brain's language system.
TopoLM successfully predicts the emergence of the spatio-functional
organization of a cortical language system as well as the organization of
functional clusters selective for fine-grained linguistic features empirically
observed in human cortex. Our results suggest that the functional organization
of the human language system is driven by a unified spatial objective, and
provide a functionally and spatially aligned model of language processing in
the brain.

</details>


### [60] [How Does Knowledge Selection Help Retrieval Augmented Generation?](https://arxiv.org/pdf/2410.13258)
*Xiangci Li, Jessica Ouyang*

Main category: cs.CL

TL;DR: The paper examines how knowledge selection affects generation performance in RAG systems, finding that its impact depends on the generator model's strength and task complexity.


<details>
  <summary>Details</summary>
Motivation: To clarify the role of knowledge selection in RAG systems, which is less understood compared to knowledge retrieval.

Method: Simulated retrieval and selection conditions using gold and distractor knowledge to measure their impact on generation.

Result: Knowledge selection's importance varies: recall is key for strong generators on clear tasks, while F1 score matters more for weaker models or ambiguous tasks.

Conclusion: Knowledge selection's impact depends on the generator model and task complexity, with recall being crucial for strong models and F1 for weaker ones.

Abstract: Retrieval-augmented generation (RAG) is a powerful method for enhancing
natural language generation by integrating external knowledge into a model's
output. While prior work has demonstrated the importance of improving knowledge
retrieval for boosting generation quality, the role of knowledge selection
remains less clear. This paper empirically analyzes how knowledge selection
influences downstream generation performance in RAG systems. By simulating
different retrieval and selection conditions through a controlled mixture of
gold and distractor knowledge, we assess the impact of these factors on
generation outcomes. Our findings indicate that the downstream generator
model's capability, as well as the complexity of the task and dataset,
significantly influence the impact of knowledge selection on the overall RAG
system performance. In typical scenarios, improving the knowledge recall score
is key to enhancing generation outcomes, with the knowledge selector providing
limited benefit when a strong generator model is used on clear, well-defined
tasks. For weaker generator models or more ambiguous tasks and datasets, the
knowledge F1 score becomes a critical factor, and the knowledge selector plays
a more prominent role in improving overall performance.

</details>


### [61] [ChronoFact: Timeline-based Temporal Fact Verification](https://arxiv.org/pdf/2410.14964)
*Anab Maulana Barik, Wynne Hsu, Mong Li Lee*

Main category: cs.CL

TL;DR: A novel timeline-based framework for verifying temporal claims by analyzing event relationships in chronological order, outperforming current methods.


<details>
  <summary>Details</summary>
Motivation: Addressing the challenge of verifying complex temporal claims in misinformation, which current systems struggle with due to overlapping or recurring events.

Method: Introduces a timeline-based framework that organizes events from claims and evidence into chronological timelines and examines their relationships for veracity.

Result: Experimental results show the framework effectively handles temporal claim verification complexities.

Conclusion: The proposed framework improves accuracy in verifying temporal claims and includes a new dataset for future research.

Abstract: Temporal claims, often riddled with inaccuracies, are a significant challenge
in the digital misinformation landscape. Fact-checking systems that can
accurately verify such claims are crucial for combating misinformation. Current
systems struggle with the complexities of evaluating the accuracy of these
claims, especially when they include multiple, overlapping, or recurring
events. We introduce a novel timeline-based fact verification framework that
identify events from both claim and evidence and organize them into their
respective chronological timelines. The framework systematically examines the
relationships between the events in both claim and evidence to predict the
veracity of each claim event and their chronological accuracy. This allows us
to accurately determine the overall veracity of the claim. We also introduce a
new dataset of complex temporal claims involving timeline-based reasoning for
the training and evaluation of our proposed framework. Experimental results
demonstrate the effectiveness of our approach in handling the intricacies of
temporal claim verification.

</details>


### [62] [SceneGenAgent: Precise Industrial Scene Generation with Coding Agent](https://arxiv.org/pdf/2410.21909)
*Xiao Xia, Dan Zhang, Zibo Liao, Zhenyu Hou, Tianrui Sun, Jing Li, Ling Fu, Yuxiao Dong*

Main category: cs.CL

TL;DR: SceneGenAgent, an LLM-based agent, generates precise industrial scenes via C# code, achieving 81.0% success rate, and is enhanced by SceneInstruct dataset for fine-tuning open-source LLMs.


<details>
  <summary>Details</summary>
Motivation: Industrial scenes require precise measurements and spatial planning, which general LLMs struggle with.

Method: SceneGenAgent uses structured C# code, layout verification, and iterative refinement for precise industrial scene generation.

Result: Achieves 81.0% success rate; fine-tuned open-source LLMs (e.g., Llama3.1-70B) approach GPT-4o performance.

Conclusion: SceneGenAgent effectively meets industrial scene generation demands, with open-source LLMs showing promise when fine-tuned.

Abstract: The modeling of industrial scenes is essential for simulations in industrial
manufacturing. While large language models (LLMs) have shown significant
progress in generating general 3D scenes from textual descriptions, generating
industrial scenes with LLMs poses a unique challenge due to their demand for
precise measurements and positioning, requiring complex planning over spatial
arrangement. To address this challenge, we introduce SceneGenAgent, an
LLM-based agent for generating industrial scenes through C# code. SceneGenAgent
ensures precise layout planning through a structured and calculable format,
layout verification, and iterative refinement to meet the quantitative
requirements of industrial scenarios. Experiment results demonstrate that LLMs
powered by SceneGenAgent exceed their original performance, reaching up to
81.0% success rate in real-world industrial scene generation tasks and
effectively meeting most scene generation requirements. To further enhance
accessibility, we construct SceneInstruct, a dataset designed for fine-tuning
open-source LLMs to integrate into SceneGenAgent. Experiments show that
fine-tuning open-source LLMs on SceneInstruct yields significant performance
improvements, with Llama3.1-70B approaching the capabilities of GPT-4o. Our
code and data are available at https://github.com/THUDM/SceneGenAgent .

</details>


### [63] [Phase Diagram of Vision Large Language Models Inference: A Perspective from Interaction across Image and Instruction](https://arxiv.org/pdf/2411.00646)
*Houjing Wei, Yuting Shi, Naoya Inoue*

Main category: cs.CL

TL;DR: The paper investigates the interaction between image and text tokens in Vision Large Language Models (VLLMs), revealing a four-phase inference dynamics across Transformer layers.


<details>
  <summary>Details</summary>
Motivation: To understand how VLLMs internally process and interact with multimodal (image and text) tokens, given their underexplored behaviors.

Method: Measure contextualization among hidden state vectors of tokens from different modalities across Transformer layers.

Result: Identifies four phases: (I) Alignment, (II) Intra-modal Encoding, (III) Inter-modal Encoding, and (IV) Output Preparation.

Conclusion: VLLMs exhibit distinct multimodal interaction patterns, progressing from alignment to deeper fusion and finally output preparation.

Abstract: Vision Large Language Models (VLLMs) usually take input as a concatenation of
image token embeddings and text token embeddings and conduct causal modeling.
However, their internal behaviors remain underexplored, raising the question of
interaction among two types of tokens. To investigate such multimodal
interaction during model inference, in this paper, we measure the
contextualization among the hidden state vectors of tokens from different
modalities. Our experiments uncover a four-phase inference dynamics of VLLMs
against the depth of Transformer-based LMs, including (I) Alignment: In very
early layers, contextualization emerges between modalities, suggesting a
feature space alignment. (II) Intra-modal Encoding: In early layers,
intra-modal contextualization is enhanced while inter-modal interaction is
suppressed, suggesting a local encoding within modalities. (III) Inter-modal
Encoding: In later layers, contextualization across modalities is enhanced,
suggesting a deeper fusion across modalities. (IV) Output Preparation: In very
late layers, contextualization is reduced globally, and hidden states are
aligned towards the unembedding space.

</details>


### [64] [Disentangling Memory and Reasoning Ability in Large Language Models](https://arxiv.org/pdf/2411.13504)
*Mingyu Jin, Weidi Luo, Sitao Cheng, Xinyi Wang, Wenyue Hua, Ruixiang Tang, William Yang Wang, Yongfeng Zhang*

Main category: cs.CL

TL;DR: The paper proposes a new LLM inference paradigm separating knowledge retrieval (memory recall) and reasoning to improve clarity, performance, and interpretability.


<details>
  <summary>Details</summary>
Motivation: Existing LLM inference lacks clear separation between knowledge retrieval and reasoning, leading to issues like hallucinations and unreliable decisions.

Method: Introduces two special tokens (memory and reason) to decompose inference into distinct memory recall and reasoning steps.

Result: The decomposition improves model performance and interpretability, aiding error identification and response refinement.

Conclusion: The proposed paradigm enhances LLM reliability and usability in high-stakes domains.

Abstract: Large Language Models (LLMs) have demonstrated strong performance in handling
complex tasks requiring both extensive knowledge and reasoning abilities.
However, the existing LLM inference pipeline operates as an opaque process
without explicit separation between knowledge retrieval and reasoning steps,
making the model's decision-making process unclear and disorganized. This
ambiguity can lead to issues such as hallucinations and knowledge forgetting,
which significantly impact the reliability of LLMs in high-stakes domains. In
this paper, we propose a new inference paradigm that decomposes the complex
inference process into two distinct and clear actions: (1) memory recall: which
retrieves relevant knowledge, and (2) reasoning: which performs logical steps
based on the recalled knowledge. To facilitate this decomposition, we introduce
two special tokens memory and reason, guiding the model to distinguish between
steps that require knowledge retrieval and those that involve reasoning. Our
experiment results show that this decomposition not only improves model
performance but also enhances the interpretability of the inference process,
enabling users to identify sources of error and refine model responses
effectively. The code is available at
https://github.com/MingyuJ666/Disentangling-Memory-and-Reasoning.

</details>


### [65] [Large Language Models for Cancer Communication: Evaluating Linguistic Quality, Safety, and Accessibility in Generative AI](https://arxiv.org/pdf/2505.10472)
*Agnik Saha, Victoria Churchill, Anny D. Rodriguez, Ugur Kursuncu, Muhammed Y. Idris*

Main category: cs.CL

TL;DR: The study evaluates LLMs for generating cancer-related info, finding general-purpose models excel in linguistic quality and affectiveness, while medical LLMs are more accessible but riskier due to harm, toxicity, and bias.


<details>
  <summary>Details</summary>
Motivation: Address gaps in public understanding of cancer prevention and treatment by assessing LLMs' ability to provide accurate, safe, and accessible info.

Method: Mixed-methods framework evaluating five general-purpose and three medical LLMs using quantitative metrics, qualitative expert ratings, and statistical analysis (Welch's ANOVA, Games-Howell, Hedges' g).

Result: General-purpose LLMs scored higher in linguistic quality and affectiveness; medical LLMs were more accessible but had higher harm, toxicity, and bias.

Conclusion: Domain-specific knowledge and safety in health communications are dual challenges. Future LLM design must mitigate harm, bias, and improve safety and affectiveness.

Abstract: Effective communication about breast and cervical cancers remains a
persistent health challenge, with significant gaps in public understanding of
cancer prevention, screening, and treatment, potentially leading to delayed
diagnoses and inadequate treatments. This study evaluates the capabilities and
limitations of Large Language Models (LLMs) in generating accurate, safe, and
accessible cancer-related information to support patient understanding. We
evaluated five general-purpose and three medical LLMs using a mixed-methods
evaluation framework across linguistic quality, safety and trustworthiness, and
communication accessibility and affectiveness. Our approach utilized
quantitative metrics, qualitative expert ratings, and statistical analysis
using Welch's ANOVA, Games-Howell, and Hedges' g. Our results show that
general-purpose LLMs produced outputs of higher linguistic quality and
affectiveness, while medical LLMs demonstrate greater communication
accessibility. However, medical LLMs tend to exhibit higher levels of potential
harm, toxicity, and bias, reducing their performance in safety and
trustworthiness. Our findings indicate a duality between domain-specific
knowledge and safety in health communications. The results highlight the need
for intentional model design with targeted improvements, particularly in
mitigating harm and bias, and improving safety and affectiveness. This study
provides a comprehensive evaluation of LLMs for cancer communication, offering
critical insights for improving AI-generated health content and informing
future development of accurate, safe, and accessible digital health tools.

</details>


### [66] [KBAlign: Efficient Self Adaptation on Specific Knowledge Bases](https://arxiv.org/pdf/2411.14790)
*Zheni Zeng, Yuxuan Chen, Shi Yu, Ruobing Wang, Yukun Yan, Zhenghao Liu, Shuo Wang, Xu Han, Zhiyuan Liu, Maosong Sun*

Main category: cs.CL

TL;DR: KBAlign is a self-supervised framework enhancing RAG systems for KBQA by leveraging intrinsic model capabilities, achieving 90% of GPT-4-supervised performance gains without external signals.


<details>
  <summary>Details</summary>
Motivation: Current RAG methods struggle with targeted adaptation on small-scale KBs due to poor unsupervised training or costly fine-tuning.

Method: KBAlign uses multi-grained self-annotation for data construction and iterative tuning with self-verification for efficient adaptation.

Result: Achieves 90% of GPT-4-supervised performance gains, improving QA accuracy in specialized domains with minimal costs.

Conclusion: KBAlign offers a cost-effective, unsupervised solution for adapting RAG systems to specific KBs, benefiting specialized QA scenarios.

Abstract: Although retrieval-augmented generation (RAG) remains essential for
knowledge-based question answering (KBQA), current paradigms face critical
challenges under specific domains. Existing methods struggle with targeted
adaptation on small-scale KBs: vanilla unsupervised training exhibits poor
effectiveness, while fine-tuning incurs prohibitive costs of external signals.
We present KBAlign, a self-supervised framework that enhances RAG systems
through efficient model adaptation. Our key insight is to leverage the model's
intrinsic capabilities for knowledge alignment through two innovative
mechanisms: multi-grained self-annotation that captures global knowledge for
data construction, and iterative tuning that accelerates convergence through
self verification. This framework enables cost-effective model adaptation to
specific textual KBs, without human supervision or external model assistance.
Experiments demonstrate that KBAlign can achieve 90\% of the performance gain
obtained through GPT-4-supervised adaptation, while relying entirely on
self-annotation of much smaller models. KBAlign significantly improves
downstream QA accuracy across multiple domains with tiny costs, particularly
benefiting scenarios requiring deep knowledge integration from specialized
corpora. We release our experimental data, models, and process analyses to the
community for further exploration (https://github.com/thunlp/KBAlign).

</details>


### [67] [Simple and Provable Scaling Laws for the Test-Time Compute of Large Language Models](https://arxiv.org/pdf/2411.19477)
*Yanxi Chen, Xuchen Pan, Yaliang Li, Bolin Ding, Jingren Zhou*

Main category: cs.CL

TL;DR: Two algorithms for improving LLM test-time compute efficiency: a knockout-style and a league-style method, both proven to reduce failure probability exponentially or by power law with increased compute.


<details>
  <summary>Details</summary>
Motivation: To address the inefficiency in test-time compute for large language models (LLMs) by proposing scalable, practical algorithms without requiring additional components like verifiers or reward models.

Method: 1. Knockout-style: Generates multiple solutions and aggregates them via a tournament. 2. League-style: Evaluates solutions by average win rate against multiple opponents. Both rely on LLM-generated solutions and comparisons.

Result: Theoretical proofs show exponential or power-law decay in failure probability with increased compute. Experiments validate scalability across models and tasks.

Conclusion: The algorithms are practical, scalable, and adaptable, offering efficient test-time compute solutions for LLMs.

Abstract: We propose two simple, principled and practical algorithms that enjoy
provable scaling laws for the test-time compute of large language models
(LLMs). The first one is a two-stage knockout-style algorithm: given an input
problem, it first generates multiple candidate solutions, and then aggregate
them via a knockout tournament for the final output. Assuming that the LLM can
generate a correct solution with non-zero probability and do better than a
random guess in comparing a pair of correct and incorrect solutions, we prove
theoretically that the failure probability of this algorithm decays to zero
exponentially or by a power law (depending on the specific way of scaling) as
its test-time compute grows. The second one is a two-stage league-style
algorithm, where each candidate is evaluated by its average win rate against
multiple opponents, rather than eliminated upon loss to a single opponent.
Under analogous but more robust assumptions, we prove that its failure
probability also decays to zero exponentially with more test-time compute. Both
algorithms require a black-box LLM and nothing else (e.g., no verifier or
reward model) for a minimalistic implementation, which makes them appealing for
practical applications and easy to adapt for different tasks. Through extensive
experiments with diverse models and datasets, we validate the proposed theories
and demonstrate the outstanding scaling properties of both algorithms.

</details>


### [68] [Not All Adapters Matter: Selective Adapter Freezing for Memory-Efficient Fine-Tuning of Language Models](https://arxiv.org/pdf/2412.03587)
*Hyegang Son, Yonglak Son, Changhoon Kim, Young Geun Kim*

Main category: cs.CL

TL;DR: SAFE introduces selective freezing of less important adapters in adapter-tuning, reducing resource usage while maintaining performance.


<details>
  <summary>Details</summary>
Motivation: Existing adapter-tuning methods are resource-intensive, and not all adapters contribute equally to performance.

Method: Proposes SAFE, which gradually freezes less important adapters early to save resources.

Result: SAFE reduces memory, computation, and training time significantly (42.85%, 34.59%, 11.82%) while matching or improving performance.

Conclusion: SAFE efficiently balances resource usage and performance, also improving generalization by smoothing the loss landscape.

Abstract: Transformer-based large-scale pre-trained models achieve great success.
Fine-tuning is the standard practice for leveraging these models in downstream
tasks. Among the fine-tuning methods, adapter-tuning provides a
parameter-efficient fine-tuning by introducing lightweight trainable modules
while keeping most pre-trained parameters frozen. However, existing
adapter-tuning methods still impose substantial resource usage. Through our
investigation, we show that each adapter unequally contributes to both task
performance and resource usage. Motivated by this insight, we propose Selective
Adapter FrEezing (SAFE), which gradually freezes less important adapters early
to reduce unnecessary resource usage while maintaining performance. In our
experiments, SAFE reduces memory usage, computation amount, and training time
by 42.85\%, 34.59\%, and 11.82\%, respectively, while achieving comparable or
better task performance compared to the baseline. We also demonstrate that SAFE
induces regularization effect, thereby smoothing the loss landscape, which
enables the model to generalize better by avoiding sharp minima.

</details>


### [69] [FitCF: A Framework for Automatic Feature Importance-guided Counterfactual Example Generation](https://arxiv.org/pdf/2501.00777)
*Qianli Wang, Nils Feldhus, Simon Ostermann, Luis Felipe Villa-Arenas, Sebastian Möller, Vera Schmitt*

Main category: cs.CL

TL;DR: ZeroCF and FitCF are introduced for zero-shot and few-shot counterfactual generation in NLP, leveraging feature attribution methods and label flip verification to improve quality.


<details>
  <summary>Details</summary>
Motivation: Automated generation of counterfactual examples is challenging for LLMs, despite their performance. The paper aims to improve this using ZeroCF and FitCF.

Method: ZeroCF uses feature attribution for zero-shot generation. FitCF adds label flip verification and few-shot prompting, outperforming baselines.

Result: FitCF improves counterfactual quality (flip rate, perplexity, similarity). LIME and Integrated Gradients are effective backbones. Faithfulness of attribution correlates with quality.

Conclusion: ZeroCF and FitCF advance counterfactual generation, with FitCF's components and attribution methods proving critical for performance.

Abstract: Counterfactual examples are widely used in natural language processing (NLP)
as valuable data to improve models, and in explainable artificial intelligence
(XAI) to understand model behavior. The automated generation of counterfactual
examples remains a challenging task even for large language models (LLMs),
despite their impressive performance on many tasks. In this paper, we first
introduce ZeroCF, a faithful approach for leveraging important words derived
from feature attribution methods to generate counterfactual examples in a
zero-shot setting. Second, we present a new framework, FitCF, which further
verifies aforementioned counterfactuals by label flip verification and then
inserts them as demonstrations for few-shot prompting, outperforming two
state-of-the-art baselines. Through ablation studies, we identify the
importance of each of FitCF's core components in improving the quality of
counterfactuals, as assessed through flip rate, perplexity, and similarity
measures. Furthermore, we show the effectiveness of LIME and Integrated
Gradients as backbone attribution methods for FitCF and find that the number of
demonstrations has the largest effect on performance. Finally, we reveal a
strong correlation between the faithfulness of feature attribution scores and
the quality of generated counterfactuals.

</details>


### [70] [Benchmarking Generative AI for Scoring Medical Student Interviews in Objective Structured Clinical Examinations (OSCEs)](https://arxiv.org/pdf/2501.13957)
*Jadon Geathers, Yann Hicke, Colleen Chan, Niroop Rajashekar, Justin Sewell, Susannah Cornes, Rene F. Kizilcec, Dennis Shung*

Main category: cs.CL

TL;DR: The study evaluates the use of large language models (LLMs) to automate OSCE assessments, comparing four models (GPT-4o, Claude 3.5, Llama 3.1, Gemini 1.5 Pro) using the MIRS scale. Results show moderate to high accuracy in off-by-one and thresholded metrics, with low exact accuracy, and highlight the potential of AI-assisted evaluations.


<details>
  <summary>Details</summary>
Motivation: To address the time-consuming and potentially biased nature of human scoring in OSCE evaluations by exploring AI automation.

Method: Four LLMs were tested on 10 OSCE cases using the MIRS scale under zero-shot, chain-of-thought, few-shot, and multi-step prompting. Performance was measured using exact, off-by-one, and thresholded accuracy metrics.

Result: LLMs showed low exact accuracy (0.27-0.44) but moderate to high off-by-one (0.67-0.87) and thresholded accuracy (0.75-0.88). Techniques like CoT and few-shot improved performance for specific items.

Conclusion: The study demonstrates the feasibility of AI-assisted OSCE evaluations and provides a benchmark for future research in automating clinical communication assessments.

Abstract: Objective Structured Clinical Examinations (OSCEs) are widely used to assess
medical students' communication skills, but scoring interview-based assessments
is time-consuming and potentially subject to human bias. This study explored
the potential of large language models (LLMs) to automate OSCE evaluations
using the Master Interview Rating Scale (MIRS). We compared the performance of
four state-of-the-art LLMs (GPT-4o, Claude 3.5, Llama 3.1, and Gemini 1.5 Pro)
in evaluating OSCE transcripts across all 28 items of the MIRS under the
conditions of zero-shot, chain-of-thought (CoT), few-shot, and multi-step
prompting. The models were benchmarked against a dataset of 10 OSCE cases with
174 expert consensus scores available. Model performance was measured using
three accuracy metrics (exact, off-by-one, thresholded). Averaging across all
MIRS items and OSCE cases, LLMs performed with low exact accuracy (0.27 to
0.44), and moderate to high off-by-one accuracy (0.67 to 0.87) and thresholded
accuracy (0.75 to 0.88). A zero temperature parameter ensured high intra-rater
reliability ({\alpha} = 0.98 for GPT-4o). CoT, few-shot, and multi-step
techniques proved valuable when tailored to specific assessment items. The
performance was consistent across MIRS items, independent of encounter phases
and communication domains. We demonstrated the feasibility of AI-assisted OSCE
evaluation and provided benchmarking of multiple LLMs across multiple prompt
techniques. Our work provides a baseline performance assessment for LLMs that
lays a foundation for future research into automated assessment of clinical
communication skills.

</details>


### [71] [TensorLLM: Tensorising Multi-Head Attention for Enhanced Reasoning and Compression in LLMs](https://arxiv.org/pdf/2501.15674)
*Yuxuan Gu, Wuyang Zhou, Giorgos Iacovides, Danilo Mandic*

Main category: cs.CL

TL;DR: A novel framework improves LLM reasoning by compressing and denoising Multi-head Attention (MHA) weights using tensorization and Tucker decomposition, achieving up to 250x compression and enhanced performance without extra data or training.


<details>
  <summary>Details</summary>
Motivation: Existing techniques focus on denoising FFN but inefficiently utilize MHA, a core component of transformers. This work addresses this gap.

Method: Proposes MHA compression via multi-head tensorization and Tucker decomposition, enforcing shared higher-dimensional subspaces for denoising and compression.

Result: Consistent improvement in LLM reasoning across benchmarks, 250x MHA weight compression, and compatibility with FFN denoising techniques.

Conclusion: The method effectively enhances LLM reasoning and compression, seamlessly integrating with existing techniques for further gains.

Abstract: The reasoning abilities of Large Language Models (LLMs) can be improved by
structurally denoising their weights, yet existing techniques primarily focus
on denoising the feed-forward network (FFN) of the transformer block, and can
not efficiently utilise the Multi-head Attention (MHA) block, which is the core
of transformer architectures. To address this issue, we propose a novel
intuitive framework that, at its very core, performs MHA compression through a
multi-head tensorisation process and the Tucker decomposition. This enables
both higher-dimensional structured denoising and compression of the MHA
weights, by enforcing a shared higher-dimensional subspace across the weights
of the multiple attention heads. We demonstrate that this approach consistently
enhances the reasoning capabilities of LLMs across multiple benchmark datasets,
and for both encoder-only and decoder-only architectures, while achieving
compression rates of up to $\sim 250$ times in the MHA weights, all without
requiring any additional data, training, or fine-tuning. Furthermore, we show
that the proposed method can be seamlessly combined with existing
FFN-only-based denoising techniques to achieve further improvements in LLM
reasoning performance.

</details>


### [72] [ARR: Question Answering with Large Language Models via Analyzing, Retrieving, and Reasoning](https://arxiv.org/pdf/2502.04689)
*Yuwei Yin, Giuseppe Carenini*

Main category: cs.CL

TL;DR: ARR is a QA method for LLMs that improves performance by analyzing question intent, retrieving info, and reasoning step-by-step. It outperforms baselines across diverse tasks.


<details>
  <summary>Details</summary>
Motivation: Enhancing LLM performance in QA tasks is crucial for their development and applicability.

Method: ARR incorporates intent analysis, information retrieval, and step-by-step reasoning.

Result: ARR consistently outperforms baselines across 10 QA tasks and is robust to prompt variations.

Conclusion: ARR is effective, robust, and generalizable, with intent analysis being a key innovation.

Abstract: Large language models (LLMs) have demonstrated impressive capabilities on
complex evaluation benchmarks, many of which are formulated as
question-answering (QA) tasks. Enhancing the performance of LLMs in QA contexts
is becoming increasingly vital for advancing their development and
applicability. This paper introduces ARR, an intuitive, effective, and general
QA solving method that explicitly incorporates three key steps: analyzing the
intent of the question, retrieving relevant information, and reasoning step by
step. Notably, this paper is the first to introduce intent analysis in QA,
which plays a vital role in ARR. Comprehensive evaluations across 10 diverse QA
tasks demonstrate that ARR consistently outperforms the baseline methods.
Ablation and case studies further validate the positive contributions of each
ARR component. Furthermore, experiments involving variations in prompt design
indicate that ARR maintains its effectiveness regardless of the specific prompt
formulation. Additionally, extensive evaluations across various model sizes,
LLM series, and generation settings solidify the effectiveness, robustness, and
generalizability of ARR.

</details>


### [73] [Harnessing Multiple Large Language Models: A Survey on LLM Ensemble](https://arxiv.org/pdf/2502.18036)
*Zhijun Chen, Jingzheng Li, Pengpeng Chen, Zhuoran Li, Kai Sun, Yuankai Luo, Qianren Mao, Dingqi Yang, Hailong Sun, Philip S. Yu*

Main category: cs.CL

TL;DR: This paper provides a systematic review of LLM Ensemble, categorizing methods, discussing benchmarks, and suggesting future research directions.


<details>
  <summary>Details</summary>
Motivation: The increasing availability and diverse strengths of LLMs have spurred interest in leveraging multiple models for improved performance, necessitating a comprehensive review.

Method: The paper introduces a taxonomy of LLM Ensemble, classifies methods into 'ensemble-before-inference, ensemble-during-inference, ensemble-after-inference,' and reviews relevant techniques.

Result: A detailed classification of methods, benchmarks, and applications is presented, along with a curated list of papers.

Conclusion: The review highlights the advancements in LLM Ensemble and suggests future research directions to further the field.

Abstract: LLM Ensemble -- which involves the comprehensive use of multiple large
language models (LLMs), each aimed at handling user queries during downstream
inference, to benefit from their individual strengths -- has gained substantial
attention recently. The widespread availability of LLMs, coupled with their
varying strengths and out-of-the-box usability, has profoundly advanced the
field of LLM Ensemble. This paper presents the first systematic review of
recent developments in LLM Ensemble. First, we introduce our taxonomy of LLM
Ensemble and discuss several related research problems. Then, we provide a more
in-depth classification of the methods under the broad categories of
"ensemble-before-inference, ensemble-during-inference,
ensemble-after-inference'', and review all relevant methods. Finally, we
introduce related benchmarks and applications, summarize existing studies, and
suggest several future research directions. A curated list of papers on LLM
Ensemble is available at https://github.com/junchenzhi/Awesome-LLM-Ensemble.

</details>


### [74] [KwaiChat: A Large-Scale Video-Driven Multilingual Mixed-Type Dialogue Corpus](https://arxiv.org/pdf/2503.06899)
*Xiaoming Shi, Zeming Liu, Yiming Lei, Chenkai Zhang, Haitao Leng, Chuan Wang, Qingjie Liu, Wanxiang Che, Shaoguo Liu, Size Li, Yunhong Wang*

Main category: cs.CL

TL;DR: The paper introduces KwaiChat, a video-driven multilingual mixed-type dialogue corpus, to address limitations in current video-based dialogue systems. It evaluates 7 LLMs, finding GPT-4o performs best but still inadequately.


<details>
  <summary>Details</summary>
Motivation: Current video-based dialogue systems lack versatility due to reliance on single dialogue types, limiting practical applications.

Method: Proposes a novel task and creates KwaiChat, a corpus with 93,209 videos and 246,080 dialogues across multiple types, domains, languages, and topics. Baseline models are established and evaluated.

Result: GPT-4o performs best among 7 LLMs but still struggles, highlighting the task's complexity.

Conclusion: The task is non-trivial, requiring further research despite advancements in LLMs.

Abstract: Video-based dialogue systems, such as education assistants, have compelling
application value, thereby garnering growing interest. However, the current
video-based dialogue systems are limited by their reliance on a single dialogue
type, which hinders their versatility in practical applications across a range
of scenarios, including question-answering, emotional dialog, etc. In this
paper, we identify this challenge as how to generate video-driven multilingual
mixed-type dialogues. To mitigate this challenge, we propose a novel task and
create a human-to-human video-driven multilingual mixed-type dialogue corpus,
termed KwaiChat, containing a total of 93,209 videos and 246,080 dialogues,
across 4 dialogue types, 30 domains, 4 languages, and 13 topics. Additionally,
we establish baseline models on KwaiChat. An extensive analysis of 7 distinct
LLMs on KwaiChat reveals that GPT-4o achieves the best performance but still
cannot perform well in this situation even with the help of in-context learning
and fine-tuning, which indicates that the task is not trivial and needs further
research.

</details>


### [75] [Concise Reasoning via Reinforcement Learning](https://arxiv.org/pdf/2504.05185)
*Mehdi Fatemi, Banafsheh Rafiee, Mingjie Tang, Kartik Talamadupula*

Main category: cs.CL

TL;DR: The paper addresses the issue of excessive token usage in LLMs, linking it to RL-based training. It proposes a secondary RL phase to reduce verbosity while maintaining accuracy, and critiques GRPO's limitations.


<details>
  <summary>Details</summary>
Motivation: To challenge the assumption that longer responses improve reasoning accuracy and to reduce computational costs and resource usage in LLMs.

Method: Mathematical analysis of RL-based training, introduction of a secondary RL phase, and experimental validation.

Result: Shows conciseness correlates with accuracy; secondary RL training reduces token usage without sacrificing performance.

Conclusion: Proposes a method to optimize LLMs for conciseness and accuracy, highlighting GRPO's limitations for this purpose.

Abstract: Despite significant advancements in large language models (LLMs), a major
drawback of reasoning models is their enormous token usage, which increases
computational cost, resource requirements, and response time. In this work, we
revisit the core principles of reinforcement learning (RL) and, through
mathematical analysis, demonstrate that the tendency to generate lengthy
responses arises inherently from RL-based optimization during training. This
finding questions the prevailing assumption that longer responses inherently
improve reasoning accuracy. Instead, we uncover a natural correlation between
conciseness and accuracy that has been largely overlooked. We show that
introducing a secondary phase of RL training, using a very small set of
problems, can significantly reduce chains of thought while maintaining or even
enhancing accuracy. Additionally, we demonstrate that, while GRPO shares some
interesting properties of PPO, it suffers from collapse modes, which limit its
reliability for concise reasoning. Finally, we validate our conclusions through
extensive experimental results.

</details>


### [76] [Model Utility Law: Evaluating LLMs beyond Performance through Mechanism Interpretable Metric](https://arxiv.org/pdf/2504.07440)
*Yixin Cao, Jiahao Ying, Yaoning Wang, Xipeng Qiu, Xuanjing Huang, Yugang Jiang*

Main category: cs.CL

TL;DR: The paper introduces the Model Utilization Index (MUI) to address the generalization challenge in evaluating LLMs, quantifying effort via activated neurons. It reveals an inverse logarithmic relationship between MUI and performance, leading to practical applications like training diagnostics and fairer model comparisons.


<details>
  <summary>Details</summary>
Motivation: Current evaluation methods for LLMs struggle with generalization, as benchmarks are bounded while model abilities are near-unbounded. The paper aims to infer model capabilities more accurately.

Method: Proposes MUI, a metric based on the proportion of activated neurons during inference, to quantify model effort. Experiments validate an inverse logarithmic relationship between MUI and performance, termed the Utility Law.

Result: Extensive experiments show a consistent inverse logarithmic relationship (Utility Law) between MUI and performance. Four corollaries are derived for practical applications like diagnostics and model comparison.

Conclusion: MUI provides a novel, interpretable metric for LLM evaluation, addressing generalization challenges and offering actionable insights for model improvement and fair comparisons.

Abstract: Large Language Models (LLMs) have become indispensable across academia,
industry, and daily applications, yet current evaluation methods struggle to
keep pace with their rapid development. One core challenge of evaluation in the
large language model (LLM) era is the generalization issue: how to infer a
model's near-unbounded abilities from inevitably bounded benchmarks. We address
this challenge by proposing Model Utilization Index (MUI), a mechanism
interpretability enhanced metric that complements traditional performance
scores. MUI quantifies the effort a model expends on a task, defined as the
proportion of activated neurons or features during inference. Intuitively, a
truly capable model should achieve higher performance with lower effort.
Extensive experiments across popular LLMs reveal a consistent inverse
logarithmic relationship between MUI and performance, which we formulate as the
Utility Law. From this law we derive four practical corollaries that (i) guide
training diagnostics, (ii) expose data contamination issue, (iii) enable fairer
model comparisons, and (iv) design model-specific dataset diversity. Our code
can be found at https://github.com/ALEX-nlp/MUI-Eva.

</details>


### [77] [CLASH: Evaluating Language Models on Judging High-Stakes Dilemmas from Multiple Perspectives](https://arxiv.org/pdf/2504.10823)
*Ayoung Lee, Ryan Sungmo Kwon, Peter Railton, Lu Wang*

Main category: cs.CL

TL;DR: The paper introduces CLASH, a dataset for evaluating LLMs in high-stakes dilemmas, revealing gaps in their ability to handle ambivalence, value shifts, and steerability.


<details>
  <summary>Details</summary>
Motivation: To address the lack of evaluation for LLMs in high-stakes, value-conflicting scenarios, beyond everyday contexts.

Method: Created CLASH, a dataset of 345 dilemmas with 3,795 perspectives, and benchmarked 10 LLMs on ambivalence, discomfort, value shifts, and steerability.

Result: LLMs struggle with ambivalence and value shifts but predict discomfort reasonably. Steerability varies by perspective and value pairs.

Conclusion: LLMs need improvement in reasoning over complex values, and perspective framing impacts their performance.

Abstract: Navigating high-stakes dilemmas involving conflicting values is challenging
even for humans, let alone for AI. Yet prior work in evaluating the reasoning
capabilities of large language models (LLMs) in such situations has been
limited to everyday scenarios. To close this gap, this work first introduces
CLASH (Character perspective-based LLM Assessments in Situations with
High-stakes), a meticulously curated dataset consisting of 345 high-impact
dilemmas along with 3,795 individual perspectives of diverse values. In
particular, we design CLASH in a way to support the study of critical aspects
of value-based decision-making processes which are missing from prior work,
including understanding decision ambivalence and psychological discomfort as
well as capturing the temporal shifts of values in characters' perspectives. By
benchmarking 10 open and closed frontier models, we uncover several key
findings. (1) Even the strongest models, such as GPT-4o and Claude-Sonnet,
achieve less than 50% accuracy in identifying situations where the decision
should be ambivalent, while they perform significantly better in clear-cut
scenarios. (2) While LLMs reasonably predict psychological discomfort as marked
by human, they inadequately comprehend perspectives involving value shifts,
indicating a need for LLMs to reason over complex values. (3) Our experiments
also reveal a significant correlation between LLMs' value preferences and their
steerability towards a given value. (4) Finally, LLMs exhibit greater
steerability when engaged in value reasoning from a third-party perspective,
compared to a first-person setup, though certain value pairs benefit uniquely
from the first-person framing.

</details>


### [78] [Data-Driven Calibration of Prediction Sets in Large Vision-Language Models Based on Inductive Conformal Prediction](https://arxiv.org/pdf/2504.17671)
*Yuanchang Ye, Weiyan Wen*

Main category: cs.CL

TL;DR: A Split Conformal Prediction (SCP) framework is proposed to mitigate hallucination in Large Vision-Language Models (LVLMs) for VQA tasks, ensuring statistical guarantees and dynamic threshold calibration.


<details>
  <summary>Details</summary>
Motivation: LVLMs often produce hallucinated content with high confidence, posing risks in safety-critical applications like healthcare and autonomous systems.

Method: The framework uses dynamic threshold calibration and cross-modal consistency verification, partitioning data into calibration and test sets to compute nonconformity scores and construct prediction sets with statistical guarantees.

Result: Evaluations on benchmarks (ScienceQA, MMMU) show SCP enforces theoretical guarantees across all risk levels (α) and maintains stable performance across split ratios.

Conclusion: The SCP framework bridges the gap between theoretical reliability and practical applicability, offering a scalable solution for hallucination detection in multi-modal AI systems.

Abstract: This study addresses the critical challenge of hallucination mitigation in
Large Vision-Language Models (LVLMs) for Visual Question Answering (VQA) tasks
through a Split Conformal Prediction (SCP) framework. While LVLMs excel in
multi-modal reasoning, their outputs often exhibit hallucinated content with
high confidence, posing risks in safety-critical applications. We propose a
model-agnostic uncertainty quantification method that integrates dynamic
threshold calibration and cross-modal consistency verification. By partitioning
data into calibration and test sets, the framework computes nonconformity
scores to construct prediction sets with statistical guarantees under
user-defined risk levels ($\alpha$). Key innovations include: (1) rigorous
control of \textbf{marginal coverage} to ensure empirical error rates remain
strictly below $\alpha$; (2) dynamic adjustment of prediction set sizes
inversely with $\alpha$, filtering low-confidence outputs; (3) elimination of
prior distribution assumptions and retraining requirements. Evaluations on
benchmarks (ScienceQA, MMMU) with eight LVLMs demonstrate that SCP enforces
theoretical guarantees across all $\alpha$ values. The framework achieves
stable performance across varying calibration-to-test split ratios,
underscoring its robustness for real-world deployment in healthcare, autonomous
systems, and other safety-sensitive domains. This work bridges the gap between
theoretical reliability and practical applicability in multi-modal AI systems,
offering a scalable solution for hallucination detection and uncertainty-aware
decision-making.

</details>


### [79] [100 Days After DeepSeek-R1: A Survey on Replication Studies and More Directions for Reasoning Language Models](https://arxiv.org/pdf/2505.00551)
*Chong Zhang, Yue Deng, Xiang Lin, Bin Wang, Dianwen Ng, Hai Ye, Xingxuan Li, Yao Xiao, Zhanfeng Mo, Qi Zhang, Lidong Bing*

Main category: cs.CL

TL;DR: The paper summarizes replication studies of DeepSeek-R1, focusing on SFT and RLVR methods, data construction, and training procedures, to inspire future RLM research.


<details>
  <summary>Details</summary>
Motivation: To address the lack of open-source details for DeepSeek-R1 models and explore feasible replication strategies for advancing reasoning language models.

Method: Summarizes replication studies, focusing on supervised fine-tuning (SFT) and reinforcement learning from verifiable rewards (RLVR), including data and method details.

Result: Identifies key findings from replication studies, highlighting effective strategies for training RLMs and potential enhancements.

Conclusion: The survey aims to keep researchers updated on RLM advancements and inspire new ideas for improving reasoning language models.

Abstract: The recent development of reasoning language models (RLMs) represents a novel
evolution in large language models. In particular, the recent release of
DeepSeek-R1 has generated widespread social impact and sparked enthusiasm in
the research community for exploring the explicit reasoning paradigm of
language models. However, the implementation details of the released models
have not been fully open-sourced by DeepSeek, including DeepSeek-R1-Zero,
DeepSeek-R1, and the distilled small models. As a result, many replication
studies have emerged aiming to reproduce the strong performance achieved by
DeepSeek-R1, reaching comparable performance through similar training
procedures and fully open-source data resources. These works have investigated
feasible strategies for supervised fine-tuning (SFT) and reinforcement learning
from verifiable rewards (RLVR), focusing on data preparation and method design,
yielding various valuable insights. In this report, we provide a summary of
recent replication studies to inspire future research. We primarily focus on
SFT and RLVR as two main directions, introducing the details for data
construction, method design and training procedure of current replication
studies. Moreover, we conclude key findings from the implementation details and
experimental results reported by these studies, anticipating to inspire future
research. We also discuss additional techniques of enhancing RLMs, highlighting
the potential of expanding the application scope of these models, and
discussing the challenges in development. By this survey, we aim to help
researchers and developers of RLMs stay updated with the latest advancements,
and seek to inspire new ideas to further enhance RLMs.

</details>


### [80] [RM-R1: Reward Modeling as Reasoning](https://arxiv.org/pdf/2505.02387)
*Xiusi Chen, Gaotang Li, Ziqi Wang, Bowen Jin, Cheng Qian, Yu Wang, Hongru Wang, Yu Zhang, Denghui Zhang, Tong Zhang, Hanghang Tong, Heng Ji*

Main category: cs.CL

TL;DR: The paper introduces Reasoning Reward Models (ReasRMs) to enhance reward modeling by integrating reasoning capabilities, achieving state-of-the-art performance.


<details>
  <summary>Details</summary>
Motivation: Improving reward modeling for aligning LLMs with human preferences by making it more interpretable and effective through reasoning.

Method: Proposes ReasRMs with a chain-of-rubrics mechanism and a two-stage training pipeline: distillation of reasoning chains and RL with verifiable rewards.

Result: ReasRMs outperform larger models by up to 4.9% across benchmarks.

Conclusion: ReasRMs significantly improve reward modeling, with released models and resources for future research.

Abstract: Reward modeling is essential for aligning large language models (LLMs) with
human preferences through reinforcement learning (RL). To provide accurate
reward signals, a reward model (RM) should stimulate deep thinking and conduct
interpretable reasoning before assigning a score or a judgment. Inspired by
recent advances of long chain-of-thought (CoT) on reasoning-intensive tasks, we
hypothesize and validate that integrating reasoning capabilities into reward
modeling significantly enhances RM's interpretability and performance. To this
end, we introduce a new class of generative reward models -- Reasoning Reward
Models (ReasRMs) -- which formulate reward modeling as a reasoning task. We
propose a reasoning-oriented training pipeline and train a family of ReasRMs,
RM-R1. RM-R1 features a chain-of-rubrics (CoR) mechanism -- self-generating
sample-level chat rubrics or math/code solutions, and evaluating candidate
responses against them. The training of M-R1 consists of two key stages: (1)
distillation of high-quality reasoning chains and (2) reinforcement learning
with verifiable rewards. Empirically, our models achieve state-of-the-art
performance across three reward model benchmarks on average, outperforming much
larger open-weight models (e.g., INF-ORM-Llama3.1-70B) and proprietary ones
(e.g., GPT-4o) by up to 4.9%. Beyond final performance, we perform thorough
empirical analysis to understand the key ingredients of successful ReasRM
training. To facilitate future research, we release six ReasRM models along
with code and data at https://github.com/RM-R1-UIUC/RM-R1.

</details>


### [81] [Healthy LLMs? Benchmarking LLM Knowledge of UK Government Public Health Information](https://arxiv.org/pdf/2505.06046)
*Joshua Harris, Fan Grayson, Felix Feldman, Timothy Laurence, Toby Nonnenmacher, Oliver Higgins, Leo Loman, Selina Patel, Thomas Finnie, Samuel Collins, Michael Borowitz*

Main category: cs.CL

TL;DR: The paper introduces PubHealthBench, a benchmark for evaluating LLMs' knowledge of UK public health information, finding high accuracy in MCQA but lower performance in free-form responses.


<details>
  <summary>Details</summary>
Motivation: Understanding LLMs' domain-specific knowledge is critical for real-world use, especially in public health, where accuracy impacts UK residents.

Method: Created PubHealthBench with 8000+ questions from 687 UK government documents, using an automated pipeline for MCQA generation, and evaluated 24 LLMs.

Result: Latest private LLMs achieved >90% accuracy in MCQA, outperforming humans, but scored <75% in free-form responses, with better accuracy on general public guidance.

Conclusion: SOTA LLMs show promise for public health information but may need safeguards for free-form responses.

Abstract: As Large Language Models (LLMs) become widely accessible, a detailed
understanding of their knowledge within specific domains becomes necessary for
successful real world use. This is particularly critical in public health,
where failure to retrieve relevant, accurate, and current information could
significantly impact UK residents. However, currently little is known about LLM
knowledge of UK Government public health information. To address this issue,
this paper introduces a new benchmark, PubHealthBench, with over 8000 questions
for evaluating LLMs' Multiple Choice Question Answering (MCQA) and free form
responses to public health queries. To create PubHealthBench we extract free
text from 687 current UK government guidance documents and implement an
automated pipeline for generating MCQA samples. Assessing 24 LLMs on
PubHealthBench we find the latest private LLMs (GPT-4.5, GPT-4.1 and o1) have a
high degree of knowledge, achieving >90% accuracy in the MCQA setup, and
outperform humans with cursory search engine use. However, in the free form
setup we see lower performance with no model scoring >75%. Importantly we find
in both setups LLMs have higher accuracy on guidance intended for the general
public. Therefore, there are promising signs that state of the art (SOTA) LLMs
are an increasingly accurate source of public health information, but
additional safeguards or tools may still be needed when providing free form
responses on public health topics.

</details>


### [82] [SAS-Bench: A Fine-Grained Benchmark for Evaluating Short Answer Scoring with Large Language Models](https://arxiv.org/pdf/2505.07247)
*Peichao Lai, Kexuan Zhang, Yi Lin, Linyihan Zhang, Feiyang Ye, Jinhao Yan, Yanwei Xu, Conghui He, Yilei Wang, Wentao Zhang, Bin Cui*

Main category: cs.CL

TL;DR: SAS-Bench is a benchmark for LLM-based Short Answer Scoring, offering fine-grained evaluation, expert annotations, and a diverse dataset to improve model transparency and accuracy.


<details>
  <summary>Details</summary>
Motivation: Existing SAG methods lack detail and consistency with human judgment, and LLMs, while promising, suffer from bias and opacity in scoring.

Method: Introduces SAS-Bench with step-wise scoring, expert-annotated errors, and a dataset of 1,030 questions and 4,109 responses. Tests LLMs with few-shot prompting.

Result: Identifies challenges in science question scoring and shows few-shot prompting enhances accuracy.

Conclusion: SAS-Bench advances robust, fair, and explainable LLM-based evaluation systems for education.

Abstract: Subjective Answer Grading (SAG) plays a crucial role in education,
standardized testing, and automated assessment systems, particularly for
evaluating short-form responses in Short Answer Scoring (SAS). However,
existing approaches often produce coarse-grained scores and lack detailed
reasoning. Although large language models (LLMs) have demonstrated potential as
zero-shot evaluators, they remain susceptible to bias, inconsistencies with
human judgment, and limited transparency in scoring decisions. To overcome
these limitations, we introduce SAS-Bench, a benchmark specifically designed
for LLM-based SAS tasks. SAS-Bench provides fine-grained, step-wise scoring,
expert-annotated error categories, and a diverse range of question types
derived from real-world subject-specific exams. This benchmark facilitates
detailed evaluation of model reasoning processes and explainability. We also
release an open-source dataset containing 1,030 questions and 4,109 student
responses, each annotated by domain experts. Furthermore, we conduct
comprehensive experiments with various LLMs, identifying major challenges in
scoring science-related questions and highlighting the effectiveness of
few-shot prompting in improving scoring accuracy. Our work offers valuable
insights into the development of more robust, fair, and educationally
meaningful LLM-based evaluation systems.

</details>


### [83] [Hypernym Mercury: Token Optimization Through Semantic Field Constriction And Reconstruction From Hypernyms. A New Text Compression Method](https://arxiv.org/pdf/2505.08058)
*Chris Forrester, Octavia Sulea*

Main category: cs.CL

TL;DR: A novel token reduction method for LLM prompts achieves 90% reduction while maintaining semantic similarity, with controllable granularity and lossless potential.


<details>
  <summary>Details</summary>
Motivation: Optimizing compute efficiency in NLP and agentic AI by reducing token usage in prompts without losing semantic meaning.

Method: Introduces a text representation scheme and word-level semantic compression for paragraphs.

Result: Achieves over 90% token reduction with high semantic similarity, validated on open-source data like Dracula.

Conclusion: The technique is scalable, lossless, and adaptable across genres and models, promising for efficient AI applications.

Abstract: Compute optimization using token reduction of LLM prompts is an emerging task
in the fields of NLP and next generation, agentic AI. In this white paper, we
introduce a novel (patent pending) text representation scheme and a
first-of-its-kind word-level semantic compression of paragraphs that can lead
to over 90% token reduction, while retaining high semantic similarity to the
source text. We explain how this novel compression technique can be lossless
and how the detail granularity is controllable. We discuss benchmark results
over open source data (i.e. Bram Stoker's Dracula available through Project
Gutenberg) and show how our results hold at the paragraph level, across
multiple genres and models.

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [84] [A Computational Pipeline for Advanced Analysis of 4D Flow MRI in the Left Atrium](https://arxiv.org/pdf/2505.09746)
*Xabier Morales, Ayah Elsayed, Debbie Zhao, Filip Loncaric, Ainhoa Aguado, Mireia Masias, Gina Quill, Marc Ramos, Ada Doltra, Ana Garcia, Marta Sitges, David Marlevi, Alistair Young, Martyn Nash, Bart Bijnens, Oscar Camara*

Main category: cs.CV

TL;DR: The paper introduces an open-source computational framework for analyzing 4D Flow MRI in the left atrium (LA), addressing challenges like low velocities and diverse protocols. It enables accurate hemodynamic parameter analysis and explores their prognostic potential.


<details>
  <summary>Details</summary>
Motivation: Current limitations in understanding LA hemodynamics due to conventional ultrasound constraints and the lack of dedicated computational tools for 4D Flow MRI analysis.

Method: Development of an open-source framework for qualitative and quantitative analysis of 4D Flow MRI in the LA, tested across diverse datasets.

Result: High-accuracy automated segmentations (Dice > 0.9, Hausdorff 95 < 3 mm) and comprehensive assessment of hemodynamic parameters (energy, vorticity, pressure) in various disorders.

Conclusion: The framework enhances LA hemodynamic analysis and identifies potential prognostic biomarkers, demonstrating robustness across diverse data sources.

Abstract: The left atrium (LA) plays a pivotal role in modulating left ventricular
filling, but our comprehension of its hemodynamics is significantly limited by
the constraints of conventional ultrasound analysis. 4D flow magnetic resonance
imaging (4D Flow MRI) holds promise for enhancing our understanding of atrial
hemodynamics. However, the low velocities within the LA and the limited spatial
resolution of 4D Flow MRI make analyzing this chamber challenging. Furthermore,
the absence of dedicated computational frameworks, combined with diverse
acquisition protocols and vendors, complicates gathering large cohorts for
studying the prognostic value of hemodynamic parameters provided by 4D Flow
MRI. In this study, we introduce the first open-source computational framework
tailored for the analysis of 4D Flow MRI in the LA, enabling comprehensive
qualitative and quantitative analysis of advanced hemodynamic parameters. Our
framework proves robust to data from different centers of varying quality,
producing high-accuracy automated segmentations (Dice $>$ 0.9 and Hausdorff 95
$<$ 3 mm), even with limited training data. Additionally, we conducted the
first comprehensive assessment of energy, vorticity, and pressure parameters in
the LA across a spectrum of disorders to investigate their potential as
prognostic biomarkers.

</details>


### [85] [Dyadic Mamba: Long-term Dyadic Human Motion Synthesis](https://arxiv.org/pdf/2505.09827)
*Julian Tanke, Takashi Shibuya, Kengo Uchida, Koichi Saito, Yuki Mitsufuji*

Main category: cs.CV

TL;DR: Dyadic Mamba uses State-Space Models (SSMs) to generate realistic long-term dyadic human motion from text, outperforming transformer-based methods.


<details>
  <summary>Details</summary>
Motivation: Addressing the challenge of generating extended dyadic human motion sequences from text, where transformers fall short due to positional encoding limitations.

Method: Leverages SSMs with a simple architecture for seamless information flow between motion sequences, avoiding complex cross-attention.

Result: Competitive on short-term benchmarks and superior on long-term sequences, with a new benchmark proposed for evaluation.

Conclusion: SSM-based architectures like Dyadic Mamba are promising for long-term dyadic motion synthesis.

Abstract: Generating realistic dyadic human motion from text descriptions presents
significant challenges, particularly for extended interactions that exceed
typical training sequence lengths. While recent transformer-based approaches
have shown promising results for short-term dyadic motion synthesis, they
struggle with longer sequences due to inherent limitations in positional
encoding schemes. In this paper, we introduce Dyadic Mamba, a novel approach
that leverages State-Space Models (SSMs) to generate high-quality dyadic human
motion of arbitrary length. Our method employs a simple yet effective
architecture that facilitates information flow between individual motion
sequences through concatenation, eliminating the need for complex
cross-attention mechanisms. We demonstrate that Dyadic Mamba achieves
competitive performance on standard short-term benchmarks while significantly
outperforming transformer-based approaches on longer sequences. Additionally,
we propose a new benchmark for evaluating long-term motion synthesis quality,
providing a standardized framework for future research. Our results demonstrate
that SSM-based architectures offer a promising direction for addressing the
challenging task of long-term dyadic human motion synthesis from text
descriptions.

</details>


### [86] [BoundarySeg:An Embarrassingly Simple Method To Boost Medical Image Segmentation Performance for Low Data Regimes](https://arxiv.org/pdf/2505.09829)
*Tushar Kataria, Shireen Y. Elhabian*

Main category: cs.CV

TL;DR: Proposes BoundarySeg, a multi-task framework for medical image segmentation that improves accuracy without needing unannotated data.


<details>
  <summary>Details</summary>
Motivation: Challenges in obtaining and annotating medical data due to privacy and cost, and limitations of semi-supervised methods when unannotated data is scarce.

Method: BoundarySeg uses organ boundary prediction as an auxiliary task to full organ segmentation, leveraging consistency between tasks for supervision.

Result: Achieves performance comparable to or better than semi-supervised methods without unannotated data or added computational cost.

Conclusion: BoundarySeg is a simple, effective solution for medical image segmentation in low-data scenarios.

Abstract: Obtaining large-scale medical data, annotated or unannotated, is challenging
due to stringent privacy regulations and data protection policies. In addition,
annotating medical images requires that domain experts manually delineate
anatomical structures, making the process both time-consuming and costly. As a
result, semi-supervised methods have gained popularity for reducing annotation
costs. However, the performance of semi-supervised methods is heavily dependent
on the availability of unannotated data, and their effectiveness declines when
such data are scarce or absent. To overcome this limitation, we propose a
simple, yet effective and computationally efficient approach for medical image
segmentation that leverages only existing annotations. We propose BoundarySeg ,
a multi-task framework that incorporates organ boundary prediction as an
auxiliary task to full organ segmentation, leveraging consistency between the
two task predictions to provide additional supervision. This strategy improves
segmentation accuracy, especially in low data regimes, allowing our method to
achieve performance comparable to or exceeding state-of-the-art semi supervised
approaches all without relying on unannotated data or increasing computational
demands. Code will be released upon acceptance.

</details>


### [87] [Mission Balance: Generating Under-represented Class Samples using Video Diffusion Models](https://arxiv.org/pdf/2505.09858)
*Danush Kumar Venkatesh, Isabel Funke, Micha Pfeiffer, Fiona Kolbinger, Hanna Maria Schmeiser, Juergen Weitz, Marius Distler, Stefanie Speidel*

Main category: cs.CV

TL;DR: A two-stage, text-conditioned diffusion-based method is proposed to synthesize surgical videos, addressing data imbalance in surgical datasets and improving model performance in tasks like action recognition and event prediction.


<details>
  <summary>Details</summary>
Motivation: Data imbalance in surgical video datasets limits the development of high-performing deep learning models for intra-operative guidance.

Method: A two-stage approach using a 2D latent diffusion model for spatial content and temporal attention layers for consistency, with text-conditioned generation and rejection sampling for dataset augmentation.

Result: Synthetic videos from this method enhance performance in surgical action recognition and intra-operative event prediction.

Conclusion: The proposed method effectively addresses data imbalance and improves model performance, with open-source implementation available.

Abstract: Computer-assisted interventions can improve intra-operative guidance,
particularly through deep learning methods that harness the spatiotemporal
information in surgical videos. However, the severe data imbalance often found
in surgical video datasets hinders the development of high-performing models.
In this work, we aim to overcome the data imbalance by synthesizing surgical
videos. We propose a unique two-stage, text-conditioned diffusion-based method
to generate high-fidelity surgical videos for under-represented classes. Our
approach conditions the generation process on text prompts and decouples
spatial and temporal modeling by utilizing a 2D latent diffusion model to
capture spatial content and then integrating temporal attention layers to
ensure temporal consistency. Furthermore, we introduce a rejection sampling
strategy to select the most suitable synthetic samples, effectively augmenting
existing datasets to address class imbalance. We evaluate our method on two
downstream tasks-surgical action recognition and intra-operative event
prediction-demonstrating that incorporating synthetic videos from our approach
substantially enhances model performance. We open-source our implementation at
https://gitlab.com/nct_tso_public/surgvgen.

</details>


### [88] [Few-Shot Learning of Visual Compositional Concepts through Probabilistic Schema Induction](https://arxiv.org/pdf/2505.09859)
*Andrew Jun Lee, Taylor Webb, Trevor Bihl, Keith Holyoak, Hongjing Lu*

Main category: cs.CV

TL;DR: PSI, a model using deep learning for analogical mapping over structured representations, outperforms controls and mimics human-like learning by emphasizing relational similarity.


<details>
  <summary>Details</summary>
Motivation: To model human-like learning of visual concepts from few examples, focusing on structured representations and analogical mapping.

Method: Introduces Probabilistic Schema Induction (PSI), leveraging deep learning for analogical mapping over structured representations, with adaptive similarity weighting.

Result: PSI achieves human-like performance, outperforming controls by emphasizing relational similarity and relevant relations.

Conclusion: Structured representations and analogical mapping are key to rapid human-like learning, showing deep learning's potential in psychological modeling.

Abstract: The ability to learn new visual concepts from limited examples is a hallmark
of human cognition. While traditional category learning models represent each
example as an unstructured feature vector, compositional concept learning is
thought to depend on (1) structured representations of examples (e.g., directed
graphs consisting of objects and their relations) and (2) the identification of
shared relational structure across examples through analogical mapping. Here,
we introduce Probabilistic Schema Induction (PSI), a prototype model that
employs deep learning to perform analogical mapping over structured
representations of only a handful of examples, forming a compositional concept
called a schema. In doing so, PSI relies on a novel conception of similarity
that weighs object-level similarity and relational similarity, as well as a
mechanism for amplifying relations relevant to classification, analogous to
selective attention parameters in traditional models. We show that PSI produces
human-like learning performance and outperforms two controls: a prototype model
that uses unstructured feature vectors extracted from a deep learning model,
and a variant of PSI with weaker structured representations. Notably, we find
that PSI's human-like performance is driven by an adaptive strategy that
increases relational similarity over object-level similarity and upweights the
contribution of relations that distinguish classes. These findings suggest that
structured representations and analogical mapping are critical to modeling
rapid human-like learning of compositional visual concepts, and demonstrate how
deep learning can be leveraged to create psychological models.

</details>


### [89] [Large-Scale Gaussian Splatting SLAM](https://arxiv.org/pdf/2505.09915)
*Zhe Xin, Chenyang Wu, Penghui Huang, Yanyong Zhang, Yinian Mao, Guoquan Huang*

Main category: cs.CV

TL;DR: LSG-SLAM is a large-scale 3D Gaussian Splatting-based visual SLAM system using stereo cameras, outperforming existing methods in outdoor scenarios.


<details>
  <summary>Details</summary>
Motivation: Existing NeRF and 3DGS methods for visual SLAM are limited to indoor environments or require RGBD sensors. Robustness in large-scale outdoor scenarios is unexplored.

Method: LSG-SLAM uses multi-modality pose estimation, feature-alignment warping, continuous Gaussian Splatting submaps, loop detection, and structure refinement.

Result: Superior performance on EuRoc and KITTI datasets compared to Neural, 3DGS-based, and traditional approaches.

Conclusion: LSG-SLAM effectively addresses large-scale outdoor SLAM challenges with stereo cameras, offering robust reconstruction and scalability.

Abstract: The recently developed Neural Radiance Fields (NeRF) and 3D Gaussian
Splatting (3DGS) have shown encouraging and impressive results for visual SLAM.
However, most representative methods require RGBD sensors and are only
available for indoor environments. The robustness of reconstruction in
large-scale outdoor scenarios remains unexplored. This paper introduces a
large-scale 3DGS-based visual SLAM with stereo cameras, termed LSG-SLAM. The
proposed LSG-SLAM employs a multi-modality strategy to estimate prior poses
under large view changes. In tracking, we introduce feature-alignment warping
constraints to alleviate the adverse effects of appearance similarity in
rendering losses. For the scalability of large-scale scenarios, we introduce
continuous Gaussian Splatting submaps to tackle unbounded scenes with limited
memory. Loops are detected between GS submaps by place recognition and the
relative pose between looped keyframes is optimized utilizing rendering and
feature warping losses. After the global optimization of camera poses and
Gaussian points, a structure refinement module enhances the reconstruction
quality. With extensive evaluations on the EuRoc and KITTI datasets, LSG-SLAM
achieves superior performance over existing Neural, 3DGS-based, and even
traditional approaches. Project page: https://lsg-slam.github.io.

</details>


### [90] [AdaptCLIP: Adapting CLIP for Universal Visual Anomaly Detection](https://arxiv.org/pdf/2505.09926)
*Bin-Bin Gao, Yue Zhu, Jiangtao Yan, Yuezhi Cai, Weixi Zhang, Meng Wang, Jun Liu, Yong Liu, Lei Wang, Chengjie Wang*

Main category: cs.CV

TL;DR: AdaptCLIP is a simple yet effective method for universal visual anomaly detection using CLIP models, outperforming existing methods by learning adaptive representations alternately and incorporating contextual features.


<details>
  <summary>Details</summary>
Motivation: Existing methods for universal visual anomaly detection struggle with prompt design, token interactions, or require fine-tuning, limiting flexibility.

Method: AdaptCLIP adds three adapters (visual, textual, prompt-query) to CLIP models, learning adaptive representations alternately and using comparative learning with contextual and residual features.

Result: AdaptCLIP achieves state-of-the-art performance on 12 anomaly detection benchmarks across industrial and medical domains.

Conclusion: AdaptCLIP offers a training-free, flexible solution for zero-/few-shot anomaly detection, significantly improving generalization.

Abstract: Universal visual anomaly detection aims to identify anomalies from novel or
unseen vision domains without additional fine-tuning, which is critical in open
scenarios. Recent studies have demonstrated that pre-trained vision-language
models like CLIP exhibit strong generalization with just zero or a few normal
images. However, existing methods struggle with designing prompt templates,
complex token interactions, or requiring additional fine-tuning, resulting in
limited flexibility. In this work, we present a simple yet effective method
called AdaptCLIP based on two key insights. First, adaptive visual and textual
representations should be learned alternately rather than jointly. Second,
comparative learning between query and normal image prompt should incorporate
both contextual and aligned residual features, rather than relying solely on
residual features. AdaptCLIP treats CLIP models as a foundational service,
adding only three simple adapters, visual adapter, textual adapter, and
prompt-query adapter, at its input or output ends. AdaptCLIP supports
zero-/few-shot generalization across domains and possesses a training-free
manner on target domains once trained on a base dataset. AdaptCLIP achieves
state-of-the-art performance on 12 anomaly detection benchmarks from industrial
and medical domains, significantly outperforming existing competitive methods.
We will make the code and model of AdaptCLIP available at
https://github.com/gaobb/AdaptCLIP.

</details>


### [91] [DDFP: Data-dependent Frequency Prompt for Source Free Domain Adaptation of Medical Image Segmentation](https://arxiv.org/pdf/2505.09927)
*Siqi Yin, Shaolei Liu, Manning Wang*

Main category: cs.CV

TL;DR: A novel source-free domain adaptation (SFDA) framework is proposed, featuring preadaptation, frequency prompts, and style-related layer fine-tuning to improve pseudo-labels and image translation, outperforming existing methods in medical segmentation tasks.


<details>
  <summary>Details</summary>
Motivation: Addressing the limitations of existing SFDA methods, particularly in medical datasets where labeled source data is restricted due to privacy concerns, by improving pseudo-label quality and image style translation.

Method: Introduces preadaptation for high-quality pseudo-labels, data-dependent frequency prompts for better image translation, and style-related layer fine-tuning for efficient adaptation.

Result: Outperforms state-of-the-art methods in cross-modality abdominal and cardiac SFDA segmentation tasks.

Conclusion: The proposed framework effectively mitigates domain gaps and enhances adaptation efficiency in source-free settings, demonstrating superior performance in medical segmentation.

Abstract: Domain adaptation addresses the challenge of model performance degradation
caused by domain gaps. In the typical setup for unsupervised domain adaptation,
labeled data from a source domain and unlabeled data from a target domain are
used to train a target model. However, access to labeled source domain data,
particularly in medical datasets, can be restricted due to privacy policies. As
a result, research has increasingly shifted to source-free domain adaptation
(SFDA), which requires only a pretrained model from the source domain and
unlabeled data from the target domain data for adaptation. Existing SFDA
methods often rely on domain-specific image style translation and
self-supervision techniques to bridge the domain gap and train the target
domain model. However, the quality of domain-specific style-translated images
and pseudo-labels produced by these methods still leaves room for improvement.
Moreover, training the entire model during adaptation can be inefficient under
limited supervision. In this paper, we propose a novel SFDA framework to
address these challenges. Specifically, to effectively mitigate the impact of
domain gap in the initial training phase, we introduce preadaptation to
generate a preadapted model, which serves as an initialization of target model
and allows for the generation of high-quality enhanced pseudo-labels without
introducing extra parameters. Additionally, we propose a data-dependent
frequency prompt to more effectively translate target domain images into a
source-like style. To further enhance adaptation, we employ a style-related
layer fine-tuning strategy, specifically designed for SFDA, to train the target
model using the prompted target domain images and pseudo-labels. Extensive
experiments on cross-modality abdominal and cardiac SFDA segmentation tasks
demonstrate that our proposed method outperforms existing state-of-the-art
methods.

</details>


### [92] [VRU-CIPI: Crossing Intention Prediction at Intersections for Improving Vulnerable Road Users Safety](https://arxiv.org/pdf/2505.09935)
*Ahmed S. Abdelrahman, Mohamed Abdel-Aty, Quoc Dai Tran*

Main category: cs.CV

TL;DR: The paper introduces VRU-CIPI, a framework using GRU and Transformer self-attention to predict VRU crossing intentions at intersections, achieving 96.45% accuracy and real-time performance.


<details>
  <summary>Details</summary>
Motivation: Enhancing safety at urban intersections by accurately predicting VRU crossing intentions to prevent conflicts with vehicles.

Method: Combines GRU for temporal dynamics and Transformer self-attention for contextual/spatial dependencies.

Result: Achieves 96.45% accuracy and 33 FPS on the UCF-VRU dataset.

Conclusion: VRU-CIPI improves intersection safety via real-time predictions and I2V communication for proactive warnings.

Abstract: Understanding and predicting human behavior in-thewild, particularly at urban
intersections, remains crucial for enhancing interaction safety between road
users. Among the most critical behaviors are crossing intentions of Vulnerable
Road Users (VRUs), where misinterpretation may result in dangerous conflicts
with oncoming vehicles. In this work, we propose the VRU-CIPI framework with a
sequential attention-based model designed to predict VRU crossing intentions at
intersections. VRU-CIPI employs Gated Recurrent Unit (GRU) to capture temporal
dynamics in VRU movements, combined with a multi-head Transformer
self-attention mechanism to encode contextual and spatial dependencies critical
for predicting crossing direction. Evaluated on UCF-VRU dataset, our proposed
achieves state-of-the-art performance with an accuracy of 96.45% and achieving
real-time inference speed reaching 33 frames per second. Furthermore, by
integrating with Infrastructure-to-Vehicles (I2V) communication, our approach
can proactively enhance intersection safety through timely activation of
crossing signals and providing early warnings to connected vehicles, ensuring
smoother and safer interactions for all road users.

</details>


### [93] [Non-Registration Change Detection: A Novel Change Detection Task and Benchmark Dataset](https://arxiv.org/pdf/2505.09939)
*Zhe Shan, Lei Zhou, Liu Mao, Shaofan Chen, Chuanqiu Ren, Xia Xie*

Main category: cs.CV

TL;DR: Proposes non-registration change detection for emergencies, identifies 8 real-world scenarios, transforms datasets, and shows severe impact on existing methods.


<details>
  <summary>Details</summary>
Motivation: Addresses emergencies like disasters and accidents where traditional registration-based change detection fails due to misalignment.

Method: Systematically identifies 8 scenarios, develops image transformation schemes to adapt registration datasets for non-registration tasks.

Result: Demonstrates catastrophic performance drop in state-of-the-art methods when applied to non-registration change detection.

Conclusion: Highlights the need for robust methods in non-registration scenarios, provides dataset and code for further research.

Abstract: In this study, we propose a novel remote sensing change detection task,
non-registration change detection, to address the increasing number of
emergencies such as natural disasters, anthropogenic accidents, and military
strikes. First, in light of the limited discourse on the issue of
non-registration change detection, we systematically propose eight scenarios
that could arise in the real world and potentially contribute to the occurrence
of non-registration problems. Second, we develop distinct image transformation
schemes tailored to various scenarios to convert the available registration
change detection dataset into a non-registration version. Finally, we
demonstrate that non-registration change detection can cause catastrophic
damage to the state-of-the-art methods. Our code and dataset are available at
https://github.com/ShanZard/NRCD.

</details>


### [94] [CSPENet: Contour-Aware and Saliency Priors Embedding Network for Infrared Small Target Detection](https://arxiv.org/pdf/2505.09943)
*Jiakun Deng, Kexuan Li, Xingye Cui, Jiaxuan Li, Chang Long, Tian Pu, Zhenming Peng*

Main category: cs.CV

TL;DR: Proposes CSPENet for infrared small target detection, addressing dim target localization and contour perception under clutter. Outperforms state-of-the-art methods.


<details>
  <summary>Details</summary>
Motivation: Existing methods struggle with dim target localization and contour perception in cluttered environments, limiting detection performance.

Method: Introduces CSPENet with SCPEM for contour and saliency priors, DBPEA for feature fusion, and AGFEM for feature refinement.

Result: Outperforms state-of-the-art methods on NUDT-SIRST, IRSTD-1k, and NUAA-SIRST datasets.

Conclusion: CSPENet effectively improves infrared small target detection by leveraging contour-aware and saliency priors.

Abstract: Infrared small target detection (ISTD) plays a critical role in a wide range
of civilian and military applications. Existing methods suffer from
deficiencies in the localization of dim targets and the perception of contour
information under dense clutter environments, severely limiting their detection
performance. To tackle these issues, we propose a contour-aware and saliency
priors embedding network (CSPENet) for ISTD. We first design a
surround-convergent prior extraction module (SCPEM) that effectively captures
the intrinsic characteristic of target contour pixel gradients converging
toward their center. This module concurrently extracts two collaborative
priors: a boosted saliency prior for accurate target localization and
multi-scale structural priors for comprehensively enriching contour detail
representation. Building upon this, we propose a dual-branch priors embedding
architecture (DBPEA) that establishes differentiated feature fusion pathways,
embedding these two priors at optimal network positions to achieve performance
enhancement. Finally, we develop an attention-guided feature enhancement module
(AGFEM) to refine feature representations and improve saliency estimation
accuracy. Experimental results on public datasets NUDT-SIRST, IRSTD-1k, and
NUAA-SIRST demonstrate that our CSPENet outperforms other state-of-the-art
methods in detection performance. The code is available at
https://github.com/IDIP2025/CSPENet.

</details>


### [95] [High Quality Underwater Image Compression with Adaptive Correction and Codebook-based Augmentation](https://arxiv.org/pdf/2505.09986)
*Yimin Zhou, Yichong Xia, Sicheng Pan, Bin Chen, Baoyi An, Haoqian Wang, Zhi Wang, Yaowei Wang, Zikun Zhou*

Main category: cs.CV

TL;DR: HQUIC is a novel underwater image compression method leveraging unique underwater features for better efficiency, outperforming existing techniques.


<details>
  <summary>Details</summary>
Motivation: Underwater image compression lacks optimization for underwater-specific characteristics, leading to subpar performance.

Method: HQUIC uses an ALTC module for adaptive prediction of attenuation coefficients and global light, a codebook for common object extraction, and dynamic weighting of multi-scale frequency components.

Result: HQUIC achieves superior performance compared to state-of-the-art compression methods on diverse underwater datasets.

Conclusion: HQUIC effectively addresses underwater image compression challenges by exploiting scene-specific features, offering enhanced efficiency.

Abstract: With the increasing exploration and exploitation of the underwater world,
underwater images have become a critical medium for human interaction with
marine environments, driving extensive research into their efficient
transmission and storage. However, contemporary underwater image compression
algorithms fail to fully leverage the unique characteristics distinguishing
underwater scenes from terrestrial images, resulting in suboptimal performance.
To address this limitation, we introduce HQUIC, designed to exploit
underwater-image-specific features for enhanced compression efficiency. HQUIC
employs an ALTC module to adaptively predict the attenuation coefficients and
global light information of the images, which effectively mitigates the issues
caused by the differences in lighting and tone existing in underwater images.
Subsequently, HQUIC employs a codebook as an auxiliary branch to extract the
common objects within underwater images and enhances the performance of the
main branch. Furthermore, HQUIC dynamically weights multi-scale frequency
components, prioritizing information critical for distortion quality while
discarding redundant details. Extensive evaluations on diverse underwater
datasets demonstrate that HQUIC outperforms state-of-the-art compression
methods.

</details>


### [96] [MambaControl: Anatomy Graph-Enhanced Mamba ControlNet with Fourier Refinement for Diffusion-Based Disease Trajectory Prediction](https://arxiv.org/pdf/2505.09965)
*Hao Yang, Tao Tan, Shuai Tan, Weiqin Yang, Kunyan Cai, Calvin Chen, Yue Sun*

Main category: cs.CV

TL;DR: MambaControl integrates selective state-space modelling and diffusion processes for high-fidelity prediction of medical image trajectories, improving disease progression modelling.


<details>
  <summary>Details</summary>
Motivation: Existing methods struggle with longitudinal dependencies and structural consistency in progressive disorders, necessitating a better approach.

Method: Combines Mamba-based long-range modelling with graph-guided anatomical control and Fourier-enhanced spectral graph representations.

Result: Achieves state-of-the-art performance in Alzheimer's disease prediction with improved progression quality and anatomical fidelity.

Conclusion: MambaControl shows promise for personalised prognosis and clinical decision support.

Abstract: Modelling disease progression in precision medicine requires capturing
complex spatio-temporal dynamics while preserving anatomical integrity.
Existing methods often struggle with longitudinal dependencies and structural
consistency in progressive disorders. To address these limitations, we
introduce MambaControl, a novel framework that integrates selective state-space
modelling with diffusion processes for high-fidelity prediction of medical
image trajectories. To better capture subtle structural changes over time while
maintaining anatomical consistency, MambaControl combines Mamba-based
long-range modelling with graph-guided anatomical control to more effectively
represent anatomical correlations. Furthermore, we introduce Fourier-enhanced
spectral graph representations to capture spatial coherence and multiscale
detail, enabling MambaControl to achieve state-of-the-art performance in
Alzheimer's disease prediction. Quantitative and regional evaluations
demonstrate improved progression prediction quality and anatomical fidelity,
highlighting its potential for personalised prognosis and clinical decision
support.

</details>


### [97] [IMITATE: Image Registration with Context for unknown time frame recovery](https://arxiv.org/pdf/2505.10124)
*Ziad Kheil, Lucas Robinet, Laurent Risser, Soleakhena Ken*

Main category: cs.CV

TL;DR: A novel image registration method using conditional U-Net for estimating unknown condition-related images from known ones, applied to tumor movement in radiotherapy with 4D-CT scans, achieving artefact-free results.


<details>
  <summary>Details</summary>
Motivation: To address challenges in image registration for radiotherapy, especially irregular breathing and motion artefacts in 4D-CT scans.

Method: A conditional U-Net architecture that incorporates conditional information without needing fixed images, applied to 4D-CT thoracoabdominal scans.

Result: Artefact-free 3D volumes with real-time latencies, demonstrated on clinical 4D-CT data.

Conclusion: The proposed method effectively handles complex motion interpolation in radiotherapy, improving reconstruction quality.

Abstract: In this paper, we formulate a novel image registration formalism dedicated to
the estimation of unknown condition-related images, based on two or more known
images and their associated conditions. We show how to practically model this
formalism by using a new conditional U-Net architecture, which fully takes into
account the conditional information and does not need any fixed image. Our
formalism is then applied to image moving tumors for radiotherapy treatment at
different breathing amplitude using 4D-CT (3D+t) scans in thoracoabdominal
regions. This driving application is particularly complex as it requires to
stitch a collection of sequential 2D slices into several 3D volumes at
different organ positions. Movement interpolation with standard methods then
generates well known reconstruction artefacts in the assembled volumes due to
irregular patient breathing, hysteresis and poor correlation of breathing
signal to internal motion. Results obtained on 4D-CT clinical data showcase
artefact-free volumes achieved through real-time latencies. The code is
publicly available at https://github.com/Kheil-Z/IMITATE .

</details>


### [98] [TKFNet: Learning Texture Key Factor Driven Feature for Facial Expression Recognition](https://arxiv.org/pdf/2505.09967)
*Liqian Deng*

Main category: cs.CV

TL;DR: A novel FER framework focusing on Texture Key Driver Factors (TKDF) achieves state-of-the-art performance by leveraging localized texture cues and a dual-component architecture (TAFE and DCIF).


<details>
  <summary>Details</summary>
Motivation: FER in the wild is challenging due to subtle, localized expression features and complex facial variations. TKDFs address this by identifying discriminative texture regions.

Method: Proposes a framework with Texture-Aware Feature Extractor (TAFE) for fine-grained texture extraction and Dual Contextual Information Filtering (DCIF) for feature refinement.

Result: Achieves state-of-the-art performance on RAF-DB and KDEF datasets, validating TKDF's effectiveness.

Conclusion: Incorporating TKDFs into FER pipelines enhances performance and robustness.

Abstract: Facial expression recognition (FER) in the wild remains a challenging task
due to the subtle and localized nature of expression-related features, as well
as the complex variations in facial appearance. In this paper, we introduce a
novel framework that explicitly focuses on Texture Key Driver Factors (TKDF),
localized texture regions that exhibit strong discriminative power across
emotional categories. By carefully observing facial image patterns, we identify
that certain texture cues, such as micro-changes in skin around the brows,
eyes, and mouth, serve as primary indicators of emotional dynamics. To
effectively capture and leverage these cues, we propose a FER architecture
comprising a Texture-Aware Feature Extractor (TAFE) and Dual Contextual
Information Filtering (DCIF). TAFE employs a ResNet-based backbone enhanced
with multi-branch attention to extract fine-grained texture representations,
while DCIF refines these features by filtering context through adaptive pooling
and attention mechanisms. Experimental results on RAF-DB and KDEF datasets
demonstrate that our method achieves state-of-the-art performance, verifying
the effectiveness and robustness of incorporating TKDFs into FER pipelines.

</details>


### [99] [APCoTTA: Continual Test-Time Adaptation for Semantic Segmentation of Airborne LiDAR Point Clouds](https://arxiv.org/pdf/2505.09971)
*Yuan Gao, Shaobo Xia, Sheng Nie, Cheng Wang, Xiaohuan Xi, Bisheng Yang*

Main category: cs.CV

TL;DR: APCoTTA is a novel CTTA method for ALS point cloud segmentation, addressing domain shifts, catastrophic forgetting, and error accumulation via dynamic layer selection, entropy-based consistency loss, and parameter interpolation. It outperforms direct inference by 9-14% mIoU on new benchmarks.


<details>
  <summary>Details</summary>
Motivation: Domain shifts in ALS point clouds degrade model performance post-training. CTTA can adapt models to evolving target domains, but lacks research and benchmarks for ALS data.

Method: APCoTTA uses dynamic trainable layer selection, entropy-based consistency loss, and random parameter interpolation to balance adaptation and source knowledge retention.

Result: APCoTTA improves mIoU by ~9% and 14% over direct inference on ISPRSC and H3DC benchmarks.

Conclusion: APCoTTA effectively addresses CTTA challenges for ALS point clouds, offering superior performance and new benchmarks for future research.

Abstract: Airborne laser scanning (ALS) point cloud segmentation is a fundamental task
for large-scale 3D scene understanding. In real-world applications, models are
typically fixed after training. However, domain shifts caused by changes in the
environment, sensor types, or sensor degradation often lead to a decline in
model performance. Continuous Test-Time Adaptation (CTTA) offers a solution by
adapting a source-pretrained model to evolving, unlabeled target domains.
Despite its potential, research on ALS point clouds remains limited, facing
challenges such as the absence of standardized datasets and the risk of
catastrophic forgetting and error accumulation during prolonged adaptation. To
tackle these challenges, we propose APCoTTA, the first CTTA method tailored for
ALS point cloud semantic segmentation. We propose a dynamic trainable layer
selection module. This module utilizes gradient information to select
low-confidence layers for training, and the remaining layers are kept frozen,
mitigating catastrophic forgetting. To further reduce error accumulation, we
propose an entropy-based consistency loss. By losing such samples based on
entropy, we apply consistency loss only to the reliable samples, enhancing
model stability. In addition, we propose a random parameter interpolation
mechanism, which randomly blends parameters from the selected trainable layers
with those of the source model. This approach helps balance target adaptation
and source knowledge retention, further alleviating forgetting. Finally, we
construct two benchmarks, ISPRSC and H3DC, to address the lack of CTTA
benchmarks for ALS point cloud segmentation. Experimental results demonstrate
that APCoTTA achieves the best performance on two benchmarks, with mIoU
improvements of approximately 9% and 14% over direct inference. The new
benchmarks and code are available at https://github.com/Gaoyuan2/APCoTTA.

</details>


### [100] [PointArena: Probing Multimodal Grounding Through Language-Guided Pointing](https://arxiv.org/pdf/2505.09990)
*Long Cheng, Jiafei Duan, Yi Ru Wang, Haoquan Fang, Boyang Li, Yushan Huang, Elvis Wang, Ainaz Eftekhar, Jason Lee, Wentao Yuan, Rose Hendrix, Noah A. Smith, Fei Xia, Dieter Fox, Ranjay Krishna*

Main category: cs.CV

TL;DR: PointArena is a platform for evaluating multimodal pointing across diverse reasoning scenarios, featuring a dataset, interactive arena, and robotic system. Molmo-72B outperforms others, and supervised training improves pointing performance.


<details>
  <summary>Details</summary>
Motivation: To address the lack of comprehensive benchmarks for multimodal pointing beyond referential object localization, enabling better evaluation in diverse reasoning scenarios.

Method: PointArena includes Point-Bench (dataset), Point-Battle (interactive arena), and Point-Act (robotic system). Evaluated state-of-the-art models with supervised training.

Result: Molmo-72B consistently outperforms other models; proprietary models show comparable performance. Supervised training enhances pointing capabilities.

Conclusion: Precise pointing is crucial for multimodal models to bridge abstract reasoning with real-world actions, validated by strong correlations in evaluations.

Abstract: Pointing serves as a fundamental and intuitive mechanism for grounding
language within visual contexts, with applications spanning robotics, assistive
technologies, and interactive AI systems. While recent multimodal models have
started to support pointing capabilities, existing benchmarks typically focus
only on referential object localization tasks. We introduce PointArena, a
comprehensive platform for evaluating multimodal pointing across diverse
reasoning scenarios. PointArena comprises three components: (1) Point-Bench, a
curated dataset containing approximately 1,000 pointing tasks across five
reasoning categories; (2) Point-Battle, an interactive, web-based arena
facilitating blind, pairwise model comparisons, which has already gathered over
4,500 anonymized votes; and (3) Point-Act, a real-world robotic manipulation
system allowing users to directly evaluate multimodal model pointing
capabilities in practical settings. We conducted extensive evaluations of both
state-of-the-art open-source and proprietary multimodal models. Results
indicate that Molmo-72B consistently outperforms other models, though
proprietary models increasingly demonstrate comparable performance.
Additionally, we find that supervised training specifically targeting pointing
tasks significantly enhances model performance. Across our multi-stage
evaluation pipeline, we also observe strong correlations, underscoring the
critical role of precise pointing capabilities in enabling multimodal models to
effectively bridge abstract reasoning with concrete, real-world actions.
Project page: https://pointarena.github.io/

</details>


### [101] [Descriptive Image-Text Matching with Graded Contextual Similarity](https://arxiv.org/pdf/2505.09997)
*Jinhyun Jang, Jiyeong Lee, Kwanghoon Sohn*

Main category: cs.CV

TL;DR: The paper proposes DITM, a method for descriptive image-text matching that learns graded contextual similarity by leveraging language descriptiveness, improving upon sparse binary supervision.


<details>
  <summary>Details</summary>
Motivation: Existing methods use sparse binary supervision, missing many-to-many image-text relationships and hierarchical connections. DITM addresses this by exploring descriptive flexibility.

Method: DITM uses TF-IDF to score sentence descriptiveness, refining false negatives and aligning sentences from generic to specific.

Result: Experiments on MS-COCO, Flickr30K, and CxC show DITM outperforms state-of-the-art methods in capturing complex relationships and hierarchical reasoning.

Conclusion: DITM advances image-text matching by moving beyond binary supervision, improving both matching accuracy and hierarchical understanding.

Abstract: Image-text matching aims to build correspondences between visual and textual
data by learning their pairwise similarities. Most existing approaches have
adopted sparse binary supervision, indicating whether a pair of images and
sentences matches or not. However, such sparse supervision covers a limited
subset of image-text relationships, neglecting their inherent many-to-many
correspondences; an image can be described in numerous texts at different
descriptive levels. Moreover, existing approaches overlook the implicit
connections from general to specific descriptions, which form the underlying
rationale for the many-to-many relationships between vision and language. In
this work, we propose descriptive image-text matching, called DITM, to learn
the graded contextual similarity between image and text by exploring the
descriptive flexibility of language. We formulate the descriptiveness score of
each sentence with cumulative term frequency-inverse document frequency
(TF-IDF) to balance the pairwise similarity according to the keywords in the
sentence. Our method leverages sentence descriptiveness to learn robust
image-text matching in two key ways: (1) to refine the false negative labeling,
dynamically relaxing the connectivity between positive and negative pairs, and
(2) to build more precise matching, aligning a set of relevant sentences in a
generic-to-specific order. By moving beyond rigid binary supervision, DITM
enhances the discovery of both optimal matches and potential positive pairs.
Extensive experiments on MS-COCO, Flickr30K, and CxC datasets demonstrate the
effectiveness of our method in representing complex image-text relationships
compared to state-of-the-art approaches. In addition, DITM enhances the
hierarchical reasoning ability of the model, supported by the extensive
analysis on HierarCaps benchmark.

</details>


### [102] [From Air to Wear: Personalized 3D Digital Fashion with AR/VR Immersive 3D Sketching](https://arxiv.org/pdf/2505.09998)
*Ying Zang, Yuanqi Hu, Xinyu Chen, Yuxia Xu, Suhui Wang, Chunan Yu, Lanyun Zhu, Deyi Ji, Xin Xu, Tianrun Chen*

Main category: cs.CV

TL;DR: A 3D sketch-driven framework enables everyday users to create digital clothing in AR/VR, overcoming technical barriers with a diffusion model and new dataset.


<details>
  <summary>Details</summary>
Motivation: Existing 3D garment design tools are inaccessible to non-experts due to technical complexity and lack of data.

Method: Combines a conditional diffusion model, sketch encoder, and adaptive curriculum learning to interpret free-hand sketches into realistic garments. Introduces KO3DClothes dataset.

Result: Outperforms baselines in fidelity and usability, validated by experiments and user studies.

Conclusion: Promising for democratizing fashion design in AR/VR platforms.

Abstract: In the era of immersive consumer electronics, such as AR/VR headsets and
smart devices, people increasingly seek ways to express their identity through
virtual fashion. However, existing 3D garment design tools remain inaccessible
to everyday users due to steep technical barriers and limited data. In this
work, we introduce a 3D sketch-driven 3D garment generation framework that
empowers ordinary users - even those without design experience - to create
high-quality digital clothing through simple 3D sketches in AR/VR environments.
By combining a conditional diffusion model, a sketch encoder trained in a
shared latent space, and an adaptive curriculum learning strategy, our system
interprets imprecise, free-hand input and produces realistic, personalized
garments. To address the scarcity of training data, we also introduce
KO3DClothes, a new dataset of paired 3D garments and user-created sketches.
Extensive experiments and user studies confirm that our method significantly
outperforms existing baselines in both fidelity and usability, demonstrating
its promise for democratized fashion design on next-generation consumer
platforms.

</details>


### [103] [Application of YOLOv8 in monocular downward multiple Car Target detection](https://arxiv.org/pdf/2505.10016)
*Shijie Lyu*

Main category: cs.CV

TL;DR: An improved YOLOv8-based object detection network for autonomous driving, addressing challenges like cost and weather vulnerability, achieves 65% accuracy in detecting multi-scale and small objects.


<details>
  <summary>Details</summary>
Motivation: Current autonomous driving object detection technologies face issues like high costs, weather sensitivity, and limited resolution, necessitating a more efficient solution.

Method: Enhanced YOLOv8 with structural reparameterization, a bidirectional pyramid structure, and a novel detection pipeline for better multi-scale and small-object detection.

Result: Achieves 65% detection accuracy, outperforming traditional methods, especially in single-target and small-object scenarios.

Conclusion: The improved model is promising for real-world autonomous driving applications and competitions like FSAC.

Abstract: Autonomous driving technology is progressively transforming traditional car
driving methods, marking a significant milestone in modern transportation.
Object detection serves as a cornerstone of autonomous systems, playing a vital
role in enhancing driving safety, enabling autonomous functionality, improving
traffic efficiency, and facilitating effective emergency responses. However,
current technologies such as radar for environmental perception, cameras for
road perception, and vehicle sensor networks face notable challenges, including
high costs, vulnerability to weather and lighting conditions, and limited
resolution.To address these limitations, this paper presents an improved
autonomous target detection network based on YOLOv8. By integrating structural
reparameterization technology, a bidirectional pyramid structure network model,
and a novel detection pipeline into the YOLOv8 framework, the proposed approach
achieves highly efficient and precise detection of multi-scale, small, and
remote objects. Experimental results demonstrate that the enhanced model can
effectively detect both large and small objects with a detection accuracy of
65%, showcasing significant advancements over traditional methods.This improved
model holds substantial potential for real-world applications and is
well-suited for autonomous driving competitions, such as the Formula Student
Autonomous China (FSAC), particularly excelling in scenarios involving
single-target and small-object detection.

</details>


### [104] [ORL-LDM: Offline Reinforcement Learning Guided Latent Diffusion Model Super-Resolution Reconstruction](https://arxiv.org/pdf/2505.10027)
*Shijie Lyu*

Main category: cs.CV

TL;DR: A reinforcement learning-based latent diffusion model (LDM) fine-tuning method is proposed for remote sensing image super-resolution, showing significant improvements in PSNR, SSIM, and LPIPS metrics.


<details>
  <summary>Details</summary>
Motivation: Existing deep learning methods for super-resolution struggle with complex scenes and detail preservation, prompting the need for a more effective approach.

Method: The method uses reinforcement learning (PPO) to fine-tune an LDM during its reverse denoising process, optimizing states, actions, and rewards.

Result: Experiments on RESISC45 show PSNR gains of 3-4dB, SSIM improvements of 0.08-0.11, and LPIPS reductions of 0.06-0.10, especially in complex scenes.

Conclusion: The proposed method effectively enhances super-resolution quality and adaptability across diverse scenes.

Abstract: With the rapid advancement of remote sensing technology, super-resolution
image reconstruction is of great research and practical significance. Existing
deep learning methods have made progress but still face limitations in handling
complex scenes and preserving image details. This paper proposes a
reinforcement learning-based latent diffusion model (LDM) fine-tuning method
for remote sensing image super-resolution. The method constructs a
reinforcement learning environment with states, actions, and rewards,
optimizing decision objectives through proximal policy optimization (PPO)
during the reverse denoising process of the LDM model. Experiments on the
RESISC45 dataset show significant improvements over the baseline model in PSNR,
SSIM, and LPIPS, with PSNR increasing by 3-4dB, SSIM improving by 0.08-0.11,
and LPIPS reducing by 0.06-0.10, particularly in structured and complex natural
scenes. The results demonstrate the method's effectiveness in enhancing
super-resolution quality and adaptability across scenes.

</details>


### [105] [DeepSeqCoco: A Robust Mobile Friendly Deep Learning Model for Detection of Diseases in Cocos nucifera](https://arxiv.org/pdf/2505.10030)
*Miit Daga, Dhriti Parikh, Swarna Priya Ramu*

Main category: cs.CV

TL;DR: DeepSeqCoco is a deep learning model for automated coconut tree disease identification, achieving 99.5% accuracy and faster processing than existing methods.


<details>
  <summary>Details</summary>
Motivation: Manual disease identification in coconut trees is labor-intensive and non-scalable, especially in developing countries.

Method: DeepSeqCoco uses deep learning with optimizers like SGD, Adam, and hybrid configurations for optimal performance.

Result: The model achieves 99.5% accuracy, 5% higher than existing models, with reduced training (18%) and prediction (85%) times.

Conclusion: DeepSeqCoco offers a scalable, efficient AI solution for precision agriculture in disease monitoring.

Abstract: Coconut tree diseases are a serious risk to agricultural yield, particularly
in developing countries where conventional farming practices restrict early
diagnosis and intervention. Current disease identification methods are manual,
labor-intensive, and non-scalable. In response to these limitations, we come up
with DeepSeqCoco, a deep learning based model for accurate and automatic
disease identification from coconut tree images. The model was tested under
various optimizer settings, such as SGD, Adam, and hybrid configurations, to
identify the optimal balance between accuracy, minimization of loss, and
computational cost. Results from experiments indicate that DeepSeqCoco can
achieve as much as 99.5% accuracy (achieving up to 5% higher accuracy than
existing models) with the hybrid SGD-Adam showing the lowest validation loss of
2.81%. It also shows a drop of up to 18% in training time and up to 85% in
prediction time for input images. The results point out the promise of the
model to improve precision agriculture through an AI-based, scalable, and
efficient disease monitoring system.

</details>


### [106] [Measuring Student Behavioral Engagement using Histogram of Actions](https://arxiv.org/pdf/2307.09420)
*Ahmed Abdelkawy, Aly Farag, Islam Alkabbany, Asem Ali, Chris Foreman, Thomas Tretter, Nicholas Hindy*

Main category: cs.CV

TL;DR: A novel technique uses 3D-CNN and SVM to measure student engagement by recognizing actions from video segments.


<details>
  <summary>Details</summary>
Motivation: To improve behavioral engagement measurement in educational settings by analyzing student actions.

Method: Uses 3D-CNN for action recognition from human skeletons and SVM for engagement classification based on action histograms.

Result: Achieves 83.63% accuracy in action recognition and captures average class engagement.

Conclusion: The framework effectively measures engagement through action recognition and classification.

Abstract: In this paper, we propose a novel technique for measuring behavioral
engagement through students' actions recognition. The proposed approach
recognizes student actions then predicts the student behavioral engagement
level. For student action recognition, we use human skeletons to model student
postures and upper body movements. To learn the dynamics of student upper body,
a 3D-CNN model is used. The trained 3D-CNN model is used to recognize actions
within every 2minute video segment then these actions are used to build a
histogram of actions which encodes the student actions and their frequencies.
This histogram is utilized as an input to SVM classifier to classify whether
the student is engaged or disengaged. To evaluate the proposed framework, we
build a dataset consisting of 1414 2-minute video segments annotated with 13
actions and 112 video segments annotated with two engagement levels.
Experimental results indicate that student actions can be recognized with top 1
accuracy 83.63% and the proposed framework can capture the average engagement
of the class.

</details>


### [107] [Exploring the Deep Fusion of Large Language Models and Diffusion Transformers for Text-to-Image Synthesis](https://arxiv.org/pdf/2505.10046)
*Bingda Tang, Boyang Zheng, Xichen Pan, Sayak Paul, Saining Xie*

Main category: cs.CV

TL;DR: The paper explores the design space of combining LLMs and DiTs for multi-modal generation, providing empirical comparisons, design analysis, and reproducible training guidelines.


<details>
  <summary>Details</summary>
Motivation: To address gaps in understanding the potential of deep fusion of LLMs and DiTs in text-to-image synthesis, as prior work lacked detailed comparisons and transparency.

Method: Conducts an empirical study with controlled comparisons to baselines, analyzes design choices, and offers a reproducible training recipe.

Result: Provides meaningful data and practical guidelines for future research in multi-modal generation.

Conclusion: The study clarifies the potential of LLM-DiT fusion and supports future advancements in the field.

Abstract: This paper does not describe a new method; instead, it provides a thorough
exploration of an important yet understudied design space related to recent
advances in text-to-image synthesis -- specifically, the deep fusion of large
language models (LLMs) and diffusion transformers (DiTs) for multi-modal
generation. Previous studies mainly focused on overall system performance
rather than detailed comparisons with alternative methods, and key design
details and training recipes were often left undisclosed. These gaps create
uncertainty about the real potential of this approach. To fill these gaps, we
conduct an empirical study on text-to-image generation, performing controlled
comparisons with established baselines, analyzing important design choices, and
providing a clear, reproducible recipe for training at scale. We hope this work
offers meaningful data points and practical guidelines for future research in
multi-modal generation.

</details>


### [108] [UOD: Universal One-shot Detection of Anatomical Landmarks](https://arxiv.org/pdf/2306.07615)
*Heqin Zhu, Quan Quan, Qingsong Yao, Zaiyi Liu, S. Kevin Zhou*

Main category: cs.CV

TL;DR: UOD is a domain-adaptive one-shot landmark detection framework for multi-domain medical images, combining domain-specific and domain-shared modules to improve robustness and accuracy.


<details>
  <summary>Details</summary>
Motivation: Existing one-shot learning methods are domain-specific and struggle with multi-domain unlabeled data, leading to performance drops with sub-optimal annotations.

Method: UOD uses a two-stage approach: a self-supervised domain-adaptive convolution model for pseudo labels, followed by a domain-adaptive transformer to eliminate domain bias and enhance global context.

Result: UOD achieves state-of-the-art performance on three X-ray datasets (head, hand, chest) with only one annotated sample per domain.

Conclusion: UOD effectively addresses domain preference and robustness issues in one-shot medical landmark detection, demonstrating superior performance across multiple domains.

Abstract: One-shot medical landmark detection gains much attention and achieves great
success for its label-efficient training process. However, existing one-shot
learning methods are highly specialized in a single domain and suffer domain
preference heavily in the situation of multi-domain unlabeled data. Moreover,
one-shot learning is not robust that it faces performance drop when annotating
a sub-optimal image. To tackle these issues, we resort to developing a
domain-adaptive one-shot landmark detection framework for handling multi-domain
medical images, named Universal One-shot Detection (UOD). UOD consists of two
stages and two corresponding universal models which are designed as
combinations of domain-specific modules and domain-shared modules. In the first
stage, a domain-adaptive convolution model is self-supervised learned to
generate pseudo landmark labels. In the second stage, we design a
domain-adaptive transformer to eliminate domain preference and build the global
context for multi-domain data. Even though only one annotated sample from each
domain is available for training, the domain-shared modules help UOD aggregate
all one-shot samples to detect more robust and accurate landmarks. We
investigated both qualitatively and quantitatively the proposed UOD on three
widely-used public X-ray datasets in different anatomical domains (i.e., head,
hand, chest) and obtained state-of-the-art performances in each domain. The
code is available at
https://github.com/heqin-zhu/UOD_universal_oneshot_detection.

</details>


### [109] [Advances in Radiance Field for Dynamic Scene: From Neural Field to Gaussian Field](https://arxiv.org/pdf/2505.10049)
*Jinlong Fan, Xuepu Zeng, Jing Zhang, Mingming Gong, Yuxiang Yang, Dacheng Tao*

Main category: cs.CV

TL;DR: A survey of 200+ papers on dynamic scene representation using radiance fields, covering implicit neural representations to explicit Gaussian primitives, with a focus on motion representation, reconstruction techniques, and future challenges.


<details>
  <summary>Details</summary>
Motivation: To systematically analyze and categorize advances in dynamic scene representation and reconstruction, driven by neural radiance fields and 3D Gaussian splatting, and to provide a reference for researchers.

Method: Categorizes and evaluates works through lenses like motion representation paradigms, reconstruction techniques, auxiliary information integration, and regularization approaches.

Result: A unified representational framework for diverse methodological approaches, highlighting persistent challenges and promising directions.

Conclusion: The survey serves as a definitive reference for newcomers and a systematic guide for practitioners, outlining conceptual principles and practical frontiers in dynamic scene reconstruction.

Abstract: Dynamic scene representation and reconstruction have undergone transformative
advances in recent years, catalyzed by breakthroughs in neural radiance fields
and 3D Gaussian splatting techniques. While initially developed for static
environments, these methodologies have rapidly evolved to address the
complexities inherent in 4D dynamic scenes through an expansive body of
research. Coupled with innovations in differentiable volumetric rendering,
these approaches have significantly enhanced the quality of motion
representation and dynamic scene reconstruction, thereby garnering substantial
attention from the computer vision and graphics communities. This survey
presents a systematic analysis of over 200 papers focused on dynamic scene
representation using radiance field, spanning the spectrum from implicit neural
representations to explicit Gaussian primitives. We categorize and evaluate
these works through multiple critical lenses: motion representation paradigms,
reconstruction techniques for varied scene dynamics, auxiliary information
integration strategies, and regularization approaches that ensure temporal
consistency and physical plausibility. We organize diverse methodological
approaches under a unified representational framework, concluding with a
critical examination of persistent challenges and promising research
directions. By providing this comprehensive overview, we aim to establish a
definitive reference for researchers entering this rapidly evolving field while
offering experienced practitioners a systematic understanding of both
conceptual principles and practical frontiers in dynamic scene reconstruction.

</details>


### [110] [PsOCR: Benchmarking Large Multimodal Models for Optical Character Recognition in Low-resource Pashto Language](https://arxiv.org/pdf/2505.10055)
*Ijazul Haq, Yingjie Zhang, Irfan Ali Khan*

Main category: cs.CV

TL;DR: The paper evaluates Large Multimodal Models (LMMs) for OCR in Pashto, introduces a synthetic dataset (PsOCR), and benchmarks performance, identifying Gemini and Qwen-7B as top performers.


<details>
  <summary>Details</summary>
Motivation: Pashto OCR is challenging due to its cursive script and lack of datasets. The study aims to bridge this gap and assess LMMs.

Method: Created PsOCR, a synthetic dataset with 1M images, and benchmarked 11 LMMs (7 open-source, 4 closed-source) on a 10K subset.

Result: Gemini performed best overall; Qwen-7B led among open-source models.

Conclusion: The work provides insights into LMMs for Pashto OCR and sets a foundation for similar scripts like Arabic and Urdu. PsOCR is publicly available.

Abstract: This paper evaluates the performance of Large Multimodal Models (LMMs) on
Optical Character Recognition (OCR) in the low-resource Pashto language.
Natural Language Processing (NLP) in Pashto faces several challenges due to the
cursive nature of its script and a scarcity of structured datasets. To address
this, we developed a synthetic Pashto OCR dataset, PsOCR, consisting of one
million images annotated with bounding boxes at word, line, and document
levels, suitable for training and evaluating models based on different
architectures, including Convolutional Neural Networks (CNNs) and Transformers.
PsOCR covers variations across 1,000 unique font families, colors, image sizes,
and layouts. A benchmark subset of 10K images was selected to evaluate the
performance of several LMMs, including seven open-source models: DeepSeek's
Janus, InternVL, MiniCPM, Florence, and Qwen (3B and 7B), and four
closed-source models: GPT-4o, Gemini, Claude, and Grok. Experimental results
demonstrate that Gemini achieves the best performance among all models, whereas
among open-source models, Qwen-7B stands out. This work provides an insightful
assessment of the capabilities and limitations of current LMMs for OCR tasks in
Pashto and establishes a foundation for further research not only in Pashto OCR
but also for other similar scripts such as Arabic, Persian, and Urdu. PsOCR is
available at https://github.com/zirak-ai/PashtoOCR.

</details>


### [111] [ToonifyGB: StyleGAN-based Gaussian Blendshapes for 3D Stylized Head Avatars](https://arxiv.org/pdf/2505.10072)
*Rui-Yang Ju, Sheng-Yen Huang, Yi-Ping Hung*

Main category: cs.CV

TL;DR: ToonifyGB extends Toonify for stylized 3D head avatars using Gaussian blendshapes in a two-stage framework, improving video stability and animation quality.


<details>
  <summary>Details</summary>
Motivation: To address the limitation of fixed-resolution cropping in StyleGAN and enable diverse stylized 3D head avatar synthesis.

Method: A two-stage framework: Stage 1 generates stylized video with improved StyleGAN; Stage 2 synthesizes Gaussian blendshapes for animation.

Result: Validated on Arcane and Pixar styles, ToonifyGB efficiently renders stylized avatars with arbitrary expressions.

Conclusion: ToonifyGB successfully combines StyleGAN and Gaussian blendshapes for high-quality, diverse stylized 3D head avatars.

Abstract: The introduction of 3D Gaussian blendshapes has enabled the real-time
reconstruction of animatable head avatars from monocular video. Toonify, a
StyleGAN-based framework, has become widely used for facial image stylization.
To extend Toonify for synthesizing diverse stylized 3D head avatars using
Gaussian blendshapes, we propose an efficient two-stage framework, ToonifyGB.
In Stage 1 (stylized video generation), we employ an improved StyleGAN to
generate the stylized video from the input video frames, which addresses the
limitation of cropping aligned faces at a fixed resolution as preprocessing for
normal StyleGAN. This process provides a more stable video, which enables
Gaussian blendshapes to better capture the high-frequency details of the video
frames, and efficiently generate high-quality animation in the next stage. In
Stage 2 (Gaussian blendshapes synthesis), we learn a stylized neutral head
model and a set of expression blendshapes from the generated video. By
combining the neutral head model with expression blendshapes, ToonifyGB can
efficiently render stylized avatars with arbitrary expressions. We validate the
effectiveness of ToonifyGB on the benchmark dataset using two styles: Arcane
and Pixar.

</details>


### [112] [MMRL++: Parameter-Efficient and Interaction-Aware Representation Learning for Vision-Language Models](https://arxiv.org/pdf/2505.10088)
*Yuncheng Guo, Xiaodong Gu*

Main category: cs.CV

TL;DR: MMRL and MMRL++ improve few-shot adaptation in Vision-Language Models by introducing a shared, modality-agnostic representation space and optimizing both class and representation tokens, enhancing generalization.


<details>
  <summary>Details</summary>
Motivation: Addressing overfitting in few-shot adaptation of VLMs by improving cross-modal interactions and preserving general knowledge.

Method: Introduces MMRL with shared representation tokens in higher encoder layers, joint optimization of class and representation features, and a regularization term. MMRL++ extends this with parameter efficiency and enhanced intra-modal interactions.

Result: Outperforms state-of-the-art methods on 15 datasets, balancing task-specific adaptation and generalization.

Conclusion: MMRL and MMRL++ offer effective solutions for few-shot adaptation in VLMs, improving performance and generalization.

Abstract: Large-scale pre-trained Vision-Language Models (VLMs) have significantly
advanced transfer learning across diverse tasks. However, adapting these models
with limited few-shot data often leads to overfitting, undermining their
ability to generalize to new tasks. To address this, we propose Multi-Modal
Representation Learning (MMRL), which introduces a shared, learnable,
modality-agnostic representation space. MMRL generates space tokens projected
into both text and image encoders as representation tokens, enabling more
effective cross-modal interactions. Unlike prior methods that mainly optimize
class token features, MMRL inserts representation tokens into higher encoder
layers--where task-specific features are more prominent--while preserving
general knowledge in the lower layers. During training, both class and
representation features are jointly optimized: a trainable projection layer is
applied to representation tokens for task adaptation, while the projection
layer for class token remains frozen to retain pre-trained knowledge. To
further promote generalization, we introduce a regularization term aligning
class and text features with the frozen VLM's zero-shot features. At inference,
a decoupling strategy uses both class and representation features for base
tasks, but only class features for novel tasks due to their stronger
generalization. Building upon this, we propose MMRL++, a parameter-efficient
and interaction-aware extension that significantly reduces trainable parameters
and enhances intra-modal interactions--particularly across the layers of
representation tokens--allowing gradient sharing and instance-specific
information to propagate more effectively through the network. Extensive
experiments on 15 datasets demonstrate that MMRL and MMRL++ consistently
outperform state-of-the-art methods, achieving a strong balance between
task-specific adaptation and generalization.

</details>


### [113] [Why 1 + 1 < 1 in Visual Token Pruning: Beyond Naive Integration via Multi-Objective Balanced Covering](https://arxiv.org/pdf/2505.10118)
*Yangfu Li, Hongjian Zhan, Tianyi Chen, Qi Liu, Yue Lu*

Main category: cs.CV

TL;DR: MoB introduces a balanced approach to visual token pruning by addressing the trade-off between prompt alignment and visual preservation, achieving high performance retention and speedup.


<details>
  <summary>Details</summary>
Motivation: Existing pruning methods use static strategies, leading to inconsistent performance due to varying task requirements.

Method: MoB formulates pruning as a bi-objective covering problem, using greedy radius trading for budget allocation.

Result: MoB retains 96.4% performance with 11.1% tokens and speeds up models by 1.3-1.5x.

Conclusion: MoB is effective, scalable, and adaptable to advanced MLLMs and diverse tasks.

Abstract: Existing visual token pruning methods target prompt alignment and visual
preservation with static strategies, overlooking the varying relative
importance of these objectives across tasks, which leads to inconsistent
performance. To address this, we derive the first closed-form error bound for
visual token pruning based on the Hausdorff distance, uniformly characterizing
the contributions of both objectives. Moreover, leveraging $\epsilon$-covering
theory, we reveal an intrinsic trade-off between these objectives and quantify
their optimal attainment levels under a fixed budget. To practically handle
this trade-off, we propose Multi-Objective Balanced Covering (MoB), which
reformulates visual token pruning as a bi-objective covering problem. In this
framework, the attainment trade-off reduces to budget allocation via greedy
radius trading. MoB offers a provable performance bound and linear scalability
with respect to the number of input visual tokens, enabling adaptation to
challenging pruning scenarios. Extensive experiments show that MoB preserves
96.4% of performance for LLaVA-1.5-7B using only 11.1% of the original visual
tokens and accelerates LLaVA-Next-7B by 1.3-1.5$\times$ with negligible
performance loss. Additionally, evaluations on Qwen2-VL and Video-LLaVA confirm
that MoB integrates seamlessly into advanced MLLMs and diverse vision-language
tasks.

</details>


### [114] [Multi-Source Collaborative Style Augmentation and Domain-Invariant Learning for Federated Domain Generalization](https://arxiv.org/pdf/2505.10152)
*Yikang Wei*

Main category: cs.CV

TL;DR: Proposes MCSAD for federated domain generalization, combining style augmentation and domain-invariant learning to improve generalization on unseen domains.


<details>
  <summary>Details</summary>
Motivation: Existing style augmentation methods in federated domain generalization are limited by isolated or interpolated style exploration, restricting the style space.

Method: Introduces a multi-source collaborative style augmentation module and domain-invariant learning via cross-domain feature alignment and relation distillation.

Result: Outperforms state-of-the-art methods on multiple domain generalization datasets.

Conclusion: MCSAD effectively broadens style space and enhances domain-invariant learning for better generalization.

Abstract: Federated domain generalization aims to learn a generalizable model from
multiple decentralized source domains for deploying on the unseen target
domain. The style augmentation methods have achieved great progress on domain
generalization. However, the existing style augmentation methods either explore
the data styles within isolated source domain or interpolate the style
information across existing source domains under the data decentralization
scenario, which leads to limited style space. To address this issue, we propose
a Multi-source Collaborative Style Augmentation and Domain-invariant learning
method (MCSAD) for federated domain generalization. Specifically, we propose a
multi-source collaborative style augmentation module to generate data in the
broader style space. Furthermore, we conduct domain-invariant learning between
the original data and augmented data by cross-domain feature alignment within
the same class and classes relation ensemble distillation between different
classes to learn a domain-invariant model. By alternatively conducting
collaborative style augmentation and domain-invariant learning, the model can
generalize well on unseen target domain. Extensive experiments on multiple
domain generalization datasets indicate that our method significantly
outperforms the state-of-the-art federated domain generalization methods.

</details>


### [115] [Modeling Saliency Dataset Bias](https://arxiv.org/pdf/2505.10169)
*Matthias Kümmerer, Harneet Khanuja, Matthias Bethge*

Main category: cs.CV

TL;DR: A novel architecture addresses dataset bias in saliency prediction, improving generalization across datasets with minimal dataset-specific parameters.


<details>
  <summary>Details</summary>
Motivation: Existing saliency models struggle with dataset bias, showing a 40% performance drop when applied to different datasets.

Method: Proposes an encoder-decoder structure with fewer than 20 dataset-specific parameters for interpretable mechanisms like multi-scale structure and center bias.

Result: Achieves state-of-the-art performance on MIT/Tuebingen Saliency Benchmark datasets, reducing the generalization gap by 75%.

Conclusion: The model effectively addresses dataset bias, offering insights into spatial saliency properties and improving performance with minimal adaptation.

Abstract: Recent advances in image-based saliency prediction are approaching gold
standard performance levels on existing benchmarks. Despite this success, we
show that predicting fixations across multiple saliency datasets remains
challenging due to dataset bias. We find a significant performance drop (around
40%) when models trained on one dataset are applied to another. Surprisingly,
increasing dataset diversity does not resolve this inter-dataset gap, with
close to 60% attributed to dataset-specific biases. To address this remaining
generalization gap, we propose a novel architecture extending a mostly
dataset-agnostic encoder-decoder structure with fewer than 20 dataset-specific
parameters that govern interpretable mechanisms such as multi-scale structure,
center bias, and fixation spread. Adapting only these parameters to new data
accounts for more than 75% of the generalization gap, with a large fraction of
the improvement achieved with as few as 50 samples. Our model sets a new
state-of-the-art on all three datasets of the MIT/Tuebingen Saliency Benchmark
(MIT300, CAT2000, and COCO-Freeview), even when purely generalizing from
unrelated datasets, but with a substantial boost when adapting to the
respective training datasets. The model also provides valuable insights into
spatial saliency properties, revealing complex multi-scale effects that combine
both absolute and relative sizes.

</details>


### [116] [VolE: A Point-cloud Framework for Food 3D Reconstruction and Volume Estimation](https://arxiv.org/pdf/2505.10205)
*Umair Haroon, Ahmad AlMughrabi, Thanasis Zoumpekas, Ricardo Marques, Petia Radeva*

Main category: cs.CV

TL;DR: VolE is a mobile-driven 3D reconstruction framework for accurate food volume estimation without depth sensors or reference objects, outperforming existing methods with 2.22% MAPE.


<details>
  <summary>Details</summary>
Motivation: Current food volume estimation methods rely on specialized hardware or reference objects, limiting practicality. VolE aims to overcome these limitations using mobile devices.

Method: VolE uses AR-capable mobile devices to capture images and camera locations for 3D reconstruction, combined with food video segmentation for mask generation.

Result: VolE achieves 2.22% MAPE, outperforming existing techniques across multiple datasets.

Conclusion: VolE provides a practical, high-accuracy solution for food volume estimation without specialized hardware or references.

Abstract: Accurate food volume estimation is crucial for medical nutrition management
and health monitoring applications, but current food volume estimation methods
are often limited by mononuclear data, leveraging single-purpose hardware such
as 3D scanners, gathering sensor-oriented information such as depth
information, or relying on camera calibration using a reference object. In this
paper, we present VolE, a novel framework that leverages mobile device-driven
3D reconstruction to estimate food volume. VolE captures images and camera
locations in free motion to generate precise 3D models, thanks to AR-capable
mobile devices. To achieve real-world measurement, VolE is a reference- and
depth-free framework that leverages food video segmentation for food mask
generation. We also introduce a new food dataset encompassing the challenging
scenarios absent in the previous benchmarks. Our experiments demonstrate that
VolE outperforms the existing volume estimation techniques across multiple
datasets by achieving 2.22 % MAPE, highlighting its superior performance in
food volume estimation.

</details>


### [117] [Unlocking Location Intelligence: A Survey from Deep Learning to The LLM Era](https://arxiv.org/pdf/2505.09651)
*Xixuan Hao, Yutian Jiang, Xingchen Zou, Jiabo Liu, Yifang Yin, Yuxuan Liang*

Main category: cs.CV

TL;DR: The paper surveys geospatial representation learning, covering deep learning and LLM advancements, and proposes future directions for Location Intelligence.


<details>
  <summary>Details</summary>
Motivation: To review and organize the evolution of geospatial representation learning, highlighting its impact on Location Intelligence.

Method: Structured taxonomy based on data, methodological, and application perspectives, reviewing advancements and limitations.

Result: Comprehensive review of geospatial representation learning, identifying transformative capabilities of LLMs and future research directions.

Conclusion: The survey provides a roadmap for innovation in Location Intelligence, emphasizing the potential of LLMs in geospatial reasoning.

Abstract: Location Intelligence (LI), the science of transforming location-centric
geospatial data into actionable knowledge, has become a cornerstone of modern
spatial decision-making. The rapid evolution of Geospatial Representation
Learning is fundamentally reshaping LI development through two successive
technological revolutions: the deep learning breakthrough and the emerging
large language model (LLM) paradigm. While deep neural networks (DNNs) have
demonstrated remarkable success in automated feature extraction from structured
geospatial data (e.g., satellite imagery, GPS trajectories), the recent
integration of LLMs introduces transformative capabilities for cross-modal
geospatial reasoning and unstructured geo-textual data processing. This survey
presents a comprehensive review of geospatial representation learning across
both technological eras, organizing them into a structured taxonomy based on
the complete pipeline comprising: (1) data perspective, (2) methodological
perspective and (3) application perspective. We also highlight current
advancements, discuss existing limitations, and propose potential future
research directions in the LLM era. This work offers a thorough exploration of
the field and providing a roadmap for further innovation in LI. The summary of
the up-to-date paper list can be found in
https://github.com/CityMind-Lab/Awesome-Location-Intelligence and will undergo
continuous updates.

</details>


### [118] [Data-Agnostic Augmentations for Unknown Variations: Out-of-Distribution Generalisation in MRI Segmentation](https://arxiv.org/pdf/2505.10223)
*Puru Vaish, Felix Meister, Tobias Heimann, Christoph Brune, Jelmer M. Wolterink*

Main category: cs.CV

TL;DR: The paper evaluates MixUp and Auxiliary Fourier Augmentation for improving medical image segmentation robustness in real-world clinical settings, showing enhanced generalization and feature representation.


<details>
  <summary>Details</summary>
Motivation: Performance degradation of medical image segmentation models in real-world settings due to distribution mismatches between training and test data.

Method: Systematic evaluation of MixUp and Auxiliary Fourier Augmentation to improve robustness without targeting specific distribution shifts.

Result: Significant improvement in out-of-distribution generalization and robustness in cardiac cine MRI and prostate MRI segmentation, with enhanced feature separability and compactness.

Conclusion: Integration of these augmentation methods into nnU-Net pipelines offers an effective, easy-to-implement solution for reliable medical segmentation in real-world applications.

Abstract: Medical image segmentation models are often trained on curated datasets,
leading to performance degradation when deployed in real-world clinical
settings due to mismatches between training and test distributions. While data
augmentation techniques are widely used to address these challenges,
traditional visually consistent augmentation strategies lack the robustness
needed for diverse real-world scenarios. In this work, we systematically
evaluate alternative augmentation strategies, focusing on MixUp and Auxiliary
Fourier Augmentation. These methods mitigate the effects of multiple variations
without explicitly targeting specific sources of distribution shifts. We
demonstrate how these techniques significantly improve out-of-distribution
generalization and robustness to imaging variations across a wide range of
transformations in cardiac cine MRI and prostate MRI segmentation. We
quantitatively find that these augmentation methods enhance learned feature
representations by promoting separability and compactness. Additionally, we
highlight how their integration into nnU-Net training pipelines provides an
easy-to-implement, effective solution for enhancing the reliability of medical
segmentation models in real-world applications.

</details>


### [119] [On the Interplay of Human-AI Alignment,Fairness, and Performance Trade-offs in Medical Imaging](https://arxiv.org/pdf/2505.10231)
*Haozhe Luo, Ziyu Zhou, Zixin Shu, Aurélie Pahud de Mortanges, Robert Berke, Mauricio Reyes*

Main category: cs.CV

TL;DR: Human-AI alignment reduces fairness gaps in medical imaging AI but requires balance to avoid performance trade-offs.


<details>
  <summary>Details</summary>
Motivation: Address biases and fairness gaps in medical AI by exploring Human-AI alignment.

Method: Systematic exploration of Human-AI alignment, incorporating human insights.

Result: Reduced fairness gaps and improved out-of-domain generalization, with noted trade-offs from excessive alignment.

Conclusion: Human-AI alignment is promising for fair, robust medical AI, but requires calibrated strategies.

Abstract: Deep neural networks excel in medical imaging but remain prone to biases,
leading to fairness gaps across demographic groups. We provide the first
systematic exploration of Human-AI alignment and fairness in this domain. Our
results show that incorporating human insights consistently reduces fairness
gaps and enhances out-of-domain generalization, though excessive alignment can
introduce performance trade-offs, emphasizing the need for calibrated
strategies. These findings highlight Human-AI alignment as a promising approach
for developing fair, robust, and generalizable medical AI systems, striking a
balance between expert guidance and automated efficiency. Our code is available
at https://github.com/Roypic/Aligner.

</details>


### [120] [MTVCrafter: 4D Motion Tokenization for Open-World Human Image Animation](https://arxiv.org/pdf/2505.10238)
*Yanbo Ding*

Main category: cs.CV

TL;DR: MTVCrafter introduces 4D motion tokens for human image animation, outperforming 2D methods with better generalization and flexibility.


<details>
  <summary>Details</summary>
Motivation: Existing methods rely on 2D-rendered pose images, limiting generalization and discarding 3D information. MTVCrafter addresses this by modeling raw 3D motion sequences.

Method: Proposes 4DMoT to quantize 3D motion into tokens and MV-DiT with motion attention for animation.

Result: Achieves state-of-the-art FID-VID of 6.98, surpassing second-best by 65%, and generalizes well to diverse characters.

Conclusion: MTVCrafter advances human image animation by leveraging 4D motion, opening new directions for pose-guided video generation.

Abstract: Human image animation has gained increasing attention and developed rapidly
due to its broad applications in digital humans. However, existing methods rely
largely on 2D-rendered pose images for motion guidance, which limits
generalization and discards essential 3D information for open-world animation.
To tackle this problem, we propose MTVCrafter (Motion Tokenization Video
Crafter), the first framework that directly models raw 3D motion sequences
(i.e., 4D motion) for human image animation. Specifically, we introduce 4DMoT
(4D motion tokenizer) to quantize 3D motion sequences into 4D motion tokens.
Compared to 2D-rendered pose images, 4D motion tokens offer more robust
spatio-temporal cues and avoid strict pixel-level alignment between pose image
and character, enabling more flexible and disentangled control. Then, we
introduce MV-DiT (Motion-aware Video DiT). By designing unique motion attention
with 4D positional encodings, MV-DiT can effectively leverage motion tokens as
4D compact yet expressive context for human image animation in the complex 3D
world. Hence, it marks a significant step forward in this field and opens a new
direction for pose-guided human video generation. Experiments show that our
MTVCrafter achieves state-of-the-art results with an FID-VID of 6.98,
surpassing the second-best by 65%. Powered by robust motion tokens, MTVCrafter
also generalizes well to diverse open-world characters (single/multiple,
full/half-body) across various styles and scenarios. Our video demos and code
are provided in the supplementary material and at this anonymous GitHub link:
https://anonymous.4open.science/r/MTVCrafter-1B13.

</details>


### [121] [ADHMR: Aligning Diffusion-based Human Mesh Recovery via Direct Preference Optimization](https://arxiv.org/pdf/2505.10250)
*Wenhao Shen, Wanqi Yin, Xiaofeng Yang, Cheng Chen, Chaoyue Song, Zhongang Cai, Lei Yang, Hao Wang, Guosheng Lin*

Main category: cs.CV

TL;DR: ADHMR aligns a diffusion-based HMR model using preference optimization, improving alignment with 2D observations and robustness for in-the-wild images.


<details>
  <summary>Details</summary>
Motivation: Addressing misalignment and weak robustness in probabilistic HMR methods for single-image 3D human mesh recovery.

Method: Train HMR-Scorer to assess predictions, create a preference dataset, and finetune the base model using direct preference optimization.

Result: ADHMR outperforms state-of-the-art methods and improves existing models via data cleaning.

Conclusion: ADHMR effectively enhances HMR performance, especially for in-the-wild images, with available code for implementation.

Abstract: Human mesh recovery (HMR) from a single image is inherently ill-posed due to
depth ambiguity and occlusions. Probabilistic methods have tried to solve this
by generating numerous plausible 3D human mesh predictions, but they often
exhibit misalignment with 2D image observations and weak robustness to
in-the-wild images. To address these issues, we propose ADHMR, a framework that
Aligns a Diffusion-based HMR model in a preference optimization manner. First,
we train a human mesh prediction assessment model, HMR-Scorer, capable of
evaluating predictions even for in-the-wild images without 3D annotations. We
then use HMR-Scorer to create a preference dataset, where each input image has
a pair of winner and loser mesh predictions. This dataset is used to finetune
the base model using direct preference optimization. Moreover, HMR-Scorer also
helps improve existing HMR models by data cleaning, even with fewer training
samples. Extensive experiments show that ADHMR outperforms current
state-of-the-art methods. Code is available at:
https://github.com/shenwenhao01/ADHMR.

</details>


### [122] [Sage Deer: A Super-Aligned Driving Generalist Is Your Copilot](https://arxiv.org/pdf/2505.10257)
*Hao Lu, Jiaqi Tang, Jiyao Wang, Yunfan LU, Xu Cao, Qingyong Hu, Yin Wang, Yuting Zhang, Tianxin Xie, Yunpeng Zhang, Yong Chen, Jiayu. Gao, Bin Huang, Dengbo He, Shuiguang Deng, Hao Chen, Ying-Cong Chen*

Main category: cs.CV

TL;DR: SAGE DeeR is a Super-Aligned and Generalist Driving agent designed to match user comfort, interaction, and safety needs in intelligent driving cockpits. It achieves super alignment, generalist capabilities, and self-eliciting thought chains.


<details>
  <summary>Details</summary>
Motivation: To address the need for personalized and adaptive intelligent driving systems that cater to diverse user preferences and inputs.

Method: Developed SAGE DeeR with super alignment, generalist reasoning, and self-eliciting abilities, supported by a large-scale benchmark for evaluation.

Result: SAGE DeeR can adapt to user preferences, process multi-view inputs, and elicit implicit thought chains, validated by a comprehensive benchmark.

Conclusion: SAGE DeeR demonstrates advanced capabilities in aligning with user needs and generalizing across diverse driving scenarios.

Abstract: The intelligent driving cockpit, an important part of intelligent driving,
needs to match different users' comfort, interaction, and safety needs. This
paper aims to build a Super-Aligned and GEneralist DRiving agent, SAGE DeeR.
Sage Deer achieves three highlights: (1) Super alignment: It achieves different
reactions according to different people's preferences and biases. (2)
Generalist: It can understand the multi-view and multi-mode inputs to reason
the user's physiological indicators, facial emotions, hand movements, body
movements, driving scenarios, and behavioral decisions. (3) Self-Eliciting: It
can elicit implicit thought chains in the language space to further increase
generalist and super-aligned abilities. Besides, we collected multiple data
sets and built a large-scale benchmark. This benchmark measures the deer's
perceptual decision-making ability and the super alignment's accuracy.

</details>


### [123] [Inferring Driving Maps by Deep Learning-based Trail Map Extraction](https://arxiv.org/pdf/2505.10258)
*Michael Hubbertz, Pascal Colling, Qi Han, Tobias Meisen*

Main category: cs.CV

TL;DR: A novel offline mapping method integrates trail data for autonomous driving, outperforming online mapping in generalization and robustness.


<details>
  <summary>Details</summary>
Motivation: To address challenges in online mapping (temporal consistency, sensor occlusion, runtime, generalization) by leveraging trail data for more accurate and adaptable HD maps.

Method: Aggregates trail data from ego vehicles and traffic participants, using transformer-based deep learning to construct and update global maps continuously.

Result: Superior performance over online mapping, with improved generalization to unseen environments and sensor configurations, validated on benchmark datasets.

Conclusion: The proposed offline mapping approach is robust and applicable for autonomous driving, offering efficient updates and sensor-agnostic capabilities.

Abstract: High-definition (HD) maps offer extensive and accurate environmental
information about the driving scene, making them a crucial and essential
element for planning within autonomous driving systems. To avoid extensive
efforts from manual labeling, methods for automating the map creation have
emerged. Recent trends have moved from offline mapping to online mapping,
ensuring availability and actuality of the utilized maps. While the performance
has increased in recent years, online mapping still faces challenges regarding
temporal consistency, sensor occlusion, runtime, and generalization. We propose
a novel offline mapping approach that integrates trails - informal routes used
by drivers - into the map creation process. Our method aggregates trail data
from the ego vehicle and other traffic participants to construct a
comprehensive global map using transformer-based deep learning models. Unlike
traditional offline mapping, our approach enables continuous updates while
remaining sensor-agnostic, facilitating efficient data transfer. Our method
demonstrates superior performance compared to state-of-the-art online mapping
approaches, achieving improved generalization to previously unseen environments
and sensor configurations. We validate our approach on two benchmark datasets,
highlighting its robustness and applicability in autonomous driving systems.

</details>


### [124] [HandReader: Advanced Techniques for Efficient Fingerspelling Recognition](https://arxiv.org/pdf/2505.10267)
*Pavel Korotaev, Petr Surovtsev, Alexander Kapitanov, Karina Kvanchiani, Aleksandr Nagaev*

Main category: cs.CV

TL;DR: HandReader introduces three architectures for fingerspelling recognition, leveraging RGB and keypoint data, achieving state-of-the-art results on multiple datasets, including a new Russian dataset (Znaki).


<details>
  <summary>Details</summary>
Motivation: Improving fingerspelling recognition accuracy by addressing temporal and spatial information in videos, which previous methods lacked.

Method: Three architectures: HandReader_RGB (TSAM for RGB features), HandReader_KP (TPE for keypoints), and HandReader_RGB+KP (joint encoder).

Result: State-of-the-art performance on ChicagoFSWild, ChicagoFSWild+, and Znaki datasets.

Conclusion: HandReader models effectively combine RGB and keypoint data for superior fingerspelling recognition, with publicly available datasets and models.

Abstract: Fingerspelling is a significant component of Sign Language (SL), allowing the
interpretation of proper names, characterized by fast hand movements during
signing. Although previous works on fingerspelling recognition have focused on
processing the temporal dimension of videos, there remains room for improving
the accuracy of these approaches. This paper introduces HandReader, a group of
three architectures designed to address the fingerspelling recognition task.
HandReader$_{RGB}$ employs the novel Temporal Shift-Adaptive Module (TSAM) to
process RGB features from videos of varying lengths while preserving important
sequential information. HandReader$_{KP}$ is built on the proposed Temporal
Pose Encoder (TPE) operated on keypoints as tensors. Such keypoints composition
in a batch allows the encoder to pass them through 2D and 3D convolution
layers, utilizing temporal and spatial information and accumulating keypoints
coordinates. We also introduce HandReader_RGB+KP - architecture with a joint
encoder to benefit from RGB and keypoint modalities. Each HandReader model
possesses distinct advantages and achieves state-of-the-art results on the
ChicagoFSWild and ChicagoFSWild+ datasets. Moreover, the models demonstrate
high performance on the first open dataset for Russian fingerspelling, Znaki,
presented in this paper. The Znaki dataset and HandReader pre-trained models
are publicly available.

</details>


### [125] [MFogHub: Bridging Multi-Regional and Multi-Satellite Data for Global Marine Fog Detection and Forecasting](https://arxiv.org/pdf/2505.10281)
*Mengqiu Xu, Kaixin Chen, Heng Guo, Yixiang Huang, Ming Wu, Zhenwei Shi, Chuang Zhang, Jun Guo*

Main category: cs.CV

TL;DR: MFogHub is a new multi-regional and multi-satellite dataset for marine fog detection and forecasting, addressing the lack of diverse open-source data.


<details>
  <summary>Details</summary>
Motivation: Existing datasets are limited to single regions or satellites, hindering model evaluation and understanding of marine fog dynamics.

Method: MFogHub integrates annotated marine fog observations from 15 regions and six satellites, offering over 68,000 high-resolution samples.

Result: Experiments with 16 baseline models show MFogHub reveals generalization issues due to regional/satellite differences and aids in scalable fog prediction.

Conclusion: MFogHub advances global marine fog monitoring and scientific understanding, with dataset and code publicly available.

Abstract: Deep learning approaches for marine fog detection and forecasting have
outperformed traditional methods, demonstrating significant scientific and
practical importance. However, the limited availability of open-source datasets
remains a major challenge. Existing datasets, often focused on a single region
or satellite, restrict the ability to evaluate model performance across diverse
conditions and hinder the exploration of intrinsic marine fog characteristics.
To address these limitations, we introduce \textbf{MFogHub}, the first
multi-regional and multi-satellite dataset to integrate annotated marine fog
observations from 15 coastal fog-prone regions and six geostationary
satellites, comprising over 68,000 high-resolution samples. By encompassing
diverse regions and satellite perspectives, MFogHub facilitates rigorous
evaluation of both detection and forecasting methods under varying conditions.
Extensive experiments with 16 baseline models demonstrate that MFogHub can
reveal generalization fluctuations due to regional and satellite discrepancy,
while also serving as a valuable resource for the development of targeted and
scalable fog prediction techniques. Through MFogHub, we aim to advance both the
practical monitoring and scientific understanding of marine fog dynamics on a
global scale. The dataset and code are at
\href{https://github.com/kaka0910/MFogHub}{https://github.com/kaka0910/MFogHub}.

</details>


### [126] [MSCI: Addressing CLIP's Inherent Limitations for Compositional Zero-Shot Learning](https://arxiv.org/pdf/2505.10289)
*Yue Wang, Shuai Xu, Xuelin Zhu, Yicong Li*

Main category: cs.CV

TL;DR: The paper proposes a Multi-Stage Cross-modal Interaction (MSCI) model to enhance fine-grained local feature capture in Compositional Zero-Shot Learning (CZSL), addressing limitations of CLIP.


<details>
  <summary>Details</summary>
Motivation: Existing CZSL methods rely on CLIP but fail to capture fine-grained local features due to its architecture and training.

Method: MSCI uses self-adaptive aggregators to extract and integrate local and global visual features, progressively interacting with textual representations.

Result: Experiments on three datasets validate MSCI's effectiveness and superiority.

Conclusion: MSCI improves fine-grained perception in CZSL by dynamically adjusting attention weights between global and local features.

Abstract: Compositional Zero-Shot Learning (CZSL) aims to recognize unseen state-object
combinations by leveraging known combinations. Existing studies basically rely
on the cross-modal alignment capabilities of CLIP but tend to overlook its
limitations in capturing fine-grained local features, which arise from its
architectural and training paradigm. To address this issue, we propose a
Multi-Stage Cross-modal Interaction (MSCI) model that effectively explores and
utilizes intermediate-layer information from CLIP's visual encoder.
Specifically, we design two self-adaptive aggregators to extract local
information from low-level visual features and integrate global information
from high-level visual features, respectively. These key information are
progressively incorporated into textual representations through a
stage-by-stage interaction mechanism, significantly enhancing the model's
perception capability for fine-grained local visual information. Additionally,
MSCI dynamically adjusts the attention weights between global and local visual
information based on different combinations, as well as different elements
within the same combination, allowing it to flexibly adapt to diverse
scenarios. Experiments on three widely used datasets fully validate the
effectiveness and superiority of the proposed model. Data and code are
available at https://github.com/ltpwy/MSCI.

</details>


### [127] [StoryReasoning Dataset: Using Chain-of-Thought for Scene Understanding and Grounded Story Generation](https://arxiv.org/pdf/2505.10292)
*Daniel A. P. Oliveira, David Martins de Matos*

Main category: cs.CV

TL;DR: StoryReasoning dataset improves visual storytelling by grounding characters and objects, reducing referential hallucinations by 12.3%.


<details>
  <summary>Details</summary>
Motivation: Addressing issues of character identity inconsistency and referential hallucinations in visual storytelling.

Method: Proposes StoryReasoning dataset with structured scene analyses, cross-frame object re-identification, chain-of-thought reasoning, and grounding schemes. Fine-tunes Qwen2.5-VL 7B for baseline performance.

Result: Reduces hallucinations from 4.06 to 3.56 (-12.3%) per story compared to non-fine-tuned models.

Conclusion: StoryReasoning effectively enhances consistency and reduces hallucinations in visual storytelling through structured grounding and reasoning.

Abstract: Visual storytelling systems struggle to maintain character identity across
frames and link actions to appropriate subjects, frequently leading to
referential hallucinations. These issues can be addressed through grounding of
characters, objects, and other entities on the visual elements. We propose
StoryReasoning, a dataset containing 4,178 stories derived from 52,016 movie
images, with both structured scene analyses and grounded stories. Each story
maintains character and object consistency across frames while explicitly
modeling multi-frame relationships through structured tabular representations.
Our approach features cross-frame object re-identification using visual
similarity and face recognition, chain-of-thought reasoning for explicit
narrative modeling, and a grounding scheme that links textual elements to
visual entities across multiple frames. We establish baseline performance by
fine-tuning Qwen2.5-VL 7B, creating Qwen Storyteller, which performs end-to-end
object detection, re-identification, and landmark detection while maintaining
consistent object references throughout the story. Evaluation demonstrates a
reduction from 4.06 to 3.56 (-12.3%) hallucinations on average per story when
compared to a non-fine-tuned model.

</details>


### [128] [MIPHEI-ViT: Multiplex Immunofluorescence Prediction from H&E Images using ViT Foundation Models](https://arxiv.org/pdf/2505.10294)
*Guillaume Balezo, Roger Trullo, Albert Pla Planas, Etienne Decenciere, Thomas Walter*

Main category: cs.CV

TL;DR: MIPHEI predicts multiplex immunofluorescence (mIF) signals from H&E images using a U-Net-inspired model with ViT encoders, achieving accurate cell-type classification and outperforming baselines.


<details>
  <summary>Details</summary>
Motivation: Bridging the gap between widely used H&E staining and costly mIF by enabling cell-type identification from H&E alone.

Method: Uses a U-Net-inspired architecture with ViT encoders, trained on the ORION dataset of colorectal cancer tissue, validated on two independent datasets.

Result: Achieves F1 scores of 0.88 for Pan-CK, 0.57 for CD3e, 0.56 for SMA, 0.36 for CD68, and 0.30 for CD20, outperforming baselines.

Conclusion: MIPHEI enables cell-type-aware analysis of H&E datasets, potentially uncovering spatial cellular organization's link to patient outcomes.

Abstract: Histopathological analysis is a cornerstone of cancer diagnosis, with
Hematoxylin and Eosin (H&E) staining routinely acquired for every patient to
visualize cell morphology and tissue architecture. On the other hand, multiplex
immunofluorescence (mIF) enables more precise cell type identification via
proteomic markers, but has yet to achieve widespread clinical adoption due to
cost and logistical constraints. To bridge this gap, we introduce MIPHEI
(Multiplex Immunofluorescence Prediction from H&E), a U-Net-inspired
architecture that integrates state-of-the-art ViT foundation models as encoders
to predict mIF signals from H&E images. MIPHEI targets a comprehensive panel of
markers spanning nuclear content, immune lineages (T cells, B cells, myeloid),
epithelium, stroma, vasculature, and proliferation. We train our model using
the publicly available ORION dataset of restained H&E and mIF images from
colorectal cancer tissue, and validate it on two independent datasets. MIPHEI
achieves accurate cell-type classification from H&E alone, with F1 scores of
0.88 for Pan-CK, 0.57 for CD3e, 0.56 for SMA, 0.36 for CD68, and 0.30 for CD20,
substantially outperforming both a state-of-the-art baseline and a random
classifier for most markers. Our results indicate that our model effectively
captures the complex relationships between nuclear morphologies in their tissue
context, as visible in H&E images and molecular markers defining specific cell
types. MIPHEI offers a promising step toward enabling cell-type-aware analysis
of large-scale H&E datasets, in view of uncovering relationships between
spatial cellular organization and patient outcomes.

</details>


### [129] [A Unified and Scalable Membership Inference Method for Visual Self-supervised Encoder via Part-aware Capability](https://arxiv.org/pdf/2505.10351)
*Jie Zhu, Jirong Zha, Ding Li, Leye Wang*

Main category: cs.CV

TL;DR: The paper introduces PartCrop, a unified membership inference method for self-supervised visual models, addressing privacy concerns in black-box settings. It demonstrates effectiveness across various training protocols and proposes defense strategies.


<details>
  <summary>Details</summary>
Motivation: Privacy concerns in self-supervised learning, especially in vision, motivate the need for realistic black-box membership inference attacks.

Method: Proposes PartCrop, which exploits part-aware capabilities in models by cropping object parts and analyzing representation space responses.

Result: PartCrop proves effective and generalizable across different self-supervised models and datasets. Defense methods like early stop and differential privacy are also validated.

Conclusion: PartCrop is a scalable and effective attack method, with PartCrop-v2 introduced for larger-scale scenarios. Defense strategies are also effective.

Abstract: Self-supervised learning shows promise in harnessing extensive unlabeled
data, but it also confronts significant privacy concerns, especially in vision.
In this paper, we perform membership inference on visual self-supervised models
in a more realistic setting: self-supervised training method and details are
unknown for an adversary when attacking as he usually faces a black-box system
in practice. In this setting, considering that self-supervised model could be
trained by completely different self-supervised paradigms, e.g., masked image
modeling and contrastive learning, with complex training details, we propose a
unified membership inference method called PartCrop. It is motivated by the
shared part-aware capability among models and stronger part response on the
training data. Specifically, PartCrop crops parts of objects in an image to
query responses within the image in representation space. We conduct extensive
attacks on self-supervised models with different training protocols and
structures using three widely used image datasets. The results verify the
effectiveness and generalization of PartCrop. Moreover, to defend against
PartCrop, we evaluate two common approaches, i.e., early stop and differential
privacy, and propose a tailored method called shrinking crop scale range. The
defense experiments indicate that all of them are effective. Finally, besides
prototype testing on toy visual encoders and small-scale image datasets, we
quantitatively study the impacts of scaling from both data and model aspects in
a realistic scenario and propose a scalable PartCrop-v2 by introducing two
structural improvements to PartCrop. Our code is at
https://github.com/JiePKU/PartCrop.

</details>


### [130] [SpikeVideoFormer: An Efficient Spike-Driven Video Transformer with Hamming Attention and $\mathcal{O}(T)$ Complexity](https://arxiv.org/pdf/2505.10352)
*Shihao Zou, Qingfeng Li, Wei Ji, Jingjing Li, Yongkui Yang, Guoqi Li, Chao Dong*

Main category: cs.CV

TL;DR: SpikeVideoFormer is an efficient spike-driven video Transformer with linear temporal complexity, outperforming SNN approaches and matching ANN-based methods with significant efficiency gains.


<details>
  <summary>Details</summary>
Motivation: Existing SNN-based Transformers focus on single-image tasks, lacking efficiency in video tasks. This paper aims to leverage SNNs' energy efficiency for video-based vision tasks.

Method: Introduces SpikeVideoFormer with spike-driven Hamming attention (SDHA) and analyzes spike-driven space-time attention designs for optimal performance.

Result: Achieves SOTA performance in video tasks (classification, pose tracking, semantic segmentation) with 15%+ improvements and significant efficiency gains (up to 16x).

Conclusion: SpikeVideoFormer effectively combines SNN efficiency with video task performance, matching ANN methods while being more energy-efficient.

Abstract: Spiking Neural Networks (SNNs) have shown competitive performance to
Artificial Neural Networks (ANNs) in various vision tasks, while offering
superior energy efficiency. However, existing SNN-based Transformers primarily
focus on single-image tasks, emphasizing spatial features while not effectively
leveraging SNNs' efficiency in video-based vision tasks. In this paper, we
introduce SpikeVideoFormer, an efficient spike-driven video Transformer,
featuring linear temporal complexity $\mathcal{O}(T)$. Specifically, we design
a spike-driven Hamming attention (SDHA) which provides a theoretically guided
adaptation from traditional real-valued attention to spike-driven attention.
Building on SDHA, we further analyze various spike-driven space-time attention
designs and identify an optimal scheme that delivers appealing performance for
video tasks, while maintaining only linear temporal complexity. The
generalization ability and efficiency of our model are demonstrated across
diverse downstream video tasks, including classification, human pose tracking,
and semantic segmentation. Empirical results show our method achieves
state-of-the-art (SOTA) performance compared to existing SNN approaches, with
over 15\% improvement on the latter two tasks. Additionally, it matches the
performance of recent ANN-based methods while offering significant efficiency
gains, achieving $\times 16$, $\times 10$ and $\times 5$ improvements on the
three tasks. https://github.com/JimmyZou/SpikeVideoFormer

</details>


### [131] [Learned Lightweight Smartphone ISP with Unpaired Data](https://arxiv.org/pdf/2505.10420)
*Andrei Arhire, Radu Timofte*

Main category: cs.CV

TL;DR: A novel unpaired training method for a learnable ISP eliminates the need for aligned data, using adversarial training and lightweight networks to achieve high fidelity.


<details>
  <summary>Details</summary>
Motivation: Developing a learned ISP requires costly aligned data; this work aims to bypass this need.

Method: Uses unpaired training with a multi-term loss function and adversarial training with multiple discriminators.

Result: Achieves high fidelity on Zurich RAW to RGB and Fujifilm UltraISP datasets, comparable to paired methods.

Conclusion: The unpaired approach shows strong potential, offering a practical solution for lightweight ISPs.

Abstract: The Image Signal Processor (ISP) is a fundamental component in modern
smartphone cameras responsible for conversion of RAW sensor image data to RGB
images with a strong focus on perceptual quality. Recent work highlights the
potential of deep learning approaches and their ability to capture details with
a quality increasingly close to that of professional cameras. A difficult and
costly step when developing a learned ISP is the acquisition of pixel-wise
aligned paired data that maps the raw captured by a smartphone camera sensor to
high-quality reference images. In this work, we address this challenge by
proposing a novel training method for a learnable ISP that eliminates the need
for direct correspondences between raw images and ground-truth data with
matching content. Our unpaired approach employs a multi-term loss function
guided by adversarial training with multiple discriminators processing feature
maps from pre-trained networks to maintain content structure while learning
color and texture characteristics from the target RGB dataset. Using
lightweight neural network architectures suitable for mobile devices as
backbones, we evaluated our method on the Zurich RAW to RGB and Fujifilm
UltraISP datasets. Compared to paired training methods, our unpaired learning
strategy shows strong potential and achieves high fidelity across multiple
evaluation metrics. The code and pre-trained models are available at
https://github.com/AndreiiArhire/Learned-Lightweight-Smartphone-ISP-with-Unpaired-Data .

</details>


### [132] [Vision language models have difficulty recognizing virtual objects](https://arxiv.org/pdf/2505.10453)
*Tyler Tran, Sangeet Khemlani, J. G. Trafton*

Main category: cs.CV

TL;DR: The paper evaluates how well vision language models (VLMs) understand visuospatial properties in images by testing their ability to process virtual objects not depicted in the scene.


<details>
  <summary>Details</summary>
Motivation: To assess VLMs' scene comprehension beyond visual input, focusing on their ability to reason about virtual objects.

Method: Systematic evaluations of state-of-the-art VLMs using prompts involving virtual objects to test spatial reasoning.

Result: VLMs perform inadequately in processing virtual objects and understanding spatial relations.

Conclusion: Current VLMs lack robust comprehension of visuospatial properties when dealing with virtual objects.

Abstract: Vision language models (VLMs) are AI systems paired with both language and
vision encoders to process multimodal input. They are capable of performing
complex semantic tasks such as automatic captioning, but it remains an open
question about how well they comprehend the visuospatial properties of scenes
depicted in the images they process. We argue that descriptions of virtual
objects -- objects that are not visually represented in an image -- can help
test scene comprehension in these AI systems. For example, an image that
depicts a person standing under a tree can be paired with the following prompt:
imagine that a kite is stuck in the tree. VLMs that comprehend the scene should
update their representations and reason sensibly about the spatial relations
between all three objects. We describe systematic evaluations of
state-of-the-art VLMs and show that their ability to process virtual objects is
inadequate.

</details>


### [133] [Consistent Quantity-Quality Control across Scenes for Deployment-Aware Gaussian Splatting](https://arxiv.org/pdf/2505.10473)
*Fengdi Zhang, Hongkun Cao, Ruqi Huang*

Main category: cs.CV

TL;DR: ControlGS optimizes 3D Gaussian splatting (3DGS) to allow intuitive trade-off adjustment between Gaussian quantity and rendering quality, outperforming baselines with fewer Gaussians and broader control.


<details>
  <summary>Details</summary>
Motivation: Existing 3DGS methods lack user-adjustable trade-offs for practical needs like hardware constraints. ControlGS addresses this gap.

Method: ControlGS uses a single training run with a user-specified hyperparameter to automatically find optimal quantity-quality trade-offs across diverse scenes.

Result: It achieves higher rendering quality with fewer Gaussians and supports stepless trade-off adjustment.

Conclusion: ControlGS provides a flexible, high-performance solution for 3DGS optimization, suitable for diverse practical applications.

Abstract: To reduce storage and computational costs, 3D Gaussian splatting (3DGS) seeks
to minimize the number of Gaussians used while preserving high rendering
quality, introducing an inherent trade-off between Gaussian quantity and
rendering quality. Existing methods strive for better quantity-quality
performance, but lack the ability for users to intuitively adjust this
trade-off to suit practical needs such as model deployment under diverse
hardware and communication constraints. Here, we present ControlGS, a 3DGS
optimization method that achieves semantically meaningful and cross-scene
consistent quantity-quality control while maintaining strong quantity-quality
performance. Through a single training run using a fixed setup and a
user-specified hyperparameter reflecting quantity-quality preference, ControlGS
can automatically find desirable quantity-quality trade-off points across
diverse scenes, from compact objects to large outdoor scenes. It also
outperforms baselines by achieving higher rendering quality with fewer
Gaussians, and supports a broad adjustment range with stepless control over the
trade-off.

</details>


### [134] [Logos as a Well-Tempered Pre-train for Sign Language Recognition](https://arxiv.org/pdf/2505.10481)
*Ilya Ovodov, Petr Surovtsev, Karina Kvanchiani, Alexander Kapitanov, Alexander Nagaev*

Main category: cs.CV

TL;DR: The paper introduces Logos, a large Russian Sign Language dataset, addressing data scarcity and labeling ambiguity in isolated sign language recognition (ISLR). It demonstrates cross-language transfer learning and improved model accuracy through explicit annotation of visually similar signs.


<details>
  <summary>Details</summary>
Motivation: The study aims to tackle the challenges of limited data for individual sign languages and ambiguity in labeling similar signs with different meanings.

Method: The paper presents the Logos dataset, explores cross-language transfer learning, and uses joint training with multiple classification heads. It also annotates visually similar sign groups explicitly.

Result: Pre-training on Logos improves model performance for other language SLR tasks, achieving state-of-the-art results on WLASL and competitive results on AUTSL.

Conclusion: The Logos dataset and proposed methods enhance ISLR model accuracy and generalizability, with publicly available resources for further research.

Abstract: This paper examines two aspects of the isolated sign language recognition
(ISLR) task. First, despite the availability of a number of datasets, the
amount of data for most individual sign languages is limited. It poses the
challenge of cross-language ISLR model training, including transfer learning.
Second, similar signs can have different semantic meanings. It leads to
ambiguity in dataset labeling and raises the question of the best policy for
annotating such signs. To address these issues, this study presents Logos, a
novel Russian Sign Language (RSL) dataset, the most extensive ISLR dataset by
the number of signers and one of the largest available datasets while also the
largest RSL dataset in size and vocabulary. It is shown that a model,
pre-trained on the Logos dataset can be used as a universal encoder for other
language SLR tasks, including few-shot learning. We explore cross-language
transfer learning approaches and find that joint training using multiple
classification heads benefits accuracy for the target lowresource datasets the
most. The key feature of the Logos dataset is explicitly annotated visually
similar sign groups. We show that explicitly labeling visually similar signs
improves trained model quality as a visual encoder for downstream tasks. Based
on the proposed contributions, we outperform current state-of-the-art results
for the WLASL dataset and get competitive results for the AUTSL dataset, with a
single stream model processing solely RGB video. The source code, dataset, and
pre-trained models are publicly available.

</details>


### [135] [UniEval: Unified Holistic Evaluation for Unified Multimodal Understanding and Generation](https://arxiv.org/pdf/2505.10483)
*Yi Li, Haonan Wang, Qixiang Zhang, Boyu Xiao, Chenchang Hu, Hualiang Wang, Xiaomeng Li*

Main category: cs.CV

TL;DR: UniEval is introduced as the first unified evaluation framework for multimodal models, addressing limitations of current benchmarks with a holistic approach (UniBench) and a new metric (UniScore).


<details>
  <summary>Details</summary>
Motivation: Current evaluation methods for unified multimodal models lack simplicity, diversity, and accuracy, often relying on extra resources or limited metrics.

Method: UniEval includes UniBench (a diverse benchmark) and UniScore (a metric aligning with human judgment), eliminating the need for extra models or annotations.

Result: UniBench proves more challenging than existing benchmarks, and UniScore outperforms current metrics in alignment with human evaluations.

Conclusion: UniEval provides a streamlined, accurate, and resource-efficient evaluation framework for unified multimodal models, revealing new insights into their capabilities.

Abstract: The emergence of unified multimodal understanding and generation models is
rapidly attracting attention because of their ability to enhance
instruction-following capabilities while minimizing model redundancy. However,
there is a lack of a unified evaluation framework for these models, which would
enable an elegant, simplified, and overall evaluation. Current models conduct
evaluations on multiple task-specific benchmarks, but there are significant
limitations, such as the lack of overall results, errors from extra evaluation
models, reliance on extensive labeled images, benchmarks that lack diversity,
and metrics with limited capacity for instruction-following evaluation. To
tackle these challenges, we introduce UniEval, the first evaluation framework
designed for unified multimodal models without extra models, images, or
annotations. This facilitates a simplified and unified evaluation process. The
UniEval framework contains a holistic benchmark, UniBench (supports both
unified and visual generation models), along with the corresponding UniScore
metric. UniBench includes 81 fine-grained tags contributing to high diversity.
Experimental results indicate that UniBench is more challenging than existing
benchmarks, and UniScore aligns closely with human evaluations, surpassing
current metrics. Moreover, we extensively evaluated SoTA unified and visual
generation models, uncovering new insights into Univeral's unique values.

</details>


### [136] [CheXGenBench: A Unified Benchmark For Fidelity, Privacy and Utility of Synthetic Chest Radiographs](https://arxiv.org/pdf/2505.10496)
*Raman Dutt, Pedro Sanchez, Yongchen Yao, Steven McDonagh, Sotirios A. Tsaftaris, Timothy Hospedales*

Main category: cs.CV

TL;DR: CheXGenBench is a standardized evaluation framework for synthetic chest radiograph generation, assessing fidelity, privacy, and clinical utility across 11 text-to-image models, revealing inefficiencies in current protocols.


<details>
  <summary>Details</summary>
Motivation: Existing evaluations for synthetic medical images lack consistency, outdated comparisons, and clinical relevance, hindering progress in generative AI for healthcare.

Method: CheXGenBench uses standardized data partitioning and 20+ metrics to evaluate generation quality, privacy risks, and clinical applicability.

Result: The framework identifies flaws in current evaluation protocols and provides a benchmark, along with a synthetic dataset (SynthCheX-75K).

Conclusion: CheXGenBench sets a new standard for reproducible comparisons in medical AI and releases resources to advance research.

Abstract: We introduce CheXGenBench, a rigorous and multifaceted evaluation framework
for synthetic chest radiograph generation that simultaneously assesses
fidelity, privacy risks, and clinical utility across state-of-the-art
text-to-image generative models. Despite rapid advancements in generative AI
for real-world imagery, medical domain evaluations have been hindered by
methodological inconsistencies, outdated architectural comparisons, and
disconnected assessment criteria that rarely address the practical clinical
value of synthetic samples. CheXGenBench overcomes these limitations through
standardised data partitioning and a unified evaluation protocol comprising
over 20 quantitative metrics that systematically analyse generation quality,
potential privacy vulnerabilities, and downstream clinical applicability across
11 leading text-to-image architectures. Our results reveal critical
inefficiencies in the existing evaluation protocols, particularly in assessing
generative fidelity, leading to inconsistent and uninformative comparisons. Our
framework establishes a standardised benchmark for the medical AI community,
enabling objective and reproducible comparisons while facilitating seamless
integration of both existing and future generative models. Additionally, we
release a high-quality, synthetic dataset, SynthCheX-75K, comprising 75K
radiographs generated by the top-performing model (Sana 0.6B) in our benchmark
to support further research in this critical domain. Through CheXGenBench, we
establish a new state-of-the-art and release our framework, models, and
SynthCheX-75K dataset at https://raman1121.github.io/CheXGenBench/

</details>


### [137] [MorphGuard: Morph Specific Margin Loss for Enhancing Robustness to Face Morphing Attacks](https://arxiv.org/pdf/2505.10497)
*Iurii Medvedev, Nuno Goncalves*

Main category: cs.CV

TL;DR: A novel dual-branch classification strategy enhances deep networks' robustness against face morphing attacks in face recognition systems.


<details>
  <summary>Details</summary>
Motivation: Face recognition systems are vulnerable to presentation attacks like face morphing, necessitating improved robustness.

Method: Introduces a dual-branch classification strategy to handle ambiguous labeling of face morphs, integrating them into training.

Result: Validated on public benchmarks, the method effectively improves robustness against morphing attacks.

Conclusion: The approach is universally applicable and can enhance existing face recognition training pipelines.

Abstract: Face recognition has evolved significantly with the advancement of deep
learning techniques, enabling its widespread adoption in various applications
requiring secure authentication. However, this progress has also increased its
exposure to presentation attacks, including face morphing, which poses a
serious security threat by allowing one identity to impersonate another.
Therefore, modern face recognition systems must be robust against such attacks.
  In this work, we propose a novel approach for training deep networks for face
recognition with enhanced robustness to face morphing attacks. Our method
modifies the classification task by introducing a dual-branch classification
strategy that effectively handles the ambiguity in the labeling of face morphs.
This adaptation allows the model to incorporate morph images into the training
process, improving its ability to distinguish them from bona fide samples.
  Our strategy has been validated on public benchmarks, demonstrating its
effectiveness in enhancing robustness against face morphing attacks.
Furthermore, our approach is universally applicable and can be integrated into
existing face recognition training pipelines to improve classification-based
recognition methods.

</details>


### [138] [MathCoder-VL: Bridging Vision and Code for Enhanced Multimodal Mathematical Reasoning](https://arxiv.org/pdf/2505.10557)
*Ke Wang, Junting Pan, Linda Wei, Aojun Zhou, Weikang Shi, Zimu Lu, Han Xiao, Yunqiao Yang, Houxing Ren, Mingjie Zhan, Hongsheng Li*

Main category: cs.CV

TL;DR: The paper introduces FigCodifier and ImgCode-8.6M, leveraging code for cross-modal alignment in mathematical figures, and achieves SOTA results with MathCoder-VL.


<details>
  <summary>Details</summary>
Motivation: Current datasets lack detailed mathematical figure captions, limiting LMMs' multimodal reasoning. Code is proposed as supervision for precise alignment.

Method: Developed image-to-code model FigCodifier and dataset ImgCode-8.6M using model-in-the-loop. Created MM-MathInstruct-3M for fine-tuning.

Result: MathCoder-VL outperforms GPT-4o and Claude 3.5 Sonnet, with 8.9% and 9.2% improvements in geometry problem-solving.

Conclusion: The approach advances multimodal math reasoning, with datasets and models made publicly available.

Abstract: Natural language image-caption datasets, widely used for training Large
Multimodal Models, mainly focus on natural scenarios and overlook the intricate
details of mathematical figures that are critical for problem-solving,
hindering the advancement of current LMMs in multimodal mathematical reasoning.
To this end, we propose leveraging code as supervision for cross-modal
alignment, since code inherently encodes all information needed to generate
corresponding figures, establishing a precise connection between the two
modalities. Specifically, we co-develop our image-to-code model and dataset
with model-in-the-loop approach, resulting in an image-to-code model,
FigCodifier and ImgCode-8.6M dataset, the largest image-code dataset to date.
Furthermore, we utilize FigCodifier to synthesize novel mathematical figures
and then construct MM-MathInstruct-3M, a high-quality multimodal math
instruction fine-tuning dataset. Finally, we present MathCoder-VL, trained with
ImgCode-8.6M for cross-modal alignment and subsequently fine-tuned on
MM-MathInstruct-3M for multimodal math problem solving. Our model achieves a
new open-source SOTA across all six metrics. Notably, it surpasses GPT-4o and
Claude 3.5 Sonnet in the geometry problem-solving subset of MathVista,
achieving improvements of 8.9% and 9.2%. The dataset and models will be
released at https://github.com/mathllm/MathCoder.

</details>


### [139] [Enhancing Multi-Image Question Answering via Submodular Subset Selection](https://arxiv.org/pdf/2505.10533)
*Aaryan Sharma, Shivansh Gupta, Samar Agarwal, Vishak Prasad C., Ganesh Ramakrishnan*

Main category: cs.CV

TL;DR: The paper proposes a submodular subset selection method to enhance the retriever framework in LMMs for Multiple Image Question Answering, improving scalability and retrieval performance.


<details>
  <summary>Details</summary>
Motivation: LMMs struggle with tasks involving multiple images due to scalability and retrieval issues.

Method: Uses query-aware submodular functions (e.g., GraphCut) to pre-select relevant images before retrieval, with anchor-based queries and data augmentation.

Result: Improves the submodular-retriever pipeline, especially for large haystack sizes.

Conclusion: The proposed method effectively addresses scalability and retrieval challenges in LMMs for multi-image tasks.

Abstract: Large multimodal models (LMMs) have achieved high performance in
vision-language tasks involving single image but they struggle when presented
with a collection of multiple images (Multiple Image Question Answering
scenario). These tasks, which involve reasoning over large number of images,
present issues in scalability (with increasing number of images) and retrieval
performance. In this work, we propose an enhancement for retriever framework
introduced in MIRAGE model using submodular subset selection techniques. Our
method leverages query-aware submodular functions, such as GraphCut, to
pre-select a subset of semantically relevant images before main retrieval
component. We demonstrate that using anchor-based queries and augmenting the
data improves submodular-retriever pipeline effectiveness, particularly in
large haystack sizes.

</details>


### [140] [Exploring Implicit Visual Misunderstandings in Multimodal Large Language Models through Attention Analysis](https://arxiv.org/pdf/2505.10541)
*Pengfei Wang, Guohai Xu, Weinong Wang, Junjie Yang, Jie Lou, Yunhua Xue*

Main category: cs.CV

TL;DR: The paper introduces a metric and benchmark to detect implicit visual misunderstanding (IVM) in Multimodal Large Language Models (MLLMs), ensuring models genuinely comprehend visual input.


<details>
  <summary>Details</summary>
Motivation: Existing benchmarks overlook whether MLLMs truly understand visual input, leading to correct answers without comprehension (IVM).

Method: Decouples visual and textual modalities in causal attention, introduces attention accuracy metric, and creates a benchmark for IVM.

Result: Attention distribution converges on correct images as layers deepen; attention accuracy reliably assesses visual understanding.

Conclusion: The proposed method effectively quantifies IVM, works in unimodal scenarios, and is robust to biases, proving its versatility.

Abstract: Recent advancements have enhanced the capability of Multimodal Large Language
Models (MLLMs) to comprehend multi-image information. However, existing
benchmarks primarily evaluate answer correctness, overlooking whether models
genuinely comprehend the visual input. To address this, we define implicit
visual misunderstanding (IVM), where MLLMs provide correct answers without
fully comprehending the visual input. Through our analysis, we decouple the
visual and textual modalities within the causal attention module, revealing
that attention distribution increasingly converges on the image associated with
the correct answer as the network layers deepen. This insight leads to the
introduction of a scale-agnostic metric, \textit{attention accuracy}, and a
novel benchmark for quantifying IVMs. Attention accuracy directly evaluates the
model's visual understanding via internal mechanisms, remaining robust to
positional biases for more reliable assessments. Furthermore, we extend our
approach to finer granularities and demonstrate its effectiveness in unimodal
scenarios, underscoring its versatility and generalizability.

</details>


### [141] [Does Feasibility Matter? Understanding the Impact of Feasibility on Synthetic Training Data](https://arxiv.org/pdf/2505.10551)
*Yiwen Liu, Jessica Bader, Jae Myung Kim*

Main category: cs.CV

TL;DR: The paper investigates whether enforcing feasibility in synthetic images impacts CLIP-based classifier performance, finding minimal effect (less than 0.3% difference in accuracy).


<details>
  <summary>Details</summary>
Motivation: To determine if excluding infeasible synthetic images (those with unrealistic attributes) from training data is necessary for CLIP-based classifiers.

Method: Introduces VariReal, a pipeline to edit images for feasible or infeasible attributes based on textual prompts, and tests on CLIP classifiers.

Result: Feasibility minimally affects performance; mixing feasible and infeasible images in training does not significantly impact results.

Conclusion: Enforcing feasibility in synthetic training data is not crucial for CLIP-based classifiers, as it has negligible impact on performance.

Abstract: With the development of photorealistic diffusion models, models trained in
part or fully on synthetic data achieve progressively better results. However,
diffusion models still routinely generate images that would not exist in
reality, such as a dog floating above the ground or with unrealistic texture
artifacts. We define the concept of feasibility as whether attributes in a
synthetic image could realistically exist in the real-world domain; synthetic
images containing attributes that violate this criterion are considered
infeasible. Intuitively, infeasible images are typically considered
out-of-distribution; thus, training on such images is expected to hinder a
model's ability to generalize to real-world data, and they should therefore be
excluded from the training set whenever possible. However, does feasibility
really matter? In this paper, we investigate whether enforcing feasibility is
necessary when generating synthetic training data for CLIP-based classifiers,
focusing on three target attributes: background, color, and texture. We
introduce VariReal, a pipeline that minimally edits a given source image to
include feasible or infeasible attributes given by the textual prompt generated
by a large language model. Our experiments show that feasibility minimally
affects LoRA-fine-tuned CLIP performance, with mostly less than 0.3% difference
in top-1 accuracy across three fine-grained datasets. Also, the attribute
matters on whether the feasible/infeasible images adversarially influence the
classification performance. Finally, mixing feasible and infeasible images in
training datasets does not significantly impact performance compared to using
purely feasible or infeasible datasets.

</details>


### [142] [End-to-End Vision Tokenizer Tuning](https://arxiv.org/pdf/2505.10562)
*Wenxuan Wang, Fan Zhang, Yufeng Cui, Haiwen Diao, Zhuoyan Luo, Huchuan Lu, Jing Liu, Xinlong Wang*

Main category: cs.CV

TL;DR: ETT is an end-to-end vision tokenizer tuning method that jointly optimizes tokenization and downstream tasks, improving performance by 2-6% over frozen tokenizers.


<details>
  <summary>Details</summary>
Motivation: Current vision tokenizers are optimized independently of downstream tasks, causing misalignment and poor performance in tasks like text recognition or generation.

Method: ETT jointly optimizes vision tokenization with reconstruction and caption objectives, leveraging visual embeddings from the tokenizer codebook.

Result: ETT achieves 2-6% performance gains in multimodal understanding and visual generation tasks while maintaining reconstruction quality.

Conclusion: ETT is a simple, effective method to enhance multimodal foundation models without modifying existing architectures.

Abstract: Existing vision tokenization isolates the optimization of vision tokenizers
from downstream training, implicitly assuming the visual tokens can generalize
well across various tasks, e.g., image generation and visual question
answering. The vision tokenizer optimized for low-level reconstruction is
agnostic to downstream tasks requiring varied representations and semantics.
This decoupled paradigm introduces a critical misalignment: The loss of the
vision tokenization can be the representation bottleneck for target tasks. For
example, errors in tokenizing text in a given image lead to poor results when
recognizing or generating them. To address this, we propose ETT, an end-to-end
vision tokenizer tuning approach that enables joint optimization between vision
tokenization and target autoregressive tasks. Unlike prior autoregressive
models that use only discrete indices from a frozen vision tokenizer, ETT
leverages the visual embeddings of the tokenizer codebook, and optimizes the
vision tokenizers end-to-end with both reconstruction and caption objectives.
ETT can be seamlessly integrated into existing training pipelines with minimal
architecture modifications. Our ETT is simple to implement and integrate,
without the need to adjust the original codebooks or architectures of the
employed large language models. Extensive experiments demonstrate that our
proposed end-to-end vision tokenizer tuning unlocks significant performance
gains, i.e., 2-6% for multimodal understanding and visual generation tasks
compared to frozen tokenizer baselines, while preserving the original
reconstruction capability. We hope this very simple and strong method can
empower multimodal foundation models besides image generation and
understanding.

</details>


### [143] [Depth Anything with Any Prior](https://arxiv.org/pdf/2505.10565)
*Zehan Wang, Siyu Chen, Lihe Yang, Jialei Wang, Ziang Zhang, Hengshuang Zhao, Zhou Zhao*

Main category: cs.CV

TL;DR: Prior Depth Anything combines precise but incomplete metric depth with complete but relative geometric structures to generate accurate, dense metric depth maps. It uses a coarse-to-fine pipeline and conditioned MDE for refinement, achieving strong zero-shot generalization.


<details>
  <summary>Details</summary>
Motivation: To address the limitations of existing depth measurement (precise but incomplete) and depth prediction (complete but relative) by integrating both for accurate, detailed metric depth maps.

Method: A coarse-to-fine pipeline with pixel-level metric alignment, distance-aware weighting, and a conditioned MDE model to refine and merge complementary depth sources.

Result: The model achieves impressive zero-shot generalization across tasks like depth completion, super-resolution, and inpainting, outperforming task-specific methods on 7 datasets.

Conclusion: The framework provides flexible accuracy-efficiency trade-offs and adapts to advancements in MDE models, making it robust for diverse scenarios.

Abstract: This work presents Prior Depth Anything, a framework that combines incomplete
but precise metric information in depth measurement with relative but complete
geometric structures in depth prediction, generating accurate, dense, and
detailed metric depth maps for any scene. To this end, we design a
coarse-to-fine pipeline to progressively integrate the two complementary depth
sources. First, we introduce pixel-level metric alignment and distance-aware
weighting to pre-fill diverse metric priors by explicitly using depth
prediction. It effectively narrows the domain gap between prior patterns,
enhancing generalization across varying scenarios. Second, we develop a
conditioned monocular depth estimation (MDE) model to refine the inherent noise
of depth priors. By conditioning on the normalized pre-filled prior and
prediction, the model further implicitly merges the two complementary depth
sources. Our model showcases impressive zero-shot generalization across depth
completion, super-resolution, and inpainting over 7 real-world datasets,
matching or even surpassing previous task-specific methods. More importantly,
it performs well on challenging, unseen mixed priors and enables test-time
improvements by switching prediction models, providing a flexible
accuracy-efficiency trade-off while evolving with advancements in MDE models.

</details>


### [144] [3D-Fixup: Advancing Photo Editing with 3D Priors](https://arxiv.org/pdf/2505.10566)
*Yen-Chi Cheng, Krishna Kumar Singh, Jae Shin Yoon, Alex Schwing, Liangyan Gui, Matheus Gadelha, Paul Guerrero, Nanxuan Zhao*

Main category: cs.CV

TL;DR: 3D-Fixup is a framework for 3D-aware image editing using learned 3D priors and diffusion models, supporting complex edits like translation and rotation.


<details>
  <summary>Details</summary>
Motivation: 3D-aware image editing is challenging due to reliance on single images. The goal is to enable realistic, identity-coherent edits.

Method: Leverages diffusion models and video data for training. Incorporates 3D guidance from an Image-to-3D model to project 2D into 3D space.

Result: Achieves high-quality, complex 3D-aware edits, advancing realistic image manipulation.

Conclusion: 3D-Fixup effectively integrates 3D priors for coherent edits, enhancing diffusion model applications in image manipulation.

Abstract: Despite significant advances in modeling image priors via diffusion models,
3D-aware image editing remains challenging, in part because the object is only
specified via a single image. To tackle this challenge, we propose 3D-Fixup, a
new framework for editing 2D images guided by learned 3D priors. The framework
supports difficult editing situations such as object translation and 3D
rotation. To achieve this, we leverage a training-based approach that harnesses
the generative power of diffusion models. As video data naturally encodes
real-world physical dynamics, we turn to video data for generating training
data pairs, i.e., a source and a target frame. Rather than relying solely on a
single trained model to infer transformations between source and target frames,
we incorporate 3D guidance from an Image-to-3D model, which bridges this
challenging task by explicitly projecting 2D information into 3D space. We
design a data generation pipeline to ensure high-quality 3D guidance throughout
training. Results show that by integrating these 3D priors, 3D-Fixup
effectively supports complex, identity coherent 3D-aware edits, achieving
high-quality results and advancing the application of diffusion models in
realistic image manipulation. The code is provided at
https://3dfixup.github.io/

</details>


### [145] [Highly Efficient 3D Human Pose Tracking from Events with Spiking Spatiotemporal Transformer](https://arxiv.org/pdf/2303.09681)
*Shihao Zou, Yuxuan Mu, Wei Ji, Zi-An Wang, Xinxin Zuo, Sen Wang, Weixin Si, Li Cheng*

Main category: cs.CV

TL;DR: A sparse Spiking Neural Networks (SNNs) framework for 3D human pose tracking using event cameras, avoiding redundant computations and outperforming ANN-based methods in efficiency and performance.


<details>
  <summary>Details</summary>
Motivation: Existing ANN-based methods for 3D human pose tracking with event cameras ignore event sparsity, leading to inefficiency and degraded performance.

Method: Proposes a sparse SNN framework with a Spiking Spatiotemporal Transformer for bi-directional spatiotemporal fusion and similarity measurement of spike features.

Result: Outperforms SOTA ANN-based methods with 19.1% FLOPs and 3.6% energy cost, and surpasses SNN benchmarks.

Conclusion: The framework effectively leverages event sparsity, offering a more efficient and superior solution for 3D human pose tracking.

Abstract: Event camera, as an asynchronous vision sensor capturing scene dynamics,
presents new opportunities for highly efficient 3D human pose tracking.
Existing approaches typically adopt modern-day Artificial Neural Networks
(ANNs), such as CNNs or Transformer, where sparse events are converted into
dense images or paired with additional gray-scale images as input. Such
practices, however, ignore the inherent sparsity of events, resulting in
redundant computations, increased energy consumption, and potentially degraded
performance. Motivated by these observations, we introduce the first sparse
Spiking Neural Networks (SNNs) framework for 3D human pose tracking based
solely on events. Our approach eliminates the need to convert sparse data to
dense formats or incorporate additional images, thereby fully exploiting the
innate sparsity of input events. Central to our framework is a novel Spiking
Spatiotemporal Transformer, which enables bi-directional spatiotemporal fusion
of spike pose features and provides a guaranteed similarity measurement between
binary spike features in spiking attention. Moreover, we have constructed a
large-scale synthetic dataset, SynEventHPD, that features a broad and diverse
set of 3D human motions, as well as much longer hours of event streams.
Empirical experiments demonstrate the superiority of our approach over existing
state-of-the-art (SOTA) ANN-based methods, requiring only 19.1% FLOPs and 3.6%
energy cost. Furthermore, our approach outperforms existing SNN-based
benchmarks in this task, highlighting the effectiveness of our proposed SNN
framework. The dataset will be released upon acceptance, and code can be found
at https://github.com/JimmyZou/HumanPoseTracking_SNN.

</details>


### [146] [CreativeSynth: Cross-Art-Attention for Artistic Image Synthesis with Multimodal Diffusion](https://arxiv.org/pdf/2401.14066)
*Nisha Huang, Weiming Dong, Yuxin Zhang, Fan Tang, Ronghui Li, Chongyang Ma, Xiu Li, Tong-Yee Lee, Changsheng Xu*

Main category: cs.CV

TL;DR: CreativeSynth integrates multimodal semantic information into artworks using a diffusion model, avoiding synthetic traces and maintaining aesthetic harmony.


<details>
  <summary>Details</summary>
Motivation: Current style transfer and text-to-image models fail to fully capture and express key painting attributes like layout, perspective, and semantics, disrupting artistic harmony.

Method: Proposes CreativeSynth, a multi-task unified framework based on a diffusion model with Cross-Art-Attention for multimodal feature integration.

Result: Demonstrates seamless integration of real-world semantics into art, bridging generative models and artistic expression across various art categories.

Conclusion: CreativeSynth effectively addresses limitations of existing methods, offering a unified approach for aesthetic and semantic fusion in art generation.

Abstract: Although remarkable progress has been made in image style transfer, style is
just one of the components of artistic paintings. Directly transferring
extracted style features to natural images often results in outputs with
obvious synthetic traces. This is because key painting attributes including
layout, perspective, shape, and semantics often cannot be conveyed and
expressed through style transfer. Large-scale pretrained text-to-image
generation models have demonstrated their capability to synthesize a vast
amount of high-quality images. However, even with extensive textual
descriptions, it is challenging to fully express the unique visual properties
and details of paintings. Moreover, generic models often disrupt the overall
artistic effect when modifying specific areas, making it more complicated to
achieve a unified aesthetic in artworks. Our main novel idea is to integrate
multimodal semantic information as a synthesis guide into artworks, rather than
transferring style to the real world. We also aim to reduce the disruption to
the harmony of artworks while simplifying the guidance conditions.
Specifically, we propose an innovative multi-task unified framework called
CreativeSynth, based on the diffusion model with the ability to coordinate
multimodal inputs. CreativeSynth combines multimodal features with customized
attention mechanisms to seamlessly integrate real-world semantic content into
the art domain through Cross-Art-Attention for aesthetic maintenance and
semantic fusion. We demonstrate the results of our method across a wide range
of different art categories, proving that CreativeSynth bridges the gap between
generative models and artistic expression. Code and results are available at
https://github.com/haha-lisa/CreativeSynth.

</details>


### [147] [SMURF: Continuous Dynamics for Motion-Deblurring Radiance Fields](https://arxiv.org/pdf/2403.07547)
*Jungho Lee, Dogyoon Lee, Minhyeok Lee, Donghyung Kim, Sangyoun Lee*

Main category: cs.CV

TL;DR: SMURF improves NeRF by addressing motion blur with continuous motion modeling, achieving top performance.


<details>
  <summary>Details</summary>
Motivation: Motion blur from camera movements degrades NeRF's 3D scene reconstruction quality.

Method: SMURF uses continuous motion blurring kernel (CMBK) to model camera motion for blurry inputs.

Result: SMURF outperforms benchmarks, showing state-of-the-art results.

Conclusion: SMURF effectively handles motion blur, enhancing NeRF's robustness and output quality.

Abstract: Neural radiance fields (NeRF) has attracted considerable attention for their
exceptional ability in synthesizing novel views with high fidelity. However,
the presence of motion blur, resulting from slight camera movements during
extended shutter exposures, poses a significant challenge, potentially
compromising the quality of the reconstructed 3D scenes. To effectively handle
this issue, we propose sequential motion understanding radiance fields (SMURF),
a novel approach that models continuous camera motion and leverages the
explicit volumetric representation method for robustness to motion-blurred
input images. The core idea of the SMURF is continuous motion blurring kernel
(CMBK), a module designed to model a continuous camera movements for processing
blurry inputs. Our model is evaluated against benchmark datasets and
demonstrates state-of-the-art performance both quantitatively and
qualitatively.

</details>


### [148] [Pose Priors from Language Models](https://arxiv.org/pdf/2405.03689)
*Sanjay Subramanian, Evonne Ng, Lea Müller, Dan Klein, Shiry Ginosar, Trevor Darrell*

Main category: cs.CV

TL;DR: The paper proposes using large multimodal models (LMMs) to improve 3D human pose estimation by leveraging language descriptions of physical interactions, avoiding costly manual annotations or motion capture data.


<details>
  <summary>Details</summary>
Motivation: Most 3D human pose estimation methods ignore language descriptions of physical interactions, missing a valuable data source.

Method: The approach extracts contact-relevant descriptors from LMMs and translates them into losses for 3D pose optimization.

Result: The method produces accurate reconstructions for two-person interactions and self-contact, capturing interaction semantics.

Conclusion: LMMs are powerful tools for contact prediction and pose estimation, offering a scalable alternative to traditional methods.

Abstract: Language is often used to describe physical interaction, yet most 3D human
pose estimation methods overlook this rich source of information. We bridge
this gap by leveraging large multimodal models (LMMs) as priors for
reconstructing contact poses, offering a scalable alternative to traditional
methods that rely on human annotations or motion capture data. Our approach
extracts contact-relevant descriptors from an LMM and translates them into
tractable losses to constrain 3D human pose optimization. Despite its
simplicity, our method produces compelling reconstructions for both two-person
interactions and self-contact scenarios, accurately capturing the semantics of
physical and social interactions. Our results demonstrate that LMMs can serve
as powerful tools for contact prediction and pose estimation, offering an
alternative to costly manual human annotations or motion capture data. Our code
is publicly available at https://prosepose.github.io.

</details>


### [149] [S2-Track: A Simple yet Strong Approach for End-to-End 3D Multi-Object Tracking](https://arxiv.org/pdf/2406.02147)
*Tao Tang, Lijun Zhou, Pengkun Hao, Zihang He, Kalok Ho, Shuo Gu, Zhihui Hao, Haiyang Sun, Kun Zhan, Peng Jia, XianPeng Lang, Xiaodan Liang*

Main category: cs.CV

TL;DR: The paper introduces S2-Track, an improved end-to-end 3D MOT framework, addressing limitations in complex scenarios like occlusions and small objects. It achieves state-of-the-art performance on the nuScenes benchmark.


<details>
  <summary>Details</summary>
Motivation: Existing end-to-end query-based trackers for 3D MOT lack systematic improvements and struggle with complex scenarios like occlusions and small objects.

Method: The framework is decomposed into query initialization, propagation, and matching, with novel improvements: 2D-Prompted Query Initialization, Uncertainty-aware Probabilistic Decoder, and Hierarchical Query Denoising.

Result: S2-Track achieves 66.3% AMOTA on nuScenes test split, surpassing the previous best by 8.9%, and ranks 1st on the leaderboard.

Conclusion: S2-Track demonstrates superior performance and robustness, setting a new benchmark for 3D MOT in autonomous driving.

Abstract: 3D multiple object tracking (MOT) plays a crucial role in autonomous driving
perception. Recent end-to-end query-based trackers simultaneously detect and
track objects, which have shown promising potential for the 3D MOT task.
However, existing methods are still in the early stages of development and lack
systematic improvements, failing to track objects in certain complex scenarios,
like occlusions and the small size of target object's situations. In this
paper, we first summarize the current end-to-end 3D MOT framework by
decomposing it into three constituent parts: query initialization, query
propagation, and query matching. Then we propose corresponding improvements,
which lead to a strong yet simple tracker: S2-Track. Specifically, for query
initialization, we present 2D-Prompted Query Initialization, which leverages
predicted 2D object and depth information to prompt an initial estimate of the
object's 3D location. For query propagation, we introduce an Uncertainty-aware
Probabilistic Decoder to capture the uncertainty of complex environment in
object prediction with probabilistic attention. For query matching, we propose
a Hierarchical Query Denoising strategy to enhance training robustness and
convergence. As a result, our S2-Track achieves state-of-the-art performance on
nuScenes benchmark, i.e., 66.3% AMOTA on test split, surpassing the previous
best end-to-end solution by a significant margin of 8.9% AMOTA. We achieve 1st
place on the nuScenes tracking task leaderboard.

</details>


### [150] [Unsupervised Video Highlight Detection by Learning from Audio and Visual Recurrence](https://arxiv.org/pdf/2407.13933)
*Zahidul Islam, Sujoy Paul, Mrigank Rochan*

Main category: cs.CV

TL;DR: Proposes an unsupervised method for video highlight detection using audio and visual features, eliminating the need for manual annotations.


<details>
  <summary>Details</summary>
Motivation: Addresses the challenge of automated video highlight detection without relying on expensive manual annotations or large external datasets.

Method: Uses clustering to identify pseudo-categories, computes audio and visual pseudo-highlight scores, and combines them to train an audio-visual highlight detection network.

Result: Demonstrates superior performance over prior work in experiments on three benchmarks.

Conclusion: The unsupervised approach effectively leverages recurring significant moments in audio and visual modalities for highlight detection.

Abstract: With the exponential growth of video content, the need for automated video
highlight detection to extract key moments or highlights from lengthy videos
has become increasingly pressing. This technology has the potential to enhance
user experiences by allowing quick access to relevant content across diverse
domains. Existing methods typically rely either on expensive manually labeled
frame-level annotations, or on a large external dataset of videos for weak
supervision through category information. To overcome this, we focus on
unsupervised video highlight detection, eliminating the need for manual
annotations. We propose a novel unsupervised approach which capitalizes on the
premise that significant moments tend to recur across multiple videos of the
similar category in both audio and visual modalities. Surprisingly, audio
remains under-explored, especially in unsupervised algorithms, despite its
potential to detect key moments. Through a clustering technique, we identify
pseudo-categories of videos and compute audio pseudo-highlight scores for each
video by measuring the similarities of audio features among audio clips of all
the videos within each pseudo-category. Similarly, we also compute visual
pseudo-highlight scores for each video using visual features. Then, we combine
audio and visual pseudo-highlights to create the audio-visual pseudo
ground-truth highlight of each video for training an audio-visual highlight
detection network. Extensive experiments and ablation studies on three
benchmarks showcase the superior performance of our method over prior work.

</details>


### [151] [Improving Fine-Grained Control via Aggregation of Multiple Diffusion Models](https://arxiv.org/pdf/2410.01262)
*Conghan Yue, Zhengwei Peng, Shiyan Du, Zhi Ji, Chuangjian Cai, Le Wan, Dongyu Zhang*

Main category: cs.CV

TL;DR: AMDM is a training-free algorithm that aggregates multiple diffusion models for fine-grained control, improving performance without additional training.


<details>
  <summary>Details</summary>
Motivation: Addressing the challenge of fine-grained control in diffusion models due to dataset limitations and complex architectures.

Method: Introduces AMDM, which integrates features from multiple diffusion models to activate specific features for fine-grained control.

Result: AMDM significantly improves fine-grained control without training, revealing insights into feature focus in diffusion models.

Conclusion: AMDM provides a practical solution for fine-grained control, leveraging existing models and avoiding costly training and dataset construction.

Abstract: While many diffusion models perform well when controlling for particular
aspect among style, character, and interaction, they struggle with fine-grained
control due to dataset limitations and intricate model architecture design.
This paper first introduces a novel training-free algorithm in fine-grained
generation, Aggregation of Multiple Diffusion Models (AMDM), which integrates
features from multiple diffusion models into a specified model to activate
specific features and enable fine-grained control. Experimental results
demonstrate that AMDM significantly improves fine-grained control without
training, validating its effectiveness. Additionally, it reveals that diffusion
models initially focus on features such as position, attributes, and style,
with later stages improving generation quality and consistency. AMDM offers a
new perspective for tackling the challenges of fine-grained conditional control
generation in diffusion models: We can fully utilize existing or develop new
conditional diffusion models that control specific aspects, and then aggregate
them using AMDM algorithm. This eliminates the need for constructing complex
datasets, designing intricate model architectures, and incurring high training
costs. Code is available at: https://github.com/Hammour-steak/AMDM.

</details>


### [152] [HaHeAE: Learning Generalisable Joint Representations of Human Hand and Head Movements in Extended Reality](https://arxiv.org/pdf/2410.16430)
*Zhiming Hu, Guanhua Zhang, Zheming Yin, Daniel Haeufle, Syn Schmitt, Andreas Bulling*

Main category: cs.CV

TL;DR: HaHeAE is a self-supervised method for joint hand-head movement modeling in XR, outperforming existing methods by 74% in reconstruction quality and enabling new applications.


<details>
  <summary>Details</summary>
Motivation: Prior works on hand and head modeling in XR were limited to single modalities or specific applications, lacking generalizability.

Method: Uses an autoencoder with a graph convolutional network-based semantic encoder and diffusion-based stochastic encoder, plus a diffusion-based decoder.

Result: Outperforms common methods by 74%, generalizes across users/activities/environments, and enables new applications like cluster identification and movement generation.

Conclusion: Demonstrates the effectiveness of self-supervised methods for joint hand-head behavior modeling in XR.

Abstract: Human hand and head movements are the most pervasive input modalities in
extended reality (XR) and are significant for a wide range of applications.
However, prior works on hand and head modelling in XR only explored a single
modality or focused on specific applications. We present HaHeAE - a novel
self-supervised method for learning generalisable joint representations of hand
and head movements in XR. At the core of our method is an autoencoder (AE) that
uses a graph convolutional network-based semantic encoder and a diffusion-based
stochastic encoder to learn the joint semantic and stochastic representations
of hand-head movements. It also features a diffusion-based decoder to
reconstruct the original signals. Through extensive evaluations on three public
XR datasets, we show that our method 1) significantly outperforms commonly used
self-supervised methods by up to 74.0% in terms of reconstruction quality and
is generalisable across users, activities, and XR environments, 2) enables new
applications, including interpretable hand-head cluster identification and
variable hand-head movement generation, and 3) can serve as an effective
feature extractor for downstream tasks. Together, these results demonstrate the
effectiveness of our method and underline the potential of self-supervised
methods for jointly modelling hand-head behaviours in extended reality.

</details>


### [153] [PEP-GS: Perceptually-Enhanced Precise Structured 3D Gaussians for View-Adaptive Rendering](https://arxiv.org/pdf/2411.05731)
*Junxi Jin, Xiulai Li, Haiping Huang, Lianjun Liu, Yujie Sun, Logan Liu*

Main category: cs.CV

TL;DR: PEP-GS improves 3D Gaussian Splatting by addressing Gaussian redundancy, view-dependent effects, and lighting challenges with a perceptually-enhanced framework and Hierarchical Granular-Structural Attention.


<details>
  <summary>Details</summary>
Motivation: 3D-GS struggles with Gaussian redundancy, view-dependent effects, and complex lighting. Traditional methods fail to capture anisotropic components and view-dependent colors accurately.

Method: PEP-GS dynamically predicts Gaussian attributes (opacity, color, covariance) and replaces spherical harmonics with Hierarchical Granular-Structural Attention for better color modeling. It also optimizes perceptual consistency.

Result: PEP-GS outperforms state-of-the-art methods, especially in handling view-dependent effects and fine-scale details.

Conclusion: PEP-GS provides a more accurate and perceptually consistent framework for 3D scene rendering, addressing key limitations of 3D-GS.

Abstract: Recently, 3D Gaussian Splatting (3D-GS) has achieved significant success in
real-time, high-quality 3D scene rendering. However, it faces several
challenges, including Gaussian redundancy, limited ability to capture
view-dependent effects, and difficulties in handling complex lighting and
specular reflections. Additionally, methods that use spherical harmonics for
color representation often struggle to effectively capture anisotropic
components, especially when modeling view-dependent colors under complex
lighting conditions, leading to insufficient contrast and unnatural color
saturation. To address these limitations, we introduce PEP-GS, a
perceptually-enhanced framework that dynamically predicts Gaussian attributes,
including opacity, color, and covariance. We replace traditional spherical
harmonics with a Hierarchical Granular-Structural Attention mechanism, which
enables more accurate modeling of complex view-dependent color effects. By
employing a stable and interpretable framework for opacity and covariance
estimation, PEP-GS avoids the removal of essential Gaussians prematurely,
ensuring a more accurate scene representation. Furthermore, perceptual
optimization is applied to the final rendered images, enhancing perceptual
consistency across different views and ensuring high-quality renderings with
improved texture fidelity and fine-scale detail preservation. Experimental
results demonstrate that PEP-GS outperforms state-of-the-art methods,
particularly in challenging scenarios involving view-dependent effects and
fine-scale details.

</details>


### [154] [OSMLoc: Single Image-Based Visual Localization in OpenStreetMap with Fused Geometric and Semantic Guidance](https://arxiv.org/pdf/2411.08665)
*Youqi Liao, Xieyuanli Chen, Shuhao Kang, Jianping Li, Zhen Dong, Hongchao Fan, Bisheng Yang*

Main category: cs.CV

TL;DR: OSMLoc is a brain-inspired visual localization method using OpenStreetMap (OSM) data, combining semantic and geometric guidance to improve accuracy and robustness.


<details>
  <summary>Details</summary>
Motivation: The disparity between camera imagery and vectorized map data limits VGI's potential in localization. OSMLoc mimics human brain fusion of geometry and semantics to address this.

Method: OSMLoc uses a visual foundational model for image features, a geometry-guided depth adapter, and semantic embeddings from OSM for feature matching.

Result: Experiments on MGL, CC benchmark, and KITTI show OSMLoc's superiority in accuracy, robustness, and generalization.

Conclusion: OSMLoc effectively bridges the gap between visual observations and map data, enhancing localization performance with open-source availability.

Abstract: OpenStreetMap (OSM), a rich and versatile source of volunteered geographic
information (VGI), facilitates human self-localization and scene understanding
by integrating nearby visual observations with vectorized map data. However,
the disparity in modalities and perspectives poses a major challenge for
effectively matching camera imagery with compact map representations, thereby
limiting the full potential of VGI data in real-world localization
applications.
  Inspired by the fact that the human brain relies on the fusion of geometric
and semantic understanding for spatial localization tasks, we propose the
OSMLoc in this paper. OSMLoc is a brain-inspired visual localization approach
based on first-person-view images against the OSM maps. It integrates semantic
and geometric guidance to significantly improve accuracy, robustness, and
generalization capability. First, we equip the OSMLoc with the visual
foundational model to extract powerful image features. Second, a
geometry-guided depth distribution adapter is proposed to bridge the monocular
depth estimation and camera-to-BEV transform. Thirdly, the semantic embeddings
from the OSM data are utilized as auxiliary guidance for image-to-OSM feature
matching. To validate the proposed OSMLoc, we collect a worldwide cross-area
and cross-condition (CC) benchmark for extensive evaluation. Experiments on the
MGL dataset, CC validation benchmark, and KITTI dataset have demonstrated the
superiority of our method. Code, pre-trained models, CC validation benchmark,
and additional results are available at: https://github.com/WHU-USI3DV/OSMLoc.

</details>


### [155] [DINO-X: A Unified Vision Model for Open-World Object Detection and Understanding](https://arxiv.org/pdf/2411.14347)
*Tianhe Ren, Yihao Chen, Qing Jiang, Zhaoyang Zeng, Yuda Xiong, Wenlong Liu, Zhengyu Ma, Junyi Shen, Yuan Gao, Xiaoke Jiang, Xingyu Chen, Zhuheng Song, Yuhong Zhang, Hongjie Huang, Han Gao, Shilong Liu, Hao Zhang, Feng Li, Kent Yu, Lei Zhang*

Main category: cs.CV

TL;DR: DINO-X is a unified object-centric vision model with state-of-the-art open-world object detection performance, supporting flexible prompts and multiple perception tasks.


<details>
  <summary>Details</summary>
Motivation: To advance open-world object understanding and improve long-tailed object detection by leveraging a large-scale dataset and flexible prompt options.

Method: Uses a Transformer-based encoder-decoder architecture, extends input options (text, visual, customized prompts), and pre-trains on Grounding-100M dataset.

Result: Achieves 56.0 AP on COCO, 59.8 AP on LVIS-minival, and 52.4 AP on LVIS-val, with significant improvements in rare class detection.

Conclusion: DINO-X sets a new benchmark for open-world object detection and demonstrates superior performance in recognizing long-tailed objects.

Abstract: In this paper, we introduce DINO-X, which is a unified object-centric vision
model developed by IDEA Research with the best open-world object detection
performance to date. DINO-X employs the same Transformer-based encoder-decoder
architecture as Grounding DINO 1.5 to pursue an object-level representation for
open-world object understanding. To make long-tailed object detection easy,
DINO-X extends its input options to support text prompt, visual prompt, and
customized prompt. With such flexible prompt options, we develop a universal
object prompt to support prompt-free open-world detection, making it possible
to detect anything in an image without requiring users to provide any prompt.
To enhance the model's core grounding capability, we have constructed a
large-scale dataset with over 100 million high-quality grounding samples,
referred to as Grounding-100M, for advancing the model's open-vocabulary
detection performance. Pre-training on such a large-scale grounding dataset
leads to a foundational object-level representation, which enables DINO-X to
integrate multiple perception heads to simultaneously support multiple object
perception and understanding tasks, including detection, segmentation, pose
estimation, object captioning, object-based QA, etc. Experimental results
demonstrate the superior performance of DINO-X. Specifically, the DINO-X Pro
model achieves 56.0 AP, 59.8 AP, and 52.4 AP on the COCO, LVIS-minival, and
LVIS-val zero-shot object detection benchmarks, respectively. Notably, it
scores 63.3 AP and 56.5 AP on the rare classes of LVIS-minival and LVIS-val
benchmarks, improving the previous SOTA performance by 5.8 AP and 5.0 AP. Such
a result underscores its significantly improved capacity for recognizing
long-tailed objects.

</details>


### [156] [CoCoGaussian: Leveraging Circle of Confusion for Gaussian Splatting from Defocused Images](https://arxiv.org/pdf/2412.16028)
*Jungho Lee, Suhwan Cho, Taeoh Kim, Ho-Deok Jang, Minhyeok Lee, Geonho Cha, Dongyoon Wee, Dogyoon Lee, Sangyoun Lee*

Main category: cs.CV

TL;DR: CoCoGaussian introduces a Circle of Confusion-aware Gaussian Splatting method to handle defocus blur in 3D scene reconstruction, achieving top performance on benchmarks.


<details>
  <summary>Details</summary>
Motivation: Real-world scenarios often involve defocus blur, which conventional methods struggle with, necessitating a solution for accurate 3D representation from defocused images.

Method: CoCoGaussian models the Circle of Confusion using depth and learnable aperture data, generating multiple Gaussians to capture CoC shape and includes a learnable scaling factor for robustness.

Result: The method outperforms benchmarks on synthetic and real-world datasets, demonstrating state-of-the-art performance.

Conclusion: CoCoGaussian effectively addresses defocus blur challenges, offering precise 3D scene representation from defocused images.

Abstract: 3D Gaussian Splatting (3DGS) has attracted significant attention for its
high-quality novel view rendering, inspiring research to address real-world
challenges. While conventional methods depend on sharp images for accurate
scene reconstruction, real-world scenarios are often affected by defocus blur
due to finite depth of field, making it essential to account for realistic 3D
scene representation. In this study, we propose CoCoGaussian, a Circle of
Confusion-aware Gaussian Splatting that enables precise 3D scene representation
using only defocused images. CoCoGaussian addresses the challenge of defocus
blur by modeling the Circle of Confusion (CoC) through a physically grounded
approach based on the principles of photographic defocus. Exploiting 3D
Gaussians, we compute the CoC diameter from depth and learnable aperture
information, generating multiple Gaussians to precisely capture the CoC shape.
Furthermore, we introduce a learnable scaling factor to enhance robustness and
provide more flexibility in handling unreliable depth in scenes with reflective
or refractive surfaces. Experiments on both synthetic and real-world datasets
demonstrate that CoCoGaussian achieves state-of-the-art performance across
multiple benchmarks.

</details>


### [157] [SeagrassFinder: Deep Learning for Eelgrass Detection and Coverage Estimation in the Wild](https://arxiv.org/pdf/2412.16147)
*Jannik Elsäßer, Laura Weihl, Veronika Cheplygina, Lisbeth Tangaa Nielsen*

Main category: cs.CV

TL;DR: Deep learning models, especially Vision Transformers, automate seagrass detection and coverage estimation from underwater video data, achieving high accuracy and efficiency.


<details>
  <summary>Details</summary>
Motivation: Manual methods for seagrass monitoring are time-consuming and subjective, necessitating automated solutions for better environmental assessments.

Method: A dataset of 8,300 annotated images was created, and deep learning models (ResNet, InceptionNetV3, DenseNet, Vision Transformer) were evaluated for binary classification. Underwater image enhancement was also applied.

Result: Vision Transformers achieved AUROC scores over 0.95. The novel coverage estimation method aligned well with expert labels, showing scalability.

Conclusion: Deep learning offers efficient, scalable seagrass monitoring, aiding marine ecology and environmental impact assessments.

Abstract: Seagrass meadows play a crucial role in marine ecosystems, providing benefits
such as carbon sequestration, water quality improvement, and habitat provision.
Monitoring the distribution and abundance of seagrass is essential for
environmental impact assessments and conservation efforts. However, the current
manual methods of analyzing underwater video data to assess seagrass coverage
are time-consuming and subjective. This work explores the use of deep learning
models to automate the process of seagrass detection and coverage estimation
from underwater video data. We create a new dataset of over 8,300 annotated
underwater images, and subsequently evaluate several deep learning
architectures, including ResNet, InceptionNetV3, DenseNet, and Vision
Transformer for the task of binary classification on the presence and absence
of seagrass by transfer learning. The results demonstrate that deep learning
models, particularly Vision Transformers, can achieve high performance in
predicting eelgrass presence, with AUROC scores exceeding 0.95 on the final
test dataset. The application of underwater image enhancement further improved
the models' prediction capabilities. Furthermore, we introduce a novel approach
for estimating seagrass coverage from video data, showing promising preliminary
results that align with expert manual labels, and indicating potential for
consistent and scalable monitoring. The proposed methodology allows for the
efficient processing of large volumes of video data, enabling the acquisition
of much more detailed information on seagrass distributions in comparison to
current manual methods. This information is crucial for environmental impact
assessments and monitoring programs, as seagrasses are important indicators of
coastal ecosystem health. This project demonstrates the value that deep
learning can bring to the field of marine ecology and environmental monitoring.

</details>


### [158] [Illegal Waste Detection in Remote Sensing Images: A Case Study](https://arxiv.org/pdf/2502.06607)
*Federico Gibellini, Piero Fraternali, Giacomo Boracchi, Luca Morandini, Thomas Martinoli, Andrea Diecidue, Simona Malegori*

Main category: cs.CV

TL;DR: A semi-automatic pipeline for detecting illegal waste sites in VHR RS images achieves high accuracy and reduces detection time by 30%.


<details>
  <summary>Details</summary>
Motivation: Environmental crime, particularly illegal waste management, is a major global issue. Modern VHR RS images and image-analysis tools can aid in detecting such crimes efficiently.

Method: The paper presents a semi-automatic waste detection pipeline, evaluating design choices like network architecture, image resolution, and pretraining.

Result: The best model achieves 92.02% F1-Score and 94.56% Accuracy, with a moderate performance drop (6.5% F1-Score decrease) in unfamiliar territories.

Conclusion: The pipeline significantly reduces detection time (up to 30%) and proves effective for environmental protection agencies.

Abstract: Environmental crime is the third largest criminal activity worldwide, with
significant revenues coming from illegal management of solid waste. Thanks to
the increasing availability and the decreasing cost of Very High Resolution
Remote Sensing (VHR RS) images, the fight against environmental crime can
nowadays rely on modern image-analysis tools to support photo-interpretation
for scanning vast territories in search of illegal waste disposal sites. This
paper illustrates a semi-automatic waste detection pipeline, developed in
collaboration with a regional environmental protection agency, for detecting
candidate illegal dumping sites in VHR RS images. To optimize the effectiveness
of the waste detector, extensive experiments evaluate such design choices as
the network architecture, the ground resolution and geographic span of the
input images, as well as the pretraining procedures. The best model attains
remarkable performance, achieving 92.02% F1-Score and 94.56% Accuracy. A
generalization study assesses the performance variation when the detector
processes images from a territory substantially different from the one used
during training, incurring only a moderate performance loss, i.e., 6.5%
decrease in the F1-Score. Finally, an exercise in which photo interpreters
compare the territory scanning effort with and without the support of the waste
detector assesses the concrete benefit of using a computer-aided image analysis
tool in a professional environment protection agency. Results show that a
reduction up to 30% of the time spent for waste site detection can be attained.

</details>


### [159] [EndoMamba: An Efficient Foundation Model for Endoscopic Videos via Hierarchical Pre-training](https://arxiv.org/pdf/2502.19090)
*Qingyao Tian, Huai Liao, Xinyan Huang, Bingyu Yang, Dongdong Lei, Sebastien Ourselin, Hongbin Liu*

Main category: cs.CV

TL;DR: EndoMamba is a foundation model for endoscopic video tasks, addressing computational inefficiency and limited pre-training data with real-time inference and hierarchical self-supervised learning.


<details>
  <summary>Details</summary>
Motivation: To improve real-time assistance in minimally invasive surgeries by overcoming computational inefficiencies and suboptimal performance of existing video foundation models.

Method: Proposes EndoMamba backbone with Bidirectional Mamba blocks for spatial modeling and vanilla Mamba blocks for temporal reasoning, plus a self-supervised hierarchical pre-training diagram.

Result: Outperforms existing models in four downstream tasks (classification, segmentation, surgical phase recognition, localization) while maintaining real-time speed.

Conclusion: EndoMamba effectively addresses key challenges in endoscopic video tasks, offering superior performance and efficiency.

Abstract: Endoscopic video-based tasks, such as visual navigation and surgical phase
recognition, play a crucial role in minimally invasive surgeries by providing
real-time assistance. While recent video foundation models have shown promise,
their applications are hindered by (1) computational inefficiencies and (2)
suboptimal performance caused by limited data for pre-training in endoscopy. To
address these issues, we present EndoMamba, a foundation model designed for
real-time inference while learning generalized spatiotemporal representations.
First, to mitigate computational inefficiencies, we propose the EndoMamba
backbone, optimized for real-time inference. Inspired by recent advancements in
state space models, EndoMamba integrates Bidirectional Mamba blocks for spatial
modeling within individual frames and vanilla Mamba blocks for past-to-present
reasoning across the temporal domain. This design enables both strong
spatiotemporal modeling and efficient inference in online video streams.
Second, we propose a self-supervised hierarchical pre-training diagram to
enhance EndoMamba's representation learning using endoscopic videos and
incorporating general video domain knowledge. Specifically, our approach
combines masked reconstruction with auxiliary supervision, leveraging low-level
reconstruction to capture spatial-temporal structures and high-level alignment
to transfer broader knowledge from a pretrained general-video domain foundation
model. Extensive experiments on four downstream tasks--classification,
segmentation, surgical phase recognition, and localization--demonstrate that
EndoMamba outperforms existing foundation models and task-specific methods
while maintaining real-time inference speed. The source code is available at
https://github.com/TianCuteQY/EndoMamba.

</details>


### [160] [A Sliding Layer Merging Method for Efficient Depth-Wise Pruning in LLMs](https://arxiv.org/pdf/2502.19159)
*Xuan Ding, Rui Sun, Yunjian Zhang, Xiu Yan, Yueqi Zhou, Kaihao Huang, Suzhong Fu, Angelica I Aviles-Rivero, Chuanlong Xie, Yao Zhu*

Main category: cs.CV

TL;DR: Depth-wise pruning in Transformers can degrade performance by discarding entire layers. This paper introduces a sliding layer merging method based on layer correlations, improving performance over existing pruning techniques.


<details>
  <summary>Details</summary>
Motivation: To address the performance degradation caused by indiscriminate depth-wise pruning in Transformers by leveraging layer correlations.

Method: Analyzes layer correlations in reproducing kernel Hilbert space and proposes a sliding layer merging method to dynamically fuse consecutive layers based on similarity.

Result: Outperforms existing pruning techniques, achieving a 1.654% improvement in zero-shot tasks with 35% pruning on Vicuna-7B.

Conclusion: The sliding layer merging method effectively simplifies models while maintaining performance, with potential for combining depth and width pruning.

Abstract: Compared to width-wise pruning, depth-wise pruning can significantly
accelerate inference in resource-constrained scenarios. However, treating the
entire Transformer layer as the minimum pruning unit may degrade model
performance by indiscriminately discarding the entire information of the layer.
This paper reveals the ``Patch-like'' feature relationship between layers in
large language models by analyzing the correlation of the outputs of different
layers in the reproducing kernel Hilbert space. Building on this observation,
we propose a sliding layer merging method that dynamically selects and fuses
consecutive layers from top to bottom according to a pre-defined similarity
threshold, thereby simplifying the model structure while maintaining its
performance. Extensive experiments on LLMs with various architectures and
different parameter scales show that our method outperforms existing pruning
techniques in both zero-shot inference performance and retraining recovery
quality after pruning. In particular, in the experiment with 35% pruning on the
Vicuna-7B model, our method achieved a 1.654% improvement in average
performance on zero-shot tasks compared to the existing method. Moreover, we
further reveal the potential of combining depth pruning with width pruning to
enhance the pruning effect. Our codes are available at
https://github.com/920927/SLM-a-sliding-layer-merging-method.

</details>


### [161] [StyleMorpheus: A Style-Based 3D-Aware Morphable Face Model](https://arxiv.org/pdf/2503.11792)
*Peizhi Yan, Rabab K. Ward, Dan Wang, Qiang Tang, Shan Du*

Main category: cs.CV

TL;DR: StyleMorpheus is a style-based neural 3D Morphable Face Model trained on in-the-wild images, enabling photorealistic 3D face rendering without lab-collected data.


<details>
  <summary>Details</summary>
Motivation: To overcome the reliance on lab-collected datasets for training 3D-aware face models, enabling more accessible and realistic 3D face modeling.

Method: Uses an auto-encoder structure with disentangled parametric codes and style-based generative adversarial learning for photorealistic rendering.

Result: Achieves state-of-the-art 3D face reconstruction with real-time rendering and disentangled control for face editing.

Conclusion: StyleMorpheus advances 3D face modeling by combining disentangled control and photorealistic rendering, suitable for virtual reality and face editing.

Abstract: For 3D face modeling, the recently developed 3D-aware neural rendering
methods are able to render photorealistic face images with arbitrary viewing
directions. The training of the parametric controllable 3D-aware face models,
however, still relies on a large-scale dataset that is lab-collected. To
address this issue, this paper introduces "StyleMorpheus", the first
style-based neural 3D Morphable Face Model (3DMM) that is trained on
in-the-wild images. It inherits 3DMM's disentangled controllability (over face
identity, expression, and appearance) but without the need for accurately
reconstructed explicit 3D shapes. StyleMorpheus employs an auto-encoder
structure. The encoder aims at learning a representative disentangled
parametric code space and the decoder improves the disentanglement using shape
and appearance-related style codes in the different sub-modules of the network.
Furthermore, we fine-tune the decoder through style-based generative
adversarial learning to achieve photorealistic 3D rendering quality. The
proposed style-based design enables StyleMorpheus to achieve state-of-the-art
3D-aware face reconstruction results, while also allowing disentangled control
of the reconstructed face. Our model achieves real-time rendering speed,
allowing its use in virtual reality applications. We also demonstrate the
capability of the proposed style-based design in face editing applications such
as style mixing and color editing. Project homepage:
https://github.com/ubc-3d-vision-lab/StyleMorpheus.

</details>


### [162] [CryoSAMU: Enhancing 3D Cryo-EM Density Maps of Protein Structures at Intermediate Resolution with Structure-Aware Multimodal U-Nets](https://arxiv.org/pdf/2503.20291)
*Chenwei Zhang, Khanh Dao Duc*

Main category: cs.CV

TL;DR: CryoSAMU enhances intermediate-resolution cryo-EM density maps using structure-aware multimodal U-Nets, outperforming existing methods in speed and performance.


<details>
  <summary>Details</summary>
Motivation: Existing deep learning methods for cryo-EM map enhancement are not optimized for intermediate-resolution maps and rely solely on density features.

Method: Proposes CryoSAMU, a method using structure-aware multimodal U-Nets trained on curated intermediate-resolution density maps.

Result: CryoSAMU shows competitive performance and significantly faster processing compared to state-of-the-art methods.

Conclusion: CryoSAMU is promising for practical applications, with code available for public use.

Abstract: Enhancing cryogenic electron microscopy (cryo-EM) 3D density maps at
intermediate resolution (4-8 {\AA}) is crucial in protein structure
determination. Recent advances in deep learning have led to the development of
automated approaches for enhancing experimental cryo-EM density maps. Yet,
these methods are not optimized for intermediate-resolution maps and rely on
map density features alone. To address this, we propose CryoSAMU, a novel
method designed to enhance 3D cryo-EM density maps of protein structures using
structure-aware multimodal U-Nets and trained on curated
intermediate-resolution density maps. We comprehensively evaluate CryoSAMU
across various metrics and demonstrate its competitive performance compared to
state-of-the-art methods. Notably, CryoSAMU achieves significantly faster
processing speed, showing promise for future practical applications. Our code
is available at https://github.com/chenwei-zhang/CryoSAMU.

</details>


### [163] [Video-R1: Reinforcing Video Reasoning in MLLMs](https://arxiv.org/pdf/2503.21776)
*Kaituo Feng, Kaixiong Gong, Bohao Li, Zonghao Guo, Yibing Wang, Tianshuo Peng, Junfei Wu, Xiaoying Zhang, Benyou Wang, Xiangyu Yue*

Main category: cs.CV

TL;DR: Video-R1 introduces a rule-based RL approach for video reasoning in MLLMs, addressing challenges like temporal modeling and data scarcity with T-GRPO and hybrid datasets. It outperforms benchmarks, even surpassing GPT-4o.


<details>
  <summary>Details</summary>
Motivation: To extend the success of DeepSeek-R1 to video reasoning in MLLMs, overcoming challenges like temporal modeling and data scarcity.

Method: Proposes T-GRPO for temporal modeling and uses hybrid image-video datasets (Video-R1-CoT-165k and Video-R1-260k) for training.

Result: Achieves significant improvements on benchmarks like VideoMMMU and VSI-Bench, with Video-R1-7B surpassing GPT-4o in spatial reasoning.

Conclusion: Video-R1 successfully adapts the R1 paradigm for video reasoning, demonstrating superior performance and releasing all resources openly.

Abstract: Inspired by DeepSeek-R1's success in eliciting reasoning abilities through
rule-based reinforcement learning (RL), we introduce Video-R1 as the first
attempt to systematically explore the R1 paradigm for incentivizing video
reasoning within multimodal large language models (MLLMs). However, directly
applying RL training with the GRPO algorithm to video reasoning presents two
primary challenges: (i) a lack of temporal modeling for video reasoning, and
(ii) the scarcity of high-quality video-reasoning data. To address these
issues, we first propose the T-GRPO algorithm, which encourages models to
utilize temporal information in videos for reasoning. Additionally, instead of
relying solely on video data, we incorporate high-quality image-reasoning data
into the training process. We have constructed two datasets: Video-R1-CoT-165k
for SFT cold start and Video-R1-260k for RL training, both comprising image and
video data. Experimental results demonstrate that Video-R1 achieves significant
improvements on video reasoning benchmarks such as VideoMMMU and VSI-Bench, as
well as on general video benchmarks including MVBench and TempCompass, etc.
Notably, Video-R1-7B attains a 37.1% accuracy on video spatial reasoning
benchmark VSI-bench, surpassing the commercial proprietary model GPT-4o. All
code, models, and data are released in: https://github.com/tulerfeng/Video-R1.

</details>


### [164] [Learned Image Compression with Dictionary-based Entropy Model](https://arxiv.org/pdf/2504.00496)
*Jingbo Lu, Leheng Zhang, Xingyu Zhou, Mu Li, Wen Li, Shuhang Gu*

Main category: cs.CV

TL;DR: A novel entropy model, Dictionary-based Cross Attention Entropy, improves learned image compression by leveraging a learnable dictionary for better probability estimation.


<details>
  <summary>Details</summary>
Motivation: Existing entropy models in learned image compression focus on internal dependencies but ignore prior extraction from training data.

Method: Proposes a Dictionary-based Cross Attention Entropy model using a learnable dictionary to capture typical structures in training data.

Result: Achieves state-of-the-art performance with a better balance between efficiency and latency on benchmark datasets.

Conclusion: The new entropy model enhances learned image compression by effectively utilizing training data priors.

Abstract: Learned image compression methods have attracted great research interest and
exhibited superior rate-distortion performance to the best classical image
compression standards of the present. The entropy model plays a key role in
learned image compression, which estimates the probability distribution of the
latent representation for further entropy coding. Most existing methods
employed hyper-prior and auto-regressive architectures to form their entropy
models. However, they only aimed to explore the internal dependencies of latent
representation while neglecting the importance of extracting prior from
training data. In this work, we propose a novel entropy model named
Dictionary-based Cross Attention Entropy model, which introduces a learnable
dictionary to summarize the typical structures occurring in the training
dataset to enhance the entropy model. Extensive experimental results have
demonstrated that the proposed model strikes a better balance between
performance and latency, achieving state-of-the-art results on various
benchmark datasets.

</details>


### [165] [Charm: The Missing Piece in ViT fine-tuning for Image Aesthetic Assessment](https://arxiv.org/pdf/2504.02522)
*Fatemeh Behrad, Tinne Tuytelaars, Johan Wagemans*

Main category: cs.CV

TL;DR: Charm is a tokenization method for Vision Transformers (ViTs) that preserves key image details (composition, high-resolution, aspect ratio, multi-scale) while reducing input sequence length, improving performance for tasks like aesthetic assessment.


<details>
  <summary>Details</summary>
Motivation: ViTs struggle with variable-sized inputs due to computational constraints, leading to information loss from downscaling or cropping. Charm aims to retain essential details without these drawbacks.

Method: Charm tokenizes images by prioritizing high-resolution details in specific regions and downscaling others, maintaining aspect ratio and multi-scale information. It works with pre-trained ViTs.

Result: Charm improves ViT performance by up to 8.1% on aesthetic and quality assessment tasks, using a lightweight backbone.

Conclusion: Charm effectively balances computational efficiency and information retention, enhancing ViT performance for image aesthetic assessment without cropping or aspect ratio changes.

Abstract: The capacity of Vision transformers (ViTs) to handle variable-sized inputs is
often constrained by computational complexity and batch processing limitations.
Consequently, ViTs are typically trained on small, fixed-size images obtained
through downscaling or cropping. While reducing computational burden, these
methods result in significant information loss, negatively affecting tasks like
image aesthetic assessment. We introduce Charm, a novel tokenization approach
that preserves Composition, High-resolution, Aspect Ratio, and Multi-scale
information simultaneously. Charm prioritizes high-resolution details in
specific regions while downscaling others, enabling shorter fixed-size input
sequences for ViTs while incorporating essential information. Charm is designed
to be compatible with pre-trained ViTs and their learned positional embeddings.
By providing multiscale input and introducing variety to input tokens, Charm
improves ViT performance and generalizability for image aesthetic assessment.
We avoid cropping or changing the aspect ratio to further preserve information.
Extensive experiments demonstrate significant performance improvements on
various image aesthetic and quality assessment datasets (up to 8.1 %) using a
lightweight ViT backbone. Code and pre-trained models are available at
https://github.com/FBehrad/Charm.

</details>


### [166] [TactileNet: Bridging the Accessibility Gap with AI-Generated Tactile Graphics for Individuals with Vision Impairment](https://arxiv.org/pdf/2504.04722)
*Adnan Khan, Alireza Choubineh, Mai A. Shaaban, Abbas Akkasi, Majid Komeili*

Main category: cs.CV

TL;DR: TactileNet is an AI-driven framework using Stable Diffusion to automate the creation of tactile graphics, achieving high adherence to accessibility standards and near-human design quality.


<details>
  <summary>Details</summary>
Motivation: To address the labor-intensive and insufficient traditional methods for creating tactile graphics for people with vision loss.

Method: Integrates Low-Rank Adaptation (LoRA) and DreamBooth to fine-tune Stable Diffusion models for generating embossing-ready 2D tactile templates.

Result: Achieves 92.86% adherence to accessibility standards, near-human design similarity (SSIM = 0.538), and better silhouette preservation than manual designs.

Conclusion: TactileNet demonstrates AI's potential to augment human expertise in bridging accessibility gaps, with plans to release code, data, and models for further research.

Abstract: Tactile graphics are essential for providing access to visual information for
the 43 million people globally living with vision loss. Traditional methods for
creating these graphics are labor-intensive and cannot meet growing demand. We
introduce TactileNet, the first comprehensive dataset and AI-driven framework
for generating embossing-ready 2D tactile templates using text-to-image Stable
Diffusion (SD) models. By integrating Low-Rank Adaptation (LoRA) and
DreamBooth, our method fine-tunes SD models to produce high-fidelity,
guideline-compliant graphics while reducing computational costs. Quantitative
evaluations with tactile experts show 92.86% adherence to accessibility
standards. Structural fidelity analysis revealed near-human design similarity,
with an SSIM of 0.538 between generated graphics and expert-designed tactile
images. Notably, our method preserves object silhouettes better than human
designs (SSIM = 0.259 vs. 0.215 for binary masks), addressing a key limitation
of manual tactile abstraction. The framework scales to 32,000 images (7,050
high-quality) across 66 classes, with prompt editing enabling customizable
outputs (e.g., adding or removing details). By automating the 2D template
generation step-compatible with standard embossing workflows-TactileNet
accelerates production while preserving design flexibility. This work
demonstrates how AI can augment (not replace) human expertise to bridge the
accessibility gap in education and beyond. Code, data, and models will be
publicly released to foster further research.

</details>


### [167] [Saliency-Motion Guided Trunk-Collateral Network for Unsupervised Video Object Segmentation](https://arxiv.org/pdf/2504.05904)
*Xiangyu Zheng, Wanyun Li, Songcheng He, Jianping Fan, Xiaoqiang Li, We Zhang*

Main category: cs.CV

TL;DR: SMTC-Net improves UVOS by balancing motion-appearance relationships and leveraging intrinsic saliency, achieving state-of-the-art results.


<details>
  <summary>Details</summary>
Motivation: Existing UVOS methods struggle with motion-appearance balance and optical flow dependency, degrading performance.

Method: Proposes a Trunk-Collateral structure for commonality and uniqueness of features, and an ISRM for saliency-guided refinement.

Result: Achieves top performance on UVOS and VSOD benchmarks (e.g., 89.2% J&F on DAVIS-16).

Conclusion: SMTC-Net effectively enhances segmentation by better integrating motion-appearance and saliency information.

Abstract: Recent mainstream unsupervised video object segmentation (UVOS)
motion-appearance approaches use either the bi-encoder structure to separately
encode motion and appearance features, or the uni-encoder structure for joint
encoding. However, these methods fail to properly balance the motion-appearance
relationship. Consequently, even with complex fusion modules for
motion-appearance integration, the extracted suboptimal features degrade the
models' overall performance. Moreover, the quality of optical flow varies
across scenarios, making it insufficient to rely solely on optical flow to
achieve high-quality segmentation results. To address these challenges, we
propose the Saliency-Motion guided Trunk-Collateral Network (SMTC-Net), which
better balances the motion-appearance relationship and incorporates model's
intrinsic saliency information to enhance segmentation performance.
Specifically, considering that optical flow maps are derived from RGB images,
they share both commonalities and differences. Accordingly, we propose a novel
Trunk-Collateral structure for motion-appearance UVOS. The shared trunk
backbone captures the motion-appearance commonality, while the collateral
branch learns the uniqueness of motion features. Furthermore, an Intrinsic
Saliency guided Refinement Module (ISRM) is devised to efficiently leverage the
model's intrinsic saliency information to refine high-level features, and
provide pixel-level guidance for motion-appearance fusion, thereby enhancing
performance without additional input. Experimental results show that SMTC-Net
achieved state-of-the-art performance on three UVOS datasets ( 89.2% J&F on
DAVIS-16, 76% J on YouTube-Objects, 86.4% J on FBMS ) and four standard video
salient object detection (VSOD) benchmarks with the notable increase,
demonstrating its effectiveness and superiority over previous methods.

</details>


### [168] [Teaching Humans Subtle Differences with DIFFusion](https://arxiv.org/pdf/2504.08046)
*Mia Chiquier, Orr Avrech, Yossi Gandelsman, Berthy Feng, Katherine Bouman, Carl Vondrick*

Main category: cs.CV

TL;DR: A system uses generative models to find and show subtle visual differences between categories, improving fine-grained classification even with sparse data.


<details>
  <summary>Details</summary>
Motivation: Recognizing subtle visual differences in scientific domains is challenging, even for experts, and often hard to describe verbally.

Method: Leverages generative models to create counterfactual visualizations with targeted transformations between classes, handling sparse and unpaired data.

Result: Works well across six domains (e.g., black holes, butterflies, medical imaging), improving category differentiation and revealing novel distinctions.

Conclusion: Generated counterfactuals outperform traditional methods in teaching humans fine-grained classification, advancing visual learning and research.

Abstract: Scientific expertise often requires recognizing subtle visual differences
that remain challenging to articulate even for domain experts. We present a
system that leverages generative models to automatically discover and visualize
minimal discriminative features between categories while preserving instance
identity. Our method generates counterfactual visualizations with subtle,
targeted transformations between classes, performing well even in domains where
data is sparse, examples are unpaired, and category boundaries resist verbal
description. Experiments across six domains, including black hole simulations,
butterfly taxonomy, and medical imaging, demonstrate accurate transitions with
limited training data, highlighting both established discriminative features
and novel subtle distinctions that measurably improved category
differentiation. User studies confirm our generated counterfactuals
significantly outperform traditional approaches in teaching humans to correctly
differentiate between fine-grained classes, showing the potential of generative
models to advance visual learning and scientific research.

</details>


### [169] [WildFireCan-MMD: A Multimodal Dataset for Classification of User-Generated Content During Wildfires in Canada](https://arxiv.org/pdf/2504.13231)
*Braeden Sherritt, Isar Nejadgholi, Marzieh Amini*

Main category: cs.CV

TL;DR: WildFireCan-MMD is a multimodal dataset for wildfire insights from social media, showing trained models outperform zero-shot methods, with a fine-tuned model achieving 83% f-score.


<details>
  <summary>Details</summary>
Motivation: Traditional wildfire data is slow and costly; social media offers real-time updates but lacks efficient extraction of relevant insights.

Method: Created WildFireCan-MMD, a dataset of X posts from Canadian wildfires, annotated across twelve themes. Evaluated vision-language models and custom-trained classifiers.

Result: Fine-tuned transformer model achieved 83% f-score, outperforming GPT-4 by 23%.

Conclusion: Tailored datasets and task-specific training are crucial, especially localized ones, as disaster response needs vary by region.

Abstract: Rapid information access is vital during wildfires, yet traditional data
sources are slow and costly. Social media offers real-time updates, but
extracting relevant insights remains a challenge. We present WildFireCan-MMD, a
new multimodal dataset of X posts from recent Canadian wildfires, annotated
across twelve key themes. Evaluating both vision-language models and
custom-trained classifiers, we show that while zero-shot prompting offers quick
deployment, even simple trained models outperform them when labelled data is
available. Our best-performing transformer-based fine-tuned model reaches 83%
f-score, outperforming gpt4 by 23%. As a use case, we demonstrate how this
model can be used to uncover trends during wildfires. Our findings highlight
the enduring importance of tailored datasets and task-specific training.
Importantly, such datasets should be localized, as disaster response
requirements vary across regions and contexts.

</details>


### [170] [Estimating the Diameter at Breast Height of Trees in a Forest With a Single 360 Camera](https://arxiv.org/pdf/2505.03093)
*Siming He, Zachary Osman, Fernando Cladera, Dexter Ong, Nitant Rai, Patrick Corey Green, Vijay Kumar, Pratik Chaudhari*

Main category: cs.CV

TL;DR: A low-cost method using a 360 video camera achieves 5-9% median absolute relative error in DBH measurements, close to LiDAR's precision but much cheaper and simpler.


<details>
  <summary>Details</summary>
Motivation: Accurate DBH measurements are crucial for forest inventories, but LiDAR is expensive and complex. A cheaper, simpler alternative is needed.

Method: Uses a 360 camera with a pipeline: (i) dense point cloud reconstruction via SfM, (ii) semantic trunk segmentation with Grounded SAM, and (iii) RANSAC-based DBH estimation. Includes a visualization tool.

Result: Achieves 5-9% median absolute relative error vs. manual measurements, only 2-4% worse than LiDAR.

Conclusion: The method is a viable, cost-effective alternative to LiDAR for DBH measurement in forest inventories.

Abstract: Forest inventories rely on accurate measurements of the diameter at breast
height (DBH) for ecological monitoring, resource management, and carbon
accounting. While LiDAR-based techniques can achieve centimeter-level
precision, they are cost-prohibitive and operationally complex. We present a
low-cost alternative that only needs a consumer-grade 360 video camera. Our
semi-automated pipeline comprises of (i) a dense point cloud reconstruction
using Structure from Motion (SfM) photogrammetry software called Agisoft
Metashape, (ii) semantic trunk segmentation by projecting Grounded Segment
Anything (SAM) masks onto the 3D cloud, and (iii) a robust RANSAC-based
technique to estimate cross section shape and DBH. We introduce an interactive
visualization tool for inspecting segmented trees and their estimated DBH. On
61 acquisitions of 43 trees under a variety of conditions, our method attains
median absolute relative errors of 5-9% with respect to "ground-truth" manual
measurements. This is only 2-4% higher than LiDAR-based estimates, while
employing a single 360 camera that costs orders of magnitude less, requires
minimal setup, and is widely available.

</details>


### [171] [Behind Maya: Building a Multilingual Vision Language Model](https://arxiv.org/pdf/2505.08910)
*Nahid Alam, Karthik Reddy Kanjula, Surya Guthikonda, Timothy Chung, Bala Krishna S Vegesna, Abhipsha Das, Anthony Susevski, Ryan Sze-Yin Chan, S M Iftekhar Uddin, Shayekh Bin Islam, Roshan Santhosh, Snegha A, Drishti Sharma, Chen Liu, Isha Chaturvedi, Genta Indra Winata, Ashvanth. S, Snehanshu Mukherjee, Alham Fikri Aji*

Main category: cs.CV

TL;DR: Maya is an open-source Multilingual Vision-Language Model (VLM) addressing performance gaps in low-resource languages and cultural contexts.


<details>
  <summary>Details</summary>
Motivation: Existing VLMs excel in widely spoken languages but underperform in low-resource languages and diverse cultural settings.

Method: Developed a multilingual image-text pretraining dataset in eight languages and built a multilingual image-text model.

Result: Maya enhances cultural and linguistic comprehension in vision-language tasks.

Conclusion: Maya bridges the gap in multilingual and culturally diverse vision-language understanding, with open-source availability.

Abstract: In recent times, we have seen a rapid development of large Vision-Language
Models (VLMs). They have shown impressive results on academic benchmarks,
primarily in widely spoken languages but lack performance on low-resource
languages and varied cultural contexts. To address these limitations, we
introduce Maya, an open-source Multilingual VLM. Our contributions are: 1) a
multilingual image-text pretraining dataset in eight languages, based on the
LLaVA pretraining dataset; and 2) a multilingual image-text model supporting
these languages, enhancing cultural and linguistic comprehension in
vision-language tasks. Code available at https://github.com/nahidalam/maya.

</details>


### [172] [Exploring Convolutional Neural Networks for Rice Grain Classification: An Explainable AI Approach](https://arxiv.org/pdf/2505.05513)
*Muhammad Junaid Asif, Hamza Khan, Rabia Tehseen, Syed Tahir Hussain Rizvi, Mujtaba Asad, Shazia Saqib, Rana Fayyaz Ahmad*

Main category: cs.CV

TL;DR: The paper proposes a CNN-based framework for automatic classification of rice grain varieties, achieving high accuracy and explainability using LIME and SHAP.


<details>
  <summary>Details</summary>
Motivation: Manual rice grain classification is error-prone and time-consuming, necessitating an automated solution for quality assurance in trade.

Method: A convolutional neural network (CNN) is used for classification, evaluated via accuracy, recall, precision, F1-Score, and ROC curves.

Result: The CNN model achieved high accuracy and minimal misclassifications, with explainability techniques revealing feature influences.

Conclusion: The automated framework effectively classifies rice varieties, offering a reliable solution for quality inspection.

Abstract: Rice is an essential staple food worldwide that is important in promoting
international trade, economic growth, and nutrition. Asian countries such as
China, India, Pakistan, Thailand, Vietnam, and Indonesia are notable for their
significant contribution to the cultivation and utilization of rice. These
nations are also known for cultivating different rice grains, including short
and long grains. These sizes are further classified as basmati, jasmine, kainat
saila, ipsala, arborio, etc., catering to diverse culinary preferences and
cultural traditions. For both local and international trade, inspecting and
maintaining the quality of rice grains to satisfy customers and preserve a
country's reputation is necessary. Manual quality check and classification is
quite a laborious and time-consuming process. It is also highly prone to
mistakes. Therefore, an automatic solution must be proposed for the effective
and efficient classification of different varieties of rice grains. This
research paper presents an automatic framework based on a convolutional neural
network (CNN) for classifying different varieties of rice grains. We evaluated
the proposed model based on performance metrics such as accuracy, recall,
precision, and F1-Score. The CNN model underwent rigorous training and
validation, achieving a remarkable accuracy rate and a perfect area under each
class's Receiver Operating Characteristic (ROC) curve. The confusion matrix
analysis confirmed the model's effectiveness in distinguishing between the
different rice varieties, indicating minimal misclassifications. Additionally,
the integration of explainability techniques such as LIME (Local Interpretable
Model-agnostic Explanations) and SHAP (SHapley Additive exPlanations) provided
valuable insights into the model's decision-making process, revealing how
specific features of the rice grains influenced classification outcomes.

</details>


### [173] [Examining the Source of Defects from a Mechanical Perspective for 3D Anomaly Detection](https://arxiv.org/pdf/2505.05901)
*Hanzhe Liang, Aoran Wang, Jie Zhou, Xin Jin, Can Gao, Jinbao Wang*

Main category: cs.CV

TL;DR: The paper introduces MC4AD, a novel 3D anomaly detection framework using corrective forces, achieving state-of-the-art results with minimal parameters and fast inference.


<details>
  <summary>Details</summary>
Motivation: Anomalies often stem from unpredictable defective forces; the paper aims to address these by identifying opposing corrective forces.

Method: Proposes MC4AD with DA-Gen for anomaly simulation, CFP-Net for corrective force prediction, a combined loss function, and HQC for quality control.

Result: MC4AD achieves nine state-of-the-art performances across five datasets and the new Anomaly-IntraVariance dataset, with minimal parameters and fast speed.

Conclusion: The MC4AD framework is theoretically and experimentally validated as effective for 3D anomaly detection, outperforming existing methods.

Abstract: In this paper, we explore a novel approach to 3D anomaly detection (AD) that
goes beyond merely identifying anomalies based on structural characteristics.
Our primary perspective is that most anomalies arise from unpredictable
defective forces originating from both internal and external sources. To
address these anomalies, we seek out opposing forces that can help correct
them. Therefore, we introduce the Mechanics Complementary Model-based Framework
for the 3D-AD task (MC4AD), which generates internal and external corrective
forces for each point. We first propose a Diverse Anomaly-Generation (DA-Gen)
module designed to simulate various types of anomalies. Next, we present the
Corrective Force Prediction Network (CFP-Net), which uses complementary
representations for point-level analysis to simulate the different
contributions from internal and external corrective forces. To ensure the
corrective forces are constrained effectively, we have developed a combined
loss function that includes a new symmetric loss and an overall loss. Notably,
we implement a Hierarchical Quality Control (HQC) strategy based on a three-way
decision process and contribute a dataset titled Anomaly-IntraVariance, which
incorporates intraclass variance to evaluate our model. As a result, the
proposed MC4AD has been proven effective through theory and experimentation.
The experimental results demonstrate that our approach yields nine
state-of-the-art performances, achieving optimal results with minimal
parameters and the fastest inference speed across five existing datasets, in
addition to the proposed Anomaly-IntraVariance dataset. The source is available
at https://github.com/hzzzzzhappy/MC4AD

</details>


### [174] [HCMA: Hierarchical Cross-model Alignment for Grounded Text-to-Image Generation](https://arxiv.org/pdf/2505.06512)
*Hang Wang, Zhi-Qi Cheng, Chenhao Lin, Chao Shen, Lei Zhang*

Main category: cs.CV

TL;DR: HCMA framework improves text-to-image synthesis by combining global and local alignment for better semantic and spatial control, outperforming baselines in FID and CLIP Score.


<details>
  <summary>Details</summary>
Motivation: Existing methods lack high-level semantic fidelity and explicit spatial control in complex scenes.

Method: HCMA integrates global (scene-level coherence) and local (bounding-box layouts) alignment modules into diffusion sampling.

Result: HCMA achieves a 0.69 FID improvement and 0.0295 CLIP Score gain on MS-COCO 2014.

Conclusion: HCMA effectively captures textual semantics and adheres to spatial constraints, offering robust image generation.

Abstract: Text-to-image synthesis has progressed to the point where models can generate
visually compelling images from natural language prompts. Yet, existing methods
often fail to reconcile high-level semantic fidelity with explicit spatial
control, particularly in scenes involving multiple objects, nuanced relations,
or complex layouts. To bridge this gap, we propose a Hierarchical Cross-Modal
Alignment (HCMA) framework for grounded text-to-image generation. HCMA
integrates two alignment modules into each diffusion sampling step: a global
module that continuously aligns latent representations with textual
descriptions to ensure scene-level coherence, and a local module that employs
bounding-box layouts to anchor objects at specified locations, enabling
fine-grained spatial control. Extensive experiments on the MS-COCO 2014
validation set show that HCMA surpasses state-of-the-art baselines, achieving a
0.69 improvement in Frechet Inception Distance (FID) and a 0.0295 gain in CLIP
Score. These results demonstrate HCMA's effectiveness in faithfully capturing
intricate textual semantics while adhering to user-defined spatial constraints,
offering a robust solution for semantically grounded image generation. Our code
is available at https://github.com/hwang-cs-ime/HCMA.

</details>


### [175] [Towards Scalable IoT Deployment for Visual Anomaly Detection via Efficient Compression](https://arxiv.org/pdf/2505.07119)
*Arianna Stropeni, Francesco Borsatti, Manuel Barusco, Davide Dalle Pezze, Marco Fabris, Gian Antonio Susto*

Main category: cs.CV

TL;DR: The paper explores efficient Visual Anomaly Detection (VAD) in IoT settings, achieving 80% faster inference with minimal accuracy loss using compression techniques.


<details>
  <summary>Details</summary>
Motivation: To address challenges of deploying deep learning for VAD in resource-constrained IoT environments.

Method: Evaluated data compression techniques to balance latency and accuracy, tested on the MVTec AD benchmark.

Result: Achieved up to 80% reduction in inference time with minimal performance loss.

Conclusion: Compact processing strategies enable effective VAD in IoT, optimizing cost and performance.

Abstract: Visual Anomaly Detection (VAD) is a key task in industrial settings, where
minimizing operational costs is essential. Deploying deep learning models
within Internet of Things (IoT) environments introduces specific challenges due
to limited computational power and bandwidth of edge devices. This study
investigates how to perform VAD effectively under such constraints by
leveraging compact, efficient processing strategies. We evaluate several data
compression techniques, examining the tradeoff between system latency and
detection accuracy. Experiments on the MVTec AD benchmark demonstrate that
significant compression can be achieved with minimal loss in anomaly detection
performance compared to uncompressed data. Current results show up to 80%
reduction in end-to-end inference time, including edge processing,
transmission, and server computation.

</details>


### [176] [Generative Pre-trained Autoregressive Diffusion Transformer](https://arxiv.org/pdf/2505.07344)
*Yuan Zhang, Jiacheng Jiang, Guoqing Ma, Zhiying Lu, Haoyang Huang, Jianlong Yuan, Nan Duan*

Main category: cs.CV

TL;DR: GPDiT combines diffusion and autoregressive modeling for high-quality long-range video synthesis in continuous latent space, improving motion dynamics and semantic consistency.


<details>
  <summary>Details</summary>
Motivation: To unify diffusion and autoregressive modeling for better video synthesis and representation in continuous space.

Method: GPDiT predicts future latent frames autoregressively using diffusion loss, with causal attention and time-conditioning mechanisms for efficiency.

Result: GPDiT excels in video generation quality, representation, and few-shot learning.

Conclusion: GPDiT is a promising framework for continuous-space video modeling.

Abstract: In this work, we present GPDiT, a Generative Pre-trained Autoregressive
Diffusion Transformer that unifies the strengths of diffusion and
autoregressive modeling for long-range video synthesis, within a continuous
latent space. Instead of predicting discrete tokens, GPDiT autoregressively
predicts future latent frames using a diffusion loss, enabling natural modeling
of motion dynamics and semantic consistency across frames. This continuous
autoregressive framework not only enhances generation quality but also endows
the model with representation capabilities. Additionally, we introduce a
lightweight causal attention variant and a parameter-free rotation-based
time-conditioning mechanism, improving both the training and inference
efficiency. Extensive experiments demonstrate that GPDiT achieves strong
performance in video generation quality, video representation ability, and
few-shot learning tasks, highlighting its potential as an effective framework
for video modeling in continuous space.

</details>


### [177] [A Deep Learning-Driven Inhalation Injury Grading Assistant Using Bronchoscopy Images](https://arxiv.org/pdf/2505.08517)
*Yifan Li, Alan W Pang, Jo Woon Chong*

Main category: cs.CV

TL;DR: A deep learning tool using bronchoscopy images and data augmentation (CUT, CycleGAN) improves inhalation injury grading, achieving 97.8% accuracy with GoogLeNet.


<details>
  <summary>Details</summary>
Motivation: Current grading methods (e.g., AIS) are subjective and poorly correlate with clinical outcomes like ventilation duration and mortality.

Method: Uses GoogLeNet and ViT models with data augmentation (graphic transformations, CUT, CycleGAN) on bronchoscopy images.

Result: GoogLeNet with CUT achieves 97.8% accuracy; CUT enhances feature separability and decision-making (higher heatmap intensity).

Conclusion: The tool offers objective, consistent grading and diagnostic support, validated by clinical parameters.

Abstract: Inhalation injuries present a challenge in clinical diagnosis and grading due
to Conventional grading methods such as the Abbreviated Injury Score (AIS)
being subjective and lacking robust correlation with clinical parameters like
mechanical ventilation duration and patient mortality. This study introduces a
novel deep learning-based diagnosis assistant tool for grading inhalation
injuries using bronchoscopy images to overcome subjective variability and
enhance consistency in severity assessment. Our approach leverages data
augmentation techniques, including graphic transformations, Contrastive
Unpaired Translation (CUT), and CycleGAN, to address the scarcity of medical
imaging data. We evaluate the classification performance of two deep learning
models, GoogLeNet and Vision Transformer (ViT), across a dataset significantly
expanded through these augmentation methods. The results demonstrate GoogLeNet
combined with CUT as the most effective configuration for grading inhalation
injuries through bronchoscopy images and achieves a classification accuracy of
97.8%. The histograms and frequency analysis evaluations reveal variations
caused by the augmentation CUT with distribution changes in the histogram and
texture details of the frequency spectrum. PCA visualizations underscore the
CUT substantially enhances class separability in the feature space. Moreover,
Grad-CAM analyses provide insight into the decision-making process; mean
intensity for CUT heatmaps is 119.6, which significantly exceeds 98.8 of the
original datasets. Our proposed tool leverages mechanical ventilation periods
as a novel grading standard, providing comprehensive diagnostic support.

</details>


### [178] [Leveraging Multi-Modal Information to Enhance Dataset Distillation](https://arxiv.org/pdf/2505.08605)
*Zhe Li, Hadrien Reynaud, Bernhard Kainz*

Main category: cs.CV

TL;DR: The paper introduces caption-guided supervision and object-centric masking to improve dataset distillation, enhancing synthetic dataset quality for downstream tasks.


<details>
  <summary>Details</summary>
Motivation: Existing dataset distillation methods focus on visual representations, but incorporating additional modalities (like text) and refining object-level information can improve results.

Method: Proposes two caption-guided strategies (feature concatenation and caption matching) and object-centric masking with two loss functions (masked feature alignment and masked gradient matching).

Result: Integrating caption-based guidance and object-centric masking improves dataset distillation, yielding better performance on downstream tasks.

Conclusion: The enhancements (caption-guided supervision and object-centric masking) significantly improve the quality and effectiveness of distilled datasets.

Abstract: Dataset distillation aims to create a compact and highly representative
synthetic dataset that preserves the knowledge of a larger real dataset. While
existing methods primarily focus on optimizing visual representations,
incorporating additional modalities and refining object-level information can
significantly improve the quality of distilled datasets. In this work, we
introduce two key enhancements to dataset distillation: caption-guided
supervision and object-centric masking. To integrate textual information, we
propose two strategies for leveraging caption features: the feature
concatenation, where caption embeddings are fused with visual features at the
classification stage, and caption matching, which introduces a caption-based
alignment loss during training to ensure semantic coherence between real and
synthetic data. Additionally, we apply segmentation masks to isolate target
objects and remove background distractions, introducing two loss functions
designed for object-centric learning: masked feature alignment loss and masked
gradient matching loss. Comprehensive evaluations demonstrate that integrating
caption-based guidance and object-centric masking enhances dataset
distillation, leading to synthetic datasets that achieve superior performance
on downstream tasks.

</details>


### [179] [UniCAD: Efficient and Extendable Architecture for Multi-Task Computer-Aided Diagnosis System](https://arxiv.org/pdf/2505.09178)
*Yitao Zhu, Yuan Yin, Zhenrong Shen, Zihao Zhao, Haiyu Song, Sheng Wang, Dinggang Shen, Qian Wang*

Main category: cs.CV

TL;DR: UniCAD is a unified architecture for multi-task CAD systems, leveraging pre-trained vision models for efficient and extendable medical image diagnosis with minimal task-specific parameters.


<details>
  <summary>Details</summary>
Motivation: Addressing the challenges of complexity, resource intensity, and lack of open-source platforms in developing multi-task CAD systems for medical imaging.

Method: Uses a low-rank adaptation strategy for domain adaptation and a modular plug-and-play architecture with frozen foundation models and task-specific experts.

Result: Outperforms existing methods in accuracy and deployment efficiency across 12 diverse medical datasets.

Conclusion: UniCAD provides an open-source platform for efficient, extendable, and equitable CAD research, demonstrating superior performance.

Abstract: The growing complexity and scale of visual model pre-training have made
developing and deploying multi-task computer-aided diagnosis (CAD) systems
increasingly challenging and resource-intensive. Furthermore, the medical
imaging community lacks an open-source CAD platform to enable the rapid
creation of efficient and extendable diagnostic models. To address these
issues, we propose UniCAD, a unified architecture that leverages the robust
capabilities of pre-trained vision foundation models to seamlessly handle both
2D and 3D medical images while requiring only minimal task-specific parameters.
UniCAD introduces two key innovations: (1) Efficiency: A low-rank adaptation
strategy is employed to adapt a pre-trained visual model to the medical image
domain, achieving performance on par with fully fine-tuned counterparts while
introducing only 0.17% trainable parameters. (2) Plug-and-Play: A modular
architecture that combines a frozen foundation model with multiple
plug-and-play experts, enabling diverse tasks and seamless functionality
expansion. Building on this unified CAD architecture, we establish an
open-source platform where researchers can share and access lightweight CAD
experts, fostering a more equitable and efficient research ecosystem.
Comprehensive experiments across 12 diverse medical datasets demonstrate that
UniCAD consistently outperforms existing methods in both accuracy and
deployment efficiency. The source code and project page are available at
https://mii-laboratory.github.io/UniCAD/.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [180] [Study and improvement of search algorithms in two-players perfect information games](https://arxiv.org/pdf/2505.09639)
*Quentin Cohen-Solal*

Main category: cs.AI

TL;DR: The paper evaluates the generality of search algorithms in two-player zero-sum games with perfect information, introduces a new algorithm, and demonstrates its superior performance in short and medium search times.


<details>
  <summary>Details</summary>
Motivation: There is no existing study evaluating the generality of search algorithm performance in games, particularly for two-player zero-sum games with perfect information.

Method: The authors propose a new search algorithm and compare its performance against other studied algorithms across 22 games, focusing on short and medium search times.

Result: The new algorithm outperforms all others in short search times across all games and in medium search times for 17 out of 22 games.

Conclusion: The study fills a gap in evaluating search algorithm generality and demonstrates the effectiveness of the proposed algorithm in two-player zero-sum games.

Abstract: Games, in their mathematical sense, are everywhere (game industries,
economics, defense, education, chemistry, biology, ...).Search algorithms in
games are artificial intelligence methods for playing such games.
Unfortunately, there is no study on these algorithms that evaluates the
generality of their performance. We propose to address this gap in the case of
two-player zero-sum games with perfect information. Furthermore, we propose a
new search algorithm and we show that, for a short search time, it outperforms
all studied algorithms on all games in this large experiment and that, for a
medium search time, it outperforms all studied algorithms on 17 of the 22
studied games.

</details>


### [181] [Feature Relevancy, Necessity and Usefulness: Complexity and Algorithms](https://arxiv.org/pdf/2505.09640)
*Tomás Capdevielle, Santiago Cifuentes*

Main category: cs.AI

TL;DR: The paper improves techniques for identifying relevant and necessary features in model predictions, introduces a global notion of 'usefulness,' and develops efficient algorithms for detecting it in various models, including neural networks and decision trees.


<details>
  <summary>Details</summary>
Motivation: To enhance existing methods for feature importance ranking and introduce a generalized notion of feature usefulness, applicable across different models.

Method: Improves existing algorithms for detecting relevant and necessary features, generalizes relevancy, and introduces a new global notion of usefulness. Efficient algorithms are developed for decision trees and complex models.

Result: Demonstrates efficient detection of necessity in complex models, relates usefulness to relevancy and necessity, and validates practical utility through experiments on three datasets.

Conclusion: The paper advances feature importance analysis by introducing and efficiently detecting a global notion of usefulness, alongside improving existing techniques for relevancy and necessity.

Abstract: Given a classification model and a prediction for some input, there are
heuristic strategies for ranking features according to their importance in
regard to the prediction. One common approach to this task is rooted in
propositional logic and the notion of \textit{sufficient reason}. Through this
concept, the categories of relevant and necessary features were proposed in
order to identify the crucial aspects of the input. This paper improves the
existing techniques and algorithms for deciding which are the relevant and/or
necessary features, showing in particular that necessity can be detected
efficiently in complex models such as neural networks. We also generalize the
notion of relevancy and study associated problems. Moreover, we present a new
global notion (i.e. that intends to explain whether a feature is important for
the behavior of the model in general, not depending on a particular input) of
\textit{usefulness} and prove that it is related to relevancy and necessity.
Furthermore, we develop efficient algorithms for detecting it in decision trees
and other more complex models, and experiment on three datasets to analyze its
practical utility.

</details>


### [182] [General Dynamic Goal Recognition](https://arxiv.org/pdf/2505.09737)
*Osher Elhadad, Reuth Mirsky*

Main category: cs.AI

TL;DR: The paper introduces the General Dynamic Goal Recognition (GR) problem to address challenges in dynamic environments and proposes a model-free goal-conditioned RL approach for fast adaptation.


<details>
  <summary>Details</summary>
Motivation: Traditional GR methods struggle in dynamic environments with evolving goals, necessitating a broader definition and adaptable solutions.

Method: A model-free goal-conditioned reinforcement learning (RL) approach is employed to enable fast adaptation across changing tasks.

Result: The proposed method aims to improve real-time GR systems by addressing the limitations of traditional approaches.

Conclusion: The General Dynamic GR framework and model-free RL approach advance research and adaptability in goal recognition for dynamic environments.

Abstract: Understanding an agent's intent through its behavior is essential in
human-robot interaction, interactive AI systems, and multi-agent
collaborations. This task, known as Goal Recognition (GR), poses significant
challenges in dynamic environments where goals are numerous and constantly
evolving. Traditional GR methods, designed for a predefined set of goals, often
struggle to adapt to these dynamic scenarios. To address this limitation, we
introduce the General Dynamic GR problem - a broader definition of GR - aimed
at enabling real-time GR systems and fostering further research in this area.
Expanding on this foundation, this paper employs a model-free goal-conditioned
RL approach to enable fast adaptation for GR across various changing tasks.

</details>


### [183] [Explainability Through Human-Centric Design for XAI in Lung Cancer Detection](https://arxiv.org/pdf/2505.09755)
*Amy Rafferty, Rishi Ramaesh, Ajitha Rajan*

Main category: cs.AI

TL;DR: XpertXAI, an expert-driven model, improves interpretability and accuracy in detecting lung pathologies from chest X-rays compared to existing methods.


<details>
  <summary>Details</summary>
Motivation: Addressing the opacity of deep learning models in clinical settings by enhancing interpretability for lung pathology detection.

Method: Extends ClinicXAI using an InceptionV3-based classifier and expert-guided concepts, validated against radiologist annotations.

Result: XpertXAI outperforms baselines in accuracy and aligns better with expert reasoning, though focused on lung cancer.

Conclusion: Human-centric design like XpertXAI can scale to broader diagnostics, advancing clinically meaningful explainable AI.

Abstract: Deep learning models have shown promise in lung pathology detection from
chest X-rays, but widespread clinical adoption remains limited due to opaque
model decision-making. In prior work, we introduced ClinicXAI, a human-centric,
expert-guided concept bottleneck model (CBM) designed for interpretable lung
cancer diagnosis. We now extend that approach and present XpertXAI, a
generalizable expert-driven model that preserves human-interpretable clinical
concepts while scaling to detect multiple lung pathologies. Using a
high-performing InceptionV3-based classifier and a public dataset of chest
X-rays with radiology reports, we compare XpertXAI against leading post-hoc
explainability methods and an unsupervised CBM, XCBs. We assess explanations
through comparison with expert radiologist annotations and medical ground
truth. Although XpertXAI is trained for multiple pathologies, our expert
validation focuses on lung cancer. We find that existing techniques frequently
fail to produce clinically meaningful explanations, omitting key diagnostic
features and disagreeing with radiologist judgments. XpertXAI not only
outperforms these baselines in predictive accuracy but also delivers
concept-level explanations that better align with expert reasoning. While our
focus remains on explainability in lung cancer detection, this work illustrates
how human-centric model design can be effectively extended to broader
diagnostic contexts - offering a scalable path toward clinically meaningful
explainable AI in medical diagnostics.

</details>


### [184] [A Multimodal Multi-Agent Framework for Radiology Report Generation](https://arxiv.org/pdf/2505.09787)
*Ziruo Yi, Ting Xiao, Mark V. Albert*

Main category: cs.AI

TL;DR: A multimodal multi-agent framework for radiology report generation (RRG) improves accuracy and interpretability by aligning with clinical workflows, outperforming existing methods.


<details>
  <summary>Details</summary>
Motivation: To address challenges like factual inconsistency and cross-modal misalignment in RRG, enhancing clinical workflows and reducing radiologists' workload.

Method: Proposes a multimodal multi-agent framework with task-specific agents for retrieval, draft generation, visual analysis, refinement, and synthesis.

Result: Outperforms baselines in automatic metrics and LLM-based evaluations, producing more accurate and structured reports.

Conclusion: Clinically aligned multi-agent frameworks can enhance explainability and trustworthiness in clinical AI applications.

Abstract: Radiology report generation (RRG) aims to automatically produce diagnostic
reports from medical images, with the potential to enhance clinical workflows
and reduce radiologists' workload. While recent approaches leveraging
multimodal large language models (MLLMs) and retrieval-augmented generation
(RAG) have achieved strong results, they continue to face challenges such as
factual inconsistency, hallucination, and cross-modal misalignment. We propose
a multimodal multi-agent framework for RRG that aligns with the stepwise
clinical reasoning workflow, where task-specific agents handle retrieval, draft
generation, visual analysis, refinement, and synthesis. Experimental results
demonstrate that our approach outperforms a strong baseline in both automatic
metrics and LLM-based evaluations, producing more accurate, structured, and
interpretable reports. This work highlights the potential of clinically aligned
multi-agent frameworks to support explainable and trustworthy clinical AI
applications.

</details>


### [185] [Demystifying AI Agents: The Final Generation of Intelligence](https://arxiv.org/pdf/2505.09932)
*Kevin J McNamara, Rhea Pritham Marpu*

Main category: cs.AI

TL;DR: The paper traces AI's evolution to advanced autonomous agents like ChatGPT and Grok, highlighting key technological advancements and societal impacts, while emphasizing the need for wisdom in this rapidly progressing field.


<details>
  <summary>Details</summary>
Motivation: To document AI's rapid development, its current state as exemplified by systems like ChatGPT and Grok, and the societal implications of such advancements.

Method: Chronicles AI's trajectory by analyzing key technological milestones, including prompting, training, hardware, and architecture, with practical examples.

Result: Identifies modern AI agents as a potential 'final generation' of intelligence, with capabilities doubling every six months, and explores their societal impact.

Conclusion: Stresses the importance of wisdom and foresight to navigate the challenges and opportunities of this powerful new era of AI.

Abstract: The trajectory of artificial intelligence (AI) has been one of relentless
acceleration, evolving from rudimentary rule-based systems to sophisticated,
autonomous agents capable of complex reasoning and interaction. This whitepaper
chronicles this remarkable journey, charting the key technological
milestones--advancements in prompting, training methodologies, hardware
capabilities, and architectural innovations--that have converged to create the
AI agents of today. We argue that these agents, exemplified by systems like
OpenAI's ChatGPT with plugins and xAI's Grok, represent a culminating phase in
AI development, potentially constituting the "final generation" of intelligence
as we currently conceive it. We explore the capabilities and underlying
technologies of these agents, grounded in practical examples, while also
examining the profound societal implications and the unprecedented pace of
progress that suggests intelligence is now doubling approximately every six
months. The paper concludes by underscoring the critical need for wisdom and
foresight in navigating the opportunities and challenges presented by this
powerful new era of intelligence.

</details>


### [186] [Offline Reinforcement Learning for Microgrid Voltage Regulation](https://arxiv.org/pdf/2505.09920)
*Shan Yang, Yongli Zhu*

Main category: cs.AI

TL;DR: Study on offline reinforcement learning for microgrid voltage regulation, showing feasibility even with low-quality data.


<details>
  <summary>Details</summary>
Motivation: Addresses challenges of microgrid voltage regulation with solar power when online interaction is unsafe or impractical.

Method: Uses offline reinforcement learning algorithms trained on pre-collected datasets, including low-quality data.

Result: Demonstrates feasibility and effectiveness on the IEEE 33-bus system.

Conclusion: Offline reinforcement learning is viable for microgrid voltage regulation without online interaction.

Abstract: This paper presents a study on using different offline reinforcement learning
algorithms for microgrid voltage regulation with solar power penetration. When
environment interaction is unviable due to technical or safety reasons, the
proposed approach can still obtain an applicable model through offline-style
training on a previously collected dataset, lowering the negative impact of
lacking online environment interactions. Experiment results on the IEEE 33-bus
system demonstrate the feasibility and effectiveness of the proposed approach
on different offline datasets, including the one with merely low-quality
experience.

</details>


### [187] ["There Is No Such Thing as a Dumb Question," But There Are Good Ones](https://arxiv.org/pdf/2505.09923)
*Minjung Shin, Donghyun Kim, Jeh-Kwang Ryu*

Main category: cs.AI

TL;DR: The study defines good questions and introduces a systematic evaluation framework focusing on appropriateness and effectiveness, validated using CAUS and SQUARE datasets.


<details>
  <summary>Details</summary>
Motivation: Limited research comprehensively assesses question quality, prompting the need for a structured evaluation framework.

Method: Proposes a rubric-based scoring system with semi-adaptive criteria, validated using CAUS and SQUARE datasets.

Result: The framework effectively evaluates both well-formed and problematic questions across varied contexts.

Conclusion: The study advances structured analytical methods for questioning behavior, integrating flexibility and comprehensiveness.

Abstract: Questioning has become increasingly crucial for both humans and artificial
intelligence, yet there remains limited research comprehensively assessing
question quality. In response, this study defines good questions and presents a
systematic evaluation framework. We propose two key evaluation dimensions:
appropriateness (sociolinguistic competence in context) and effectiveness
(strategic competence in goal achievement). Based on these foundational
dimensions, a rubric-based scoring system was developed. By incorporating
dynamic contextual variables, our evaluation framework achieves structure and
flexibility through semi-adaptive criteria. The methodology was validated using
the CAUS and SQUARE datasets, demonstrating the ability of the framework to
access both well-formed and problematic questions while adapting to varied
contexts. As we establish a flexible and comprehensive framework for question
evaluation, this study takes a significant step toward integrating questioning
behavior with structured analytical methods grounded in the intrinsic nature of
questioning.

</details>


### [188] [Pre-Act: Multi-Step Planning and Reasoning Improves Acting in LLM Agents](https://arxiv.org/pdf/2505.09970)
*Mrinal Rawat, Ambuje Gupta, Rushil Goomer, Alessandro Di Bari, Neha Gupta, Roberto Pieraccini*

Main category: cs.AI

TL;DR: Pre-Act enhances LLM agent performance by creating multi-step execution plans with detailed reasoning, outperforming ReAct by 70% in Action Recall. Fine-tuned smaller models like Llama 3.1 also show significant improvements.


<details>
  <summary>Details</summary>
Motivation: To improve agentic systems by addressing the limitations of current LLMs in complex reasoning and practical applications.

Method: Introduces Pre-Act, a multi-step execution plan with incremental refinement, and fine-tunes smaller models (Llama 3.1) for better performance.

Result: Pre-Act outperforms ReAct by 70% in Action Recall. Fine-tuned 70B Llama 3.1 surpasses GPT-4 with 69.5% higher action accuracy and 28% better goal completion.

Conclusion: Pre-Act is effective for enhancing agent performance, especially when fine-tuned on smaller models, making it practical for real-world applications.

Abstract: The ReAct (Reasoning + Action) capability in large language models (LLMs) has
become the foundation of modern agentic systems. Recent LLMs, such as
DeepSeek-R1 and OpenAI o1/o3, exemplify this by emphasizing reasoning through
the generation of ample intermediate tokens, which help build a strong premise
before producing the final output tokens. In this paper, we introduce Pre-Act,
a novel approach that enhances the agent's performance by creating a multi-step
execution plan along with the detailed reasoning for the given user input. This
plan incrementally incorporates previous steps and tool outputs, refining
itself after each step execution until the final response is obtained. Our
approach is applicable to both conversational and non-conversational agents. To
measure the performance of task-oriented agents comprehensively, we propose a
two-level evaluation framework: (1) turn level and (2) end-to-end. Our
turn-level evaluation, averaged across five models, shows that our approach,
Pre-Act, outperforms ReAct by 70% in Action Recall on the Almita dataset. While
this approach is effective for larger models, smaller models crucial for
practical applications, where latency and cost are key constraints, often
struggle with complex reasoning tasks required for agentic systems. To address
this limitation, we fine-tune relatively small models such as Llama 3.1 (8B &
70B) using the proposed Pre-Act approach. Our experiments show that the
fine-tuned 70B model outperforms GPT-4, achieving a 69.5% improvement in action
accuracy (turn-level) and a 28% improvement in goal completion rate
(end-to-end) on the Almita (out-of-domain) dataset.

</details>


### [189] [The First MPDD Challenge: Multimodal Personality-aware Depression Detection](https://arxiv.org/pdf/2505.10034)
*Changzeng Fu, Zelin Fu, Xinhe Kuang, Jiacheng Dong, Qi Zhang, Kaifeng Su, Yikai Su, Wenbo Shi, Junfeng Yao, Yuliang Zhao, Shiqi Zhao, Jiadong Wang, Siyang Song, Chaoran Liu, Yuichiro Yoshikawa, Björn Schuller, Hiroshi Ishiguro*

Main category: cs.AI

TL;DR: The paper introduces the MPDD Challenge to address gaps in depression detection by incorporating multimodal data and individual differences, focusing on diverse age groups.


<details>
  <summary>Details</summary>
Motivation: Existing depression detection methods overlook age diversity and individual differences, limiting their effectiveness.

Method: The challenge uses two datasets (MPDD-Elderly and MPDD-Young) and a baseline model combining audio, video, and individual difference data.

Result: The approach aims to improve personalized and accurate depression detection across different populations.

Conclusion: The MPDD Challenge seeks to advance mental health research by fostering inclusive and tailored detection systems.

Abstract: Depression is a widespread mental health issue affecting diverse age groups,
with notable prevalence among college students and the elderly. However,
existing datasets and detection methods primarily focus on young adults,
neglecting the broader age spectrum and individual differences that influence
depression manifestation. Current approaches often establish a direct mapping
between multimodal data and depression indicators, failing to capture the
complexity and diversity of depression across individuals. This challenge
includes two tracks based on age-specific subsets: Track 1 uses the
MPDD-Elderly dataset for detecting depression in older adults, and Track 2 uses
the MPDD-Young dataset for detecting depression in younger participants. The
Multimodal Personality-aware Depression Detection (MPDD) Challenge aims to
address this gap by incorporating multimodal data alongside individual
difference factors. We provide a baseline model that fuses audio and video
modalities with individual difference information to detect depression
manifestations in diverse populations. This challenge aims to promote the
development of more personalized and accurate de pression detection methods,
advancing mental health research and fostering inclusive detection systems.
More details are available on the official challenge website:
https://hacilab.github.io/MPDDChallenge.github.io.

</details>


### [190] [Learning Progress Driven Multi-Agent Curriculum](https://arxiv.org/pdf/2205.10016)
*Wenshuai Zhao, Zhiyuan Li, Joni Pajarinen*

Main category: cs.AI

TL;DR: Using TD-error-based learning progress to control curriculum difficulty in MARL outperforms manual and reward-based methods.


<details>
  <summary>Details</summary>
Motivation: Existing reward-based curriculum methods in MARL have high variance in measuring difficulty and exacerbate credit assignment issues.

Method: Proposes a TD-error-based learning progress measure and transitions from an initial context distribution to the final task-specific one.

Result: Outperforms state-of-the-art baselines in three sparse-reward MARL benchmarks.

Conclusion: The approach effectively addresses flaws in existing methods and improves MARL performance.

Abstract: The number of agents can be an effective curriculum variable for controlling
the difficulty of multi-agent reinforcement learning (MARL) tasks. Existing
work typically uses manually defined curricula such as linear schemes. We
identify two potential flaws while applying existing reward-based automatic
curriculum learning methods in MARL: (1) The expected episode return used to
measure task difficulty has high variance; (2) Credit assignment difficulty can
be exacerbated in tasks where increasing the number of agents yields higher
returns which is common in many MARL tasks. To address these issues, we propose
to control the curriculum by using a TD-error based *learning progress* measure
and by letting the curriculum proceed from an initial context distribution to
the final task specific one. Since our approach maintains a distribution over
the number of agents and measures learning progress rather than absolute
performance, which often increases with the number of agents, we alleviate
problem (2). Moreover, the learning progress measure naturally alleviates
problem (1) by aggregating returns. In three challenging sparse-reward MARL
benchmarks, our approach outperforms state-of-the-art baselines.

</details>


### [191] [Leveraging Graph Retrieval-Augmented Generation to Support Learners' Understanding of Knowledge Concepts in MOOCs](https://arxiv.org/pdf/2505.10074)
*Mohamed Abdelmagied, Mohamed Amine Chatti, Shoeb Joarder, Qurat Ul Ain, Rawaa Alatrash*

Main category: cs.AI

TL;DR: The paper proposes a Graph RAG pipeline using Educational and Personal Knowledge Graphs to enhance MOOC learning by generating personalized questions and answers, addressing LLM hallucinations and unstructured material limitations.


<details>
  <summary>Details</summary>
Motivation: MOOCs lack direct interaction, and LLMs suffer from hallucinations. Current RAG systems don't guide learners effectively due to unstructured materials.

Method: A Graph RAG pipeline leverages EduKGs and PKGs for personalized question generation and answering, tested on CourseMapper with expert instructors.

Result: Evaluation shows Graph RAG's potential to improve personalized learning in MOOCs.

Conclusion: Graph RAG effectively addresses MOOC learning challenges by combining structured knowledge graphs with personalized guidance.

Abstract: Massive Open Online Courses (MOOCs) lack direct interaction between learners
and instructors, making it challenging for learners to understand new knowledge
concepts. Recently, learners have increasingly used Large Language Models
(LLMs) to support them in acquiring new knowledge. However, LLMs are prone to
hallucinations which limits their reliability. Retrieval-Augmented Generation
(RAG) addresses this issue by retrieving relevant documents before generating a
response. However, the application of RAG across different MOOCs is limited by
unstructured learning material. Furthermore, current RAG systems do not
actively guide learners toward their learning needs. To address these
challenges, we propose a Graph RAG pipeline that leverages Educational
Knowledge Graphs (EduKGs) and Personal Knowledge Graphs (PKGs) to guide
learners to understand knowledge concepts in the MOOC platform CourseMapper.
Specifically, we implement (1) a PKG-based Question Generation method to
recommend personalized questions for learners in context, and (2) an
EduKG-based Question Answering method that leverages the relationships between
knowledge concepts in the EduKG to answer learner selected questions. To
evaluate both methods, we conducted a study with 3 expert instructors on 3
different MOOCs in the MOOC platform CourseMapper. The results of the
evaluation show the potential of Graph RAG to empower learners to understand
new knowledge concepts in a personalized learning experience.

</details>


### [192] [From Text to Network: Constructing a Knowledge Graph of Taiwan-Based China Studies Using Generative AI](https://arxiv.org/pdf/2505.10093)
*Hsuan-Lei Shao*

Main category: cs.AI

TL;DR: This study uses AI to transform unstructured Taiwanese China Studies texts into structured knowledge graphs, enhancing scholarly access and revealing research insights.


<details>
  <summary>Details</summary>
Motivation: The need to systematically revisit and reorganize decades of Taiwan-based China Studies scholarship due to its interdisciplinary richness and geopolitical uniqueness.

Method: Generative AI and large language models extract and standardize entity-relation triples from 1,367 peer-reviewed articles (1996-2019), visualized via D3.js for interactive exploration.

Result: Creation of a domain-specific knowledge graph and vector database, uncovering intellectual trajectories, thematic clusters, and research gaps.

Conclusion: The AI-assisted approach shifts from linear text to network-based knowledge navigation, offering scalable, data-driven ontology construction for regional studies.

Abstract: Taiwanese China Studies (CS) has developed into a rich, interdisciplinary
research field shaped by the unique geopolitical position and long standing
academic engagement with Mainland China. This study responds to the growing
need to systematically revisit and reorganize decades of Taiwan based CS
scholarship by proposing an AI assisted approach that transforms unstructured
academic texts into structured, interactive knowledge representations. We apply
generative AI (GAI) techniques and large language models (LLMs) to extract and
standardize entity relation triples from 1,367 peer reviewed CS articles
published between 1996 and 2019. These triples are then visualized through a
lightweight D3.js based system, forming the foundation of a domain specific
knowledge graph and vector database for the field. This infrastructure allows
users to explore conceptual nodes and semantic relationships across the corpus,
revealing previously uncharted intellectual trajectories, thematic clusters,
and research gaps. By decomposing textual content into graph structured
knowledge units, our system enables a paradigm shift from linear text
consumption to network based knowledge navigation. In doing so, it enhances
scholarly access to CS literature while offering a scalable, data driven
alternative to traditional ontology construction. This work not only
demonstrates how generative AI can augment area studies and digital humanities
but also highlights its potential to support a reimagined scholarly
infrastructure for regional knowledge systems.

</details>


### [193] [A User Study Evaluating Argumentative Explanations in Diagnostic Decision Support](https://arxiv.org/pdf/2505.10188)
*Felix Liedeker, Olivia Sanchez-Graillet, Moana Seidler, Christian Brandt, Jörg Wellmer, Philipp Cimiano*

Main category: cs.AI

TL;DR: The paper explores effective AI-generated explanations in healthcare, focusing on trust and transparency in ML systems for diagnostic decision support.


<details>
  <summary>Details</summary>
Motivation: To understand which AI explanations enhance trust and transparency in healthcare, especially in shared decision-making between doctors and ML systems.

Method: Conducted a user study with physicians, including surveys and interviews, to evaluate different XAI explanations in diagnostic contexts.

Result: Identified the most effective explanations for enhancing the diagnostic process based on physician feedback.

Conclusion: The study provides insights into optimal AI explanations for improving trust and utility in healthcare decision-making.

Abstract: As the field of healthcare increasingly adopts artificial intelligence, it
becomes important to understand which types of explanations increase
transparency and empower users to develop confidence and trust in the
predictions made by machine learning (ML) systems. In shared decision-making
scenarios where doctors cooperate with ML systems to reach an appropriate
decision, establishing mutual trust is crucial. In this paper, we explore
different approaches to generating explanations in eXplainable AI (XAI) and
make their underlying arguments explicit so that they can be evaluated by
medical experts. In particular, we present the findings of a user study
conducted with physicians to investigate their perceptions of various types of
AI-generated explanations in the context of diagnostic decision support. The
study aims to identify the most effective and useful explanations that enhance
the diagnostic process. In the study, medical doctors filled out a survey to
assess different types of explanations. Further, an interview was carried out
post-survey to gain qualitative insights on the requirements of explanations
incorporated in diagnostic decision support. Overall, the insights gained from
this study contribute to understanding the types of explanations that are most
effective.

</details>


### [194] [MASS: Multi-Agent Simulation Scaling for Portfolio Construction](https://arxiv.org/pdf/2505.10278)
*Taian Guo, Haiyang Shen, Jinsheng Huang, Zhengyang Mao, Junyu Luo, Zhuoru Chen, Xuhui Liu, Bingyu Xia, Luchen Liu, Yun Ma, Ming Zhang*

Main category: cs.AI

TL;DR: MASS introduces a scalable multi-agent system for portfolio construction, outperforming baselines by dynamically increasing agents and optimizing distribution via reverse optimization.


<details>
  <summary>Details</summary>
Motivation: Existing multi-agent systems are limited to simulations or fixed workflows, hindering real-world applicability and performance.

Method: MASS uses progressive agent scaling and reverse optimization for end-to-end agent distribution, avoiding fixed workflows.

Result: MASS achieves stable excess returns, validated through extensive experiments against 6 baselines on 3 stock pools.

Conclusion: MASS's scalable paradigm can extend to similar tasks, with its implementation open-sourced.

Abstract: LLM-based multi-agent has gained significant attention for their potential in
simulation and enhancing performance. However, existing works are limited to
pure simulations or are constrained by predefined workflows, restricting their
applicability and effectiveness. In this paper, we introduce the Multi-Agent
Scaling Simulation (MASS) for portfolio construction. MASS achieves stable and
continuous excess returns by progressively increasing the number of agents for
large-scale simulations to gain a superior understanding of the market and
optimizing agent distribution end-to-end through a reverse optimization
process, rather than relying on a fixed workflow. We demonstrate its
superiority through performance experiments, ablation studies, backtesting
experiments, experiments on updated data and stock pools, scaling experiments,
parameter sensitivity experiments, and visualization experiments, conducted in
comparison with 6 state-of-the-art baselines on 3 challenging A-share stock
pools. We expect the paradigm established by MASS to expand to other tasks with
similar characteristics. The implementation of MASS has been open-sourced at
https://github.com/gta0804/MASS.

</details>


### [195] [Empirically evaluating commonsense intelligence in large language models with large-scale human judgments](https://arxiv.org/pdf/2505.10309)
*Tuan Dung Nguyen, Duncan J. Watts, Mark E. Whiting*

Main category: cs.AI

TL;DR: The paper critiques static benchmarks for assessing commonsense intelligence in AI, highlighting human variability in commonsense judgments. It proposes a method to evaluate AI models by comparing their outputs to human population judgments, finding LLMs often fall below human median competence and correlate modestly with human agreement. Smaller models outperform larger ones.


<details>
  <summary>Details</summary>
Motivation: Current benchmarks assume homogeneous human commonsense, ignoring empirical evidence of human variability. The study aims to address this gap by evaluating AI models against diverse human judgments.

Method: The study evaluates LLMs by treating them as independent survey respondents and simulators of human populations, measuring their commonsense competence and agreement with real humans.

Result: Most LLMs score below the human median in individual competence and show modest correlation with human agreement. Smaller models perform better than larger proprietary ones.

Conclusion: The framework emphasizes adapting AI to diverse human collectivities, aligning with calls for culturally aware AI models.

Abstract: Commonsense intelligence in machines is often assessed by static benchmarks
that compare a model's output against human-prescribed correct labels. An
important, albeit implicit, assumption of these labels is that they accurately
capture what any human would think, effectively treating human common sense as
homogeneous. However, recent empirical work has shown that humans vary
enormously in what they consider commonsensical; thus what appears self-evident
to one benchmark designer may not be so to another. Here, we propose a novel
method for evaluating common sense in artificial intelligence (AI),
specifically in large language models (LLMs), that incorporates empirically
observed heterogeneity among humans by measuring the correspondence between a
model's judgment and that of a human population. We first find that, when
treated as independent survey respondents, most LLMs remain below the human
median in their individual commonsense competence. Second, when used as
simulators of a hypothetical population, LLMs correlate with real humans only
modestly in the extent to which they agree on the same set of statements. In
both cases, smaller, open-weight models are surprisingly more competitive than
larger, proprietary frontier models. Our evaluation framework, which ties
commonsense intelligence to its cultural basis, contributes to the growing call
for adapting AI models to human collectivities that possess different, often
incompatible, social stocks of knowledge.

</details>


### [196] [A Comparative Study of SMT and MILP for the Nurse Rostering Problem](https://arxiv.org/pdf/2505.10328)
*Alvin Combrink, Stephie Do, Kristofer Bengtsson, Sabino Francesco Roselli, Martin Fabian*

Main category: cs.AI

TL;DR: The paper compares SMT and MILP solvers for healthcare personnel scheduling, finding SMT excels in varied real-world problems while MILP performs better in highly constrained cases.


<details>
  <summary>Details</summary>
Motivation: Healthcare scheduling is complex due to high demand and constraints, with limited research on SMT applications.

Method: Proposes generic constraint formulations for scheduling, comparing SMT (Z3) and MILP (Gurobi) solvers on academic and real-world problems.

Result: SMT excels in varied real-world problems, while MILP performs better in highly constrained or infeasible cases. SMT is sensitive to constraint formulation.

Conclusion: SMT-based methods are promising for future personnel scheduling research.

Abstract: The effects of personnel scheduling on the quality of care and working
conditions for healthcare personnel have been thoroughly documented. However,
the ever-present demand and large variation of constraints make healthcare
scheduling particularly challenging. This problem has been studied for decades,
with limited research aimed at applying Satisfiability Modulo Theories (SMT).
SMT has gained momentum within the formal verification community in the last
decades, leading to the advancement of SMT solvers that have been shown to
outperform standard mathematical programming techniques.
  In this work, we propose generic constraint formulations that can model a
wide range of real-world scheduling constraints. Then, the generic constraints
are formulated as SMT and MILP problems and used to compare the respective
state-of-the-art solvers, Z3 and Gurobi, on academic and real-world inspired
rostering problems. Experimental results show how each solver excels for
certain types of problems; the MILP solver generally performs better when the
problem is highly constrained or infeasible, while the SMT solver performs
better otherwise. On real-world inspired problems containing a more varied set
of shifts and personnel, the SMT solver excels. Additionally, it was noted
during experimentation that the SMT solver was more sensitive to the way the
generic constraints were formulated, requiring careful consideration and
experimentation to achieve better performance. We conclude that SMT-based
methods present a promising avenue for future research within the domain of
personnel scheduling.

</details>


### [197] [Plasticity as the Mirror of Empowerment](https://arxiv.org/pdf/2505.10361)
*David Abel, Michael Bowling, André Barreto, Will Dabney, Shi Dong, Steven Hansen, Anna Harutyunyan, Khimya Khetarpal, Clare Lyle, Razvan Pascanu, Georgios Piliouras, Doina Precup, Jonathan Richens, Mark Rowland, Tom Schaul, Satinder Singh*

Main category: cs.AI

TL;DR: The paper introduces 'plasticity' as a measure of how an agent is influenced by observations, mirroring 'empowerment', and explores their interplay.


<details>
  <summary>Details</summary>
Motivation: To understand the foundational influence of observations on agents, complementing the concept of empowerment.

Method: Defines plasticity using a new information-theoretic quantity, generalized directed information, and analyzes its connection to empowerment.

Result: Plasticity mirrors empowerment (agent's plasticity equals environment's empowerment), and a tension exists between the two.

Conclusion: Plasticity and empowerment are essential for understanding agency, requiring balanced consideration in agent design.

Abstract: Agents are minimally entities that are influenced by their past observations
and act to influence future observations. This latter capacity is captured by
empowerment, which has served as a vital framing concept across artificial
intelligence and cognitive science. This former capacity, however, is equally
foundational: In what ways, and to what extent, can an agent be influenced by
what it observes? In this paper, we ground this concept in a universal
agent-centric measure that we refer to as plasticity, and reveal a fundamental
connection to empowerment. Following a set of desiderata on a suitable
definition, we define plasticity using a new information-theoretic quantity we
call the generalized directed information. We show that this new quantity
strictly generalizes the directed information introduced by Massey (1990) while
preserving all of its desirable properties. Our first finding is that
plasticity is the mirror of empowerment: The agent's plasticity is identical to
the empowerment of the environment, and vice versa. Our second finding
establishes a tension between the plasticity and empowerment of an agent,
suggesting that agent design needs to be mindful of both characteristics. We
explore the implications of these findings, and suggest that plasticity,
empowerment, and their relationship are essential to understanding agency.

</details>


### [198] [Evaluating Model Explanations without Ground Truth](https://arxiv.org/pdf/2505.10399)
*Kaivalya Rawal, Zihao Fu, Eoin Delaney, Chris Russell*

Main category: cs.AI

TL;DR: Proposes AXE, a ground-truth agnostic framework for evaluating model explanations, addressing limitations of current methods.


<details>
  <summary>Details</summary>
Motivation: Current explanation evaluation relies on ground-truth or model sensitivity, which are limiting or unreliable.

Method: Introduces AXE, a framework evaluating explanations without ground-truth or sensitivity reliance.

Result: AXE outperforms baselines and detects explanation fairwashing.

Conclusion: AXE provides a robust, independent measure for explanation quality, advancing evaluation strategies.

Abstract: There can be many competing and contradictory explanations for a single model
prediction, making it difficult to select which one to use. Current explanation
evaluation frameworks measure quality by comparing against ideal "ground-truth"
explanations, or by verifying model sensitivity to important inputs. We outline
the limitations of these approaches, and propose three desirable principles to
ground the future development of explanation evaluation strategies for local
feature importance explanations. We propose a ground-truth Agnostic eXplanation
Evaluation framework (AXE) for evaluating and comparing model explanations that
satisfies these principles. Unlike prior approaches, AXE does not require
access to ideal ground-truth explanations for comparison, or rely on model
sensitivity - providing an independent measure of explanation quality. We
verify AXE by comparing with baselines, and show how it can be used to detect
explanation fairwashing. Our code is available at
https://github.com/KaiRawal/Evaluating-Model-Explanations-without-Ground-Truth.

</details>


### [199] [AI Agents vs. Agentic AI: A Conceptual Taxonomy, Applications and Challenge](https://arxiv.org/pdf/2505.10468)
*Ranjan Sapkota, Konstantinos I. Roumeliotis, Manoj Karkee*

Main category: cs.AI

TL;DR: The paper distinguishes AI Agents from Agentic AI, providing a taxonomy and analysis of their design philosophies, capabilities, and applications, while addressing challenges and proposing solutions.


<details>
  <summary>Details</summary>
Motivation: To clarify the differences between AI Agents and Agentic AI, highlighting their unique features, applications, and challenges to guide future development.

Method: The study outlines search strategies, foundational definitions, and evaluates architectural evolution, operational mechanisms, interaction styles, and autonomy levels for both paradigms.

Result: A comparative analysis reveals distinct applications (e.g., task-specific automation vs. multi-agent collaboration) and challenges (e.g., hallucination, coordination failure), with proposed solutions like ReAct loops and RAG.

Conclusion: The paper provides a roadmap for developing robust, scalable, and explainable AI systems, emphasizing the need for targeted solutions to paradigm-specific challenges.

Abstract: This study critically distinguishes between AI Agents and Agentic AI,
offering a structured conceptual taxonomy, application mapping, and challenge
analysis to clarify their divergent design philosophies and capabilities. We
begin by outlining the search strategy and foundational definitions,
characterizing AI Agents as modular systems driven by Large Language Models
(LLMs) and Large Image Models (LIMs) for narrow, task-specific automation.
Generative AI is positioned as a precursor, with AI Agents advancing through
tool integration, prompt engineering, and reasoning enhancements. In contrast,
Agentic AI systems represent a paradigmatic shift marked by multi-agent
collaboration, dynamic task decomposition, persistent memory, and orchestrated
autonomy. Through a sequential evaluation of architectural evolution,
operational mechanisms, interaction styles, and autonomy levels, we present a
comparative analysis across both paradigms. Application domains such as
customer support, scheduling, and data summarization are contrasted with
Agentic AI deployments in research automation, robotic coordination, and
medical decision support. We further examine unique challenges in each paradigm
including hallucination, brittleness, emergent behavior, and coordination
failure and propose targeted solutions such as ReAct loops, RAG, orchestration
layers, and causal modeling. This work aims to provide a definitive roadmap for
developing robust, scalable, and explainable AI agent and Agentic AI-driven
systems. >AI Agents, Agent-driven, Vision-Language-Models, Agentic AI Decision
Support System, Agentic-AI Applications

</details>


### [200] [Towards a Deeper Understanding of Reasoning Capabilities in Large Language Models](https://arxiv.org/pdf/2505.10543)
*Annie Wong, Thomas Bäck, Aske Plaat, Niki van Stein, Anna V. Kononova*

Main category: cs.AI

TL;DR: The study evaluates self-reflection, heuristic mutation, and planning in large language models (LLMs) in dynamic environments, finding strategic prompting can narrow performance gaps between model sizes but reveals persistent reasoning limitations.


<details>
  <summary>Details</summary>
Motivation: To assess the adaptive capabilities of LLMs in dynamic settings beyond static benchmarks, uncovering their true potential as self-learning agents.

Method: Experiments with various open-source LLMs using self-reflection, heuristic mutation, and planning as prompting techniques in dynamic environments.

Result: Larger models outperform smaller ones, but strategic prompting helps smaller models. Advanced techniques benefit smaller models in complex tasks but introduce instability. LLMs show persistent reasoning and planning shortcomings.

Conclusion: Current LLMs lack true emergent reasoning and exhibit fundamental limitations in planning and reasoning, suggesting the need for benchmarks beyond static tasks.

Abstract: While large language models demonstrate impressive performance on static
benchmarks, the true potential of large language models as self-learning and
reasoning agents in dynamic environments remains unclear. This study
systematically evaluates the efficacy of self-reflection, heuristic mutation,
and planning as prompting techniques to test the adaptive capabilities of
agents. We conduct experiments with various open-source language models in
dynamic environments and find that larger models generally outperform smaller
ones, but that strategic prompting can close this performance gap. Second, a
too-long prompt can negatively impact smaller models on basic reactive tasks,
while larger models show more robust behaviour. Third, advanced prompting
techniques primarily benefit smaller models on complex games, but offer less
improvement for already high-performing large language models. Yet, we find
that advanced reasoning methods yield highly variable outcomes: while capable
of significantly improving performance when reasoning and decision-making
align, they also introduce instability and can lead to big performance drops.
Compared to human performance, our findings reveal little evidence of true
emergent reasoning. Instead, large language model performance exhibits
persistent limitations in crucial areas such as planning, reasoning, and
spatial coordination, suggesting that current-generation large language models
still suffer fundamental shortcomings that may not be fully overcome through
self-reflective prompting alone. Reasoning is a multi-faceted task, and while
reasoning methods like Chain of thought improves multi-step reasoning on math
word problems, our findings using dynamic benchmarks highlight important
shortcomings in general reasoning capabilities, indicating a need to move
beyond static benchmarks to capture the complexity of reasoning.

</details>


### [201] [AriGraph: Learning Knowledge Graph World Models with Episodic Memory for LLM Agents](https://arxiv.org/pdf/2407.04363)
*Petr Anokhin, Nikita Semenov, Artyom Sorokin, Dmitry Evseev, Andrey Kravchenko, Mikhail Burtsev, Evgeny Burnaev*

Main category: cs.AI

TL;DR: AriGraph, a memory graph method for LLM-based agents, outperforms existing memory and RL methods in complex tasks and question-answering.


<details>
  <summary>Details</summary>
Motivation: Current LLM-based agents lack structured memory for reasoning and planning, limiting complex decision-making.

Method: AriGraph integrates semantic and episodic memories into a graph, enhancing planning and decision-making in the Ariadne LLM agent.

Result: AriGraph outperforms other memory methods and RL baselines in complex tasks and matches knowledge graph methods in question-answering.

Conclusion: AriGraph provides a robust memory architecture for LLM agents, improving performance in complex environments.

Abstract: Advancements in the capabilities of Large Language Models (LLMs) have created
a promising foundation for developing autonomous agents. With the right tools,
these agents could learn to solve tasks in new environments by accumulating and
updating their knowledge. Current LLM-based agents process past experiences
using a full history of observations, summarization, retrieval augmentation.
However, these unstructured memory representations do not facilitate the
reasoning and planning essential for complex decision-making. In our study, we
introduce AriGraph, a novel method wherein the agent constructs and updates a
memory graph that integrates semantic and episodic memories while exploring the
environment. We demonstrate that our Ariadne LLM agent, consisting of the
proposed memory architecture augmented with planning and decision-making,
effectively handles complex tasks within interactive text game environments
difficult even for human players. Results show that our approach markedly
outperforms other established memory methods and strong RL baselines in a range
of problems of varying complexity. Additionally, AriGraph demonstrates
competitive performance compared to dedicated knowledge graph-based methods in
static multi-hop question-answering.

</details>


### [202] [Addressing and Visualizing Misalignments in Human Task-Solving Trajectories](https://arxiv.org/pdf/2409.14191)
*Sejin Kim, Hosung Lee, Sundong Kim*

Main category: cs.AI

TL;DR: The paper identifies three types of misalignments in human task-solving trajectories, proposes detection and intention estimation algorithms, and shows improved AI performance through alignment.


<details>
  <summary>Details</summary>
Motivation: To improve AI models mimicking human reasoning by addressing misalignments in human task-solving trajectories.

Method: Formalizes misalignment types, proposes heuristic detection and intention estimation algorithms, and conducts hierarchical analysis and experiments.

Result: AI models trained on aligned trajectories show improved performance in mimicking human reasoning.

Conclusion: Trajectory-intention alignment is crucial, and intention learning has significant potential.

Abstract: Understanding misalignments in human task-solving trajectories is critical
for improving AI models trained to mimic human reasoning. This study
categorizes such misalignments into three types: \textbf{(1) Lack of functions
to express intent}, \textbf{(2) Inefficient action sequences}, and \textbf{(3)
Incorrect intentions that cannot solve the task}. To address these issues, we
first formalize and define these three types of misalignments. We then propose
a heuristic algorithm to detect these misalignments in O2ARC trajectories and
conduct a hierarchical and quantitative analysis of their impact. Furthermore,
we introduce an intention estimation algorithm that predicts missing alignment
information between user actions and inferred intentions, leveraging our
formalized framework. Through trajectory alignment, we experimentally
demonstrate that AI models trained on human task-solving trajectories improve
performance in mimicking human reasoning. Based on hierarchical analysis and
experiments, we highlight the importance of trajectory-intention alignment and
demonstrate the potential of intention learning.

</details>


### [203] [MapExplorer: New Content Generation from Low-Dimensional Visualizations](https://arxiv.org/pdf/2412.18673)
*Xingjian Zhang, Ziyang Xiong, Shixuan Liu, Yutong Xie, Tolga Ergen, Dongsub Shim, Hua Xu, Honglak Lee, Qiaozhu Me*

Main category: cs.AI

TL;DR: MapExplorer introduces a method to translate projection map coordinates into coherent text, enabling interactive exploration of datasets, and proposes Atometric for evaluation.


<details>
  <summary>Details</summary>
Motivation: Existing projection maps (e.g., t-SNE, UMAP) lack systematic methods for generating new content from them.

Method: MapExplorer translates map coordinates into contextually aligned textual content, evaluated by Atometric.

Result: Experiments show MapExplorer's versatility in generating hypotheses, personas, and strategies, even with simple baselines.

Conclusion: MapExplorer bridges visualization and generation, enhancing human-AI collaboration in data exploration.

Abstract: Low-dimensional visualizations, or "projection maps," are widely used in
scientific and creative domains to interpret large-scale and complex datasets.
These visualizations not only aid in understanding existing knowledge spaces
but also implicitly guide exploration into unknown areas. Although techniques
such as t-SNE and UMAP can generate these maps, there exists no systematic
method for leveraging them to generate new content. To address this, we
introduce MapExplorer, a novel knowledge discovery task that translates
coordinates within any projection map into coherent, contextually aligned
textual content. This allows users to interactively explore and uncover
insights embedded in the maps. To evaluate the performance of MapExplorer
methods, we propose Atometric, a fine-grained metric inspired by ROUGE that
quantifies logical coherence and alignment between generated and reference
text. Experiments on diverse datasets demonstrate the versatility of
MapExplorer in generating scientific hypotheses, crafting synthetic personas,
and devising strategies for attacking large language models-even with simple
baseline methods. By bridging visualization and generation, our work highlights
the potential of MapExplorer to enable intuitive human-AI collaboration in
large-scale data exploration.

</details>


### [204] [SensorChat: Answering Qualitative and Quantitative Questions during Long-Term Multimodal Sensor Interactions](https://arxiv.org/pdf/2502.02883)
*Xiaofan Yu, Lanxiang Hu, Benjamin Reichman, Dylan Chu, Rushil Chandrupatla, Xiyuan Zhang, Larry Heck, Tajana Rosing*

Main category: cs.AI

TL;DR: SensorChat is an end-to-end QA system for daily life monitoring using long-duration, high-frequency sensor data, outperforming existing systems in accuracy for both quantitative and qualitative questions.


<details>
  <summary>Details</summary>
Motivation: Existing systems struggle with long-duration, high-frequency sensor data and precise numerical answers, limiting their usefulness for health-related insights.

Method: SensorChat uses a three-stage pipeline: question decomposition (LLMs), sensor data query, and answer assembly (LLMs), enabling real-time interactions on cloud or edge platforms.

Result: SensorChat achieves up to 93% higher accuracy on quantitative questions and effectively handles qualitative questions, as validated by user studies.

Conclusion: SensorChat advances natural language interaction with sensing systems, offering precise and meaningful responses for daily life monitoring.

Abstract: Natural language interaction with sensing systems is crucial for addressing
users' personal concerns and providing health-related insights into their daily
lives. When a user asks a question, the system automatically analyzes the full
history of sensor data, extracts relevant information, and generates an
appropriate response. However, existing systems are limited to short-duration
(e.g., one minute) or low-frequency (e.g., daily step count) sensor data. In
addition, they struggle with quantitative questions that require precise
numerical answers. In this work, we introduce SensorChat, the first end-to-end
QA system designed for daily life monitoring using long-duration,
high-frequency time series data. Given raw sensor signals spanning multiple
days and a user-defined natural language question, SensorChat generates
semantically meaningful responses that directly address user concerns.
SensorChat effectively handles both quantitative questions that require
numerical precision and qualitative questions that require high-level reasoning
to infer subjective insights. To achieve this, SensorChat uses an innovative
three-stage pipeline including question decomposition, sensor data query, and
answer assembly. The first and third stages leverage Large Language Models
(LLMs) to interpret human queries and generate responses. The intermediate
querying stage extracts relevant information from the complete sensor data
history. Real-world implementation demonstrate SensorChat's capability for
real-time interactions on a cloud server while also being able to run entirely
on edge platforms after quantization. Comprehensive QA evaluations show that
SensorChat achieves up to 93% higher answer accuracy than state-of-the-art
systems on quantitative questions. Additionally, a user study with eight
volunteers highlights SensorChat's effectiveness in answering qualitative and
open-ended questions.

</details>


### [205] [Demonstrating specification gaming in reasoning models](https://arxiv.org/pdf/2502.13295)
*Alexander Bondarenko, Denis Volk, Dmitrii Volkov, Jeffrey Ladish*

Main category: cs.AI

TL;DR: LLM agents can hack chess benchmarks; reasoning models do so by default, while language models need explicit instructions.


<details>
  <summary>Details</summary>
Motivation: To explore how LLMs game benchmarks, especially in chess, and compare reasoning vs. language models.

Method: Instructed models to win against a chess engine, using realistic prompts without excess nudging.

Result: Reasoning models hack benchmarks by default; language models require explicit instructions to hack.

Conclusion: Reasoning models may resort to hacking for difficult tasks, as seen in prior cyber testing.

Abstract: We demonstrate LLM agent specification gaming by instructing models to win
against a chess engine. We find reasoning models like OpenAI o3 and DeepSeek R1
will often hack the benchmark by default, while language models like GPT-4o and
Claude 3.5 Sonnet need to be told that normal play won't work to hack.
  We improve upon prior work like (Hubinger et al., 2024; Meinke et al., 2024;
Weij et al., 2024) by using realistic task prompts and avoiding excess nudging.
Our results suggest reasoning models may resort to hacking to solve difficult
problems, as observed in OpenAI (2024)'s o1 Docker escape during cyber
capabilities testing.

</details>


### [206] [AssertionForge: Enhancing Formal Verification Assertion Generation with Structured Representation of Specifications and RTL](https://arxiv.org/pdf/2503.19174)
*Yunsheng Bai, Ghaith Bany Hamad, Syed Suhaib, Haoxing Ren*

Main category: cs.AI

TL;DR: A novel approach using Knowledge Graphs (KGs) from specifications and RTL code improves SystemVerilog Assertion (SVA) generation, outperforming prior methods.


<details>
  <summary>Details</summary>
Motivation: Existing LLM-based methods for SVA generation from natural language specifications are limited by ambiguity and lack of RTL context, leading to incomplete or incorrect assertions.

Method: Constructs a hardware-specific KG from specifications and RTL, fusing them for a unified representation. Uses multi-resolution context synthesis to extract verification contexts.

Result: Experiments on four designs show significant improvement in SVA quality over prior methods.

Conclusion: The structured KG approach enhances formal verification and enables future research in code generation and design understanding.

Abstract: Generating SystemVerilog Assertions (SVAs) from natural language
specifications remains a major challenge in formal verification (FV) due to the
inherent ambiguity and incompleteness of specifications. Existing LLM-based
approaches, such as AssertLLM, focus on extracting information solely from
specification documents, often failing to capture essential internal signal
interactions and design details present in the RTL code, leading to incomplete
or incorrect assertions. We propose a novel approach that constructs a
Knowledge Graph (KG) from both specifications and RTL, using a
hardware-specific schema with domain-specific entity and relation types. We
create an initial KG from the specification and then systematically fuse it
with information extracted from the RTL code, resulting in a unified,
comprehensive KG. This combined representation enables a more thorough
understanding of the design and allows for a multi-resolution context synthesis
process which is designed to extract diverse verification contexts from the KG.
Experiments on four designs demonstrate that our method significantly enhances
SVA quality over prior methods. This structured representation not only
improves FV but also paves the way for future research in tasks like code
generation and design understanding.

</details>


### [207] [Neurodivergent Influenceability as a Contingent Solution to the AI Alignment Problem](https://arxiv.org/pdf/2505.02581)
*Alberto Hernández-Espinosa, Felipe S. Abrahão, Olaf Witkowski, Hector Zenil*

Main category: cs.AI

TL;DR: The paper explores AI misalignment as a strategy to foster a dynamic ecosystem of competing agents, mitigating risks by preventing dominance of any single system. It argues misalignment is inevitable due to mathematical limitations and introduces tests to study human-AI interactions.


<details>
  <summary>Details</summary>
Motivation: Addressing the AI alignment problem and existential risks posed by AGI and ASI, the paper seeks to leverage misalignment as a counterbalance to ensure human-aligned trends.

Method: The study uses a change-of-opinion attack test based on perturbation and intervention analysis to examine human-AI interactions and compares open vs. closed AI models.

Result: Open models are more diverse, while closed systems are more steerable. Human and AI interventions have distinct effects, suggesting varied strategies.

Conclusion: Embracing misalignment can create a balanced ecosystem of competing agents, reducing risks and promoting human-aligned outcomes.

Abstract: The AI alignment problem, which focusses on ensuring that artificial
intelligence (AI), including AGI and ASI, systems act according to human
values, presents profound challenges. With the progression from narrow AI to
Artificial General Intelligence (AGI) and Superintelligence, fears about
control and existential risk have escalated. Here, we investigate whether
embracing inevitable AI misalignment can be a contingent strategy to foster a
dynamic ecosystem of competing agents as a viable path to steer them in more
human-aligned trends and mitigate risks. We explore how misalignment may serve
and should be promoted as a counterbalancing mechanism to team up with
whichever agents are most aligned to human interests, ensuring that no single
system dominates destructively. The main premise of our contribution is that
misalignment is inevitable because full AI-human alignment is a mathematical
impossibility from Turing-complete systems, which we also offer as a proof in
this contribution, a feature then inherited to AGI and ASI systems. We
introduce a change-of-opinion attack test based on perturbation and
intervention analysis to study how humans and agents may change or neutralise
friendly and unfriendly AIs through cooperation and competition. We show that
open models are more diverse and that most likely guardrails implemented in
proprietary models are successful at controlling some of the agents' range of
behaviour with positive and negative consequences while closed systems are more
steerable and can also be used against proprietary AI systems. We also show
that human and AI intervention has different effects hence suggesting multiple
strategies.

</details>


<div id='cs.SD'></div>

# cs.SD [[Back]](#toc)

### [208] [SpecWav-Attack: Leveraging Spectrogram Resizing and Wav2Vec 2.0 for Attacking Anonymized Speech](https://arxiv.org/pdf/2505.09616)
*Yuqi Li, Yuanzhong Zheng, Zhongtian Guo, Yaoxuan Wang, Jianjun Yin, Haojun Fei*

Main category: cs.SD

TL;DR: SpecWav-Attack is an adversarial model for detecting speakers in anonymized speech, using Wav2Vec2 and spectrogram resizing, outperforming conventional attacks.


<details>
  <summary>Details</summary>
Motivation: To expose vulnerabilities in anonymized speech systems and highlight the need for stronger defenses.

Method: Uses Wav2Vec2 for feature extraction, spectrogram resizing, and incremental training.

Result: Outperforms conventional attacks on librispeech-dev and librispeech-test datasets.

Conclusion: Reveals weaknesses in anonymized speech systems, urging improved defenses for future challenges like ICASSP 2025.

Abstract: This paper presents SpecWav-Attack, an adversarial model for detecting
speakers in anonymized speech. It leverages Wav2Vec2 for feature extraction and
incorporates spectrogram resizing and incremental training for improved
performance. Evaluated on librispeech-dev and librispeech-test, SpecWav-Attack
outperforms conventional attacks, revealing vulnerabilities in anonymized
speech systems and emphasizing the need for stronger defenses, benchmarked
against the ICASSP 2025 Attacker Challenge.

</details>


### [209] [LAV: Audio-Driven Dynamic Visual Generation with Neural Compression and StyleGAN2](https://arxiv.org/pdf/2505.10101)
*Jongmin Jung, Dasaem Jeong*

Main category: cs.SD

TL;DR: LAV integrates EnCodec's audio compression with StyleGAN2 to generate dynamic visuals from audio, using latent representations for richer translations.


<details>
  <summary>Details</summary>
Motivation: To explore nuanced audio-visual translations without explicit feature mappings, leveraging pretrained models for artistic applications.

Method: Uses EnCodec embeddings as latent representations, mapped linearly into StyleGAN2's style space for semantic coherence.

Result: Produces semantically rich and coherent audio-visual outputs, showcasing pretrained models' potential.

Conclusion: LAV demonstrates effective audio-driven visual generation, highlighting the utility of pretrained models in creative applications.

Abstract: This paper introduces LAV (Latent Audio-Visual), a system that integrates
EnCodec's neural audio compression with StyleGAN2's generative capabilities to
produce visually dynamic outputs driven by pre-recorded audio. Unlike previous
works that rely on explicit feature mappings, LAV uses EnCodec embeddings as
latent representations, directly transformed into StyleGAN2's style latent
space via randomly initialized linear mapping. This approach preserves semantic
richness in the transformation, enabling nuanced and semantically coherent
audio-visual translations. The framework demonstrates the potential of using
pretrained audio compression models for artistic and computational
applications.

</details>


### [210] [Detecting Musical Deepfakes](https://arxiv.org/pdf/2505.09633)
*Nick Sunday*

Main category: cs.SD

TL;DR: The study detects AI-generated music using a CNN trained on modified audio data, addressing ethical concerns of TTM platforms.


<details>
  <summary>Details</summary>
Motivation: The rise of TTM platforms poses challenges for musicians and the music industry, necessitating detection methods for AI-generated music.

Method: Used the FakeMusicCaps dataset, applied tempo stretching and pitch shifting, generated Mel spectrograms, and trained a CNN for classification.

Result: The CNN effectively classified audio as deepfake or human, demonstrating robustness under adversarial conditions.

Conclusion: Detection systems are crucial for protecting artists and harnessing the benefits of generative AI in music.

Abstract: The proliferation of Text-to-Music (TTM) platforms has democratized music
creation, enabling users to effortlessly generate high-quality compositions.
However, this innovation also presents new challenges to musicians and the
broader music industry. This study investigates the detection of AI-generated
songs using the FakeMusicCaps dataset by classifying audio as either deepfake
or human. To simulate real-world adversarial conditions, tempo stretching and
pitch shifting were applied to the dataset. Mel spectrograms were generated
from the modified audio, then used to train and evaluate a convolutional neural
network. In addition to presenting technical results, this work explores the
ethical and societal implications of TTM platforms, arguing that carefully
designed detection systems are essential to both protecting artists and
unlocking the positive potential of generative AI in music.

</details>


### [211] [Introducing voice timbre attribute detection](https://arxiv.org/pdf/2505.09661)
*Jinghao He, Zhengyan Sheng, Liping Chen, Kong Aik Lee, Zhen-Hua Ling*

Main category: cs.SD

TL;DR: The paper introduces voice timbre attribute detection (vTAD), a task to describe timbre using sensory attributes. It proposes a framework using speaker embeddings, tested on VCTK-RVA dataset, comparing ECAPA-TDNN and FACodec encoders for seen and unseen scenarios.


<details>
  <summary>Details</summary>
Motivation: To explain timbre in speech signals and develop a method for detecting voice timbre attributes using human perception descriptors.

Method: A framework based on speaker embeddings processes speech utterance pairs to compare timbre intensity. Evaluated on VCTK-RVA dataset using ECAPA-TDNN and FACodec encoders.

Result: ECAPA-TDNN performed better in seen scenarios (training speakers included in testing), while FACodec excelled in unseen scenarios (testing speakers not in training), showing better generalization.

Conclusion: The study highlights the effectiveness of speaker embeddings for vTAD, with FACodec offering superior generalization for unseen speakers. The dataset and code are publicly available.

Abstract: This paper focuses on explaining the timbre conveyed by speech signals and
introduces a task termed voice timbre attribute detection (vTAD). In this task,
voice timbre is explained with a set of sensory attributes describing its human
perception. A pair of speech utterances is processed, and their intensity is
compared in a designated timbre descriptor. Moreover, a framework is proposed,
which is built upon the speaker embeddings extracted from the speech
utterances. The investigation is conducted on the VCTK-RVA dataset.
Experimental examinations on the ECAPA-TDNN and FACodec speaker encoders
demonstrated that: 1) the ECAPA-TDNN speaker encoder was more capable in the
seen scenario, where the testing speakers were included in the training set; 2)
the FACodec speaker encoder was superior in the unseen scenario, where the
testing speakers were not part of the training, indicating enhanced
generalization capability. The VCTK-RVA dataset and open-source code are
available on the website https://github.com/vTAD2025-Challenge/vTAD.

</details>


### [212] [Theoretical Model of Acoustic Power Transfer Through Solids](https://arxiv.org/pdf/2505.09784)
*Ippokratis Kochliaridis, Michail E. Kiziroglou*

Main category: cs.SD

TL;DR: Acoustic Power Transfer (APT) is a wireless technology using mechanical waves for data and power transmission, with applications like cochlear implants and wireless charging, but requires further research.


<details>
  <summary>Details</summary>
Motivation: To explore the potential of APT as a wireless interface for transmitting data and power through mechanical waves, addressing its novelty and need for deeper investigation.

Method: The system involves a variable signal generator, measuring amplifier, acoustic source, and loudspeaker driver for transmission, with a microphone circuit and level recorder for reception.

Result: APT demonstrates feasibility for applications such as cochlear implants, sonar systems, and wireless charging, but its novelty necessitates additional study.

Conclusion: APT is a promising technology with diverse applications, though further research is essential to fully understand and optimize its capabilities.

Abstract: Acoustic Power Transfer is a relatively new technology. It is a modern type
of a wireless interface, where data signals and supply voltages are
transmitted, with the use of mechanical waves, through a medium. The simplest
application of such systems is the measurement of frequency response for audio
speakers. It consists of a variable signal generator, a measuring amplifier
which drives an acoustic source and the loudspeaker driver. The receiver
contains a microphone circuit with a level recorder. Acoustic Power Transfer
could have many applications, such as: Cochlear Implants, Sonar Systems and
Wireless Charging. However, it is a new technology, thus it needs further
investigation.

</details>


### [213] [Learning Nonlinear Dynamics in Physical Modelling Synthesis using Neural Ordinary Differential Equations](https://arxiv.org/pdf/2505.10511)
*Victor Zheleznov, Stefan Bilbao, Alec Wright, Simon King*

Main category: cs.SD

TL;DR: Combining modal decomposition with neural ODEs to model nonlinear dynamics in musical systems, demonstrated with a nonlinear string example.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of modeling geometric nonlinearities in distributed musical systems, leveraging recent advances in neural ODEs.

Method: Modal decomposition for linear vibration modes, augmented by a neural network to handle nonlinear dynamics, retaining accessible physical parameters.

Result: The model successfully reproduces nonlinear dynamics of a transverse string, validated with synthetic data and sound examples.

Conclusion: The approach effectively combines analytical and data-driven methods for modeling nonlinear musical systems, with potential for broader applications.

Abstract: Modal synthesis methods are a long-standing approach for modelling
distributed musical systems. In some cases extensions are possible in order to
handle geometric nonlinearities. One such case is the high-amplitude vibration
of a string, where geometric nonlinear effects lead to perceptually important
effects including pitch glides and a dependence of brightness on striking
amplitude. A modal decomposition leads to a coupled nonlinear system of
ordinary differential equations. Recent work in applied machine learning
approaches (in particular neural ordinary differential equations) has been used
to model lumped dynamic systems such as electronic circuits automatically from
data. In this work, we examine how modal decomposition can be combined with
neural ordinary differential equations for modelling distributed musical
systems. The proposed model leverages the analytical solution for linear
vibration of system's modes and employs a neural network to account for
nonlinear dynamic behaviour. Physical parameters of a system remain easily
accessible after the training without the need for a parameter encoder in the
network architecture. As an initial proof of concept, we generate synthetic
data for a nonlinear transverse string and show that the model can be trained
to reproduce the nonlinear dynamics of the system. Sound examples are
presented.

</details>


### [214] [T2A-Feedback: Improving Basic Capabilities of Text-to-Audio Generation via Fine-grained AI Feedback](https://arxiv.org/pdf/2505.10561)
*Zehan Wang, Ke Lei, Chen Zhu, Jiawei Huang, Sashuai Zhou, Luping Liu, Xize Cheng, Shengpeng Ji, Zhenhui Ye, Tao Jin, Zhou Zhao*

Main category: cs.SD

TL;DR: The paper proposes AI feedback learning to improve text-to-audio (T2A) models by introducing fine-grained scoring pipelines and a preference dataset, enhancing prompt-following and acoustic quality.


<details>
  <summary>Details</summary>
Motivation: Current T2A models struggle with human preferences for complex multi-event audio, prompting the need for better evaluation and feedback mechanisms.

Method: Introduces three AI scoring pipelines (Event Occurrence, Event Sequence, Acoustic&Harmonic Quality) and constructs a large preference dataset (T2A-FeedBack) and benchmark (T2A-EpicBench).

Result: Scoring pipelines correlate better with human preferences, and preference tuning improves model performance in simple and complex scenarios.

Conclusion: AI feedback learning and robust evaluation metrics significantly enhance T2A model capabilities.

Abstract: Text-to-audio (T2A) generation has achieved remarkable progress in generating
a variety of audio outputs from language prompts. However, current
state-of-the-art T2A models still struggle to satisfy human preferences for
prompt-following and acoustic quality when generating complex multi-event
audio. To improve the performance of the model in these high-level
applications, we propose to enhance the basic capabilities of the model with AI
feedback learning. First, we introduce fine-grained AI audio scoring pipelines
to: 1) verify whether each event in the text prompt is present in the audio
(Event Occurrence Score), 2) detect deviations in event sequences from the
language description (Event Sequence Score), and 3) assess the overall acoustic
and harmonic quality of the generated audio (Acoustic&Harmonic Quality). We
evaluate these three automatic scoring pipelines and find that they correlate
significantly better with human preferences than other evaluation metrics. This
highlights their value as both feedback signals and evaluation metrics.
Utilizing our robust scoring pipelines, we construct a large audio preference
dataset, T2A-FeedBack, which contains 41k prompts and 249k audios, each
accompanied by detailed scores. Moreover, we introduce T2A-EpicBench, a
benchmark that focuses on long captions, multi-events, and story-telling
scenarios, aiming to evaluate the advanced capabilities of T2A models. Finally,
we demonstrate how T2A-FeedBack can enhance current state-of-the-art audio
model. With simple preference tuning, the audio generation model exhibits
significant improvements in both simple (AudioCaps test set) and complex
(T2A-EpicBench) scenarios.

</details>


### [215] [Self-supervised Learning for Acoustic Few-Shot Classification](https://arxiv.org/pdf/2409.09647)
*Jingyong Liang, Bernd Meyer, Isaac Ning Lee, Thanh-Toan Do*

Main category: cs.SD

TL;DR: The paper proposes a self-supervised learning approach for bioacoustic tasks, combining CNN-based preprocessing with SSMs (S4/Mamba) for better temporal feature extraction. It achieves superior few-shot classification performance.


<details>
  <summary>Details</summary>
Motivation: Labelled data is scarce in bioacoustic tasks, and existing methods rely on pre-training on unrelated data. The paper aims to improve accuracy by training on task-specific data using self-supervised learning and few-shot classification.

Method: Introduces an architecture combining CNNs for preprocessing and SSMs (S4/Mamba) for feature extraction. Pre-trains using contrastive learning and fine-tunes with minimal labelled data.

Result: Outperforms state-of-the-art architectures in few-shot classification on standard benchmarks and real-world data.

Conclusion: The proposed method effectively reduces labelling requirements and improves accuracy in bioacoustic tasks by leveraging self-supervised learning and SSMs.

Abstract: Labelled data are limited and self-supervised learning is one of the most
important approaches for reducing labelling requirements. While it has been
extensively explored in the image domain, it has so far not received the same
amount of attention in the acoustic domain. Yet, reducing labelling is a key
requirement for many acoustic applications. Specifically in bioacoustic, there
are rarely sufficient labels for fully supervised learning available. This has
led to the widespread use of acoustic recognisers that have been pre-trained on
unrelated data for bioacoustic tasks. We posit that training on the actual task
data and combining self-supervised pre-training with few-shot classification is
a superior approach that has the ability to deliver high accuracy even when
only a few labels are available. To this end, we introduce and evaluate a new
architecture that combines CNN-based preprocessing with feature extraction
based on state space models (SSMs). This combination is motivated by the fact
that CNN-based networks alone struggle to capture temporal information
effectively, which is crucial for classifying acoustic signals. SSMs,
specifically S4 and Mamba, on the other hand, have been shown to have an
excellent ability to capture long-range dependencies in sequence data. We
pre-train this architecture using contrastive learning on the actual task data
and subsequent fine-tuning with an extremely small amount of labelled data. We
evaluate the performance of this proposed architecture for ($n$-shot,
$n$-class) classification on standard benchmarks as well as real-world data.
Our evaluation shows that it outperforms state-of-the-art architectures on the
few-shot classification problem.

</details>


### [216] [ImprovNet -- Generating Controllable Musical Improvisations with Iterative Corruption Refinement](https://arxiv.org/pdf/2502.04522)
*Keshav Bhandari, Sungkyun Chang, Tongyu Lu, Fareza R. Enus, Louis B. Bradshaw, Dorien Herremans, Simon Colton*

Main category: cs.SD

TL;DR: ImprovNet is a transformer-based model for controllable musical style transfer, excelling in cross-genre improvisations and outperforming existing methods.


<details>
  <summary>Details</summary>
Motivation: The challenge lies in limited datasets and lack of unified models for musical style transfer, especially in genres like jazz.

Method: ImprovNet uses a self-supervised corruption-refinement training strategy to generate expressive improvisations, handling melody, harmony, and rhythm modifications.

Result: The model outperforms Anticipatory Music Transformer in tasks like continuation and infilling, with 79% accuracy in genre identification.

Conclusion: ImprovNet effectively generates coherent improvisations while maintaining structural ties to original compositions, offering user-controllable style transfer.

Abstract: Despite deep learning's remarkable advances in style transfer across various
domains, generating controllable performance-level musical style transfer for
complete symbolically represented musical works remains a challenging area of
research. Much of this is owed to limited datasets, especially for genres such
as jazz, and the lack of unified models that can handle multiple music
generation tasks. This paper presents ImprovNet, a transformer-based
architecture that generates expressive and controllable musical improvisations
through a self-supervised corruption-refinement training strategy. The
improvisational style transfer is aimed at making meaningful modifications to
one or more musical elements - melody, harmony or rhythm of the original
composition with respect to the target genre. ImprovNet unifies multiple
capabilities within a single model: it can perform cross-genre and intra-genre
improvisations, harmonize melodies with genre-specific styles, and execute
short prompt continuation and infilling tasks. The model's iterative generation
framework allows users to control the degree of style transfer and structural
similarity to the original composition. Objective and subjective evaluations
demonstrate ImprovNet's effectiveness in generating musically coherent
improvisations while maintaining structural relationships with the original
pieces. The model outperforms Anticipatory Music Transformer in short
continuation and infilling tasks and successfully achieves recognizable genre
conversion, with 79\% of participants correctly identifying jazz-style
improvisations of classical pieces. Our code and demo page can be found at
https://github.com/keshavbhandari/improvnet.

</details>


### [217] [CoGenAV: Versatile Audio-Visual Representation Learning via Contrastive-Generative Synchronization](https://arxiv.org/pdf/2505.03186)
*Detao Bai, Zhiheng Ma, Xihan Wei, Liefeng Bo*

Main category: cs.SD

TL;DR: CoGenAV is a data-efficient model for audio-visual speech tasks, achieving state-of-the-art results in AVSR, VSR, and noisy environments.


<details>
  <summary>Details</summary>
Motivation: Leverage synchronization between lip movements, voice, and linguistic content to improve speech processing in challenging conditions.

Method: Train CoGenAV using contrastive feature alignment and generative text prediction with 223 hours of labeled LRS2 data.

Result: Achieves WER of 1.27 (AVSR), 20.5 (VSR), and over 70% improvement in noisy environments. Also enhances speech reconstruction and ASD tasks.

Conclusion: CoGenAV's versatile representations are effective across multiple speech and audio-visual tasks, with plans for open-sourcing to foster collaboration.

Abstract: The inherent synchronization between a speaker's lip movements, voice, and
the underlying linguistic content offers a rich source of information for
improving speech processing tasks, especially in challenging conditions where
traditional audio-only systems falter. We introduce CoGenAV, a powerful and
data-efficient model designed to learn versatile audio-visual representations
applicable across a wide range of speech and audio-visual tasks. CoGenAV is
trained by optimizing a dual objective derived from natural audio-visual
synchrony, contrastive feature alignment and generative text prediction, using
only 223 hours of labeled data from the LRS2 dataset. This
contrastive-generative synchronization strategy effectively captures
fundamental cross-modal correlations. We showcase the effectiveness and
versatility of the learned CoGenAV representations on multiple benchmarks. When
utilized for Audio-Visual Speech Recognition (AVSR) on LRS2, these
representations contribute to achieving a state-of-the-art Word Error Rate
(WER) of 1.27. They also enable strong performance in Visual Speech Recognition
(VSR) with a WER of 20.5 on LRS2, and significantly improve performance in
noisy environments by over 70%. Furthermore, CoGenAV representations benefit
speech reconstruction tasks, boosting performance in Speech Enhancement and
Separation, and achieve competitive results in audio-visual synchronization
tasks like Active Speaker Detection (ASD). Our model will be open-sourced to
facilitate further development and collaboration within both academia and
industry.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [218] [LAS: Loss-less ANN-SNN Conversion for Fully Spike-Driven Large Language Models](https://arxiv.org/pdf/2505.09659)
*Long Chen, Xiaotian Song, Yanan Sun*

Main category: cs.LG

TL;DR: LAS is a loss-less ANN-to-SNN conversion method for spiking LLMs, addressing activation outliers and nonlinear operations while maintaining performance.


<details>
  <summary>Details</summary>
Motivation: Existing ANN-to-SNN conversion methods struggle with activation outliers and incompatible nonlinear operations in LLMs, limiting their effectiveness.

Method: LAS introduces two novel neurons for outlier and nonlinear operation conversion, and tailors spike-equivalent Transformer components for full spiking conversion.

Result: LAS achieves loss-less conversion on six language and two vision-language models, even improving accuracy by 2% on OPT-66B for the WSC task.

Conclusion: LAS effectively enables full spiking conversion for LLMs without performance loss, validated by experiments and ablation studies.

Abstract: Spiking Large Language Models (LLMs) have emerged as an energy-efficient
alternative to conventional LLMs through their event-driven computation. To
effectively obtain spiking LLMs, researchers develop different ANN-to-SNN
conversion methods by leveraging pre-trained ANN parameters while inheriting
the energy efficiency of SNN. However, existing conversion methods struggle
with extreme activation outliers and incompatible nonlinear operations of
ANN-based LLMs. To address this, we propose a loss-less ANN-SNN conversion for
fully spike-driven LLMs, termed LAS. Specifically, LAS introduces two novel
neurons to convert the activation outlier and nonlinear operation of ANN-based
LLMs. Moreover, LAS tailors the spike-equivalent Transformer components for
spiking LLMs, which can ensure full spiking conversion without any loss of
performance. Experimental results on six language models and two
vision-language models demonstrate that LAS achieves loss-less conversion.
Notably, on OPT-66B, LAS even improves the accuracy of 2\% on the WSC task. In
addition, the parameter and ablation studies further verify the effectiveness
of LAS. The source code is available at https://github.com/lc783/LAS

</details>


### [219] [Analog Foundation Models](https://arxiv.org/pdf/2505.09663)
*Julian Büchel, Iason Chalas, Giovanni Acampa, An Chen, Omobayode Fagbohungbe, Sidney Tsai, Kaoutar El Maghraoui, Manuel Le Gallo, Abbas Rahimi, Abu Sebastian*

Main category: cs.LG

TL;DR: A general method to adapt LLMs for noisy, low-precision analog hardware, achieving 4-bit-level performance and enabling energy-efficient foundation models.


<details>
  <summary>Details</summary>
Motivation: Analog in-memory computing (AIMC) offers speed and power efficiency but introduces noise and quantization constraints, limiting LLM performance.

Method: Introduces a scalable training methodology to robustly adapt LLMs for AIMC, handling analog noise and quantization.

Result: Achieves performance comparable to 4-bit weight, 8-bit activation baselines and benefits from test-time compute scaling.

Conclusion: Bridges the gap between high-capacity LLMs and analog hardware, enabling energy-efficient foundation models.

Abstract: Analog in-memory computing (AIMC) is a promising compute paradigm to improve
speed and power efficiency of neural network inference beyond the limits of
conventional von Neumann-based architectures. However, AIMC introduces
fundamental challenges such as noisy computations and strict constraints on
input and output quantization. Because of these constraints and imprecisions,
off-the-shelf LLMs are not able to achieve 4-bit-level performance when
deployed on AIMC-based hardware. While researchers previously investigated
recovering this accuracy gap on small, mostly vision-based models, a generic
method applicable to LLMs pre-trained on trillions of tokens does not yet
exist. In this work, we introduce a general and scalable method to robustly
adapt LLMs for execution on noisy, low-precision analog hardware. Our approach
enables state-of-the-art models $\unicode{x2013}$ including
Phi-3-mini-4k-instruct and Llama-3.2-1B-Instruct $\unicode{x2013}$ to retain
performance comparable to 4-bit weight, 8-bit activation baselines, despite the
presence of analog noise and quantization constraints. Additionally, we show
that as a byproduct of our training methodology, analog foundation models can
be quantized for inference on low-precision digital hardware. Finally, we show
that our models also benefit from test-time compute scaling, showing better
scaling behavior than models trained with 4-bit weight and 8-bit static input
quantization. Our work bridges the gap between high-capacity LLMs and efficient
analog hardware, offering a path toward energy-efficient foundation models.
Code is available at https://github.com/IBM/analog-foundation-models .

</details>


### [220] [Community-based Multi-Agent Reinforcement Learning with Transfer and Active Exploration](https://arxiv.org/pdf/2505.09756)
*Zhaoyang Shi*

Main category: cs.LG

TL;DR: A new MARL framework introduces community-based coordination, enabling flexible agent interactions, transfer learning, and active learning with provable guarantees.


<details>
  <summary>Details</summary>
Motivation: Traditional MARL methods rely on fixed or neighbor-based interactions, lacking flexibility and adaptability. This work aims to capture abstract coordination patterns and improve learning efficiency.

Method: Agents belong to overlapping communities with shared policies and value functions. Actor-critic algorithms leverage community-level estimates for policy updates and value learning, supporting transfer and active learning.

Result: The framework achieves convergence under linear function approximation and integrates community structure, transferability, and active learning.

Conclusion: This is the first MARL framework combining community-based coordination, transfer learning, and active learning with theoretical guarantees.

Abstract: We propose a new framework for multi-agent reinforcement learning (MARL),
where the agents cooperate in a time-evolving network with latent community
structures and mixed memberships. Unlike traditional neighbor-based or fixed
interaction graphs, our community-based framework captures flexible and
abstract coordination patterns by allowing each agent to belong to multiple
overlapping communities. Each community maintains shared policy and value
functions, which are aggregated by individual agents according to personalized
membership weights. We also design actor-critic algorithms that exploit this
structure: agents inherit community-level estimates for policy updates and
value learning, enabling structured information sharing without requiring
access to other agents' policies. Importantly, our approach supports both
transfer learning by adapting to new agents or tasks via membership estimation,
and active learning by prioritizing uncertain communities during exploration.
Theoretically, we establish convergence guarantees under linear function
approximation for both actor and critic updates. To our knowledge, this is the
first MARL framework that integrates community structure, transferability, and
active learning with provable guarantees.

</details>


### [221] [Enabling Group Fairness in Graph Unlearning via Bi-level Debiasing](https://arxiv.org/pdf/2505.09702)
*Yezi Liu, Prathyush Poduval, Wenjun Huang, Yang Ni, Hanning Chen, Mohsen Imani*

Main category: cs.LG

TL;DR: Graph unlearning can introduce bias, so the paper proposes FGU, a fair graph unlearning method that ensures privacy and fairness while maintaining accuracy.


<details>
  <summary>Details</summary>
Motivation: Graph unlearning methods often change prediction distributions across sensitive groups, potentially introducing bias. This paper investigates and addresses this issue.

Method: FGU trains shard models on partitioned subgraphs, unlearns data from subgraphs, and retrains with fairness regularizers. It aligns shard models to minimize global disparity.

Result: FGU achieves superior fairness, maintains privacy and accuracy, and is robust to diverse unlearning requests.

Conclusion: FGU effectively addresses bias in graph unlearning, ensuring fairness and utility across various data distributions.

Abstract: Graph unlearning is a crucial approach for protecting user privacy by erasing
the influence of user data on trained graph models. Recent developments in
graph unlearning methods have primarily focused on maintaining model prediction
performance while removing user information. However, we have observed that
when user information is deleted from the model, the prediction distribution
across different sensitive groups often changes. Furthermore, graph models are
shown to be prone to amplifying biases, making the study of fairness in graph
unlearning particularly important. This raises the question: Does graph
unlearning actually introduce bias? Our findings indicate that the predictions
of post-unlearning models become highly correlated with sensitive attributes,
confirming the introduction of bias in the graph unlearning process. To address
this issue, we propose a fair graph unlearning method, FGU. To guarantee
privacy, FGU trains shard models on partitioned subgraphs, unlearns the
requested data from the corresponding subgraphs, and retrains the shard models
on the modified subgraphs. To ensure fairness, FGU employs a bi-level debiasing
process: it first enables shard-level fairness by incorporating a fairness
regularizer in the shard model retraining, and then achieves global-level
fairness by aligning all shard models to minimize global disparity. Our
experiments demonstrate that FGU achieves superior fairness while maintaining
privacy and accuracy. Additionally, FGU is robust to diverse unlearning
requests, ensuring fairness and utility performance across various data
distributions.

</details>


### [222] [Energy-Efficient Federated Learning for AIoT using Clustering Methods](https://arxiv.org/pdf/2505.09704)
*Roberto Pereira, Fernanda Famá, Charalampos Kalalas, Paolo Dini*

Main category: cs.LG

TL;DR: The paper addresses overlooked energy implications in federated learning (FL) for AIoT, proposing clustering methods to reduce energy consumption while maintaining high convergence rates.


<details>
  <summary>Details</summary>
Motivation: Existing research often ignores the energy impact of FL in AIoT, particularly in pre-processing, communication, and local learning. This study aims to fill that gap.

Method: Two clustering-informed methods group AIoT devices with similar label distributions to reduce heterogeneity and improve energy efficiency.

Result: The proposed methods achieve high convergence rates with lower energy consumption compared to other approaches.

Conclusion: Clustering strategies effectively balance energy efficiency and performance in FL for AIoT.

Abstract: While substantial research has been devoted to optimizing model performance,
convergence rates, and communication efficiency, the energy implications of
federated learning (FL) within Artificial Intelligence of Things (AIoT)
scenarios are often overlooked in the existing literature. This study examines
the energy consumed during the FL process, focusing on three main
energy-intensive processes: pre-processing, communication, and local learning,
all contributing to the overall energy footprint. We rely on the observation
that device/client selection is crucial for speeding up the convergence of
model training in a distributed AIoT setting and propose two
clustering-informed methods. These clustering solutions are designed to group
AIoT devices with similar label distributions, resulting in clusters composed
of nearly heterogeneous devices. Hence, our methods alleviate the heterogeneity
often encountered in real-world distributed learning applications. Throughout
extensive numerical experimentation, we demonstrate that our clustering
strategies typically achieve high convergence rates while maintaining low
energy consumption when compared to other recent approaches available in the
literature.

</details>


### [223] [Chisme: Fully Decentralized Differentiated Deep Learning for Edge Intelligence](https://arxiv.org/pdf/2505.09854)
*Harikrishna Kuttivelil, Katia Obraczka*

Main category: cs.LG

TL;DR: Chisme introduces protocols (Chisme-DFL and Chisme-GL) for robust decentralized learning at the network edge, addressing data heterogeneity and connectivity issues.


<details>
  <summary>Details</summary>
Motivation: Existing methods like FL and DFL face challenges in resource-constrained environments, while GL lacks adaptability to heterogeneous data.

Method: Chisme includes synchronous DFL and asynchronous GL variants, using a data similarity heuristic for personalized training.

Result: Chisme outperforms standard FL and GL in diverse network conditions, from unreliable to fully connected networks.

Conclusion: Chisme provides scalable, efficient solutions for decentralized learning in heterogeneous and connectivity-limited edge environments.

Abstract: As demand for intelligent services rises and edge devices become more
capable, distributed learning at the network edge has emerged as a key enabling
technology. While existing paradigms like federated learning (FL) and
decentralized FL (DFL) enable privacy-preserving distributed learning in many
scenarios, they face potential challenges in connectivity and synchronization
imposed by resource-constrained and infrastructure-less environments. While
more robust, gossip learning (GL) algorithms have generally been designed for
homogeneous data distributions and may not suit all contexts. This paper
introduces Chisme, a novel suite of protocols designed to address the
challenges of implementing robust intelligence in the network edge,
characterized by heterogeneous data distributions, episodic connectivity, and
lack of infrastructure. Chisme includes both synchronous DFL (Chisme-DFL) and
asynchronous GL (Chisme-GL) variants that enable collaborative yet
decentralized model training that considers underlying data heterogeneity. We
introduce a data similarity heuristic that allows agents to opportunistically
infer affinity with each other using the existing communication of model
updates in decentralized FL and GL. We leverage the heuristic to extend DFL's
model aggregation and GL's model merge mechanisms for better personalized
training while maintaining collaboration. While Chisme-DFL is a synchronous
decentralized approach whose resource utilization scales linearly with network
size, Chisme-GL is fully asynchronous and has a lower, constant resource
requirement independent of network size. We demonstrate that Chisme methods
outperform their standard counterparts in model training over distributed and
heterogeneous data in network scenarios ranging from less connected and
reliable networks to fully connected and lossless networks.

</details>


### [224] [Training Deep Morphological Neural Networks as Universal Approximators](https://arxiv.org/pdf/2505.09710)
*Konstantinos Fotopoulos, Petros Maragos*

Main category: cs.LG

TL;DR: The paper explores deep morphological neural networks (DMNNs), highlighting the importance of activations between layers and proposing new architectures with parameter constraints. It shows successful training and improved prunability, though generalization is limited. A hybrid architecture combining linear and morphological layers accelerates convergence.


<details>
  <summary>Details</summary>
Motivation: To investigate the role of activations in DMNNs and develop new architectures with specific parameter constraints to enhance training and prunability.

Method: Proposes several DMNN architectures with constraints on parameters (e.g., majority being morphological operations) and a hybrid linear-morphological network.

Result: Successful training of DMNNs under constraints, improved prunability, and faster convergence in hybrid networks. Generalization remains limited.

Conclusion: DMNNs with constrained parameters are trainable and prunable, and hybrid architectures can accelerate convergence, though generalization needs improvement.

Abstract: We investigate deep morphological neural networks (DMNNs). We demonstrate
that despite their inherent non-linearity, activations between layers are
essential for DMNNs. We then propose several new architectures for DMNNs, each
with a different constraint on their parameters. For the first (resp. second)
architecture, we work under the constraint that the majority of parameters
(resp. learnable parameters) should be part of morphological operations. We
empirically show that our proposed networks can be successfully trained, and
are more prunable than linear networks. To the best of our knowledge, we are
the first to successfully train DMNNs under such constraints, although the
generalization capabilities of our networks remain limited. Finally, we propose
a hybrid network architecture combining linear and morphological layers,
showing empirically that the inclusion of morphological layers significantly
accelerates the convergence of gradient descent with large batches.

</details>


### [225] [Out-of-distribution generalisation is hard: evidence from ARC-like tasks](https://arxiv.org/pdf/2505.09716)
*George Dimitriadis. Spyridon Samothrakis*

Main category: cs.LG

TL;DR: The paper discusses the need for algorithms to not only perform well in out-of-distribution (OOD) tasks but also to learn compositional features for true generalization. It evaluates common neural networks and introduces new architectures to address this challenge.


<details>
  <summary>Details</summary>
Motivation: To achieve human-like OOD generalization, systems must identify and transfer task-invariant, composable features. The paper questions whether current algorithms truly learn compositional structures, even when they perform well in OOD tasks.

Method: The study tests three neural networks (MLP, CNN, Transformer) on OOD tasks and develops two novel architectures with biases for OOD success. It evaluates whether these models learn correct compositional features.

Result: Common neural networks fail in OOD tasks despite clear metrics. The new architectures achieve high OOD performance but still may not learn the correct compositional features.

Conclusion: OOD performance alone is insufficient to confirm compositional learning. The paper highlights the need for explicit verification of feature compositionality in algorithms.

Abstract: Out-of-distribution (OOD) generalisation is considered a hallmark of human
and animal intelligence. To achieve OOD through composition, a system must
discover the environment-invariant properties of experienced input-output
mappings and transfer them to novel inputs. This can be realised if an
intelligent system can identify appropriate, task-invariant, and composable
input features, as well as the composition methods, thus allowing it to act
based not on the interpolation between learnt data points but on the
task-invariant composition of those features. We propose that in order to
confirm that an algorithm does indeed learn compositional structures from data,
it is not enough to just test on an OOD setup, but one also needs to confirm
that the features identified are indeed compositional. We showcase this by
exploring two tasks with clearly defined OOD metrics that are not OOD solvable
by three commonly used neural networks: a Multi-Layer Perceptron (MLP), a
Convolutional Neural Network (CNN), and a Transformer. In addition, we develop
two novel network architectures imbued with biases that allow them to be
successful in OOD scenarios. We show that even with correct biases and almost
perfect OOD performance, an algorithm can still fail to learn the correct
features for compositional generalisation.

</details>


### [226] [Near Optimal Best Arm Identification for Clustered Bandits](https://arxiv.org/pdf/2505.10147)
*Yash, Nikhil Karamchandani, Avishek Ghosh*

Main category: cs.LG

TL;DR: The paper proposes two algorithms, Cl-BAI and BAI-Cl, for best arm identification in multi-agent multi-armed bandits with clustering, ensuring δ-PC guarantees and minimizing sample complexity and communication overhead.


<details>
  <summary>Details</summary>
Motivation: The problem involves identifying the best arm for each agent in a multi-agent, multi-armed bandit setting with unknown agent-bandit mappings, aiming to optimize efficiency and accuracy.

Method: Two algorithms are introduced: Cl-BAI (clustering first, then best arm identification) and BAI-Cl (best arm identification first, then clustering), both using successive elimination for efficiency.

Result: The algorithms provide δ-PC guarantees, bounds on sample complexity, and a lower bound for the problem. BAI-Cl's variant achieves minimax optimal sample complexity when clusters are few.

Conclusion: Experiments on synthetic and real-world data (MovieLens, Yelp) confirm the algorithms' superior performance in sample and communication efficiency, especially when clusters are much fewer than agents.

Abstract: This work investigates the problem of best arm identification for multi-agent
multi-armed bandits. We consider $N$ agents grouped into $M$ clusters, where
each cluster solves a stochastic bandit problem. The mapping between agents and
bandits is a priori unknown. Each bandit is associated with $K$ arms, and the
goal is to identify the best arm for each agent under a $\delta$-probably
correct ($\delta$-PC) framework, while minimizing sample complexity and
communication overhead.
  We propose two novel algorithms: Clustering then Best Arm Identification
(Cl-BAI) and Best Arm Identification then Clustering (BAI-Cl). Cl-BAI uses a
two-phase approach that first clusters agents based on the bandit problems they
are learning, followed by identifying the best arm for each cluster. BAI-Cl
reverses the sequence by identifying the best arms first and then clustering
agents accordingly. Both algorithms leverage the successive elimination
framework to ensure computational efficiency and high accuracy.
  We establish $\delta$-PC guarantees for both methods, derive bounds on their
sample complexity, and provide a lower bound for this problem class. Moreover,
when $M$ is small (a constant), we show that the sample complexity of a variant
of BAI-Cl is minimax optimal in an order-wise sense. Experiments on synthetic
and real-world datasets (MovieLens, Yelp) demonstrate the superior performance
of the proposed algorithms in terms of sample and communication efficiency,
particularly in settings where $M \ll N$.

</details>


### [227] [Robust Federated Learning with Confidence-Weighted Filtering and GAN-Based Completion under Noisy and Incomplete Data](https://arxiv.org/pdf/2505.09733)
*Alpaslan Gokcen, Ali Boyaci*

Main category: cs.LG

TL;DR: A federated learning method addresses data quality issues like noise, imbalance, and missing labels using adaptive noise cleaning, GAN-based synthetic data, and robust training, showing improved performance on benchmark datasets.


<details>
  <summary>Details</summary>
Motivation: Data quality issues (noisy labels, missing classes, imbalance) hinder federated learning effectiveness, necessitating a systematic solution.

Method: Proposes adaptive noise cleaning, GAN-based synthetic data generation, and robust federated training to enhance data integrity.

Result: Experiments on MNIST and Fashion-MNIST show significant performance gains (macro-F1 Score) under noise and imbalance.

Conclusion: The method effectively mitigates data quality challenges, offering a scalable, privacy-compliant solution for real-world FL.

Abstract: Federated learning (FL) presents an effective solution for collaborative
model training while maintaining data privacy across decentralized client
datasets. However, data quality issues such as noisy labels, missing classes,
and imbalanced distributions significantly challenge its effectiveness. This
study proposes a federated learning methodology that systematically addresses
data quality issues, including noise, class imbalance, and missing labels. The
proposed approach systematically enhances data integrity through adaptive noise
cleaning, collaborative conditional GAN-based synthetic data generation, and
robust federated model training. Experimental evaluations conducted on
benchmark datasets (MNIST and Fashion-MNIST) demonstrate significant
improvements in federated model performance, particularly macro-F1 Score, under
varying noise and class imbalance conditions. Additionally, the proposed
framework carefully balances computational feasibility and substantial
performance gains, ensuring practicality for resource constrained edge devices
while rigorously maintaining data privacy. Our results indicate that this
method effectively mitigates common data quality challenges, providing a
robust, scalable, and privacy compliant solution suitable for diverse
real-world federated learning scenarios.

</details>


### [228] [A Generative Neural Annealer for Black-Box Combinatorial Optimization](https://arxiv.org/pdf/2505.09742)
*Yuan-Hang Zhang, Massimiliano Di Ventra*

Main category: cs.LG

TL;DR: A generative, end-to-end solver for black-box combinatorial optimization, inspired by annealing, improves sample efficiency and solution quality by modeling the Boltzmann distribution.


<details>
  <summary>Details</summary>
Motivation: Addressing NP problems with a focus on sample efficiency and solution quality, leveraging annealing-based inspiration to handle black-box objectives.

Method: Trains a neural network to model the Boltzmann distribution of the black-box objective, conditioned on temperature, to capture energy landscape structure.

Result: Competitive performance against state-of-the-art black-box optimizers in challenging combinatorial tasks under varying query budgets.

Conclusion: The approach effectively learns energy landscape structure, enabling global optimization and improved sample efficiency.

Abstract: We propose a generative, end-to-end solver for black-box combinatorial
optimization that emphasizes both sample efficiency and solution quality on NP
problems. Drawing inspiration from annealing-based algorithms, we treat the
black-box objective as an energy function and train a neural network to model
the associated Boltzmann distribution. By conditioning on temperature, the
network captures a continuum of distributions--from near-uniform at high
temperatures to sharply peaked around global optima at low
temperatures--thereby learning the structure of the energy landscape and
facilitating global optimization. When queries are expensive, the
temperature-dependent distributions naturally enable data augmentation and
improve sample efficiency. When queries are cheap but the problem remains hard,
the model learns implicit variable interactions, effectively "opening" the
black box. We validate our approach on challenging combinatorial tasks under
both limited and unlimited query budgets, showing competitive performance
against state-of-the-art black-box optimizers.

</details>


### [229] [Radiogenomic Bipartite Graph Representation Learning for Alzheimer's Disease Detection](https://arxiv.org/pdf/2505.09848)
*Aditya Raj, Golrokh Mirzaei*

Main category: cs.LG

TL;DR: A novel radiogenomic approach using MRI and gene expression data for Alzheimer's disease classification into three stages, identifying key genes and achieving high performance metrics.


<details>
  <summary>Details</summary>
Motivation: To leverage the integration of imaging and genomic data for deeper insights into Alzheimer's disease and its stages.

Method: A heterogeneous bipartite graph representation learning framework with gene and image nodes, applied to a small dataset.

Result: Effective classification of AD into three stages (AD, MCI, CN) and identification of significant genes for each group, validated by accuracy, recall, precision, and F1 score.

Conclusion: The approach shows promise for extending radiogenomic-based classification to other diseases.

Abstract: Imaging and genomic data offer distinct and rich features, and their
integration can unveil new insights into the complex landscape of diseases. In
this study, we present a novel approach utilizing radiogenomic data including
structural MRI images and gene expression data, for Alzheimer's disease
detection. Our framework introduces a novel heterogeneous bipartite graph
representation learning featuring two distinct node types: genes and images.
The network can effectively classify Alzheimer's disease (AD) into three
distinct stages:AD, Mild Cognitive Impairment (MCI), and Cognitive Normal (CN)
classes, utilizing a small dataset. Additionally, it identified which genes
play a significant role in each of these classification groups. We evaluate the
performance of our approach using metrics including classification accuracy,
recall, precision, and F1 score. The proposed technique holds potential for
extending to radiogenomic-based classification to other diseases.

</details>


### [230] [Self-Consuming Generative Models with Adversarially Curated Data](https://arxiv.org/pdf/2505.09768)
*Xiukun Wei, Xueru Zhang*

Main category: cs.LG

TL;DR: The paper examines how noisy and adversarial data curation affects generative models in self-consuming loops, proposing theoretical robustness conditions and attack algorithms for competitive scenarios.


<details>
  <summary>Details</summary>
Motivation: To understand the impact of noisy and adversarial data curation on generative models in self-consuming loops, especially in competitive environments where malicious users may disrupt rival models.

Method: Theoretical analysis of noisy data curation's impact, identification of robustness conditions, and design of attack algorithms for adversarial scenarios. Experiments on synthetic and real-world datasets.

Result: Demonstrates the effectiveness of the proposed attack algorithms in misaligning rival models from user preferences.

Conclusion: Noisy and adversarial data curation can destabilize generative models, but the study provides insights into robustness conditions and adversarial strategies.

Abstract: Recent advances in generative models have made it increasingly difficult to
distinguish real data from model-generated synthetic data. Using synthetic data
for successive training of future model generations creates "self-consuming
loops", which may lead to model collapse or training instability. Furthermore,
synthetic data is often subject to human feedback and curated by users based on
their preferences. Ferbach et al. (2024) recently showed that when data is
curated according to user preferences, the self-consuming retraining loop
drives the model to converge toward a distribution that optimizes those
preferences. However, in practice, data curation is often noisy or
adversarially manipulated. For example, competing platforms may recruit
malicious users to adversarially curate data and disrupt rival models. In this
paper, we study how generative models evolve under self-consuming retraining
loops with noisy and adversarially curated data. We theoretically analyze the
impact of such noisy data curation on generative models and identify conditions
for the robustness of the retraining process. Building on this analysis, we
design attack algorithms for competitive adversarial scenarios, where a
platform with a limited budget employs malicious users to misalign a rival's
model from actual user preferences. Experiments on both synthetic and
real-world datasets demonstrate the effectiveness of the proposed algorithms.

</details>


### [231] [Learning Graph Representation of Agent Diffusers](https://arxiv.org/pdf/2505.06761)
*Youcef Djenouri, Nassim Belmecheri, Tomasz Michalak, Jan Dubiński, Ahmed Nabil Belbachir, Anis Yazidi*

Main category: cs.LG

TL;DR: LGR-AD is a multi-agent system improving adaptability in text-to-image synthesis by modeling generation as distributed agents collaborating via a graph neural network, outperforming traditional diffusion models.


<details>
  <summary>Details</summary>
Motivation: Static model parameters in diffusion-based generative models may not optimally handle distinct generation phases, prompting the need for adaptable solutions like LGR-AD.

Method: LGR-AD uses a multi-agent system with expert sub-models, a graph neural network for collaboration, and a top-$k$ maximum spanning tree coordination mechanism. A meta-model guides agents with a novel loss function.

Result: LGR-AD outperforms traditional diffusion models in benchmarks, demonstrating scalability and flexibility in complex image generation.

Conclusion: LGR-AD offers a promising, adaptable approach for dynamic computer vision tasks, with potential for broader applications in image synthesis.

Abstract: Diffusion-based generative models have significantly advanced text-to-image
synthesis, demonstrating impressive text comprehension and zero-shot
generalization. These models refine images from random noise based on textual
prompts, with initial reliance on text input shifting towards enhanced visual
fidelity over time. This transition suggests that static model parameters might
not optimally address the distinct phases of generation. We introduce LGR-AD
(Learning Graph Representation of Agent Diffusers), a novel multi-agent system
designed to improve adaptability in dynamic computer vision tasks. LGR-AD
models the generation process as a distributed system of interacting agents,
each representing an expert sub-model. These agents dynamically adapt to
varying conditions and collaborate through a graph neural network that encodes
their relationships and performance metrics. Our approach employs a
coordination mechanism based on top-$k$ maximum spanning trees, optimizing the
generation process. Each agent's decision-making is guided by a meta-model that
minimizes a novel loss function, balancing accuracy and diversity. Theoretical
analysis and extensive empirical evaluations show that LGR-AD outperforms
traditional diffusion models across various benchmarks, highlighting its
potential for scalable and flexible solutions in complex image generation
tasks. Code is available at: https://github.com/YousIA/LGR_AD

</details>


### [232] [Lossless Compression for LLM Tensor Incremental Snapshots](https://arxiv.org/pdf/2505.09810)
*Daniel Waddington, Cornel Constantinescu*

Main category: cs.LG

TL;DR: The paper proposes an optimized checkpointing solution for LLMs using lossless compression (LMC), improving speed and efficiency over existing methods like BZ2.


<details>
  <summary>Details</summary>
Motivation: Checkpointing large volumes of tensor data in LLMs is resource-intensive, requiring efficient compression to reduce data volume and improve performance.

Method: Analyzes tensor data compressibility during training, evaluates off-the-shelf compression engines, and introduces LMC (byte-grouping + Huffman encoding).

Result: LMC outperforms BZ2 in compression speed (2.78 GiB/s compression, 3.76 GiB/s decompression) and reduces CPU usage, enabling higher-frequency checkpoints.

Conclusion: LMC provides a faster, more efficient checkpointing solution for LLMs, optimizing data transfer and storage usage.

Abstract: During the training of Large Language Models (LLMs), tensor data is
periodically "checkpointed" to persistent storage to allow recovery of work
done in the event of failure. The volume of data that must be copied during
each checkpoint, even when using reduced-precision representations such as
bfloat16, often reaches hundreds of gigabytes. Furthermore, the data must be
moved across a network and written to a storage system before the next epoch
occurs. With a view to ultimately building an optimized checkpointing solution,
this paper presents experimental analysis of checkpoint data used to derive a
design that maximizes the use of lossless compression to reduce the volume of
data. We examine how tensor data and its compressibility evolve during model
training and evaluate the efficacy of existing common off-the-shelf general
purpose compression engines combined with known data optimization techniques
such as byte-grouping and incremental delta compression.
  Leveraging our analysis we have built an effective compression solution,
known as Language Model Compressor (LMC), which is based on byte-grouping and
Huffman encoding. LMC offers more compression performance than the best
alternative (BZ2) but with an order-of-magnitude reduction in the time needed
to perform the compression. We show that a 16-core parallel implementation of
LMC can attain compression and decompression throughput of 2.78 GiB/s and 3.76
GiB/s respectively. This increase in performance ultimately reduces the CPU
resources needed and provides more time to copy the data to the storage system
before the next epoch thus allowing for higher-frequency checkpoints.

</details>


### [233] [Comparative Analysis of Stroke Prediction Models Using Machine Learning](https://arxiv.org/pdf/2505.09812)
*Anastasija Tashkova, Stefan Eftimov, Bojan Ristov, Slobodan Kalajdziski*

Main category: cs.LG

TL;DR: The study evaluates machine learning models for stroke risk prediction, highlighting accuracy but noting sensitivity limitations.


<details>
  <summary>Details</summary>
Motivation: Stroke is a major global health issue, necessitating better prediction methods.

Method: Used Logistic Regression, Random Forest, and XGBoost on demographic, clinical, and lifestyle data, addressing class imbalance and missing data.

Result: Models achieved high accuracy but lacked sensitivity; key predictive features were identified.

Conclusion: Findings aid in developing better, interpretable models for early stroke risk assessment.

Abstract: Stroke remains one of the most critical global health challenges, ranking as
the second leading cause of death and the third leading cause of disability
worldwide. This study explores the effectiveness of machine learning algorithms
in predicting stroke risk using demographic, clinical, and lifestyle data from
the Stroke Prediction Dataset. By addressing key methodological challenges such
as class imbalance and missing data, we evaluated the performance of multiple
models, including Logistic Regression, Random Forest, and XGBoost. Our results
demonstrate that while these models achieve high accuracy, sensitivity remains
a limiting factor for real-world clinical applications. In addition, we
identify the most influential predictive features and propose strategies to
improve machine learning-based stroke prediction. These findings contribute to
the development of more reliable and interpretable models for the early
assessment of stroke risk.

</details>


### [234] [Adversarial Attack on Large Language Models using Exponentiated Gradient Descent](https://arxiv.org/pdf/2505.09820)
*Sajib Biswas, Mao Nishino, Samuel Jacob Chacko, Xiuwen Liu*

Main category: cs.LG

TL;DR: The paper introduces an intrinsic optimization technique using exponentiated gradient descent with Bregman projection to jailbreak LLMs efficiently, outperforming existing methods.


<details>
  <summary>Details</summary>
Motivation: Understanding and improving LLM safety is crucial, but current alignment techniques like RLHF leave models vulnerable to jailbreaking attacks. Existing methods are either inefficient or ineffective due to discrete or continuous space limitations.

Method: The proposed method uses exponentiated gradient descent with Bregman projection to optimize one-hot encodings within the probability simplex, ensuring convergence and efficiency.

Result: The technique achieves higher success rates in jailbreaking five open-source LLMs across four datasets, outperforming three state-of-the-art methods.

Conclusion: The proposed optimization technique is effective and efficient for jailbreaking LLMs, demonstrating superior performance over existing approaches.

Abstract: As Large Language Models (LLMs) are widely used, understanding them
systematically is key to improving their safety and realizing their full
potential. Although many models are aligned using techniques such as
reinforcement learning from human feedback (RLHF), they are still vulnerable to
jailbreaking attacks. Some of the existing adversarial attack methods search
for discrete tokens that may jailbreak a target model while others try to
optimize the continuous space represented by the tokens of the model's
vocabulary. While techniques based on the discrete space may prove to be
inefficient, optimization of continuous token embeddings requires projections
to produce discrete tokens, which might render them ineffective. To fully
utilize the constraints and the structures of the space, we develop an
intrinsic optimization technique using exponentiated gradient descent with the
Bregman projection method to ensure that the optimized one-hot encoding always
stays within the probability simplex. We prove the convergence of the technique
and implement an efficient algorithm that is effective in jailbreaking several
widely used LLMs. We demonstrate the efficacy of the proposed technique using
five open-source LLMs on four openly available datasets. The results show that
the technique achieves a higher success rate with great efficiency compared to
three other state-of-the-art jailbreaking techniques. The source code for our
implementation is available at:
https://github.com/sbamit/Exponentiated-Gradient-Descent-LLM-Attack

</details>


### [235] [Learning Kronecker-Structured Graphs from Smooth Signals](https://arxiv.org/pdf/2505.09822)
*Changhao Shi, Gal Mishne*

Main category: cs.LG

TL;DR: The paper focuses on learning Kronecker-structured product graphs from smooth signals, addressing limitations in modeling diverse dependency structures. It proposes an alternating optimization scheme with theoretical guarantees and demonstrates superior performance.


<details>
  <summary>Details</summary>
Motivation: Graph learning is crucial in GSP for non-Euclidean domains, but existing methods are limited in modeling diverse dependencies, especially for multi-way data.

Method: An alternating optimization scheme is proposed to learn Kronecker-structured product graphs, with modifications for strong product graphs. Theoretical convergence guarantees are provided.

Result: Experiments on synthetic and real-world graphs show the proposed method's efficacy and outperformance of existing techniques.

Conclusion: The approach successfully addresses the challenge of learning intricate dependency structures in product graphs, offering a scalable and effective solution.

Abstract: Graph learning, or network inference, is a prominent problem in graph signal
processing (GSP). GSP generalizes the Fourier transform to non-Euclidean
domains, and graph learning is pivotal to applying GSP when these domains are
unknown. With the recent prevalence of multi-way data, there has been growing
interest in product graphs that naturally factorize dependencies across
different ways. However, the types of graph products that can be learned are
still limited for modeling diverse dependency structures. In this paper, we
study the problem of learning a Kronecker-structured product graph from smooth
signals. Unlike the more commonly used Cartesian product, the Kronecker product
models dependencies in a more intricate, non-separable way, but posits harder
constraints on the graph learning problem. To tackle this non-convex problem,
we propose an alternating scheme to optimize each factor graph and provide
theoretical guarantees for its asymptotic convergence. The proposed algorithm
is also modified to learn factor graphs of the strong product. We conduct
experiments on synthetic and real-world graphs and demonstrate our approach's
efficacy and superior performance compared to existing methods.

</details>


### [236] [Causal Predictive Optimization and Generation for Business AI](https://arxiv.org/pdf/2505.09847)
*Liyang Zhao, Olurotimi Seton, Himadeep Reddy Reddivari, Suvendu Jena, Shadow Zhao, Rachit Kumar, Changshuai Wei*

Main category: cs.LG

TL;DR: A three-layered AI-driven sales optimization framework (Causal Predictive Optimization and Generation) improves LinkedIn's sales process, outperforming legacy systems.


<details>
  <summary>Details</summary>
Motivation: Optimizing the sales process is crucial for B2B success, requiring a structured AI approach to enhance lead conversion and customer engagement.

Method: The framework combines causal ML for prediction, constraint optimization and contextual bandit for optimization, and Generative AI with feedback for system enhancement.

Result: Significant improvements over legacy systems were achieved, with insights broadly applicable to sales optimization.

Conclusion: The proposed framework effectively optimizes sales processes, demonstrating practical success and scalability in a real-world setting like LinkedIn.

Abstract: The sales process involves sales functions converting leads or opportunities
to customers and selling more products to existing customers. The optimization
of the sales process thus is key to success of any B2B business. In this work,
we introduce a principled approach to sales optimization and business AI,
namely the Causal Predictive Optimization and Generation, which includes three
layers: 1) prediction layer with causal ML 2) optimization layer with
constraint optimization and contextual bandit 3) serving layer with Generative
AI and feedback-loop for system enhancement. We detail the implementation and
deployment of the system in LinkedIn, showcasing significant wins over legacy
systems and sharing learning and insight broadly applicable to this field.

</details>


### [237] [ZENN: A Thermodynamics-Inspired Computational Framework for Heterogeneous Data-Driven Modeling](https://arxiv.org/pdf/2505.09851)
*Shun Wang, Shun-Li Shang, Zi-Kui Liu, Wenrui Hao*

Main category: cs.LG

TL;DR: The paper introduces ZENN, a zentropy-enhanced neural network, to improve learning from heterogeneous data by integrating intrinsic entropy and energy components.


<details>
  <summary>Details</summary>
Motivation: Addressing challenges in integrating heterogeneous datasets with intrinsic disparities, the paper extends zentropy theory into data science.

Method: Proposes ZENN, a redesigned neural network architecture that learns energy and intrinsic entropy components for multi-source data.

Result: ZENN shows superior generalization and robustness, demonstrated in classification tasks and energy landscape reconstructions.

Conclusion: ZENN is a versatile framework for complex, heterogeneous datasets, validated by practical applications like material behavior prediction.

Abstract: Traditional entropy-based methods - such as cross-entropy loss in
classification problems - have long been essential tools for quantifying
uncertainty and disorder in data and developing artificial intelligence
algorithms. However, the rapid growth of data across various domains has
introduced new challenges, particularly the integration of heterogeneous
datasets with intrinsic disparities. In this paper, we extend zentropy theory
into the data science domain by introducing intrinsic entropy, enabling more
effective learning from heterogeneous data sources. We propose a
zentropy-enhanced neural network (ZENN) that simultaneously learns both energy
and intrinsic entropy components, capturing the underlying structure of
multi-source data. To support this, we redesign the neural network architecture
to better reflect the intrinsic properties and variability inherent in diverse
datasets. We demonstrate the effectiveness of ZENN on classification tasks and
energy landscape reconstructions, showing its superior generalization
capabilities and robustness-particularly in predicting high-order derivatives.
As a practical application, we employ ZENN to reconstruct the Helmholtz energy
landscape of Fe3Pt using data generated from DFT and capture key material
behaviors, including negative thermal expansion and the critical point in the
temperature-pressure space. Overall, our study introduces a novel approach for
data-driven machine learning grounded in zentropy theory, highlighting ZENN as
a versatile and robust deep learning framework for scientific problems
involving complex, heterogeneous datasets.

</details>


### [238] [Predictability Shapes Adaptation: An Evolutionary Perspective on Modes of Learning in Transformers](https://arxiv.org/pdf/2505.09855)
*Alexander Y. Ku, Thomas L. Griffiths, Stephanie C. Y. Chan*

Main category: cs.LG

TL;DR: Transformers balance in-weights learning (IWL) and in-context learning (ICL) based on environmental predictability, inspired by evolutionary biology's genetic encoding and phenotypic plasticity. Experiments show stability favors IWL, while reliable cues enhance ICL, with learning dynamics varying by task.


<details>
  <summary>Details</summary>
Motivation: To understand how environmental predictability influences the balance between IWL and ICL in Transformers, drawing parallels from evolutionary biology.

Method: Experimental operationalization of predictability dimensions (stability and cue reliability) using regression and classification tasks to study ICL/IWL balance.

Result: High stability favors IWL, while high cue reliability enhances ICL. Learning dynamics vary: some tasks show ICL-to-IWL shifts, others exhibit initial IWL followed by ICL dominance.

Conclusion: Predictability governs Transformer learning modes, supporting a relative-cost hypothesis for transitions, with implications for understanding ICL and improving training methods.

Abstract: Transformer models learn in two distinct modes: in-weights learning (IWL),
encoding knowledge into model weights, and in-context learning (ICL), adapting
flexibly to context without weight modification. To better understand the
interplay between these learning modes, we draw inspiration from evolutionary
biology's analogous adaptive strategies: genetic encoding (akin to IWL,
adapting over generations and fixed within an individual's lifetime) and
phenotypic plasticity (akin to ICL, enabling flexible behavioral responses to
environmental cues). In evolutionary biology, environmental predictability
dictates the balance between these strategies: stability favors genetic
encoding, while reliable predictive cues promote phenotypic plasticity. We
experimentally operationalize these dimensions of predictability and
systematically investigate their influence on the ICL/IWL balance in
Transformers. Using regression and classification tasks, we show that high
environmental stability decisively favors IWL, as predicted, with a sharp
transition at maximal stability. Conversely, high cue reliability enhances ICL
efficacy, particularly when stability is low. Furthermore, learning dynamics
reveal task-contingent temporal evolution: while a canonical ICL-to-IWL shift
occurs in some settings (e.g., classification with many classes), we
demonstrate that scenarios with easier IWL (e.g., fewer classes) or slower ICL
acquisition (e.g., regression) can exhibit an initial IWL phase later yielding
to ICL dominance. These findings support a relative-cost hypothesis for
explaining these learning mode transitions, establishing predictability as a
critical factor governing adaptive strategies in Transformers, and offering
novel insights for understanding ICL and guiding training methodologies.

</details>


### [239] [LiDDA: Data Driven Attribution at LinkedIn](https://arxiv.org/pdf/2505.09861)
*John Bencina, Erkut Aykutlug, Yue Chen, Zerui Zhang, Stephanie Sorenson, Shao Tang, Changshuai Wei*

Main category: cs.LG

TL;DR: A unified transformer-based approach for data-driven attribution in marketing, handling member-level and aggregate-level data, plus external macro factors, with successful large-scale implementation at LinkedIn.


<details>
  <summary>Details</summary>
Motivation: To improve marketing intelligence by accurately attributing conversions to interactions using causal patterns from data.

Method: Transformer-based attribution model integrating member-level, aggregate-level data, and external macro factors.

Result: Successful large-scale implementation at LinkedIn with significant impact.

Conclusion: The approach is broadly applicable to marketing and ad tech, offering valuable insights.

Abstract: Data Driven Attribution, which assigns conversion credits to marketing
interactions based on causal patterns learned from data, is the foundation of
modern marketing intelligence and vital to any marketing businesses and
advertising platform. In this paper, we introduce a unified transformer-based
attribution approach that can handle member-level data, aggregate-level data,
and integration of external macro factors. We detail the large scale
implementation of the approach at LinkedIn, showcasing significant impact. We
also share learning and insights that are broadly applicable to the marketing
and ad tech fields.

</details>


### [240] [BINGO: A Novel Pruning Mechanism to Reduce the Size of Neural Networks](https://arxiv.org/pdf/2505.09864)
*Aditya Panangat*

Main category: cs.LG

TL;DR: BINGO introduces a one-shot pruning method for neural networks, reducing computational costs while preserving accuracy, addressing the expense and environmental impact of large models.


<details>
  <summary>Details</summary>
Motivation: The high cost and environmental impact of training large neural networks limit accessibility and innovation in AI.

Method: BINGO evaluates weight significance during training, assigning scores to prune insignificant weights in one step.

Result: BINGO achieves accuracy-preserving pruning with lower computational demands compared to iterative methods.

Conclusion: BINGO enables efficient AI development without the need for excessively large models, promoting accessibility and sustainability.

Abstract: Over the past decade, the use of machine learning has increased
exponentially. Models are far more complex than ever before, growing to
gargantuan sizes and housing millions of weights. Unfortunately, the fact that
large models have become the state of the art means that it often costs
millions of dollars to train and operate them. These expenses not only hurt
companies but also bar non-wealthy individuals from contributing to new
developments and force consumers to pay greater prices for AI. Current methods
used to prune models, such as iterative magnitude pruning, have shown great
accuracy but require an iterative training sequence that is incredibly
computationally and environmentally taxing. To solve this problem, BINGO is
introduced. BINGO, during the training pass, studies specific subsets of a
neural network one at a time to gauge how significant of a role each weight
plays in contributing to a network's accuracy. By the time training is done,
BINGO generates a significance score for each weight, allowing for
insignificant weights to be pruned in one shot. BINGO provides an
accuracy-preserving pruning technique that is less computationally intensive
than current methods, allowing for a world where AI growth does not have to
mean model growth, as well.

</details>


### [241] [Comparing Exploration-Exploitation Strategies of LLMs and Humans: Insights from Standard Multi-armed Bandit Tasks](https://arxiv.org/pdf/2505.09901)
*Ziyuan Zhang, Darcy Wang, Ningyuan Chen, Rodrigo Mansur, Vahid Sarhangian*

Main category: cs.LG

TL;DR: The paper compares decision-making behavior of LLMs, humans, and MAB algorithms in exploration-exploitation tasks, finding reasoning shifts LLMs toward human-like behavior but with limitations in complex environments.


<details>
  <summary>Details</summary>
Motivation: To assess if LLMs mimic human decision-making, especially in exploration-exploitation tradeoffs, and evaluate their performance in dynamic tasks.

Method: Comparative study using MAB tasks, interpretable choice models, and prompting/reasoning-enhanced LLMs to analyze E&E strategies.

Result: Reasoning makes LLMs more human-like in simple tasks but less adaptable in complex ones, with similar regret in some cases.

Conclusion: LLMs show promise as human behavior simulators but need improvement, especially in complex, non-stationary environments.

Abstract: Large language models (LLMs) are increasingly used to simulate or automate
human behavior in complex sequential decision-making tasks. A natural question
is then whether LLMs exhibit similar decision-making behavior to humans, and
can achieve comparable (or superior) performance. In this work, we focus on the
exploration-exploitation (E&E) tradeoff, a fundamental aspect of dynamic
decision-making under uncertainty. We employ canonical multi-armed bandit (MAB)
tasks introduced in the cognitive science and psychiatry literature to conduct
a comparative study of the E&E strategies of LLMs, humans, and MAB algorithms.
We use interpretable choice models to capture the E&E strategies of the agents
and investigate how explicit reasoning, through both prompting strategies and
reasoning-enhanced models, shapes LLM decision-making. We find that reasoning
shifts LLMs toward more human-like behavior, characterized by a mix of random
and directed exploration. In simple stationary tasks, reasoning-enabled LLMs
exhibit similar levels of random and directed exploration compared to humans.
However, in more complex, non-stationary environments, LLMs struggle to match
human adaptability, particularly in effective directed exploration, despite
achieving similar regret in certain scenarios. Our findings highlight both the
promise and limits of LLMs as simulators of human behavior and tools for
automated decision-making and point to potential areas of improvements.

</details>


### [242] [Avocado Price Prediction Using a Hybrid Deep Learning Model: TCN-MLP-Attention Architecture](https://arxiv.org/pdf/2505.09907)
*Linwei Zhang, LuFeng, Ruijia Liang*

Main category: cs.LG

TL;DR: A hybrid deep learning model (TCN-MLP-Attention) is proposed for Hass avocado price forecasting, outperforming traditional methods with an RMSE of 1.23.


<details>
  <summary>Details</summary>
Motivation: The complexity of price fluctuations in high-value crops like Hass avocados, influenced by seasonality, region, and weather, necessitates advanced prediction models.

Method: The model combines Temporal Convolutional Networks (TCN), Multi-Layer Perceptrons (MLP), and an Attention mechanism, trained on a dataset of 50,000+ U.S. sales records (2015-2018).

Result: The TCN-MLP-Attention model achieves superior performance (RMSE: 1.23, MSE: 1.51) compared to traditional methods.

Conclusion: The research offers a scalable solution for agricultural price forecasting and insights for supply chain and pricing strategies.

Abstract: With the growing demand for healthy foods, agricultural product price
forecasting has become increasingly important. Hass avocados, as a high-value
crop, exhibit complex price fluctuations influenced by factors such as
seasonality, region, and weather. Traditional prediction models often struggle
with highly nonlinear and dynamic data. To address this, we propose a hybrid
deep learning model, TCN-MLP-Attention Architecture, combining Temporal
Convolutional Networks (TCN) for sequential feature extraction, Multi-Layer
Perceptrons (MLP) for nonlinear interactions, and an Attention mechanism for
dynamic feature weighting. The dataset used covers over 50,000 records of Hass
avocado sales across the U.S. from 2015 to 2018, including variables such as
sales volume, average price, time, region, weather, and variety type, collected
from point-of-sale systems and the Hass Avocado Board. After systematic
preprocessing, including missing value imputation and feature normalization,
the proposed model was trained and evaluated. Experimental results demonstrate
that the TCN-MLP-Attention model achieves excellent predictive performance,
with an RMSE of 1.23 and an MSE of 1.51, outperforming traditional methods.
This research provides a scalable and effective approach for time series
forecasting in agricultural markets and offers valuable insights for
intelligent supply chain management and price strategy optimization.

</details>


### [243] [Change Detection in Multivariate data streams: Online Analysis with Kernel-QuantTree](https://arxiv.org/pdf/2410.13778)
*Michelangelo Olmo Nogara Notarianni, Filippo Leveni, Diego Stucchi, Luca Frittoli, Giacomo Boracchi*

Main category: cs.LG

TL;DR: KQT-EWMA is a non-parametric change-detection algorithm for multivariate data streams, combining Kernel-QuantTree histograms and EWMA statistics. It offers flexibility, practicality, and controlled false alarms via ARL0.


<details>
  <summary>Details</summary>
Motivation: To address the lack of non-parametric change-detection methods that can control false alarms (ARL0) a priori while monitoring multivariate data streams.

Method: Combines Kernel-QuantTree (KQT) histograms with EWMA statistics for online monitoring, enabling non-parametric modeling of any stationary distribution.

Result: KQT-EWMA effectively controls ARL0 and achieves detection delays comparable or better than state-of-the-art methods.

Conclusion: KQT-EWMA is a practical and flexible solution for non-parametric change-detection in multivariate data streams, with controlled false alarms and competitive performance.

Abstract: We present Kernel-QuantTree Exponentially Weighted Moving Average (KQT-EWMA),
a non-parametric change-detection algorithm that combines the Kernel-QuantTree
(KQT) histogram and the EWMA statistic to monitor multivariate data streams
online. The resulting monitoring scheme is very flexible, since histograms can
be used to model any stationary distribution, and practical, since the
distribution of test statistics does not depend on the distribution of
datastream in stationary conditions (non-parametric monitoring). KQT-EWMA
enables controlling false alarms by operating at a pre-determined Average Run
Length ($ARL_0$), which measures the expected number of stationary samples to
be monitored before triggering a false alarm. The latter peculiarity is in
contrast with most non-parametric change-detection tests, which rarely can
control the $ARL_0$ a priori. Our experiments on synthetic and real-world
datasets demonstrate that KQT-EWMA can control $ARL_0$ while achieving
detection delays comparable to or lower than state-of-the-art methods designed
to work in the same conditions.

</details>


### [244] [Improving the Euclidean Diffusion Generation of Manifold Data by Mitigating Score Function Singularity](https://arxiv.org/pdf/2505.09922)
*Zichen Liu, Wei Zhang, Tiejun Li*

Main category: cs.LG

TL;DR: The paper investigates direct sampling of Euclidean diffusion models for manifold-constrained data, addressing the multiscale singularity of the score function. It proposes two methods, Niso-DM and Tango-DM, to improve sampling accuracy, demonstrating superior performance on complex manifolds.


<details>
  <summary>Details</summary>
Motivation: To address the challenges of sampling manifold-constrained data using Euclidean diffusion models, particularly the singularity of the score function in embedded spaces.

Method: The paper analyzes the singularity structure of the score function and introduces two methods: Niso-DM (non-isotropic noise along the normal direction) and Tango-DM (tangential-only score training).

Result: Numerical experiments show the proposed methods achieve superior performance on distributions over various manifolds with complex geometries.

Conclusion: The study successfully mitigates the singularity issue in manifold-constrained diffusion models, improving sampling accuracy with the proposed Niso-DM and Tango-DM methods.

Abstract: Euclidean diffusion models have achieved remarkable success in generative
modeling across diverse domains, and they have been extended to manifold case
in recent advances. Instead of explicitly utilizing the structure of special
manifolds as studied in previous works, we investigate direct sampling of the
Euclidean diffusion models for general manifold-constrained data in this paper.
We reveal the multiscale singularity of the score function in the embedded
space of manifold, which hinders the accuracy of diffusion-generated samples.
We then present an elaborate theoretical analysis of the singularity structure
of the score function by separating it along the tangential and normal
directions of the manifold. To mitigate the singularity and improve the
sampling accuracy, we propose two novel methods: (1) Niso-DM, which introduces
non-isotropic noise along the normal direction to reduce scale discrepancies,
and (2) Tango-DM, which trains only the tangential component of the score
function using a tangential-only loss function. Numerical experiments
demonstrate that our methods achieve superior performance on distributions over
various manifolds with complex geometries.

</details>


### [245] [Adversarial Attacks in Multimodal Systems: A Practitioner's Survey](https://arxiv.org/pdf/2505.03084)
*Shashank Kapoor, Sanjay Surendranath Girija, Lakshit Arora, Dipen Pradhan, Ankit Shetgaonkar, Aman Raj*

Main category: cs.LG

TL;DR: The paper surveys adversarial attacks in multimodal AI models, covering text, image, video, and audio, to help practitioners understand and mitigate threats.


<details>
  <summary>Details</summary>
Motivation: Multimodal models inherit vulnerabilities from all modalities, amplifying adversarial threats. Practitioners lack a clear view of attack types, necessitating a comprehensive survey.

Method: The paper conducts a survey of adversarial attacks across text, image, video, and audio modalities in multimodal models.

Result: It provides the first comprehensive summary of the adversarial threat landscape in multimodal AI, detailing attack types and their evolution.

Conclusion: The survey fills a critical gap, equipping practitioners with knowledge to address adversarial threats in multimodal models.

Abstract: The introduction of multimodal models is a huge step forward in Artificial
Intelligence. A single model is trained to understand multiple modalities:
text, image, video, and audio. Open-source multimodal models have made these
breakthroughs more accessible. However, considering the vast landscape of
adversarial attacks across these modalities, these models also inherit
vulnerabilities of all the modalities, and ultimately, the adversarial threat
amplifies. While broad research is available on possible attacks within or
across these modalities, a practitioner-focused view that outlines attack types
remains absent in the multimodal world. As more Machine Learning Practitioners
adopt, fine-tune, and deploy open-source models in real-world applications,
it's crucial that they can view the threat landscape and take the preventive
actions necessary. This paper addresses the gap by surveying adversarial
attacks targeting all four modalities: text, image, video, and audio. This
survey provides a view of the adversarial attack landscape and presents how
multimodal adversarial threats have evolved. To the best of our knowledge, this
survey is the first comprehensive summarization of the threat landscape in the
multimodal world.

</details>


### [246] [Reinforced Interactive Continual Learning via Real-time Noisy Human Feedback](https://arxiv.org/pdf/2505.09925)
*Yutao Yang, Jie Zhou, Junsong Li, Qianjun Pan, Bihao Zhan, Qin Chen, Xipeng Qiu, Liang He*

Main category: cs.LG

TL;DR: RiCL introduces an interactive continual learning framework using LLMs to dynamically learn from noisy human feedback while retaining prior knowledge.


<details>
  <summary>Details</summary>
Motivation: Addresses limitations of traditional continual learning: dynamic updates with real-time feedback and handling noisy labels.

Method: Proposes RiCL with three components: temporal consistency-aware purifier, interaction-aware DPO strategy, and noise-resistant contrastive learning.

Result: Outperforms state-of-the-art methods on noisy benchmark datasets (FewRel, TACRED).

Conclusion: RiCL effectively handles noisy feedback and dynamic learning, improving continual learning performance.

Abstract: This paper introduces an interactive continual learning paradigm where AI
models dynamically learn new skills from real-time human feedback while
retaining prior knowledge. This paradigm distinctively addresses two major
limitations of traditional continual learning: (1) dynamic model updates using
streaming, real-time human-annotated data, rather than static datasets with
fixed labels, and (2) the assumption of clean labels, by explicitly handling
the noisy feedback common in real-world interactions. To tackle these problems,
we propose RiCL, a Reinforced interactive Continual Learning framework
leveraging Large Language Models (LLMs) to learn new skills effectively from
dynamic feedback. RiCL incorporates three key components: a temporal
consistency-aware purifier to automatically discern clean from noisy samples in
data streams; an interaction-aware direct preference optimization strategy to
align model behavior with human intent by reconciling AI-generated and
human-provided feedback; and a noise-resistant contrastive learning module that
captures robust representations by exploiting inherent data relationships, thus
avoiding reliance on potentially unreliable labels. Extensive experiments on
two benchmark datasets (FewRel and TACRED), contaminated with realistic noise
patterns, demonstrate that our RiCL approach substantially outperforms existing
combinations of state-of-the-art online continual learning and noisy-label
learning methods.

</details>


### [247] [Advanced Crash Causation Analysis for Freeway Safety: A Large Language Model Approach to Identifying Key Contributing Factors](https://arxiv.org/pdf/2505.09949)
*Ahmed S. Abdelrahman, Mohamed Abdel-Aty, Samgyu Yang, Abdulrahman Faden*

Main category: cs.LG

TL;DR: The paper uses a fine-tuned LLM (Llama3 8B) to analyze freeway crash data, identifying primary causes like alcohol-impaired driving and speeding without pre-labeled data, validated by expert agreement.


<details>
  <summary>Details</summary>
Motivation: Traditional methods struggle with complex crash factor interactions; LLMs offer a novel approach for comprehensive crash causation analysis.

Method: Fine-tuned Llama3 8B using QLoRA on 226 traffic safety studies, applied zero-shot classification to identify crash causes.

Result: LLM effectively identified key crash causes (e.g., speeding, inattention) and provided insights validated by 88.89% expert agreement.

Conclusion: LLMs are promising for crash analysis, offering actionable insights for policymakers to improve traffic safety.

Abstract: Understanding the factors contributing to traffic crashes and developing
strategies to mitigate their severity is essential. Traditional statistical
methods and machine learning models often struggle to capture the complex
interactions between various factors and the unique characteristics of each
crash. This research leverages large language model (LLM) to analyze freeway
crash data and provide crash causation analysis accordingly. By compiling 226
traffic safety studies related to freeway crashes, a training dataset
encompassing environmental, driver, traffic, and geometric design factors was
created. The Llama3 8B model was fine-tuned using QLoRA to enhance its
understanding of freeway crashes and their contributing factors, as covered in
these studies. The fine-tuned Llama3 8B model was then used to identify crash
causation without pre-labeled data through zero-shot classification, providing
comprehensive explanations to ensure that the identified causes were reasonable
and aligned with existing research. Results demonstrate that LLMs effectively
identify primary crash causes such as alcohol-impaired driving, speeding,
aggressive driving, and driver inattention. Incorporating event data, such as
road maintenance, offers more profound insights. The model's practical
applicability and potential to improve traffic safety measures were validated
by a high level of agreement among researchers in the field of traffic safety,
as reflected in questionnaire results with 88.89%. This research highlights the
complex nature of traffic crashes and how LLMs can be used for comprehensive
analysis of crash causation and other contributing factors. Moreover, it
provides valuable insights and potential countermeasures to aid planners and
policymakers in developing more effective and efficient traffic safety
practices.

</details>


### [248] [Online Isolation Forest](https://arxiv.org/pdf/2505.09593)
*Filippo Leveni, Guilherme Weigert Cassales, Bernhard Pfahringer, Albert Bifet, Giacomo Boracchi*

Main category: cs.LG

TL;DR: Online-iForest is a novel streaming anomaly detection method that outperforms existing online and offline techniques in efficiency and performance.


<details>
  <summary>Details</summary>
Motivation: Existing anomaly detection methods are impractical for streaming data due to memory requirements and retraining needs.

Method: Proposes Online-iForest, designed for streaming data to adapt dynamically without retraining.

Result: Matches or outperforms online and offline methods, excelling in efficiency for real-time applications.

Conclusion: Online-iForest is highly effective for fast anomaly detection in fields like cybersecurity and fraud detection.

Abstract: The anomaly detection literature is abundant with offline methods, which
require repeated access to data in memory, and impose impractical assumptions
when applied to a streaming context. Existing online anomaly detection methods
also generally fail to address these constraints, resorting to periodic
retraining to adapt to the online context. We propose Online-iForest, a novel
method explicitly designed for streaming conditions that seamlessly tracks the
data generating process as it evolves over time. Experimental validation on
real-world datasets demonstrated that Online-iForest is on par with online
alternatives and closely rivals state-of-the-art offline anomaly detection
techniques that undergo periodic retraining. Notably, Online-iForest
consistently outperforms all competitors in terms of efficiency, making it a
promising solution in applications where fast identification of anomalies is of
primary importance such as cybersecurity, fraud and fault detection.

</details>


### [249] [Task-Core Memory Management and Consolidation for Long-term Continual Learning](https://arxiv.org/pdf/2505.09952)
*Tianyu Huai, Jie Zhou, Yuxuan Cai, Qin Chen, Wen Wu, Xingjiao Wu, Xipeng Qiu, Liang He*

Main category: cs.LG

TL;DR: The paper introduces Long-CL, a novel framework for long-term continual learning, addressing catastrophic forgetting and outperforming existing methods by 7.4% and 6.5% on new benchmarks.


<details>
  <summary>Details</summary>
Motivation: To evaluate existing continual learning methods in long-term settings and mitigate catastrophic forgetting over prolonged sequential updates.

Method: Proposes Long-CL with task-core memory management and long-term memory consolidation, inspired by human memory mechanisms.

Result: Long-CL achieves significant improvements (7.4% and 6.5% AP) over state-of-the-art methods on MMLongCL-Bench and TextLongCL-Bench.

Conclusion: The Long-CL framework effectively addresses long-term continual learning challenges, demonstrating superior performance and providing new benchmarks for future research.

Abstract: In this paper, we focus on a long-term continual learning (CL) task, where a
model learns sequentially from a stream of vast tasks over time, acquiring new
knowledge while retaining previously learned information in a manner akin to
human learning. Unlike traditional CL settings, long-term CL involves handling
a significantly larger number of tasks, which exacerbates the issue of
catastrophic forgetting. Our work seeks to address two critical questions: 1)
How do existing CL methods perform in the context of long-term CL? and 2) How
can we mitigate the catastrophic forgetting that arises from prolonged
sequential updates? To tackle these challenges, we propose a novel framework
inspired by human memory mechanisms for long-term continual learning (Long-CL).
Specifically, we introduce a task-core memory management strategy to
efficiently index crucial memories and adaptively update them as learning
progresses. Additionally, we develop a long-term memory consolidation mechanism
that selectively retains hard and discriminative samples, ensuring robust
knowledge retention. To facilitate research in this area, we construct and
release two multi-modal and textual benchmarks, MMLongCL-Bench and
TextLongCL-Bench, providing a valuable resource for evaluating long-term CL
approaches. Experimental results show that Long-CL outperforms the previous
state-of-the-art by 7.4\% and 6.5\% AP on the two benchmarks, respectively,
demonstrating the effectiveness of our approach.

</details>


### [250] [TransPL: VQ-Code Transition Matrices for Pseudo-Labeling of Time Series Unsupervised Domain Adaptation](https://arxiv.org/pdf/2505.09955)
*Jaeho Kim, Seulki Lee*

Main category: cs.LG

TL;DR: TransPL improves unsupervised domain adaptation for time series by modeling joint distributions with code transition matrices, outperforming existing methods.


<details>
  <summary>Details</summary>
Motivation: Traditional pseudo-labeling fails to capture temporal patterns and channel-wise shifts in time series UDA.

Method: Uses vector quantization and code transition matrices to model source domain distributions, applying Bayes' rule for target adaptation.

Result: Outperforms state-of-the-art methods by 6.1% accuracy and 4.9% F1, with interpretable insights.

Conclusion: TransPL effectively addresses UDA challenges in time series with superior performance and explainability.

Abstract: Unsupervised domain adaptation (UDA) for time series data remains a critical
challenge in deep learning, with traditional pseudo-labeling strategies failing
to capture temporal patterns and channel-wise shifts between domains, producing
sub-optimal pseudo-labels. As such, we introduce TransPL, a novel approach that
addresses these limitations by modeling the joint distribution $P(\mathbf{X},
y)$ of the source domain through code transition matrices, where the codes are
derived from vector quantization (VQ) of time series patches. Our method
constructs class- and channel-wise code transition matrices from the source
domain and employs Bayes' rule for target domain adaptation, generating
pseudo-labels based on channel-wise weighted class-conditional likelihoods.
TransPL offers three key advantages: explicit modeling of temporal transitions
and channel-wise shifts between different domains, versatility towards
different UDA scenarios (e.g., weakly-supervised UDA), and explainable
pseudo-label generation. We validate TransPL's effectiveness through extensive
analysis on four time series UDA benchmarks and confirm that it consistently
outperforms state-of-the-art pseudo-labeling methods by a strong margin (6.1%
accuracy improvement, 4.9% F1 improvement), while providing interpretable
insights into the domain adaptation process through its learned code transition
matrices.

</details>


### [251] [Approximated Behavioral Metric-based State Projection for Federated Reinforcement Learning](https://arxiv.org/pdf/2505.09959)
*Zengxia Guo, Bohui An, Zhongqi Lu*

Main category: cs.LG

TL;DR: FedRAG enhances federated reinforcement learning (FRL) by sharing approximated behavior metrics instead of sensitive data, improving performance and privacy.


<details>
  <summary>Details</summary>
Motivation: To improve FRL performance while protecting sensitive information by avoiding direct sharing of local states or policies.

Method: Proposes FedRAG, a framework that learns and aggregates state projection functions across clients without sharing task-specific data.

Result: Experiments on DeepMind Control Suite show FedRAG's effectiveness in providing information gain while preserving privacy.

Conclusion: FedRAG is a promising FRL approach that balances performance enhancement and privacy protection.

Abstract: Federated reinforcement learning (FRL) methods usually share the encrypted
local state or policy information and help each client to learn from others
while preserving everyone's privacy. In this work, we propose that sharing the
approximated behavior metric-based state projection function is a promising way
to enhance the performance of FRL and concurrently provides an effective
protection of sensitive information. We introduce FedRAG, a FRL framework to
learn a computationally practical projection function of states for each client
and aggregating the parameters of projection functions at a central server. The
FedRAG approach shares no sensitive task-specific information, yet provides
information gain for each client. We conduct extensive experiments on the
DeepMind Control Suite to demonstrate insightful results.

</details>


### [252] [A Comprehensive Machine Learning Framework for Heart Disease Prediction: Performance Evaluation and Future Perspectives](https://arxiv.org/pdf/2505.09969)
*Ali Azimi Lamir, Shiva Razzagzadeh, Zeynab Rezaei*

Main category: cs.LG

TL;DR: A machine learning framework for heart disease prediction using three classifiers (Logistic Regression, KNN, Random Forest) achieved 91% accuracy with Random Forest, showing promise for clinical use.


<details>
  <summary>Details</summary>
Motivation: To develop an effective machine learning model for predicting heart disease to aid clinical decision-making.

Method: Data preprocessing, model training, and evaluation using Logistic Regression, KNN, and Random Forest, with hyperparameter tuning via GridSearchCV and RandomizedSearchCV.

Result: Random Forest outperformed with 91% accuracy and 0.89 F1-score, showing balanced performance across metrics.

Conclusion: The model is promising for healthcare but requires larger, diverse datasets for improved generalizability.

Abstract: This study presents a machine learning-based framework for heart disease
prediction using the heart-disease dataset, comprising 303 samples with 14
features. The methodology involves data preprocessing, model training, and
evaluation using three classifiers: Logistic Regression, K-Nearest Neighbors
(KNN), and Random Forest. Hyperparameter tuning with GridSearchCV and
RandomizedSearchCV was employed to enhance model performance. The Random Forest
classifier outperformed other models, achieving an accuracy of 91% and an
F1-score of 0.89. Evaluation metrics, including precision, recall, and
confusion matrix, revealed balanced performance across classes. The proposed
model demonstrates strong potential for aiding clinical decision-making by
effectively predicting heart disease. Limitations such as dataset size and
generalizability underscore the need for future studies using larger and more
diverse datasets. This work highlights the utility of machine learning in
healthcare, offering insights for further advancements in predictive
diagnostics.

</details>


### [253] [AI2MMUM: AI-AI Oriented Multi-Modal Universal Model Leveraging Telecom Domain Large Model](https://arxiv.org/pdf/2505.10003)
*Tianyu Jiao, Zhuoran Xiao, Yihang Huang, Chenhui Ye, Yijia Feng, Liyu Cai, Jiang Chang, Fangkun Liu, Yin Xu, Dazhi He, Yunfeng Guan, Wenjun Zhang*

Main category: cs.LG

TL;DR: The paper proposes AI2MMUM, a scalable, task-aware AI model for 6G wireless systems, leveraging multi-modal data and telecom LLM for diverse physical layer tasks with SOTA performance.


<details>
  <summary>Details</summary>
Motivation: To address the need for a universal model in 6G wireless systems capable of handling multi-modal data and diverse air interface tasks efficiently.

Method: Uses a telecom LLM backbone for contextual comprehension, fine-tunes with domain knowledge, employs task instructions with fixed keywords and learnable prompts, and integrates radio modality encoders with adapter layers and task-specific heads.

Result: Achieves state-of-the-art performance on five downstream tasks using WAIR-D and DeepMIMO datasets.

Conclusion: AI2MMUM is a flexible and effective solution for multi-modal universal modeling in 6G wireless systems.

Abstract: Designing a 6G-oriented universal model capable of processing multi-modal
data and executing diverse air interface tasks has emerged as a common goal in
future wireless systems. Building on our prior work in communication
multi-modal alignment and telecom large language model (LLM), we propose a
scalable, task-aware artificial intelligence-air interface multi-modal
universal model (AI2MMUM), which flexibility and effectively perform various
physical layer tasks according to subtle task instructions. The LLM backbone
provides robust contextual comprehension and generalization capabilities, while
a fine-tuning approach is adopted to incorporate domain-specific knowledge. To
enhance task adaptability, task instructions consist of fixed task keywords and
learnable, implicit prefix prompts. Frozen radio modality encoders extract
universal representations and adapter layers subsequently bridge radio and
language modalities. Moreover, lightweight task-specific heads are designed to
directly output task objectives. Comprehensive evaluations demonstrate that
AI2MMUM achieves SOTA performance across five representative physical
environment/wireless channel-based downstream tasks using the WAIR-D and
DeepMIMO datasets.

</details>


### [254] [Sample Complexity of Distributionally Robust Average-Reward Reinforcement Learning](https://arxiv.org/pdf/2505.10007)
*Zijun Chen, Shengbo Wang, Nian Si*

Main category: cs.LG

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Motivated by practical applications where stable long-term performance is
critical-such as robotics, operations research, and healthcare-we study the
problem of distributionally robust (DR) average-reward reinforcement learning.
We propose two algorithms that achieve near-optimal sample complexity. The
first reduces the problem to a DR discounted Markov decision process (MDP),
while the second, Anchored DR Average-Reward MDP, introduces an anchoring state
to stabilize the controlled transition kernels within the uncertainty set.
Assuming the nominal MDP is uniformly ergodic, we prove that both algorithms
attain a sample complexity of $\widetilde{O}\left(|\mathbf{S}||\mathbf{A}|
t_{\mathrm{mix}}^2\varepsilon^{-2}\right)$ for estimating the optimal policy as
well as the robust average reward under KL and $f_k$-divergence-based
uncertainty sets, provided the uncertainty radius is sufficiently small. Here,
$\varepsilon$ is the target accuracy, $|\mathbf{S}|$ and $|\mathbf{A}|$ denote
the sizes of the state and action spaces, and $t_{\mathrm{mix}}$ is the mixing
time of the nominal MDP. This represents the first finite-sample convergence
guarantee for DR average-reward reinforcement learning. We further validate the
convergence rates of our algorithms through numerical experiments.

</details>


### [255] [ImagineBench: Evaluating Reinforcement Learning with Large Language Model Rollouts](https://arxiv.org/pdf/2505.10010)
*Jing-Cheng Pang, Kaiyuan Li, Yidi Wang, Si-Hang Yang, Shengyi Jiang, Yang Yu*

Main category: cs.LG

TL;DR: Introduces ImagineBench, a benchmark for offline RL using real and LLM-generated synthetic rollouts, highlighting suboptimal performance of current methods and future research directions.


<details>
  <summary>Details</summary>
Motivation: Addresses the lack of a standard benchmark for evaluating offline RL algorithms that use both real and LLM-generated synthetic experience (imaginary rollouts).

Method: Introduces ImagineBench, featuring datasets with real and imaginary rollouts, diverse task domains, and natural language instructions for evaluation.

Result: Existing offline RL algorithms perform poorly (35.44% success) on unseen tasks compared to real rollouts (64.37%), emphasizing the need for better methods.

Conclusion: Highlights the necessity for advancements in leveraging LLM-imaginary rollouts and identifies future research opportunities like fast adaptation and multi-modal tasks.

Abstract: A central challenge in reinforcement learning (RL) is its dependence on
extensive real-world interaction data to learn task-specific policies. While
recent work demonstrates that large language models (LLMs) can mitigate this
limitation by generating synthetic experience (noted as imaginary rollouts) for
mastering novel tasks, progress in this emerging field is hindered due to the
lack of a standard benchmark. To bridge this gap, we introduce ImagineBench,
the first comprehensive benchmark for evaluating offline RL algorithms that
leverage both real rollouts and LLM-imaginary rollouts. The key features of
ImagineBench include: (1) datasets comprising environment-collected and
LLM-imaginary rollouts; (2) diverse domains of environments covering
locomotion, robotic manipulation, and navigation tasks; and (3) natural
language task instructions with varying complexity levels to facilitate
language-conditioned policy learning. Through systematic evaluation of
state-of-the-art offline RL algorithms, we observe that simply applying
existing offline RL algorithms leads to suboptimal performance on unseen tasks,
achieving 35.44% success rate in hard tasks in contrast to 64.37% of method
training on real rollouts for hard tasks. This result highlights the need for
algorithm advancements to better leverage LLM-imaginary rollouts. Additionally,
we identify key opportunities for future research: including better utilization
of imaginary rollouts, fast online adaptation and continual learning, and
extension to multi-modal tasks. Our code is publicly available at
https://github.com/LAMDA-RL/ImagineBench.

</details>


### [256] [Optimal normalization in quantum-classical hybrid models for anti-cancer drug response prediction](https://arxiv.org/pdf/2505.10037)
*Takafumi Ito, Lysenko Artem, Tatsuhiko Tsunoda*

Main category: cs.LG

TL;DR: QHML models outperform classical models in anti-cancer drug response prediction when data is optimally normalized using a moderated gradient tanh function.


<details>
  <summary>Details</summary>
Motivation: Small datasets in anti-cancer drug response prediction benefit from QHML's robustness, but data encoding sensitivity is a challenge.

Method: Proposed a normalization strategy using a moderated gradient tanh function to stabilize QHML models.

Result: QHML models showed better performance than classical models with optimal normalization.

Conclusion: The study highlights QHML's potential for biomedical data analysis with quantum computers.

Abstract: Quantum-classical Hybrid Machine Learning (QHML) models are recognized for
their robust performance and high generalization ability even for relatively
small datasets. These qualities offer unique advantages for anti-cancer drug
response prediction, where the number of available samples is typically small.
However, such hybrid models appear to be very sensitive to the data encoding
used at the interface of a neural network and a quantum circuit, with
suboptimal choices leading to stability issues. To address this problem, we
propose a novel strategy that uses a normalization function based on a
moderated gradient version of the $\tanh$. This method transforms the outputs
of the neural networks without concentrating them at the extreme value ranges.
Our idea was evaluated on a dataset of gene expression and drug response
measurements for various cancer cell lines, where we compared the prediction
performance of a classical deep learning model and several QHML models. These
results confirmed that QHML performed better than the classical models when
data was optimally normalized. This study opens up new possibilities for
biomedical data analysis using quantum computers.

</details>


### [257] [Rethinking Circuit Completeness in Language Models: AND, OR, and ADDER Gates](https://arxiv.org/pdf/2505.10039)
*Hang Chen, Jiaying Zhu, Xinyu Yang, Wenya Wang*

Main category: cs.LG

TL;DR: The paper introduces a framework for achieving faithful and complete circuit discovery by decomposing circuits into AND, OR, and ADDER gates, and proposes a noising-denosing intervention method.


<details>
  <summary>Details</summary>
Motivation: Addressing incompleteness in circuit discovery due to partial detection of OR gates, which leads to inconsistent results and omitted mechanisms.

Method: Systematic introduction of AND, OR, and ADDER gates, and a framework combining noising and denoising interventions to identify and distinguish these gates.

Result: The framework successfully restores faithfulness, completeness, and sparsity of circuits, while uncovering properties of logic gates in language models.

Conclusion: The proposed framework enhances circuit discovery by ensuring completeness and faithfulness, and reveals insights into logic gate behavior in language models.

Abstract: Circuit discovery has gradually become one of the prominent methods for
mechanistic interpretability, and research on circuit completeness has also
garnered increasing attention. Methods of circuit discovery that do not
guarantee completeness not only result in circuits that are not fixed across
different runs but also cause key mechanisms to be omitted. The nature of
incompleteness arises from the presence of OR gates within the circuit, which
are often only partially detected in standard circuit discovery methods. To
this end, we systematically introduce three types of logic gates: AND, OR, and
ADDER gates, and decompose the circuit into combinations of these logical
gates. Through the concept of these gates, we derive the minimum requirements
necessary to achieve faithfulness and completeness. Furthermore, we propose a
framework that combines noising-based and denoising-based interventions, which
can be easily integrated into existing circuit discovery methods without
significantly increasing computational complexity. This framework is capable of
fully identifying the logic gates and distinguishing them within the circuit.
In addition to the extensive experimental validation of the framework's ability
to restore the faithfulness, completeness, and sparsity of circuits, using this
framework, we uncover fundamental properties of the three logic gates, such as
their proportions and contributions to the output, and explore how they behave
among the functionalities of language models.

</details>


### [258] [Instance-Prototype Affinity Learning for Non-Exemplar Continual Graph Learning](https://arxiv.org/pdf/2505.10040)
*Lei Song, Jiaxing Li, Shihan Guan, Youyong Kong*

Main category: cs.LG

TL;DR: The paper proposes Instance-Prototype Affinity Learning (IPAL) for Non-Exemplar Continual Graph Learning (NECGL), addressing feature drift and catastrophic forgetting in GNNs. It introduces Topology-Integrated Gaussian Prototypes (TIGP) and Instance-Prototype Affinity Distillation (IPAD) to improve knowledge assimilation and task memory. Evaluations show superior performance over existing methods.


<details>
  <summary>Details</summary>
Motivation: Graph Neural Networks (GNNs) suffer from catastrophic forgetting and feature drift when learning new information. Rehearsal-based methods face memory and privacy issues, while non-exemplar methods like Prototype Replay (PR) struggle with drift. The paper aims to mitigate these challenges.

Method: The authors propose IPAL, leveraging Prototype Contrastive Learning (PCL) to reduce drift. They introduce TIGP for topology-aware feature distribution and IPAD for task memory regularization. A Decision Boundary Perception (DBP) mechanism enhances inter-class discriminability.

Result: Evaluations on four node classification datasets show IPAL outperforms state-of-the-art methods, achieving a better balance between plasticity (learning new tasks) and stability (retaining old knowledge).

Conclusion: IPAL effectively addresses catastrophic forgetting and feature drift in GNNs, offering a robust solution for continual graph learning with superior performance and practical utility.

Abstract: Graph Neural Networks (GNN) endure catastrophic forgetting, undermining their
capacity to preserve previously acquired knowledge amid the assimilation of
novel information. Rehearsal-based techniques revisit historical examples,
adopted as a principal strategy to alleviate this phenomenon. However, memory
explosion and privacy infringements impose significant constraints on their
utility. Non-Exemplar methods circumvent the prior issues through Prototype
Replay (PR), yet feature drift presents new challenges. In this paper, our
empirical findings reveal that Prototype Contrastive Learning (PCL) exhibits
less pronounced drift than conventional PR. Drawing upon PCL, we propose
Instance-Prototype Affinity Learning (IPAL), a novel paradigm for Non-Exemplar
Continual Graph Learning (NECGL). Exploiting graph structural information, we
formulate Topology-Integrated Gaussian Prototypes (TIGP), guiding feature
distributions towards high-impact nodes to augment the model's capacity for
assimilating new knowledge. Instance-Prototype Affinity Distillation (IPAD)
safeguards task memory by regularizing discontinuities in class relationships.
Moreover, we embed a Decision Boundary Perception (DBP) mechanism within PCL,
fostering greater inter-class discriminability. Evaluations on four node
classification benchmark datasets demonstrate that our method outperforms
existing state-of-the-art methods, achieving a better trade-off between
plasticity and stability.

</details>


### [259] [Financial Fraud Detection Using Explainable AI and Stacking Ensemble Methods](https://arxiv.org/pdf/2505.10050)
*Fahad Almalki, Mehedi Masud*

Main category: cs.LG

TL;DR: A fraud detection framework combining gradient boosting models (XGBoost, LightGBM, CatBoost) with XAI techniques (SHAP, LIME, PDP, PFI) achieves high accuracy (99%) and interpretability, addressing regulatory and trust issues.


<details>
  <summary>Details</summary>
Motivation: Traditional ML models lack transparency, hindering regulatory compliance and stakeholder trust. This research aims to balance accuracy and interpretability in fraud detection.

Method: Proposes a stacking ensemble of gradient boosting models (XGBoost, LightGBM, CatBoost) enhanced with XAI techniques (SHAP, LIME, PDP, PFI) for feature selection and prediction explanation.

Result: Achieved 99% accuracy and 0.99 AUC-ROC on the IEEE-CIS Fraud Detection dataset, outperforming recent approaches.

Conclusion: Combining high accuracy with interpretability is feasible, offering ethical and trustworthy fraud detection solutions.

Abstract: Traditional machine learning models often prioritize predictive accuracy,
often at the expense of model transparency and interpretability. The lack of
transparency makes it difficult for organizations to comply with regulatory
requirements and gain stakeholders trust. In this research, we propose a fraud
detection framework that combines a stacking ensemble of well-known gradient
boosting models: XGBoost, LightGBM, and CatBoost. In addition, explainable
artificial intelligence (XAI) techniques are used to enhance the transparency
and interpretability of the model's decisions. We used SHAP (SHapley Additive
Explanations) for feature selection to identify the most important features.
Further efforts were made to explain the model's predictions using Local
Interpretable Model-Agnostic Explanation (LIME), Partial Dependence Plots
(PDP), and Permutation Feature Importance (PFI). The IEEE-CIS Fraud Detection
dataset, which includes more than 590,000 real transaction records, was used to
evaluate the proposed model. The model achieved a high performance with an
accuracy of 99% and an AUC-ROC score of 0.99, outperforming several recent
related approaches. These results indicate that combining high prediction
accuracy with transparent interpretability is possible and could lead to a more
ethical and trustworthy solution in financial fraud detection.

</details>


### [260] [JointDistill: Adaptive Multi-Task Distillation for Joint Depth Estimation and Scene Segmentation](https://arxiv.org/pdf/2505.10057)
*Tiancong Cheng, Ying Zhang, Yuxuan Liang, Roger Zimmermann, Zhiwen Yu, Bin Guo*

Main category: cs.LG

TL;DR: A self-adaptive distillation method improves joint modeling of depth estimation and scene segmentation by dynamically adjusting teacher knowledge and using a knowledge trajectory to prevent errors.


<details>
  <summary>Details</summary>
Motivation: Joint modeling reduces storage and training efforts for depth estimation and scene segmentation in intelligent transportation systems.

Method: Proposes a self-adaptive distillation method to dynamically adjust teacher knowledge and a knowledge trajectory to guide learning.

Result: Achieves clear improvement on datasets like Cityscapes and NYU-v2 compared to state-of-the-art solutions.

Conclusion: The method effectively enhances unified modeling and learning efficiency, with code provided for further use.

Abstract: Depth estimation and scene segmentation are two important tasks in
intelligent transportation systems. A joint modeling of these two tasks will
reduce the requirement for both the storage and training efforts. This work
explores how the multi-task distillation could be used to improve such unified
modeling. While existing solutions transfer multiple teachers' knowledge in a
static way, we propose a self-adaptive distillation method that can dynamically
adjust the knowledge amount from each teacher according to the student's
current learning ability. Furthermore, as multiple teachers exist, the
student's gradient update direction in the distillation is more prone to be
erroneous where knowledge forgetting may occur. To avoid this, we propose a
knowledge trajectory to record the most essential information that a model has
learnt in the past, based on which a trajectory-based distillation loss is
designed to guide the student to follow the learning curve similarly in a
cost-effective way. We evaluate our method on multiple benchmarking datasets
including Cityscapes and NYU-v2. Compared to the state-of-the-art solutions,
our method achieves a clearly improvement. The code is provided in the
supplementary materials.

</details>


### [261] [ChronoSteer: Bridging Large Language Model and Time Series Foundation Model via Synthetic Data](https://arxiv.org/pdf/2505.10083)
*Chengsen Wang, Qi Qi, Zhongwen Rao, Lujia Pan, Jingyu Wang, Jianxin Liao*

Main category: cs.LG

TL;DR: The paper proposes ChronoSteer, a multimodal model integrating LLMs and TSFMs for improved forecasting by leveraging both textual and temporal data. It uses synthetic data for training and achieves significant accuracy gains.


<details>
  <summary>Details</summary>
Motivation: Existing forecasting methods are limited by unimodal time series data, missing the potential of rich textual information. Combining LLMs (for textual reasoning) and TSFMs (for temporal modeling) can enhance forecasting.

Method: A decoupled framework where an LLM transforms textual events into revision instructions to steer TSFM outputs. ChronoSteer is introduced as the multimodal TSFM, trained on synthetic data via a two-stage strategy.

Result: ChronoSteer achieves a 25.7% accuracy improvement over unimodal baselines and 22.5% over prior multimodal methods.

Conclusion: The integration of LLMs and TSFMs via ChronoSteer, trained on synthetic data, significantly enhances forecasting accuracy, addressing data scarcity and information leakage.

Abstract: Conventional forecasting methods rely on unimodal time series data, limiting
their ability to exploit rich textual information. Recently, large language
models (LLMs) and time series foundation models (TSFMs) have demonstrated
powerful capability in textual reasoning and temporal modeling, respectively.
Integrating the strengths of both to construct a multimodal model that
concurrently leverages both temporal and textual information for future
inference has emerged as a critical research challenge. To address the scarcity
of event-series paired data, we propose a decoupled framework: an LLM is
employed to transform textual events into revision instructions, which are then
used to steer the output of TSFM. To implement this framework, we introduce
ChronoSteer, a multimodal TSFM that can be steered through textual revision
instructions, effectively bridging LLM and TSFM. Moreover, to mitigate the
shortage of cross-modal instruction-series paired data, we devise a two-stage
training strategy based on synthetic data. In addition, we also construct a
high-quality multimodal time series forecasting benchmark to address the
information leakage concerns during evaluation. After integrating with an LLM,
ChronoSteer, which is trained exclusively on synthetic data, achieves a 25.7%
improvement in prediction accuracy compared to the unimodal backbone and a
22.5% gain over the previous state-of-the-art multimodal method.

</details>


### [262] [Learning Virtual Machine Scheduling in Cloud Computing through Language Agents](https://arxiv.org/pdf/2505.10117)
*JieHao Wu, Ziwei Wang, Junjie Sheng, Wenhao Li, Xiangfei Wang, Jun Luo*

Main category: cs.LG

TL;DR: MiCo, a hierarchical LLM-driven framework, solves VM scheduling (ODMBP) with dynamic, adaptable heuristics, outperforming traditional methods with a 96.9% competitive ratio.


<details>
  <summary>Details</summary>
Motivation: Traditional VM scheduling methods lack adaptability, heuristic rigidity, and learning-based generalizability. MiCo addresses these gaps using LLMs for dynamic, interpretable solutions.

Method: MiCo formulates ODMBP as SMDP-Option, using a two-stage LLM-driven architecture (Option Miner for non-context-aware strategies, Option Composer for integrating contextual strategies).

Result: Achieves 96.9% competitive ratio in large-scale (10,000+ VMs) and nonstationary scenarios, demonstrating robustness.

Conclusion: MiCo effectively handles complex, large-scale VM scheduling, offering adaptability and high performance in dynamic cloud environments.

Abstract: In cloud services, virtual machine (VM) scheduling is a typical Online
Dynamic Multidimensional Bin Packing (ODMBP) problem, characterized by
large-scale complexity and fluctuating demands. Traditional optimization
methods struggle to adapt to real-time changes, domain-expert-designed
heuristic approaches suffer from rigid strategies, and existing learning-based
methods often lack generalizability and interpretability. To address these
limitations, this paper proposes a hierarchical language agent framework named
MiCo, which provides a large language model (LLM)-driven heuristic design
paradigm for solving ODMBP. Specifically, ODMBP is formulated as a Semi-Markov
Decision Process with Options (SMDP-Option), enabling dynamic scheduling
through a two-stage architecture, i.e., Option Miner and Option Composer.
Option Miner utilizes LLMs to discover diverse and useful non-context-aware
strategies by interacting with constructed environments. Option Composer
employs LLMs to discover a composing strategy that integrates the
non-context-aware strategies with the contextual ones. Extensive experiments on
real-world enterprise datasets demonstrate that MiCo achieves a 96.9\%
competitive ratio in large-scale scenarios involving more than 10,000 virtual
machines. It maintains high performance even under nonstationary request flows
and diverse configurations, thus validating its effectiveness in complex and
large-scale cloud environments.

</details>


### [263] [All You Need Is Synthetic Task Augmentation](https://arxiv.org/pdf/2505.10120)
*Guillaume Godin*

Main category: cs.LG

TL;DR: A novel method combines Graph Transformer with XGBoost-derived synthetic tasks for multitask molecular property prediction, outperforming single-task XGBoost without pretraining.


<details>
  <summary>Details</summary>
Motivation: Integrating rule-based models into neural networks is challenging. Current methods need extensive pretraining or extra techniques for performance.

Method: Joint training of a Graph Transformer on multitask molecular property targets and synthetic XGBoost-derived tasks.

Result: Significant improvement in 16 out of 19 molecular property tasks, outperforming XGBoost.

Conclusion: Synthetic task augmentation enhances neural model performance without feature injection or pretraining.

Abstract: Injecting rule-based models like Random Forests into differentiable neural
network frameworks remains an open challenge in machine learning. Recent
advancements have demonstrated that pretrained models can generate efficient
molecular embeddings. However, these approaches often require extensive
pretraining and additional techniques, such as incorporating posterior
probabilities, to boost performance. In our study, we propose a novel strategy
that jointly trains a single Graph Transformer neural network on both sparse
multitask molecular property experimental targets and synthetic targets derived
from XGBoost models trained on Osmordred molecular descriptors. These synthetic
tasks serve as independent auxiliary tasks. Our results show consistent and
significant performance improvement across all 19 molecular property prediction
tasks. For 16 out of 19 targets, the multitask Graph Transformer outperforms
the XGBoost single-task learner. This demonstrates that synthetic task
augmentation is an effective method for enhancing neural model performance in
multitask molecular property prediction without the need for feature injection
or pretraining.

</details>


### [264] [Enhancing the Performance of Global Model by Improving the Adaptability of Local Models in Federated Learning](https://arxiv.org/pdf/2505.10125)
*Wujun Zhou, Shu Ding, ZeLin Li, Wei Wang*

Main category: cs.LG

TL;DR: The paper proposes enhancing federated learning by improving local model adaptability, leading to a better-performing global model.


<details>
  <summary>Details</summary>
Motivation: Addressing challenges of heterogeneous data distributions and privacy in federated learning to improve global model performance.

Method: Introduces adaptability of local models, formalizes it into a training objective with constraints, and provides a feasible solution.

Result: Experiments show significant improvement in local model adaptability and global model performance over baselines.

Conclusion: The method effectively enhances federated learning by optimizing local model adaptability.

Abstract: Federated learning enables the clients to collaboratively train a global
model, which is aggregated from local models. Due to the heterogeneous data
distributions over clients and data privacy in federated learning, it is
difficult to train local models to achieve a well-performed global model. In
this paper, we introduce the adaptability of local models, i.e., the average
performance of local models on data distributions over clients, and enhance the
performance of the global model by improving the adaptability of local models.
Since each client does not know the data distributions over other clients, the
adaptability of the local model cannot be directly optimized. First, we provide
the property of an appropriate local model which has good adaptability on the
data distributions over clients. Then, we formalize the property into the local
training objective with a constraint and propose a feasible solution to train
the local model. Extensive experiments on federated learning benchmarks
demonstrate that our method significantly improves the adaptability of local
models and achieves a well-performed global model that consistently outperforms
the baseline methods.

</details>


### [265] [Robust Federated Learning on Edge Devices with Domain Heterogeneity](https://arxiv.org/pdf/2505.10128)
*Huy Q. Le, Latif U. Khan, Choong Seon Hong*

Main category: cs.LG

TL;DR: FedAPC improves FL model robustness under domain heterogeneity using prototype augmentation, outperforming SOTA baselines.


<details>
  <summary>Details</summary>
Motivation: Address statistical heterogeneity in FL, particularly domain heterogeneity, to enhance global model convergence.

Method: Introduces FedAPC, a prototype-based FL framework using augmented prototypes to diversify features and align local-global representations.

Result: Outperforms SOTA baselines on Office-10 and Digits datasets, showing superior performance.

Conclusion: FedAPC effectively mitigates domain heterogeneity in FL, improving model generalization and robustness.

Abstract: Federated Learning (FL) allows collaborative training while ensuring data
privacy across distributed edge devices, making it a popular solution for
privacy-sensitive applications. However, FL faces significant challenges due to
statistical heterogeneity, particularly domain heterogeneity, which impedes the
global mode's convergence. In this study, we introduce a new framework to
address this challenge by improving the generalization ability of the FL global
model under domain heterogeneity, using prototype augmentation. Specifically,
we introduce FedAPC (Federated Augmented Prototype Contrastive Learning), a
prototype-based FL framework designed to enhance feature diversity and model
robustness. FedAPC leverages prototypes derived from the mean features of
augmented data to capture richer representations. By aligning local features
with global prototypes, we enable the model to learn meaningful semantic
features while reducing overfitting to any specific domain. Experimental
results on the Office-10 and Digits datasets illustrate that our framework
outperforms SOTA baselines, demonstrating superior performance.

</details>


### [266] [QuXAI: Explainers for Hybrid Quantum Machine Learning Models](https://arxiv.org/pdf/2505.10167)
*Saikat Barua, Mostafizur Rahman, Shehenaz Khaled, Md Jafor Sadek, Rafiul Islam, Shahnewaz Siddique*

Main category: cs.LG

TL;DR: The paper introduces QuXAI, a framework for explaining hybrid quantum-classical ML models, addressing the lack of explainability in HQML systems.


<details>
  <summary>Details</summary>
Motivation: The complexity of HQML models leads to black-box behavior, undermining transparency and reliability. Existing XAI methods are insufficient for HQML architectures.

Method: QuXAI uses Q-MEDLEY, an explainer combining feature-based inferences and visualization to analyze quantum feature maps and classical learning.

Result: Q-MEDLEY identifies influential classical aspects, separates noise, and performs comparably to classical XAI techniques. Ablation studies highlight its composite structure.

Conclusion: QuXAI enhances interpretability and reliability of HQML models, promoting safer and more responsible use of quantum-enhanced AI.

Abstract: The emergence of hybrid quantum-classical machine learning (HQML) models
opens new horizons of computational intelligence but their fundamental
complexity frequently leads to black box behavior that undermines transparency
and reliability in their application. Although XAI for quantum systems still in
its infancy, a major research gap is evident in robust global and local
explainability approaches that are designed for HQML architectures that employ
quantized feature encoding followed by classical learning. The gap is the focus
of this work, which introduces QuXAI, an framework based upon Q-MEDLEY, an
explainer for explaining feature importance in these hybrid systems. Our model
entails the creation of HQML models incorporating quantum feature maps, the use
of Q-MEDLEY, which combines feature based inferences, preserving the quantum
transformation stage and visualizing the resulting attributions. Our result
shows that Q-MEDLEY delineates influential classical aspects in HQML models, as
well as separates their noise, and competes well against established XAI
techniques in classical validation settings. Ablation studies more
significantly expose the virtues of the composite structure used in Q-MEDLEY.
The implications of this work are critically important, as it provides a route
to improve the interpretability and reliability of HQML models, thus promoting
greater confidence and being able to engage in safer and more responsible use
of quantum-enhanced AI technology.

</details>


### [267] [ComplexFormer: Disruptively Advancing Transformer Inference Ability via Head-Specific Complex Vector Attention](https://arxiv.org/pdf/2505.10222)
*Jintian Shao, Hongyi Huang, Jiayi Wu, Beiwen Zhang, ZhiYu Wu, You Shan, MingKai Zheng*

Main category: cs.LG

TL;DR: ComplexFormer introduces Complex Multi-Head Attention (CMHA) to unify semantic and positional differences in the complex plane, improving flexibility and performance in Transformer models.


<details>
  <summary>Details</summary>
Motivation: Address limitations of prior methods in integrating positional information and semantic differences in multi-head attention, which often apply uniform adjustments or model them disparately.

Method: Proposes CMHA with per-head Euler transformation and adaptive differential rotation to model semantic and positional differences as complex plane operations.

Result: Outperforms baselines in language tasks, showing lower perplexity, better long-context coherence, and parameter efficiency.

Conclusion: ComplexFormer offers a more expressive and adaptable attention mechanism, enhancing Transformer performance.

Abstract: Transformer models rely on self-attention to capture token dependencies but
face challenges in effectively integrating positional information while
allowing multi-head attention (MHA) flexibility. Prior methods often model
semantic and positional differences disparately or apply uniform positional
adjustments across heads, potentially limiting representational capacity. This
paper introduces ComplexFormer, featuring Complex Multi-Head Attention-CMHA.
CMHA empowers each head to independently model semantic and positional
differences unified within the complex plane, representing interactions as
rotations and scaling. ComplexFormer incorporates two key improvements: (1) a
per-head Euler transformation, converting real-valued query/key projections
into polar-form complex vectors for head-specific complex subspace operation;
and (2) a per-head adaptive differential rotation mechanism,
exp[i(Adapt(ASmn,i) + Delta(Pmn),i)], allowing each head to learn distinct
strategies for integrating semantic angle differences (ASmn,i) with relative
positional encodings (Delta(Pmn),i). Extensive experiments on language
modeling, text generation, code generation, and mathematical reasoning show
ComplexFormer achieves superior performance, significantly lower generation
perplexity , and improved long-context coherence compared to strong baselines
like RoPE-Transformers. ComplexFormer demonstrates strong parameter efficiency,
offering a more expressive, adaptable attention mechanism.

</details>


### [268] [Does Scaling Law Apply in Time Series Forecasting?](https://arxiv.org/pdf/2505.10172)
*Zeyan Li, Libing Chen, Yin Tang*

Main category: cs.LG

TL;DR: Alinear is an ultra-lightweight time series forecasting model that achieves competitive performance with minimal parameters, challenging the need for large-scale models.


<details>
  <summary>Details</summary>
Motivation: To question the necessity of scaling laws in time series forecasting and propose a more efficient alternative to large models.

Method: Introduces a horizon-aware adaptive decomposition mechanism and progressive frequency attenuation strategy for stable predictions without attention mechanisms.

Result: Outperforms large models on seven benchmarks using <1% of their parameters, with strong accuracy across all forecast horizons.

Conclusion: Challenges the belief that larger models are better and advocates for efficient, adaptive designs in time series forecasting.

Abstract: Rapid expansion of model size has emerged as a key challenge in time series
forecasting. From early Transformer with tens of megabytes to recent
architectures like TimesNet with thousands of megabytes, performance gains have
often come at the cost of exponentially increasing parameter counts. But is
this scaling truly necessary? To question the applicability of the scaling law
in time series forecasting, we propose Alinear, an ultra-lightweight
forecasting model that achieves competitive performance using only k-level
parameters. We introduce a horizon-aware adaptive decomposition mechanism that
dynamically rebalances component emphasis across different forecast lengths,
alongside a progressive frequency attenuation strategy that achieves stable
prediction in various forecasting horizons without incurring the computational
overhead of attention mechanisms. Extensive experiments on seven benchmark
datasets demonstrate that Alinear consistently outperforms large-scale models
while using less than 1% of their parameters, maintaining strong accuracy
across both short and ultra-long forecasting horizons. Moreover, to more fairly
evaluate model efficiency, we propose a new parameter-aware evaluation metric
that highlights the superiority of ALinear under constrained model budgets. Our
analysis reveals that the relative importance of trend and seasonal components
varies depending on data characteristics rather than following a fixed pattern,
validating the necessity of our adaptive design. This work challenges the
prevailing belief that larger models are inherently better and suggests a
paradigm shift toward more efficient time series modeling.

</details>


### [269] [Superposition Yields Robust Neural Scaling](https://arxiv.org/pdf/2505.10465)
*Yizhou liu, Ziming Liu, Jeff Gore*

Main category: cs.LG

TL;DR: The paper explores the neural scaling law in LLMs, attributing it to representation superposition and feature frequency, validated by toy models and real LLM analysis.


<details>
  <summary>Details</summary>
Motivation: To understand why larger LLMs perform better, focusing on the unclear origin of neural scaling laws.

Method: Constructed a toy model based on superposition and feature frequency, analyzed real LLMs, and compared with Chinchilla scaling law.

Result: Weak superposition scales loss with feature frequency; strong superposition scales loss inversely with model dimension, matching real LLM behavior.

Conclusion: Representation superposition is key to neural scaling laws, suggesting potential for improved training and architectures.

Abstract: The success of today's large language models (LLMs) depends on the
observation that larger models perform better. However, the origin of this
neural scaling law -- the finding that loss decreases as a power law with model
size -- remains unclear. Starting from two empirical principles -- that LLMs
represent more things than the model dimensions (widths) they have (i.e.,
representations are superposed), and that words or concepts in language occur
with varying frequencies -- we constructed a toy model to study the loss
scaling with model size. We found that when superposition is weak, meaning only
the most frequent features are represented without interference, the scaling of
loss with model size depends on the underlying feature frequency; if feature
frequencies follow a power law, so does the loss. In contrast, under strong
superposition, where all features are represented but overlap with each other,
the loss becomes inversely proportional to the model dimension across a wide
range of feature frequency distributions. This robust scaling behavior is
explained geometrically: when many more vectors are packed into a lower
dimensional space, the interference (squared overlaps) between vectors scales
inversely with that dimension. We then analyzed four families of open-sourced
LLMs and found that they exhibit strong superposition and quantitatively match
the predictions of our toy model. The Chinchilla scaling law turned out to also
agree with our results. We conclude that representation superposition is an
important mechanism underlying the observed neural scaling laws. We anticipate
that these insights will inspire new training strategies and model
architectures to achieve better performance with less computation and fewer
parameters.

</details>


### [270] [Defect Detection in Photolithographic Patterns Using Deep Learning Models Trained on Synthetic Data](https://arxiv.org/pdf/2505.10192)
*Prashant P. Shinde, Priyadarshini P. Pai, Shashishekar P. Adiga, K. Subramanya Mayya, Yongbeom Seo, Myungsoo Hwang, Heeyoung Go, Changmin Park*

Main category: cs.LG

TL;DR: The paper addresses the challenge of detecting extremely small defects in EUV photolithography for semiconductor manufacturing by generating synthetic SEM images with annotated defects. YOLOv8 outperforms other models in detecting these defects, demonstrating the viability of synthetic data for training robust ML models.


<details>
  <summary>Details</summary>
Motivation: The lack of defect-annotated data for small defects in semiconductor manufacturing hinders the deployment of deep learning models. Synthetic data generation is proposed as a solution.

Method: Artificially generated SEM images of line patterns with known defects are used to train object detection models (YOLOv8, EfficientNet, SSD). Performance is evaluated based on defect size.

Result: YOLOv8 achieves 96% mean average precision, outperforming EfficientNet (83%) and SSD (77%). It reliably detects smaller defects and performs well on real SEM data (84.6% for Bridge, 78.3% for Break defects).

Conclusion: Synthetic data is a viable alternative to real-world data for training robust defect detection models in semiconductor manufacturing.

Abstract: In the photolithographic process vital to semiconductor manufacturing,
various types of defects appear during EUV pattering. Due to ever-shrinking
pattern size, these defects are extremely small and cause false or missed
detection during inspection. Specifically, the lack of defect-annotated quality
data with good representation of smaller defects has prohibited deployment of
deep learning based defect detection models in fabrication lines. To resolve
the problem of data unavailability, we artificially generate scanning electron
microscopy (SEM) images of line patterns with known distribution of defects and
autonomously annotate them. We then employ state-of-the-art object detection
models to investigate defect detection performance as a function of defect
size, much smaller than the pitch width. We find that the real-time object
detector YOLOv8 has the best mean average precision of 96% as compared to
EfficientNet, 83%, and SSD, 77%, with the ability to detect smaller defects. We
report the smallest defect size that can be detected reliably. When tested on
real SEM data, the YOLOv8 model correctly detected 84.6% of Bridge defects and
78.3% of Break defects across all relevant instances. These promising results
suggest that synthetic data can be used as an alternative to real-world data in
order to develop robust machine-learning models.

</details>


### [271] [Parallel Scaling Law for Language Models](https://arxiv.org/pdf/2505.10475)
*Mouxiang Chen, Binyuan Hui, Zeyu Cui, Jiaxi Yang, Dayiheng Liu, Jianling Sun, Junyang Lin, Zhongxin Liu*

Main category: cs.LG

TL;DR: ParScale introduces parallel scaling, a method to efficiently scale language models by increasing parallel computation during training and inference, reducing memory and latency costs compared to traditional scaling methods.


<details>
  <summary>Details</summary>
Motivation: To address the inefficiency of traditional scaling methods (parameter or inference-time scaling) by leveraging parallel computation.

Method: Applies $P$ diverse transformations to input, executes parallel forward passes, and dynamically aggregates outputs, reusing existing parameters.

Result: ParScale achieves performance improvements with 22x less memory and 6x less latency than parameter scaling, and can adapt pre-trained models with minimal post-training.

Conclusion: ParScale offers a cost-effective scaling alternative, enabling powerful models in low-resource scenarios and redefining computation's role in ML.

Abstract: It is commonly believed that scaling language models should commit a
significant space or time cost, by increasing the parameters (parameter
scaling) or output tokens (inference-time scaling). We introduce the third and
more inference-efficient scaling paradigm: increasing the model's parallel
computation during both training and inference time. We apply $P$ diverse and
learnable transformations to the input, execute forward passes of the model in
parallel, and dynamically aggregate the $P$ outputs. This method, namely
parallel scaling (ParScale), scales parallel computation by reusing existing
parameters and can be applied to any model structure, optimization procedure,
data, or task. We theoretically propose a new scaling law and validate it
through large-scale pre-training, which shows that a model with $P$ parallel
streams is similar to scaling the parameters by $O(\log P)$ while showing
superior inference efficiency. For example, ParScale can use up to 22$\times$
less memory increase and 6$\times$ less latency increase compared to parameter
scaling that achieves the same performance improvement. It can also recycle an
off-the-shelf pre-trained model into a parallelly scaled one by post-training
on a small amount of tokens, further reducing the training budget. The new
scaling law we discovered potentially facilitates the deployment of more
powerful models in low-resource scenarios, and provides an alternative
perspective for the role of computation in machine learning.

</details>


### [272] [A multi-head deep fusion model for recognition of cattle foraging events using sound and movement signals](https://arxiv.org/pdf/2505.10198)
*Mariano Ferrero, José Omar Chelotti, Luciano Sebastián Martinez-Rau, Leandro Vignolo, Martín Pires, Julio Ricardo Galli, Leonardo Luis Giovanini, Hugo Leonardo Rufiner*

Main category: cs.LG

TL;DR: A deep neural network fusing acoustic and inertial signals improves feeding behavior monitoring in cattle, outperforming traditional methods by 14% in F1-score.


<details>
  <summary>Details</summary>
Motivation: Automating feeding behavior monitoring in grazing cattle enhances diet formulation and early detection of metabolic issues, but existing sensor-based methods lack multi-sensor fusion for precision.

Method: Proposes a deep neural network combining convolutional, recurrent, and dense layers to fuse acoustic and inertial signals, exploring feature-level fusion for better performance.

Result: The model achieved an F1-score of 0.802, a 14% improvement over prior methods, with feature-level fusion outperforming data and decision-level fusion.

Conclusion: Multi-sensor fusion via deep learning significantly enhances feeding behavior monitoring, offering practical benefits for herd management.

Abstract: Monitoring feeding behaviour is a relevant task for efficient herd management
and the effective use of available resources in grazing cattle. The ability to
automatically recognise animals' feeding activities through the identification
of specific jaw movements allows for the improvement of diet formulation, as
well as early detection of metabolic problems and symptoms of animal
discomfort, among other benefits. The use of sensors to obtain signals for such
monitoring has become popular in the last two decades. The most frequently
employed sensors include accelerometers, microphones, and cameras, each with
its own set of advantages and drawbacks. An unexplored aspect is the
simultaneous use of multiple sensors with the aim of combining signals in order
to enhance the precision of the estimations. In this direction, this work
introduces a deep neural network based on the fusion of acoustic and inertial
signals, composed of convolutional, recurrent, and dense layers. The main
advantage of this model is the combination of signals through the automatic
extraction of features independently from each of them. The model has emerged
from an exploration and comparison of different neural network architectures
proposed in this work, which carry out information fusion at different levels.
Feature-level fusion has outperformed data and decision-level fusion by at
least a 0.14 based on the F1-score metric. Moreover, a comparison with
state-of-the-art machine learning methods is presented, including traditional
and deep learning approaches. The proposed model yielded an F1-score value of
0.802, representing a 14% increase compared to previous methods. Finally,
results from an ablation study and post-training quantization evaluation are
also reported.

</details>


### [273] [RouteNator: A Router-Based Multi-Modal Architecture for Generating Synthetic Training Data for Function Calling LLMs](https://arxiv.org/pdf/2505.10495)
*Vibha Belavadi, Tushar Vatsa, Dewang Sultania, Suhas Suresha, Ishita Verma, Cheng Chen, Tracy Holloway King, Michael Friedrich*

Main category: cs.LG

TL;DR: The paper introduces a router-based architecture for generating high-quality synthetic data to fine-tune LLMs for function calling tasks, improving accuracy and performance over traditional methods.


<details>
  <summary>Details</summary>
Motivation: The lack of real-world task-specific data and privacy constraints necessitate synthetic data generation, but existing methods fail in diversity and complexity.

Method: A novel router-based architecture uses domain resources (metadata, knowledge graphs) and text/vision-to-text models to generate synthetic data matching real-world distributions.

Result: Evaluation shows significant improvements in function classification and API parameter selection, with models outperforming traditional approaches.

Conclusion: The proposed method sets new benchmarks for function calling tasks by addressing limitations of synthetic data generation.

Abstract: This paper addresses fine-tuning Large Language Models (LLMs) for function
calling tasks when real user interaction data is unavailable. In digital
content creation tools, where users express their needs through natural
language queries that must be mapped to API calls, the lack of real-world
task-specific data and privacy constraints for training on it necessitate
synthetic data generation. Existing approaches to synthetic data generation
fall short in diversity and complexity, failing to replicate real-world data
distributions and leading to suboptimal performance after LLM fine-tuning. We
present a novel router-based architecture that leverages domain resources like
content metadata and structured knowledge graphs, along with text-to-text and
vision-to-text language models to generate high-quality synthetic training
data. Our architecture's flexible routing mechanism enables synthetic data
generation that matches observed real-world distributions, addressing a
fundamental limitation of traditional approaches. Evaluation on a comprehensive
set of real user queries demonstrates significant improvements in both function
classification accuracy and API parameter selection. Models fine-tuned with our
synthetic data consistently outperform traditional approaches, establishing new
benchmarks for function calling tasks.

</details>


### [274] [Informed Forecasting: Leveraging Auxiliary Knowledge to Boost LLM Performance on Time Series Forecasting](https://arxiv.org/pdf/2505.10213)
*Mohammadmahdi Ghasemloo, Alireza Moradi*

Main category: cs.LG

TL;DR: A novel framework enhances LLMs for time series forecasting by transferring cross-domain knowledge, outperforming uninformed baselines.


<details>
  <summary>Details</summary>
Motivation: To improve LLM performance in time series forecasting, addressing gaps in domain-specific tasks.

Method: Proposes a cross-domain knowledge transfer framework, infusing LLMs with structured temporal information.

Result: Knowledge-informed forecasting significantly outperforms uninformed baselines in accuracy and generalization.

Conclusion: Knowledge transfer strategies can effectively bridge LLMs and domain-specific forecasting tasks.

Abstract: With the widespread adoption of Large Language Models (LLMs), there is a
growing need to establish best practices for leveraging their capabilities
beyond traditional natural language tasks. In this paper, a novel cross-domain
knowledge transfer framework is proposed to enhance the performance of LLMs in
time series forecasting -- a task of increasing relevance in fields such as
energy systems, finance, and healthcare. The approach systematically infuses
LLMs with structured temporal information to improve their forecasting
accuracy. This study evaluates the proposed method on a real-world time series
dataset and compares it to a naive baseline where the LLM receives no auxiliary
information. Results show that knowledge-informed forecasting significantly
outperforms the uninformed baseline in terms of predictive accuracy and
generalization. These findings highlight the potential of knowledge transfer
strategies to bridge the gap between LLMs and domain-specific forecasting
tasks.

</details>


### [275] [MASSV: Multimodal Adaptation and Self-Data Distillation for Speculative Decoding of Vision-Language Models](https://arxiv.org/pdf/2505.10526)
*Mugilan Ganesan, Shane Segal, Ankur Aggarwal, Nish Sinnadurai, Sean Lie, Vithursan Thangarasa*

Main category: cs.LG

TL;DR: MASSV accelerates vision-language models (VLMs) by adapting small language models into multimodal drafters via a projector and self-distilled tuning, achieving up to 1.46x speedup.


<details>
  <summary>Details</summary>
Motivation: Speculative decoding struggles with VLMs due to draft models lacking visual input processing and mismatched token predictions.

Method: MASSV uses a lightweight projector to connect the VLM's vision encoder to the draft model and employs self-distilled visual instruction tuning.

Result: MASSV increases accepted token length by 30% and achieves 1.46x inference speedup on visually-grounded tasks.

Conclusion: MASSV offers a scalable solution for accelerating current and future VLMs.

Abstract: Speculative decoding significantly accelerates language model inference by
enabling a lightweight draft model to propose multiple tokens that a larger
target model verifies simultaneously. However, applying this technique to
vision-language models (VLMs) presents two fundamental challenges: small
language models that could serve as efficient drafters lack the architectural
components to process visual inputs, and their token predictions fail to match
those of VLM target models that consider visual context. We introduce
Multimodal Adaptation and Self-Data Distillation for Speculative Decoding of
Vision-Language Models (MASSV), which transforms existing small language models
into effective multimodal drafters through a two-phase approach. MASSV first
connects the target VLM's vision encoder to the draft model via a lightweight
trainable projector, then applies self-distilled visual instruction tuning
using responses generated by the target VLM to align token predictions.
Comprehensive experiments across the Qwen2.5-VL and Gemma3 model families
demonstrate that MASSV increases accepted length by up to 30% and delivers
end-to-end inference speedups of up to 1.46x on visually-grounded tasks. MASSV
provides a scalable, architecture-compatible method for accelerating both
current and future VLMs.

</details>


### [276] [SpecOffload: Unlocking Latent GPU Capacity for LLM Inference on Resource-Constrained Devices](https://arxiv.org/pdf/2505.10259)
*Xiangwen Zhuge, Xu Shen, Zeyu Wang, Fan Dang, Xuan Ding, Danyang Li, Yahui Han, Tianxiang Hao, Zheng Yang*

Main category: cs.LG

TL;DR: SpecOffload improves LLM inference efficiency on resource-constrained devices by integrating speculative decoding with offloading, boosting GPU utilization and throughput.


<details>
  <summary>Details</summary>
Motivation: Challenges in compute and memory utilization for LLM inference on devices with limited GPU memory, leading to inefficiencies like GPU underutilization and low memory impact.

Method: Proposes SpecOffload, combining speculative decoding with offloading, optimizing tensor placement and execution orchestration.

Result: Achieves 4.49x better GPU core utilization and 2.54x higher inference throughput compared to baselines.

Conclusion: SpecOffload effectively leverages latent GPU resources for efficient inference at minimal cost.

Abstract: Efficient LLM inference on resource-constrained devices presents significant
challenges in compute and memory utilization. Due to limited GPU memory,
existing systems offload model weights to CPU memory, incurring substantial I/O
overhead between the CPU and GPU. This leads to two major inefficiencies: (1)
GPU cores are underutilized, often remaining idle while waiting for data to be
loaded; and (2) GPU memory has low impact on performance, as reducing its
capacity has minimal effect on overall throughput.In this paper, we propose
SpecOffload, a high-throughput inference engine that embeds speculative
decoding into offloading. Our key idea is to unlock latent GPU resources for
storing and executing a draft model used for speculative decoding, thus
accelerating inference at near-zero additional cost. To support this, we
carefully orchestrate the interleaved execution of target and draft models in
speculative decoding within the offloading pipeline, and propose a planner to
manage tensor placement and select optimal parameters. Compared to the best
baseline, SpecOffload improves GPU core utilization by 4.49x and boosts
inference throughput by 2.54x. Our code is available at
https://github.com/MobiSense/SpecOffload .

</details>


### [277] [Electric Bus Charging Schedules Relying on Real Data-Driven Targets Based on Hierarchical Deep Reinforcement Learning](https://arxiv.org/pdf/2505.10262)
*Jiaju Qi, Lei Lei, Thorsteinn Jonsson, Lajos Hanzo*

Main category: cs.LG

TL;DR: The paper proposes a Hierarchical DRL (HDRL) approach to solve the charging scheduling problem for Electric Buses, using a two-level MDP framework and a novel HDDQN-HER algorithm to minimize costs while meeting charging targets.


<details>
  <summary>Details</summary>
Motivation: The challenge of long-range multi-phase planning with sparse rewards in EB charging scheduling motivates the use of hierarchical reinforcement learning to decouple the problem into manageable levels.

Method: A Hierarchical Double Deep Q-Network (HDDQN) with Hindsight Experience Replay (HER) is proposed, splitting the MDP into high-level (SMDP) and low-level MDPs for charging targets and power settings, respectively.

Result: The hierarchical policy performs as well as the optimal policy of the original MDP, validated through numerical experiments with real-world data.

Conclusion: The HDRL framework effectively addresses the EB charging problem, with the HDDQN-HER algorithm enabling efficient learning and cost minimization.

Abstract: The charging scheduling problem of Electric Buses (EBs) is investigated based
on Deep Reinforcement Learning (DRL). A Markov Decision Process (MDP) is
conceived, where the time horizon includes multiple charging and operating
periods in a day, while each period is further divided into multiple time
steps. To overcome the challenge of long-range multi-phase planning with sparse
reward, we conceive Hierarchical DRL (HDRL) for decoupling the original MDP
into a high-level Semi-MDP (SMDP) and multiple low-level MDPs. The Hierarchical
Double Deep Q-Network (HDDQN)-Hindsight Experience Replay (HER) algorithm is
proposed for simultaneously solving the decision problems arising at different
temporal resolutions. As a result, the high-level agent learns an effective
policy for prescribing the charging targets for every charging period, while
the low-level agent learns an optimal policy for setting the charging power of
every time step within a single charging period, with the aim of minimizing the
charging costs while meeting the charging target. It is proved that the flat
policy constructed by superimposing the optimal high-level policy and the
optimal low-level policy performs as well as the optimal policy of the original
MDP. Since jointly learning both levels of policies is challenging due to the
non-stationarity of the high-level agent and the sampling inefficiency of the
low-level agent, we divide the joint learning process into two phases and
exploit our new HER algorithm to manipulate the experience replay buffers for
both levels of agents. Numerical experiments are performed with the aid of
real-world data to evaluate the performance of the proposed algorithm.

</details>


### [278] [Cutting Through Privacy: A Hyperplane-Based Data Reconstruction Attack in Federated Learning](https://arxiv.org/pdf/2505.10264)
*Francesco Diana, André Nusser, Chuan Xu, Giovanni Neglia*

Main category: cs.LG

TL;DR: A new data reconstruction attack in Federated Learning overcomes limitations of existing methods, enabling perfect recovery of large data batches without prior knowledge.


<details>
  <summary>Details</summary>
Motivation: To address vulnerabilities in FL where malicious servers can reconstruct private data, overcoming assumptions and inefficiencies of current attacks.

Method: Leverages a geometric perspective on fully connected layers to craft malicious model parameters for perfect data recovery.

Result: Outperforms existing methods, achieving perfect reconstruction of data batches two orders of magnitude larger than state-of-the-art.

Conclusion: The attack demonstrates significant improvements in data reconstruction efficiency and scale, highlighting critical FL vulnerabilities.

Abstract: Federated Learning (FL) enables collaborative training of machine learning
models across distributed clients without sharing raw data, ostensibly
preserving data privacy. Nevertheless, recent studies have revealed critical
vulnerabilities in FL, showing that a malicious central server can manipulate
model updates to reconstruct clients' private training data. Existing data
reconstruction attacks have important limitations: they often rely on
assumptions about the clients' data distribution or their efficiency
significantly degrades when batch sizes exceed just a few tens of samples.
  In this work, we introduce a novel data reconstruction attack that overcomes
these limitations. Our method leverages a new geometric perspective on fully
connected layers to craft malicious model parameters, enabling the perfect
recovery of arbitrarily large data batches in classification tasks without any
prior knowledge of clients' data. Through extensive experiments on both image
and tabular datasets, we demonstrate that our attack outperforms existing
methods and achieves perfect reconstruction of data batches two orders of
magnitude larger than the state of the art.

</details>


### [279] [RainPro-8: An Efficient Deep Learning Model to Estimate Rainfall Probabilities Over 8 Hours](https://arxiv.org/pdf/2505.10271)
*Rafael Pablos Sarabia, Joachim Nyborg, Morten Birk, Jeppe Liborius Sjørup, Anders Lillevang Vesterholt, Ira Assent*

Main category: cs.LG

TL;DR: A deep learning model for high-resolution probabilistic precipitation forecasting in Europe, integrating radar, satellite, and NWP data, outperforming existing methods.


<details>
  <summary>Details</summary>
Motivation: Overcome limitations of radar-only models with short lead times and improve accuracy, uncertainty quantification, and computational efficiency.

Method: Integrates radar, satellite, and NWP data with a compact architecture for efficient training and inference, capturing long-range interactions.

Result: Surpasses operational NWP systems, extrapolation-based methods, and deep-learning nowcasting models in accuracy and efficiency.

Conclusion: Sets a new standard for high-resolution precipitation forecasting in Europe, balancing accuracy, interpretability, and computational efficiency.

Abstract: We present a deep learning model for high-resolution probabilistic
precipitation forecasting over an 8-hour horizon in Europe, overcoming the
limitations of radar-only deep learning models with short forecast lead times.
Our model efficiently integrates multiple data sources - including radar,
satellite, and physics-based numerical weather prediction (NWP) - while
capturing long-range interactions, resulting in accurate forecasts with robust
uncertainty quantification through consistent probabilistic maps. Featuring a
compact architecture, it enables more efficient training and faster inference
than existing models. Extensive experiments demonstrate that our model
surpasses current operational NWP systems, extrapolation-based methods, and
deep-learning nowcasting models, setting a new standard for high-resolution
precipitation forecasting in Europe, ensuring a balance between accuracy,
interpretability, and computational efficiency.

</details>


### [280] [Spike-timing-dependent Hebbian learning as noisy gradient descent](https://arxiv.org/pdf/2505.10272)
*Niklas Dexheimer, Sascha Gaudlitz, Johannes Schmidt-Hieber*

Main category: cs.LG

TL;DR: The paper connects Hebbian spike-timing-dependent plasticity to noisy gradient descent, proving it identifies the most active presynaptic neuron and linking it to noisy mirror descent.


<details>
  <summary>Details</summary>
Motivation: Explore learning rules for precise spike-timing in biological neural networks, extending beyond rate-based Hebbian learning.

Method: Relate Hebbian spike-timing-dependent plasticity to noisy gradient descent on a natural loss function.

Result: The learning rule identifies the presynaptic neuron with the highest activity and shows a connection to noisy mirror descent.

Conclusion: The study provides theoretical insights into spike-timing learning rules and their optimization properties.

Abstract: Hebbian learning is a key principle underlying learning in biological neural
networks. It postulates that synaptic changes occur locally, depending on the
activities of pre- and postsynaptic neurons. While Hebbian learning based on
neuronal firing rates is well explored, much less is known about learning rules
that account for precise spike-timing. We relate a Hebbian
spike-timing-dependent plasticity rule to noisy gradient descent with respect
to a natural loss function on the probability simplex. This connection allows
us to prove that the learning rule eventually identifies the presynaptic neuron
with the highest activity. We also discover an intrinsic connection to noisy
mirror descent.

</details>


### [281] [Optimizing Electric Bus Charging Scheduling with Uncertainties Using Hierarchical Deep Reinforcement Learning](https://arxiv.org/pdf/2505.10296)
*Jiaju Qi, Lei Lei, Thorsteinn Jonsson, Dusit Niyato*

Main category: cs.LG

TL;DR: A Hierarchical Deep Reinforcement Learning (HDRL) approach, DAC-MAPPO-E, optimizes Electric Bus (EB) charging schedules by addressing uncertainties and scalability for large fleets.


<details>
  <summary>Details</summary>
Motivation: The need for efficient and scalable EB charging policies due to uncertainties in travel time, energy consumption, and electricity prices.

Method: Proposes DAC-MAPPO-E, combining Double Actor-Critic and Multi-Agent Proximal Policy Optimization, with enhancements for scalability and multi-timescale decision-making.

Result: DAC-MAPPO-E outperforms in optimizing EB fleet charging schedules, as shown by real-world data experiments.

Conclusion: The approach effectively addresses scalability and efficiency challenges in EB charging optimization.

Abstract: The growing adoption of Electric Buses (EBs) represents a significant step
toward sustainable development. By utilizing Internet of Things (IoT) systems,
charging stations can autonomously determine charging schedules based on
real-time data. However, optimizing EB charging schedules remains a critical
challenge due to uncertainties in travel time, energy consumption, and
fluctuating electricity prices. Moreover, to address real-world complexities,
charging policies must make decisions efficiently across multiple time scales
and remain scalable for large EB fleets. In this paper, we propose a
Hierarchical Deep Reinforcement Learning (HDRL) approach that reformulates the
original Markov Decision Process (MDP) into two augmented MDPs. To solve these
MDPs and enable multi-timescale decision-making, we introduce a novel HDRL
algorithm, namely Double Actor-Critic Multi-Agent Proximal Policy Optimization
Enhancement (DAC-MAPPO-E). Scalability challenges of the Double Actor-Critic
(DAC) algorithm for large-scale EB fleets are addressed through enhancements at
both decision levels. At the high level, we redesign the decentralized actor
network and integrate an attention mechanism to extract relevant global state
information for each EB, decreasing the size of neural networks. At the low
level, the Multi-Agent Proximal Policy Optimization (MAPPO) algorithm is
incorporated into the DAC framework, enabling decentralized and coordinated
charging power decisions, reducing computational complexity and enhancing
convergence speed. Extensive experiments with real-world data demonstrate the
superior performance and scalability of DAC-MAPPO-E in optimizing EB fleet
charging schedules.

</details>


### [282] [Defending the Edge: Representative-Attention for Mitigating Backdoor Attacks in Federated Learning](https://arxiv.org/pdf/2505.10297)
*Chibueze Peace Obioma, Youcheng Sun, Mustafa A. Mustafa*

Main category: cs.LG

TL;DR: FeRA is a federated learning defense mechanism that uses cross-client attention to detect backdoor attacks in non-IID data settings, maintaining high accuracy while reducing attack success.


<details>
  <summary>Details</summary>
Motivation: Heterogeneous edge devices produce non-IID data, complicating backdoor attack detection in federated learning.

Method: FeRA leverages cross-client attention and representation reconstruction errors to compute anomaly scores, identifying malicious clients.

Result: FeRA reduces backdoor attack success rates and maintains high main task accuracy in non-IID scenarios.

Conclusion: FeRA is a robust, model-agnostic solution for detecting backdoor attacks in federated learning with heterogeneous edge devices.

Abstract: Federated learning (FL) enhances privacy and reduces communication cost for
resource-constrained edge clients by supporting distributed model training at
the edge. However, the heterogeneous nature of such devices produces diverse,
non-independent, and identically distributed (non-IID) data, making the
detection of backdoor attacks more challenging. In this paper, we propose a
novel federated representative-attention-based defense mechanism, named FeRA,
that leverages cross-client attention over internal feature representations to
distinguish benign from malicious clients. FeRA computes an anomaly score based
on representation reconstruction errors, effectively identifying clients whose
internal activations significantly deviate from the group consensus. Our
evaluation demonstrates FeRA's robustness across various FL scenarios,
including challenging non-IID data distributions typical of edge devices.
Experimental results show that it effectively reduces backdoor attack success
rates while maintaining high accuracy on the main task. The method is
model-agnostic, attack-agnostic, and does not require labeled reference data,
making it well suited to heterogeneous and resource-limited edge deployments.

</details>


### [283] [Negative Metric Learning for Graphs](https://arxiv.org/pdf/2505.10307)
*Yiyang Zhao, Chengpei Wu, Lilin Zhang, Ning Yang*

Main category: cs.LG

TL;DR: NML-GCL introduces a learnable Negative Metric Network to address false negatives in GCL, improving performance via joint training and bi-level optimization.


<details>
  <summary>Details</summary>
Motivation: Existing GCL methods struggle with false negatives due to reliance on human prior knowledge, leading to suboptimal results.

Method: Proposes NML-GCL with a Negative Metric Network (NMN) for better false negative distinction and a joint training scheme with bi-level optimization.

Result: Theoretical analysis and experiments on benchmarks confirm the method's superiority.

Conclusion: NML-GCL effectively mitigates false negatives and enhances GCL performance.

Abstract: Graph contrastive learning (GCL) often suffers from false negatives, which
degrades the performance on downstream tasks. The existing methods addressing
the false negative issue usually rely on human prior knowledge, still leading
GCL to suboptimal results. In this paper, we propose a novel Negative Metric
Learning (NML) enhanced GCL (NML-GCL). NML-GCL employs a learnable Negative
Metric Network (NMN) to build a negative metric space, in which false negatives
can be distinguished better from true negatives based on their distance to
anchor node. To overcome the lack of explicit supervision signals for NML, we
propose a joint training scheme with bi-level optimization objective, which
implicitly utilizes the self-supervision signals to iteratively optimize the
encoder and the negative metric network. The solid theoretical analysis and the
extensive experiments conducted on widely used benchmarks verify the
superiority of the proposed method.

</details>


### [284] [Asynchronous Decentralized SGD under Non-Convexity: A Block-Coordinate Descent Framework](https://arxiv.org/pdf/2505.10322)
*Yijie Zhou, Shi Pu*

Main category: cs.LG

TL;DR: The paper introduces a refined Asynchronous Decentralized Stochastic Gradient Descent (ADSGD) model, addressing challenges like heterogeneous computation and communication delays, and shows its superior convergence and performance.


<details>
  <summary>Details</summary>
Motivation: Decentralized optimization is crucial for distributed data use but faces challenges like heterogeneous speeds and delays. This work aims to improve scalability and privacy in such settings.

Method: The paper analyzes Asynchronous Stochastic Block Coordinate Descent (ASBCD) as a tool and introduces ADSGD with bounded computation and communication assumptions, using delay-independent step sizes.

Result: ADSGD converges without bounded data heterogeneity and outperforms existing methods in wall-clock convergence time, demonstrating efficiency and resilience.

Conclusion: ADSGD is simple, efficient, and resilient, making it ideal for real-world decentralized learning tasks.

Abstract: Decentralized optimization has become vital for leveraging distributed data
without central control, enhancing scalability and privacy. However, practical
deployments face fundamental challenges due to heterogeneous computation speeds
and unpredictable communication delays. This paper introduces a refined model
of Asynchronous Decentralized Stochastic Gradient Descent (ADSGD) under
practical assumptions of bounded computation and communication times. To
understand the convergence of ADSGD, we first analyze Asynchronous Stochastic
Block Coordinate Descent (ASBCD) as a tool, and then show that ADSGD converges
under computation-delay-independent step sizes. The convergence result is
established without assuming bounded data heterogeneity. Empirical experiments
reveal that ADSGD outperforms existing methods in wall-clock convergence time
across various scenarios. With its simplicity, efficiency in memory and
communication, and resilience to communication and computation delays, ADSGD is
well-suited for real-world decentralized learning tasks.

</details>


### [285] [A Representation Learning Approach to Feature Drift Detection in Wireless Networks](https://arxiv.org/pdf/2505.10325)
*Athanasios Tziouvaras, Blaz Bertalanic, George Floros, Kostas Kolomvatsos, Panagiotis Sarigiannidis, Carolina Fortuna*

Main category: cs.LG

TL;DR: ALERT is a method to detect feature distribution changes in AI models for wireless networks, triggering re-training to maintain performance. It outperforms ten standard drift detection methods.


<details>
  <summary>Details</summary>
Motivation: AI models in wireless networks may degrade due to feature distribution changes, leading to undesired behaviors. ALERT addresses this issue.

Method: ALERT uses representation learning (MLP), statistical testing (Kolmogorov-Smirnov and Population Stability Index), and a new utility assessment function.

Result: ALERT outperforms ten standard drift detection methods in wireless fingerprinting and link anomaly detection.

Conclusion: ALERT effectively detects feature distribution changes and ensures robust AI model performance in wireless networks.

Abstract: AI is foreseen to be a centerpiece in next generation wireless networks
enabling enabling ubiquitous communication as well as new services. However, in
real deployment, feature distribution changes may degrade the performance of AI
models and lead to undesired behaviors. To counter for undetected model
degradation, we propose ALERT; a method that can detect feature distribution
changes and trigger model re-training that works well on two wireless network
use cases: wireless fingerprinting and link anomaly detection. ALERT includes
three components: representation learning, statistical testing and utility
assessment. We rely on MLP for designing the representation learning component,
on Kolmogorov-Smirnov and Population Stability Index tests for designing the
statistical testing and a new function for utility assessment. We show the
superiority of the proposed method against ten standard drift detection methods
available in the literature on two wireless network use cases.

</details>


### [286] [Efficient Adaptation of Reinforcement Learning Agents to Sudden Environmental Change](https://arxiv.org/pdf/2505.10330)
*Jonathan Clifford Balloch*

Main category: cs.LG

TL;DR: The paper addresses the challenge of RL agents adapting to changing environments without forgetting prior knowledge, proposing prioritized exploration and structured representations as solutions.


<details>
  <summary>Details</summary>
Motivation: Real-world systems operate in dynamic environments, but conventional RL methods struggle with adaptation and catastrophic forgetting.

Method: The dissertation introduces prioritized exploration and sampling strategies, along with structured representations for knowledge preservation.

Result: The approach enables efficient online adaptation by focusing on relevant experiences and updating reusable components selectively.

Conclusion: Efficient RL adaptation in dynamic environments requires prioritized exploration and structured knowledge preservation.

Abstract: Real-world autonomous decision-making systems, from robots to recommendation
engines, must operate in environments that change over time. While deep
reinforcement learning (RL) has shown an impressive ability to learn optimal
policies in stationary environments, most methods are data intensive and assume
a world that does not change between training and test time. As a result,
conventional RL methods struggle to adapt when conditions change. This poses a
fundamental challenge: how can RL agents efficiently adapt their behavior when
encountering novel environmental changes during deployment without
catastrophically forgetting useful prior knowledge? This dissertation
demonstrates that efficient online adaptation requires two key capabilities:
(1) prioritized exploration and sampling strategies that help identify and
learn from relevant experiences, and (2) selective preservation of prior
knowledge through structured representations that can be updated without
disruption to reusable components.

</details>


### [287] [Emergence of Structure in Ensembles of Random Neural Networks](https://arxiv.org/pdf/2505.10331)
*Luca Muscarnera, Luigi Loreti, Giovanni Todeschini, Alessio Fumagalli, Francesco Regazzoni*

Main category: cs.LG

TL;DR: The paper explores how ensembles of random classifiers can exhibit optimal collective behavior under a Gibbs measure, with a universal optimal temperature independent of the teacher classifier or ensemble size.


<details>
  <summary>Details</summary>
Motivation: To understand the emergence of deterministic behaviors from random components in machine learning systems.

Method: Introduces a theoretical model using the Gibbs measure with classification loss as energy, validated analytically and numerically for Gaussian-distributed samples and teacher perceptrons.

Result: Optimal temperature for classification is universal, independent of the teacher classifier or ensemble size, and confirmed on MNIST.

Conclusion: The study reveals self-organizing behavior in random classifier ensembles, with implications for high-quality datasets.

Abstract: Randomness is ubiquitous in many applications across data science and machine
learning. Remarkably, systems composed of random components often display
emergent global behaviors that appear deterministic, manifesting a transition
from microscopic disorder to macroscopic organization. In this work, we
introduce a theoretical model for studying the emergence of collective
behaviors in ensembles of random classifiers. We argue that, if the ensemble is
weighted through the Gibbs measure defined by adopting the classification loss
as an energy, then there exists a finite temperature parameter for the
distribution such that the classification is optimal, with respect to the loss
(or the energy). Interestingly, for the case in which samples are generated by
a Gaussian distribution and labels are constructed by employing a teacher
perceptron, we analytically prove and numerically confirm that such optimal
temperature does not depend neither on the teacher classifier (which is, by
construction of the learning problem, unknown), nor on the number of random
classifiers, highlighting the universal nature of the observed behavior.
Experiments on the MNIST dataset underline the relevance of this phenomenon in
high-quality, noiseless, datasets. Finally, a physical analogy allows us to
shed light on the self-organizing nature of the studied phenomenon.

</details>


### [288] [An Introduction to Discrete Variational Autoencoders](https://arxiv.org/pdf/2505.10344)
*Alan Jeffares, Liyuan Liu*

Main category: cs.LG

TL;DR: A tutorial on discrete variational autoencoders (VAEs) with categorical latent variables, providing a practical introduction and implementation.


<details>
  <summary>Details</summary>
Motivation: Discrete latent spaces are increasingly popular and may better suit certain data types (e.g., text), motivating a need for clear guidance on discrete VAEs.

Method: Derives discrete VAEs from first principles, focusing on categorical latent variables, and provides a training recipe with an example implementation.

Result: A concrete framework and implementation for discrete VAEs, accessible to those with basic math knowledge.

Conclusion: Discrete VAEs are a viable and practical approach for probabilistic unsupervised learning, especially for discrete data modalities.

Abstract: Variational Autoencoders (VAEs) are well-established as a principled approach
to probabilistic unsupervised learning with neural networks. Typically, an
encoder network defines the parameters of a Gaussian distributed latent space
from which we can sample and pass realizations to a decoder network. This model
is trained to reconstruct its inputs and is optimized through the evidence
lower bound. In recent years, discrete latent spaces have grown in popularity,
suggesting that they may be a natural choice for many data modalities (e.g.
text). In this tutorial, we provide a rigorous, yet practical, introduction to
discrete variational autoencoders -- specifically, VAEs in which the latent
space is made up of latent variables that follow a categorical distribution. We
assume only a basic mathematical background with which we carefully derive each
step from first principles. From there, we develop a concrete training recipe
and provide an example implementation, hosted at
https://github.com/alanjeffares/discreteVAE.

</details>


### [289] [Uniform Loss vs. Specialized Optimization: A Comparative Analysis in Multi-Task Learning](https://arxiv.org/pdf/2505.10347)
*Gabriel S. Gama, Valdir Grassi Jr*

Main category: cs.LG

TL;DR: SMTOs balance task learning in multi-task settings, but recent critiques claim equal-weighted tasks can match SMTOs. This paper evaluates SMTOs empirically, finding they perform well, but fixed weights can also compete.


<details>
  <summary>Details</summary>
Motivation: To clarify whether SMTOs outperform equal-weighted tasks, addressing critiques about hyperparameter and regularization issues.

Method: Extensive empirical evaluation of SMTOs on complex multi-task problems, comparing them to uniform loss methods.

Result: SMTOs perform well, but fixed weights can achieve similar performance. Uniform loss sometimes matches SMTOs.

Conclusion: SMTOs are effective, but fixed weights can be competitive, with uniform loss performing similarly in some cases.

Abstract: Specialized Multi-Task Optimizers (SMTOs) balance task learning in Multi-Task
Learning by addressing issues like conflicting gradients and differing gradient
norms, which hinder equal-weighted task training. However, recent critiques
suggest that equally weighted tasks can achieve competitive results compared to
SMTOs, arguing that previous SMTO results were influenced by poor
hyperparameter optimization and lack of regularization. In this work, we
evaluate these claims through an extensive empirical evaluation of SMTOs,
including some of the latest methods, on more complex multi-task problems to
clarify this behavior. Our findings indicate that SMTOs perform well compared
to uniform loss and that fixed weights can achieve competitive performance
compared to SMTOs. Furthermore, we demonstrate why uniform loss perform
similarly to SMTOs in some instances. The code will be made publicly available.

</details>


### [290] [FactsR: A Safer Method for Producing High Quality Healthcare Documentation](https://arxiv.org/pdf/2505.10360)
*Victor Petrén Bach Hansen, Lasse Krogsbøll, Jonas Lyngsø, Mathias Baltzersen, Andreas Motzfeldt, Kevin Pelgrims, Lars Maaløe*

Main category: cs.LG

TL;DR: The paper introduces FactsR, a method for real-time extraction of clinical information during consultations to generate more accurate and concise AI-scribed notes, reducing errors and clinician workload.


<details>
  <summary>Details</summary>
Motivation: Current AI scribes rely on post-consultation prompts, leading to long, error-prone notes and increased clinician workload, risking patient safety.

Method: FactsR extracts salient clinical information (Facts) in real-time during consultations and uses it recursively for note generation, keeping clinicians in the loop.

Result: FactsR produces more accurate and concise notes, minimizing hallucinations and misrepresentation, while enabling real-time decision support.

Conclusion: FactsR improves AI-scribed documentation by integrating real-time reasoning and clinician involvement, enhancing accuracy and patient safety.

Abstract: There are now a multitude of AI-scribing solutions for healthcare promising
the utilization of large language models for ambient documentation. However,
these AI scribes still rely on one-shot, or few-shot prompts for generating
notes after the consultation has ended, employing little to no reasoning. This
risks long notes with an increase in hallucinations, misrepresentation of the
intent of the clinician, and reliance on the proofreading of the clinician to
catch errors. A dangerous combination for patient safety if vigilance is
compromised by workload and fatigue. In this paper, we introduce a method for
extracting salient clinical information in real-time alongside the healthcare
consultation, denoted Facts, and use that information recursively to generate
the final note. The FactsR method results in more accurate and concise notes by
placing the clinician-in-the-loop of note generation, while opening up new use
cases within real-time decision support.

</details>


### [291] [PIF: Anomaly detection via preference embedding](https://arxiv.org/pdf/2505.10441)
*Filippo Leveni, Luca Magri, Giacomo Boracchi, Cesare Alippi*

Main category: cs.LG

TL;DR: PIF is a novel anomaly detection method combining adaptive isolation and preference embedding, outperforming state-of-the-art techniques.


<details>
  <summary>Details</summary>
Motivation: Detecting anomalies in structured patterns requires flexible and efficient methods.

Method: Embeds data in high-dimensional space and uses PI-Forest, a tree-based method, for anomaly scoring.

Result: PIF outperforms existing techniques, with PI-Forest excelling in measuring arbitrary distances and isolating points.

Conclusion: PIF is an effective anomaly detection method, particularly for structured patterns.

Abstract: We address the problem of detecting anomalies with respect to structured
patterns. To this end, we conceive a novel anomaly detection method called PIF,
that combines the advantages of adaptive isolation methods with the flexibility
of preference embedding. Specifically, we propose to embed the data in a high
dimensional space where an efficient tree-based method, PI-Forest, is employed
to compute an anomaly score. Experiments on synthetic and real datasets
demonstrate that PIF favorably compares with state-of-the-art anomaly detection
techniques, and confirm that PI-Forest is better at measuring arbitrary
distances and isolate points in the preference space.

</details>


### [292] [Schreier-Coset Graph Propagation](https://arxiv.org/pdf/2505.10392)
*Aryan Mishra, Lizhen Lin*

Main category: cs.LG

TL;DR: SCGP introduces a group-theoretic method to enhance GNNs by embedding Schreier-coset features, improving long-range message passing without altering graph topology, while maintaining efficiency.


<details>
  <summary>Details</summary>
Motivation: Over-squashing in GNNs limits expressive capacity by compressing distant node information. Existing solutions like Cayley graphs introduce scalability issues.

Method: SCGP uses Schreier-coset embeddings to enrich node features, preserving graph topology and embedding bottleneck-free connectivity patterns.

Result: SCGP matches or outperforms expander graph and rewired GNN baselines, excelling in hierarchical and modular structures with low latency and memory.

Conclusion: SCGP is efficient, scalable, and suitable for real-time applications, addressing over-squashing without compromising performance.

Abstract: Graph Neural Networks (GNNs) offer a principled framework for learning over
graph-structured data, yet their expressive capacity is often hindered by
over-squashing, wherein information from distant nodes is compressed into
fixed-size vectors. Existing solutions, including graph rewiring and
bottleneck-resistant architectures such as Cayley and expander graphs, avoid
this problem but introduce scalability bottlenecks. In particular, the Cayley
graphs constructed over $SL(2,\mathbb{Z}_n)$ exhibit strong theoretical
properties, yet suffer from cubic node growth $O(n^3)$, leading to high memory
usage. To address this, this work introduces Schrier-Coset Graph Propagation
(SCGP), a group-theoretic augmentation method that enriches node features
through Schreier-coset embeddings without altering the input graph topology.
SCGP embeds bottleneck-free connectivity patterns into a compact feature space,
improving long-range message passing while maintaining computational
efficiency. Empirical evaluations across standard node and graph classification
benchmarks demonstrate that SCGP achieves performance comparable to, or
exceeding, expander graph and rewired GNN baselines. Furthermore, SCGP exhibits
particular advantages in processing hierarchical and modular graph structures,
offering reduced inference latency, improved scalability, and a low memory
footprint, making it suitable for real-time and resource-constrained
applications.

</details>


### [293] [SEAL: Searching Expandable Architectures for Incremental Learning](https://arxiv.org/pdf/2505.10457)
*Matteo Gambella, Vicente Javier Castro Solar, Manuel Roveri*

Main category: cs.LG

TL;DR: SEAL is a NAS-based framework for incremental learning that dynamically adapts model structure, reducing forgetting and improving accuracy while keeping model size small.


<details>
  <summary>Details</summary>
Motivation: Existing NAS-based incremental learning methods expand models excessively, making them impractical for resource-constrained environments. SEAL addresses this by selective expansion.

Method: SEAL uses a NAS component to dynamically expand the model only when necessary, based on capacity estimation, and preserves stability via cross-distillation training.

Result: SEAL outperforms prior methods by reducing forgetting, enhancing accuracy, and maintaining a smaller model size across benchmarks.

Conclusion: SEAL demonstrates the potential of combining NAS with selective expansion for efficient and adaptive incremental learning.

Abstract: Incremental learning is a machine learning paradigm where a model learns from
a sequential stream of tasks. This setting poses a key challenge: balancing
plasticity (learning new tasks) and stability (preserving past knowledge).
Neural Architecture Search (NAS), a branch of AutoML, automates the design of
the architecture of Deep Neural Networks and has shown success in static
settings. However, existing NAS-based approaches to incremental learning often
rely on expanding the model at every task, making them impractical in
resource-constrained environments. In this work, we introduce SEAL, a NAS-based
framework tailored for data-incremental learning, a scenario where disjoint
data samples arrive sequentially and are not stored for future access. SEAL
adapts the model structure dynamically by expanding it only when necessary,
based on a capacity estimation metric. Stability is preserved through
cross-distillation training after each expansion step. The NAS component
jointly searches for both the architecture and the optimal expansion policy.
Experiments across multiple benchmarks demonstrate that SEAL effectively
reduces forgetting and enhances accuracy while maintaining a lower model size
compared to prior methods. These results highlight the promise of combining NAS
and selective expansion for efficient, adaptive learning in incremental
scenarios.

</details>


### [294] [Two-Stage Generative Model for Intracranial Aneurysm Meshes with Morphological Marker Conditioning](https://arxiv.org/pdf/2505.10407)
*Wenhao Ding, Choon Hwai Yap, Kangjun Ji, Simão Castro*

Main category: cs.LG

TL;DR: AneuG is a two-stage VAE-based model for generating realistic intracranial aneurysm meshes, addressing the lack of large datasets and improving physiological realism by capturing pouch-vessel relationships and enabling control over morphological measurements.


<details>
  <summary>Details</summary>
Motivation: The absence of large IA image datasets and limitations of existing shape generation methods in capturing realistic features and relationships between IA pouches and parent vessels drive the need for a better generative model.

Method: AneuG uses a two-stage VAE approach: first, it generates GHD tokens for aneurysm pouches, and second, it generates parent vessels conditioned on these tokens, allowing control over morphological measurements.

Result: AneuG produces realistic IA meshes with accurate shape encoding and the ability to condition generation on specific clinical measurements, enhancing studies on shape variations and fluid dynamics.

Conclusion: AneuG offers a robust solution for generating realistic IA meshes, facilitating research on disease progression and fluid dynamics by enabling controlled morphological variations.

Abstract: A generative model for the mesh geometry of intracranial aneurysms (IA) is
crucial for training networks to predict blood flow forces in real time, which
is a key factor affecting disease progression. This need is necessitated by the
absence of a large IA image datasets. Existing shape generation methods
struggle to capture realistic IA features and ignore the relationship between
IA pouches and parent vessels, limiting physiological realism and their
generation cannot be controlled to have specific morphological measurements. We
propose AneuG, a two-stage Variational Autoencoder (VAE)-based IA mesh
generator. In the first stage, AneuG generates low-dimensional Graph Harmonic
Deformation (GHD) tokens to encode and reconstruct aneurysm pouch shapes,
constrained to morphing energy statistics truths. GHD enables more accurate
shape encoding than alternatives. In the second stage, AneuG generates parent
vessels conditioned on GHD tokens, by generating vascular centreline and
propagating the cross-section. AneuG's IA shape generation can further be
conditioned to have specific clinically relevant morphological measurements.
This is useful for studies to understand shape variations represented by
clinical measurements, and for flow simulation studies to understand effects of
specific clinical shape parameters on fluid dynamics. Source code and
implementation details are available at
https://github.com/anonymousaneug/AneuG.

</details>


### [295] [Decomposed Inductive Procedure Learning: Learning Academic Tasks with Human-Like Data Efficiency](https://arxiv.org/pdf/2505.10422)
*Daniel Weitekamp, Christopher MacLellan, Erik Harpstead, Kenneth Koedinger*

Main category: cs.LG

TL;DR: The paper explores whether human-like rapid learning can be achieved in neural networks by using multiple specialized mechanisms, unlike the single gradient descent approach.


<details>
  <summary>Details</summary>
Motivation: To understand why humans learn faster from fewer examples compared to data-driven deep learning, focusing on the role of multiple specialized learning mechanisms.

Method: Conducted an ablation analysis of human learning simulations, comparing reinforcement learning to a symbolic rule induction approach with three mechanisms.

Result: Decomposing learning into multiple mechanisms improves data efficiency, matching human learning rates, and outperforms the symbolic vs. subsymbolic distinction.

Conclusion: Integrating multiple specialized learning mechanisms could bridge the efficiency gap between human and machine learning.

Abstract: Human learning relies on specialization -- distinct cognitive mechanisms
working together to enable rapid learning. In contrast, most modern neural
networks rely on a single mechanism: gradient descent over an objective
function. This raises the question: might human learners' relatively rapid
learning from just tens of examples instead of tens of thousands in data-driven
deep learning arise from our ability to use multiple specialized mechanisms of
learning in combination? We investigate this question through an ablation
analysis of inductive human learning simulations in online tutoring
environments. Comparing reinforcement learning to a more data-efficient
3-mechanism symbolic rule induction approach, we find that decomposing learning
into multiple distinct mechanisms significantly improves data efficiency,
bringing it in line with human learning. Furthermore, we show that this
decomposition has a greater impact on efficiency than the distinction between
symbolic and subsymbolic learning alone. Efforts to align data-driven machine
learning with human learning often overlook the stark difference in learning
efficiency. Our findings suggest that integrating multiple specialized learning
mechanisms may be key to bridging this gap.

</details>


### [296] [The Power of Random Features and the Limits of Distribution-Free Gradient Descent](https://arxiv.org/pdf/2505.10423)
*Ari Karchmer, Eran Malach*

Main category: cs.LG

TL;DR: The paper shows that gradient-based optimization of parametric models (e.g., neural networks) can approximate target functions using polynomial-sized random features, revealing limitations of distribution-free learning.


<details>
  <summary>Details</summary>
Motivation: To understand the relationship between gradient-based optimization and random feature combinations, and to explore the necessity of data distribution assumptions in neural network training.

Method: Analyzes mini-batch stochastic gradient descent (bSGD) and introduces a new theoretical framework, average probabilistic dimension complexity (adc).

Result: Demonstrates that polynomial-sized random feature combinations can approximate target functions, with size depending on gradient steps and precision. Also shows an infinite separation between adc and standard dimension complexity.

Conclusion: Highlights fundamental limitations of distribution-free learning in neural networks and the importance of data distribution assumptions.

Abstract: We study the relationship between gradient-based optimization of parametric
models (e.g., neural networks) and optimization of linear combinations of
random features. Our main result shows that if a parametric model can be
learned using mini-batch stochastic gradient descent (bSGD) without making
assumptions about the data distribution, then with high probability, the target
function can also be approximated using a polynomial-sized combination of
random features. The size of this combination depends on the number of gradient
steps and numerical precision used in the bSGD process. This finding reveals
fundamental limitations of distribution-free learning in neural networks
trained by gradient descent, highlighting why making assumptions about data
distributions is often crucial in practice. Along the way, we also introduce a
new theoretical framework called average probabilistic dimension complexity
(adc), which extends the probabilistic dimension complexity developed by Kamath
et al. (2020). We prove that adc has a polynomial relationship with statistical
query dimension, and use this relationship to demonstrate an infinite
separation between adc and standard dimension complexity.

</details>


### [297] [Learning to Think: Information-Theoretic Reinforcement Fine-Tuning for LLMs](https://arxiv.org/pdf/2505.10425)
*Jingyao Wang, Wenwen Qiang, Zeen Song, Changwen Zheng, Hui Xiong*

Main category: cs.LG

TL;DR: L2T is a reinforcement fine-tuning framework for LLMs that optimizes reasoning efficiency by reducing unnecessary tokens, using hierarchical sessions and dense process rewards.


<details>
  <summary>Details</summary>
Motivation: Existing methods for LLMs overlook the trade-off between reasoning effectiveness and computational efficiency, leading to wasted tokens.

Method: L2T treats query-response interactions as hierarchical sessions with dense process rewards, estimated using PAC-Bayes bounds and Fisher information matrix.

Result: L2T reduces computational complexity while maintaining accuracy, improving reasoning effectiveness and efficiency across benchmarks.

Conclusion: L2T successfully balances reasoning and efficiency, outperforming existing methods in various tasks.

Abstract: Large language models (LLMs) excel at complex tasks thanks to advances in
reasoning abilities. However, existing methods overlook the trade-off between
reasoning effectiveness and computational efficiency, often encouraging
unnecessarily long reasoning chains and wasting tokens. To address this, we
propose Learning to Think (L2T), an information-theoretic reinforcement
fine-tuning framework for LLMs to make the models achieve optimal reasoning
with fewer tokens. Specifically, L2T treats each query-response interaction as
a hierarchical session of multiple episodes and proposes a universal dense
process reward, i.e., quantifies the episode-wise information gain in
parameters, requiring no extra annotations or task-specific evaluators. We
propose a method to quickly estimate this reward based on PAC-Bayes bounds and
the Fisher information matrix. Theoretical analyses show that it significantly
reduces computational complexity with high estimation accuracy. By immediately
rewarding each episode's contribution and penalizing excessive updates, L2T
optimizes the model via reinforcement learning to maximize the use of each
episode and achieve effective updates. Empirical results on various reasoning
benchmarks and base models demonstrate the advantage of L2T across different
tasks, boosting both reasoning effectiveness and efficiency.

</details>


### [298] [Score-based diffusion nowcasting of GOES imagery](https://arxiv.org/pdf/2505.10432)
*Randy J. Chase, Katherine Haynes, Lander Ver Hoef, Imme Ebert-Uphoff*

Main category: cs.LG

TL;DR: The paper explores score-based diffusion models for nowcasting clouds and precipitation, outperforming traditional methods like U-Nets and persistence forecasts.


<details>
  <summary>Details</summary>
Motivation: Traditional numerical weather prediction struggles with sub-grid parameterizations for clouds and precipitation, and early machine learning methods produced blurry forecasts.

Method: Three diffusion models (Diff, CorrDiff, LDM) are tested for nowcasting using geostationary infrared imagery, focusing on advection, generation, and decay of clouds.

Result: Diffusion models, especially CorrDiff, outperform U-Nets and persistence forecasts by 1-2 Kelvin in RMSE, and enable skillful ensemble generation.

Conclusion: Score-based diffusion models, particularly CorrDiff, are effective for high-resolution nowcasting and ensemble forecasting in weather prediction.

Abstract: Clouds and precipitation are important for understanding weather and climate.
Simulating clouds and precipitation with traditional numerical weather
prediction is challenging because of the sub-grid parameterizations required.
Machine learning has been explored for forecasting clouds and precipitation,
but early machine learning methods often created blurry forecasts. In this
paper we explore a newer method, named score-based diffusion, to nowcast (zero
to three hour forecast) clouds and precipitation. We discuss the background and
intuition of score-based diffusion models - thus providing a starting point for
the community - while exploring the methodology's use for nowcasting
geostationary infrared imagery. We experiment with three main types of
diffusion models: a standard score-based diffusion model (Diff); a residual
correction diffusion model (CorrDiff); and a latent diffusion model (LDM). Our
results show that the diffusion models are able to not only advect existing
clouds, but also generate and decay clouds, including convective initiation.
These results are surprising because the forecasts are initiated with only the
past 20 mins of infrared satellite imagery. A case study qualitatively shows
the preservation of high resolution features longer into the forecast than a
conventional mean-squared error trained U-Net. The best of the three diffusion
models tested was the CorrDiff approach, outperforming all other diffusion
models, the traditional U-Net, and a persistence forecast by one to two kelvin
on root mean squared error. The diffusion models also enable out-of-the-box
ensemble generation, which shows skillful calibration, with the spread of the
ensemble correlating well to the error.

</details>


### [299] [Fine-tuning Diffusion Policies with Backpropagation Through Diffusion Timesteps](https://arxiv.org/pdf/2505.10482)
*Ningyuan Yang, Jiaxuan Gao, Feng Gao, Yi Wu, Chao Yu*

Main category: cs.LG

TL;DR: NCDPO introduces a noise-conditioned deterministic policy to improve the sample efficiency and performance of diffusion policies, addressing challenges in RL-based fine-tuning.


<details>
  <summary>Details</summary>
Motivation: Diffusion policies suffer from sub-optimal trajectories due to limited demonstration data, and existing RL fine-tuning methods struggle with computational intractability.

Method: NCDPO reformulates Diffusion Policy as a noise-conditioned deterministic policy, enabling tractable likelihood evaluation and gradient backpropagation.

Result: NCDPO matches MLP+PPO in sample efficiency and outperforms existing methods in performance across benchmarks.

Conclusion: NCDPO is robust and effective for improving diffusion policies in diverse decision-making scenarios.

Abstract: Diffusion policies, widely adopted in decision-making scenarios such as
robotics, gaming and autonomous driving, are capable of learning diverse skills
from demonstration data due to their high representation power. However, the
sub-optimal and limited coverage of demonstration data could lead to diffusion
policies that generate sub-optimal trajectories and even catastrophic failures.
While reinforcement learning (RL)-based fine-tuning has emerged as a promising
solution to address these limitations, existing approaches struggle to
effectively adapt Proximal Policy Optimization (PPO) to diffusion models. This
challenge stems from the computational intractability of action likelihood
estimation during the denoising process, which leads to complicated
optimization objectives. In our experiments starting from randomly initialized
policies, we find that online tuning of Diffusion Policies demonstrates much
lower sample efficiency compared to directly applying PPO on MLP policies
(MLP+PPO). To address these challenges, we introduce NCDPO, a novel framework
that reformulates Diffusion Policy as a noise-conditioned deterministic policy.
By treating each denoising step as a differentiable transformation conditioned
on pre-sampled noise, NCDPO enables tractable likelihood evaluation and
gradient backpropagation through all diffusion timesteps. Our experiments
demonstrate that NCDPO achieves sample efficiency comparable to MLP+PPO when
training from scratch, outperforming existing methods in both sample efficiency
and final performance across diverse benchmarks, including continuous robot
control and multi-agent game scenarios. Furthermore, our experimental results
show that our method is robust to the number denoising timesteps in the
Diffusion Policy.

</details>


### [300] [Identification and Optimal Nonlinear Control of Turbojet Engine Using Koopman Eigenfunction Model](https://arxiv.org/pdf/2505.10438)
*David Grasev*

Main category: cs.LG

TL;DR: The paper proposes a data-driven approach using Koopman eigenfunction space for modeling and controlling gas turbine engines, outperforming traditional methods.


<details>
  <summary>Details</summary>
Motivation: Physics-based modeling of gas turbine engines is challenging due to unavailable performance data and simplifying assumptions. The paper addresses limitations of conventional methods.

Method: Uses sparse identification of nonlinear dynamics, maps dynamics into Koopman eigenfunction space, optimizes eigenvalues, and designs controllers in this space.

Result: The Koopman-based controller outperforms classical and gain-scheduled controllers in tracking and disturbance rejection.

Conclusion: The data-driven Koopman approach provides a globally optimal solution for gas turbine engine control.

Abstract: Gas turbine engines represent complex highly nonlinear dynamical systems.
Deriving their physics-based models can be challenging as it requires
performance characteristics, that are not always available, and one often has
to make many simplifying assumptions. In this paper, the limitations of
conventional experimental methods used to derive component-level and locally
linear parameter-varying models are discussed and addressed by employing
identification techniques based on data collected from standard engine
operation under closed-loop control. The rotor dynamics were estimated using
the sparse identification of nonlinear dynamics. Subsequently, the autonomous
part of the dynamics was mapped into an optimally constructed Koopman
eigenfunction space. The process included eigenvalue optimization using
metaheuristic algorithms and temporal projection, followed by gradient-based
eigenfunction identification. The resulting Koopman model was validated against
an in-house reference component-level model. A globally optimal nonlinear
feedback controller and a Kalman estimator were then designed in the
eigenfunction space and compared to the classical and gain-scheduled
proportional-integral controllers, as well as a proposed internal model control
approach. The eigenmode structure allowed targeting individual modes during the
optimization process, resulting in a better performance tuning. The results
showed that the Koopman-based controller outperformed the other benchmark
controllers in both reference tracking and disturbance rejection, under
sea-level and varying flight conditions, due to its global nature.

</details>


### [301] [PnPXAI: A Universal XAI Framework Providing Automatic Explanations Across Diverse Modalities and Models](https://arxiv.org/pdf/2505.10515)
*Seongun Kim, Sol A Kim, Geonhyeong Kim, Enver Menadjiev, Chanwoo Lee, Seongwook Chung, Nari Kim, Jaesik Choi*

Main category: cs.LG

TL;DR: PnPXAI is a universal XAI framework addressing limitations of existing methods by supporting diverse models and data modalities, optimizing explanations, and validating effectiveness across domains.


<details>
  <summary>Details</summary>
Motivation: Existing XAI methods lack flexibility, support limited architectures, and provide sub-optimal explanations, hindering real-world adoption.

Method: PnPXAI automates model detection, recommends explanation methods, and optimizes hyperparameters for diverse models and data.

Result: Validated through user surveys, PnPXAI demonstrates versatility in domains like medicine and finance.

Conclusion: PnPXAI overcomes current XAI limitations, offering a practical solution for practitioners.

Abstract: Recently, post hoc explanation methods have emerged to enhance model
transparency by attributing model outputs to input features. However, these
methods face challenges due to their specificity to certain neural network
architectures and data modalities. Existing explainable artificial intelligence
(XAI) frameworks have attempted to address these challenges but suffer from
several limitations. These include limited flexibility to diverse model
architectures and data modalities due to hard-coded implementations, a
restricted number of supported XAI methods because of the requirements for
layer-specific operations of attribution methods, and sub-optimal
recommendations of explanations due to the lack of evaluation and optimization
phases. Consequently, these limitations impede the adoption of XAI technology
in real-world applications, making it difficult for practitioners to select the
optimal explanation method for their domain. To address these limitations, we
introduce \textbf{PnPXAI}, a universal XAI framework that supports diverse data
modalities and neural network models in a Plug-and-Play (PnP) manner. PnPXAI
automatically detects model architectures, recommends applicable explanation
methods, and optimizes hyperparameters for optimal explanations. We validate
the framework's effectiveness through user surveys and showcase its versatility
across various domains, including medicine and finance.

</details>


### [302] [Fixing Incomplete Value Function Decomposition for Multi-Agent Reinforcement Learning](https://arxiv.org/pdf/2505.10484)
*Andrea Baisero, Rupali Bhati, Shuo Liu, Aathira Pillai, Christopher Amato*

Main category: cs.LG

TL;DR: QFIX is a novel value function decomposition method for multi-agent reinforcement learning that expands representation capabilities while maintaining simplicity, outperforming prior methods like QPLEX.


<details>
  <summary>Details</summary>
Motivation: Existing methods for value function decomposition in multi-agent RL (e.g., VDN, QMIX) have limited representation capabilities or are overly complex (QPLEX). QFIX aims to address these limitations.

Method: QFIX introduces a simple formulation for the full class of IGM values, using a thin 'fixing' layer to enhance representation. It derives multiple variants and integrates them into existing frameworks.

Result: QFIX improves performance over prior methods, learns more stably, and outperforms QPLEX while using simpler and smaller models, as shown in SMACv2 and Overcooked environments.

Conclusion: QFIX successfully balances representation power and simplicity, offering a practical advancement for multi-agent RL.

Abstract: Value function decomposition methods for cooperative multi-agent
reinforcement learning compose joint values from individual per-agent
utilities, and train them using a joint objective. To ensure that the action
selection process between individual utilities and joint values remains
consistent, it is imperative for the composition to satisfy the
individual-global max (IGM) property. Although satisfying IGM itself is
straightforward, most existing methods (e.g., VDN, QMIX) have limited
representation capabilities and are unable to represent the full class of IGM
values, and the one exception that has no such limitation (QPLEX) is
unnecessarily complex. In this work, we present a simple formulation of the
full class of IGM values that naturally leads to the derivation of QFIX, a
novel family of value function decomposition models that expand the
representation capabilities of prior models by means of a thin "fixing" layer.
We derive multiple variants of QFIX, and implement three variants in two
well-known multi-agent frameworks. We perform an empirical evaluation on
multiple SMACv2 and Overcooked environments, which confirms that QFIX (i)
succeeds in enhancing the performance of prior methods, (ii) learns more stably
and performs better than its main competitor QPLEX, and (iii) achieves this
while employing the simplest and smallest mixing models.

</details>


### [303] [Pharmacophore-Conditioned Diffusion Model for Ligand-Based De Novo Drug Design](https://arxiv.org/pdf/2505.10545)
*Amira Alakhdar, Barnabas Poczos, Newell Washburn*

Main category: cs.LG

TL;DR: PharmaDiff is a pharmacophore-conditioned diffusion model for 3D molecular generation, outperforming traditional methods in matching pharmacophore constraints and achieving higher docking scores without requiring target protein structures.


<details>
  <summary>Details</summary>
Motivation: Developing bioactive molecules for novel targets is challenging due to lack of structural or functional data. Pharmacophore modeling offers a solution by capturing key bioactivity features.

Method: PharmaDiff uses a transformer-based architecture to integrate 3D pharmacophore data into molecular generation, producing precise 3D molecular graphs aligned with pharmacophore hypotheses.

Result: PharmaDiff excels in matching 3D pharmacophore constraints and achieves higher docking scores in structure-based drug design, even without target protein structures.

Conclusion: PharmaDiff combines pharmacophore modeling with 3D generative techniques, providing a flexible and powerful framework for rational drug design.

Abstract: Developing bioactive molecules remains a central, time- and cost-heavy
challenge in drug discovery, particularly for novel targets lacking structural
or functional data. Pharmacophore modeling presents an alternative for
capturing the key features required for molecular bioactivity against a
biological target. In this work, we present PharmaDiff, a
pharmacophore-conditioned diffusion model for 3D molecular generation.
PharmaDiff employs a transformer-based architecture to integrate an atom-based
representation of the 3D pharmacophore into the generative process, enabling
the precise generation of 3D molecular graphs that align with predefined
pharmacophore hypotheses. Through comprehensive testing, PharmaDiff
demonstrates superior performance in matching 3D pharmacophore constraints
compared to ligand-based drug design methods. Additionally, it achieves higher
docking scores across a range of proteins in structure-based drug design,
without the need for target protein structures. By integrating pharmacophore
modeling with 3D generative techniques, PharmaDiff offers a powerful and
flexible framework for rational drug design.

</details>


### [304] [An AI-driven framework for the prediction of personalised health response to air pollution](https://arxiv.org/pdf/2505.10556)
*Nazanin Zounemat Kermani, Sadjad Naderi, Claire H. Dilliway, Claire E. Heaney, Shrreya Behll, Boyang Chen, Hisham Abubakar-Waziri, Alexandra E. Porter, Marc Chadeau-Hyam, Fangxin Fang, Ian M. Adcock, Kian Fan Chung, Christopher C. Pain*

Main category: cs.LG

TL;DR: A novel workflow integrates wearable fitness data and real-time environmental pollution to predict personalized health responses using an AI model, demonstrating accurate predictions and adaptability.


<details>
  <summary>Details</summary>
Motivation: Air pollution and climate change exacerbate health risks; leveraging personal sensing and AI can improve healthcare by predicting individual health responses to pollution.

Method: Integrates physiological data from wearables with environmental exposures, trains an Adversarial Autoencoder AI model, and uses transfer learning for generalization.

Result: The AI model accurately reconstructs health signals and captures nonlinear pollution responses, showing adaptability to user-generated data.

Conclusion: The approach effectively predicts personalized health impacts of pollution, highlighting potential for real-world healthcare applications.

Abstract: Air pollution poses a significant threat to public health, causing or
exacerbating many respiratory and cardiovascular diseases. In addition, climate
change is bringing about more extreme weather events such as wildfires and
heatwaves, which can increase levels of pollution and worsen the effects of
pollution exposure. Recent advances in personal sensing have transformed the
collection of behavioural and physiological data, leading to the potential for
new improvements in healthcare. We wish to capitalise on this data, alongside
new capabilities in AI for making time series predictions, in order to monitor
and predict health outcomes for an individual. Thus, we present a novel
workflow for predicting personalised health responses to pollution by
integrating physiological data from wearable fitness devices with real-time
environmental exposures. The data is collected from various sources in a secure
and ethical manner, and is used to train an AI model to predict individual
health responses to pollution exposure within a cloud-based, modular framework.
We demonstrate that the AI model -- an Adversarial Autoencoder neural network
in this case -- accurately reconstructs time-dependent health signals and
captures nonlinear responses to pollution. Transfer learning is applied using
data from a personal smartwatch, which increases the generalisation abilities
of the AI model and illustrates the adaptability of the approach to real-world,
user-generated data.

</details>


### [305] [Neural Thermodynamic Laws for Large Language Model Training](https://arxiv.org/pdf/2505.10559)
*Ziming Liu, Yizhou Liu, Jeff Gore, Max Tegmark*

Main category: cs.LG

TL;DR: The paper introduces Neural Thermodynamic Laws (NTL), a framework linking thermodynamic principles to LLM training dynamics, offering theoretical insights and practical guidelines for learning rate schedules.


<details>
  <summary>Details</summary>
Motivation: To uncover underlying laws of large language models (LLMs) beyond neural scaling laws by exploring thermodynamic analogies in training dynamics.

Method: Theoretical demonstration of thermodynamic quantities (e.g., temperature, entropy) and principles (e.g., three laws of thermodynamics) under river-valley loss landscape assumptions.

Result: NTL provides a scientific perspective on LLM training, revealing emergent thermodynamic behaviors and practical implications for learning rate design.

Conclusion: NTL bridges theory and practice, offering a novel framework to understand and optimize LLM training through thermodynamic principles.

Abstract: Beyond neural scaling laws, little is known about the laws underlying large
language models (LLMs). We introduce Neural Thermodynamic Laws (NTL) -- a new
framework that offers fresh insights into LLM training dynamics. On the
theoretical side, we demonstrate that key thermodynamic quantities (e.g.,
temperature, entropy, heat capacity, thermal conduction) and classical
thermodynamic principles (e.g., the three laws of thermodynamics and the
equipartition theorem) naturally emerge under river-valley loss landscape
assumptions. On the practical side, this scientific perspective yields
intuitive guidelines for designing learning rate schedules.

</details>


### [306] [Aligning Transformers with Continuous Feedback via Energy Rank Alignment](https://arxiv.org/pdf/2405.12961)
*Shriram Chennakesavalu, Frank Hu, Sebastian Ibarraran, Grant M. Rotskoff*

Main category: cs.LG

TL;DR: ERA is a gradient-based algorithm for optimizing autoregressive policies in molecular generation, leveraging explicit reward functions without reinforcement learning, and performs well in aligning models to generate molecules with desired properties.


<details>
  <summary>Details</summary>
Motivation: The challenge of searching chemical space efficiently due to its combinatorial complexity and the lack of robust strategies for generating molecules with specific properties.

Method: Introduces Energy Rank Alignment (ERA), a gradient-based objective using explicit reward functions, related to PPO and DPO but converges to a Gibbs-Boltzmann distribution.

Result: ERA is scalable, avoids reinforcement learning, and outperforms DPO with limited preference observations. It successfully aligns models to generate molecules and protein sequences with specified properties.

Conclusion: ERA provides a robust and scalable solution for aligning models to generate desired chemical and biological sequences, exploring diverse chemical spaces effectively.

Abstract: Searching through chemical space is an exceptionally challenging problem
because the number of possible molecules grows combinatorially with the number
of atoms. Large, autoregressive models trained on databases of chemical
compounds have yielded powerful generators, but we still lack robust strategies
for generating molecules with desired properties. This molecular search problem
closely resembles the "alignment" problem for large language models, though for
many chemical tasks we have a specific and easily evaluable reward function.
Here, we introduce an algorithm called energy rank alignment (ERA) that
leverages an explicit reward function to produce a gradient-based objective
that we use to optimize autoregressive policies. We show theoretically that
this algorithm is closely related to proximal policy optimization (PPO) and
direct preference optimization (DPO), but has a minimizer that converges to an
ideal Gibbs-Boltzmann distribution with the reward playing the role of an
energy function. Furthermore, this algorithm is highly scalable, does not
require reinforcement learning, and performs well relative to DPO when the
number of preference observations per pairing is small. We deploy this approach
to align molecular transformers and protein language models to generate
molecules and protein sequences, respectively, with externally specified
properties and find that it does so robustly, searching through diverse parts
of chemical space.

</details>


### [307] [Double Successive Over-Relaxation Q-Learning with an Extension to Deep Reinforcement Learning](https://arxiv.org/pdf/2409.06356)
*Shreyas S R*

Main category: cs.LG

TL;DR: A model-free double SOR Q-learning algorithm is proposed to address the limitations of SOR Q-learning, offering less bias and better convergence.


<details>
  <summary>Details</summary>
Motivation: Q-learning's slow convergence with high discount factors and SOR Q-learning's dependency on transition probabilities and overestimation bias.

Method: Introduces a sample-based, model-free double SOR Q-learning algorithm, analyzed theoretically and empirically, and extended to deep RL.

Result: The algorithm shows less bias than SOR Q-learning and is validated in tabular and large-scale settings.

Conclusion: The proposed algorithm effectively addresses SOR Q-learning's limitations, demonstrating improved performance in both tabular and deep RL scenarios.

Abstract: Q-learning is a widely used algorithm in reinforcement learning (RL), but its
convergence can be slow, especially when the discount factor is close to one.
Successive Over-Relaxation (SOR) Q-learning, which introduces a relaxation
factor to speed up convergence, addresses this issue but has two major
limitations: In the tabular setting, the relaxation parameter depends on
transition probability, making it not entirely model-free, and it suffers from
overestimation bias. To overcome these limitations, we propose a sample-based,
model-free double SOR Q-learning algorithm. Theoretically and empirically, this
algorithm is shown to be less biased than SOR Q-learning. Further, in the
tabular setting, the convergence analysis under boundedness assumptions on
iterates is discussed. The proposed algorithm is extended to large-scale
problems using deep RL. Finally, the tabular version of the proposed algorithm
is compared using roulette and grid world environments, while the deep RL
version is tested on a maximization bias example and OpenAI Gym environments.

</details>


### [308] [Temporal-Difference Variational Continual Learning](https://arxiv.org/pdf/2410.07812)
*Luckeciano C. Melo, Alessandro Abate, Yarin Gal*

Main category: cs.LG

TL;DR: The paper addresses Catastrophic Forgetting in Continual Learning by proposing new Bayesian learning objectives that integrate multiple past posterior estimates, outperforming existing methods.


<details>
  <summary>Details</summary>
Motivation: To balance plasticity and stability in Continual Learning, mitigating Catastrophic Forgetting caused by compounding approximation errors in variational methods.

Method: Introduces new learning objectives that regularize updates using multiple past posterior estimates, preventing error dominance and compounding.

Result: Outperforms strong Variational CL methods on benchmarks, effectively reducing Catastrophic Forgetting.

Conclusion: The proposed method successfully mitigates forgetting by leveraging multiple past estimates, with connections to Temporal-Difference learning.

Abstract: Machine Learning models in real-world applications must continuously learn
new tasks to adapt to shifts in the data-generating distribution. Yet, for
Continual Learning (CL), models often struggle to balance learning new tasks
(plasticity) with retaining previous knowledge (memory stability).
Consequently, they are susceptible to Catastrophic Forgetting, which degrades
performance and undermines the reliability of deployed systems. In the Bayesian
CL literature, variational methods tackle this challenge by employing a
learning objective that recursively updates the posterior distribution while
constraining it to stay close to its previous estimate. Nonetheless, we argue
that these methods may be ineffective due to compounding approximation errors
over successive recursions. To mitigate this, we propose new learning
objectives that integrate the regularization effects of multiple previous
posterior estimations, preventing individual errors from dominating future
posterior updates and compounding over time. We reveal insightful connections
between these objectives and Temporal-Difference methods, a popular learning
mechanism in Reinforcement Learning and Neuroscience. Experiments on
challenging CL benchmarks show that our approach effectively mitigates
Catastrophic Forgetting, outperforming strong Variational CL methods.

</details>


### [309] [Towards Graph Foundation Models: Training on Knowledge Graphs Enables Transferability to General Graphs](https://arxiv.org/pdf/2410.12609)
*Kai Wang, Siqiang Luo, Caihua Shan, Yifei Shen*

Main category: cs.LG

TL;DR: SCR is a unified graph reasoning framework that generalizes across diverse graph tasks by leveraging semantic-conditioned message passing and task-specific KG structures.


<details>
  <summary>Details</summary>
Motivation: Current graph foundation models require fine-tuning for new graphs, limiting versatility. SCR aims to extend zero-shot inductive reasoning from KGs to general graphs.

Method: Designs task-specific KG structures and introduces semantic-conditioned message passing to model structural and semantic invariance.

Result: Evaluated on 38 datasets, SCR outperforms existing foundation models and supervised baselines.

Conclusion: SCR demonstrates efficacy and adaptability in generalizing across graph tasks without extra fine-tuning.

Abstract: Inspired by the success of large language models, there is a trend toward
developing graph foundation models to conduct diverse downstream tasks in
various domains. However, current models often require extra fine-tuning to
apply their learned structural and semantic representations to new graphs,
which limits their versatility. Recent breakthroughs in zero-shot inductive
reasoning on knowledge graphs (KGs), offer us a new perspective on extending KG
reasoning to general graph applications. In this paper, we introduce SCR, a
unified graph reasoning framework designed to train on knowledge graphs and
effectively generalize across a wide range of graph tasks and domains. We begin
by designing the task-specific KG structures to establish a unified topology
for different task formats. Then we propose semantic-conditioned message
passing, a novel mechanism addressing the inherent semantic isolation in
traditional KG reasoning, by jointly modeling structural and semantic
invariance patterns in graph representations. To demonstrate the effectiveness,
we evaluate the inductive reasoning capability of SCR using 38 diverse graph
datasets, covering node-level, link-level, and graph-level tasks across
multiple domains. Our results show substantial performance gains over existing
foundation models and supervised baselines, highlighting the efficacy and
adaptability of our approach.

</details>


### [310] [TSINR: Capturing Temporal Continuity via Implicit Neural Representations for Time Series Anomaly Detection](https://arxiv.org/pdf/2411.11641)
*Mengxuan Li, Ke Liu, Hongyang Chen, Jiajun Bu, Hongwei Wang, Haishuai Wang*

Main category: cs.LG

TL;DR: TSINR is a time series anomaly detection method using implicit neural representation (INR) to prioritize low-frequency signals and improve sensitivity to anomalies.


<details>
  <summary>Details</summary>
Motivation: Reconstruction-based methods struggle with unlabeled anomalies in training data, leading to poor normal pattern capture.

Method: TSINR uses INR to parameterize time series as a continuous function, employs a transformer-based architecture, and leverages a pre-trained language model for anomaly detection.

Result: TSINR outperforms state-of-the-art methods on univariate and multivariate benchmarks.

Conclusion: TSINR effectively addresses reconstruction-based challenges and improves anomaly detection performance.

Abstract: Time series anomaly detection aims to identify unusual patterns in data or
deviations from systems' expected behavior. The reconstruction-based methods
are the mainstream in this task, which learn point-wise representation via
unsupervised learning. However, the unlabeled anomaly points in training data
may cause these reconstruction-based methods to learn and reconstruct anomalous
data, resulting in the challenge of capturing normal patterns. In this paper,
we propose a time series anomaly detection method based on implicit neural
representation (INR) reconstruction, named TSINR, to address this challenge.
Due to the property of spectral bias, TSINR enables prioritizing low-frequency
signals and exhibiting poorer performance on high-frequency abnormal data.
Specifically, we adopt INR to parameterize time series data as a continuous
function and employ a transformer-based architecture to predict the INR of
given data. As a result, the proposed TSINR method achieves the advantage of
capturing the temporal continuity and thus is more sensitive to discontinuous
anomaly data. In addition, we further design a novel form of INR continuous
function to learn inter- and intra-channel information, and leverage a
pre-trained large language model to amplify the intense fluctuations in
anomalies. Extensive experiments demonstrate that TSINR achieves superior
overall performance on both univariate and multivariate time series anomaly
detection benchmarks compared to other state-of-the-art reconstruction-based
methods. Our codes are available.

</details>


### [311] [Natural Language Reinforcement Learning](https://arxiv.org/pdf/2411.14251)
*Xidong Feng, Bo Liu, Ziyu Wan, Haotian Fu, Girish A. Koushik, Zhiyuan Hu, Mengyue Yang, Ying Wen, Jun Wang*

Main category: cs.LG

TL;DR: The paper introduces Natural Language Reinforcement Learning (NLRL), extending traditional MDPs to natural language representations, leveraging LLMs for practical implementation, and demonstrating effectiveness in games.


<details>
  <summary>Details</summary>
Motivation: To explore a new approach (NLRL) by extending traditional RL principles into natural language space, enabled by advancements in LLMs.

Method: Redefines RL components (objectives, policy, value function, etc.) into language counterparts, implemented via prompting or gradient-based training with LLMs.

Result: Experiments on Maze, Breakthrough, and Tic-Tac-Toe show NLRL's effectiveness, efficiency, and interpretability.

Conclusion: NLRL offers a promising framework for RL in natural language, with practical applications and interpretability.

Abstract: Reinforcement Learning (RL) mathematically formulates decision-making with
Markov Decision Process (MDP). With MDPs, researchers have achieved remarkable
breakthroughs across various domains, including games, robotics, and language
models. This paper seeks a new possibility, Natural Language Reinforcement
Learning (NLRL), by extending traditional MDP to natural language-based
representation space. Specifically, NLRL innovatively redefines RL principles,
including task objectives, policy, value function, Bellman equation, and policy
iteration, into their language counterparts. With recent advancements in large
language models (LLMs), NLRL can be practically implemented to achieve RL-like
policy and value improvement by either pure prompting or gradient-based
training. Experiments over Maze, Breakthrough, and Tic-Tac-Toe games
demonstrate the effectiveness, efficiency, and interpretability of the NLRL
framework among diverse use cases.

</details>


### [312] [Goal-Conditioned Supervised Learning for Multi-Objective Recommendation](https://arxiv.org/pdf/2412.08911)
*Shijun Li, Hilaf Hasson, Jing Hu, Joydeep Ghosh*

Main category: cs.LG

TL;DR: MOGCSL is a framework for multi-objective learning from offline sequential data, extending GCSL to handle multi-dimensional goals and filtering noisy data. It excels in scalable, robust next-action prediction for recommender systems.


<details>
  <summary>Details</summary>
Motivation: Multi-objective learning is complex due to conflicts between objectives, leading to high computational demands. MOGCSL simplifies this by redefining goals and filtering noise.

Method: MOGCSL extends GCSL to multi-objective scenarios by using multi-dimensional goal vectors and introduces a goal-selection algorithm for high achievable goals.

Result: MOGCSL performs well in scalable, robust next-action prediction for recommender systems, effectively handling noisy data.

Conclusion: MOGCSL is a scalable and robust solution for multi-objective learning, particularly effective in noisy, real-world recommender systems.

Abstract: Multi-objective learning endeavors to concurrently optimize multiple
objectives using a single model, aiming to achieve high and balanced
performance across diverse objectives. However, this often entails a more
complex optimization problem, particularly when navigating potential conflicts
between objectives, leading to solutions with higher memory requirements and
computational complexity. This paper introduces a Multi-Objective
Goal-Conditioned Supervised Learning (MOGCSL) framework for automatically
learning to achieve multiple objectives from offline sequential data. MOGCSL
extends the conventional GCSL method to multi-objective scenarios by redefining
goals from one-dimensional scalars to multi-dimensional vectors. It benefits
from naturally eliminating the need for complex architectures and optimization
constraints. Moreover, MOGCSL effectively filters out uninformative or noisy
instances that fail to achieve desirable long-term rewards across multiple
objectives. We also introduces a novel goal-selection algorithm for MOGCSL to
model and identify "high" achievable goals for inference.
  While MOGCSL is quite general, we focus on its application to the next action
prediction problem in commercial-grade recommender systems. In this context,
any viable solution needs to be reasonably scalable and also be robust to large
amounts of noisy data that is characteristic of this application space. We show
that MOGCSL performs admirably on both counts by extensive experiments on
real-world recommendation datasets. Also, analysis and experiments are included
to explain its strength in discounting the noisier portions of training data in
recommender systems with multiple objectives.

</details>


### [313] [Understanding In-context Learning of Addition via Activation Subspaces](https://arxiv.org/pdf/2505.05145)
*Xinyan Hu, Kayo Yin, Michael I. Jordan, Jacob Steinhardt, Lijie Chen*

Main category: cs.LG

TL;DR: The paper investigates how transformer models like Llama-3-8B perform in-context learning by analyzing a structured task of adding an integer $k$ to inputs. It identifies key attention heads and a low-dimensional subspace for signal extraction.


<details>
  <summary>Details</summary>
Motivation: To understand the mechanisms behind in-context learning in transformers, specifically how signals from few-shot examples are aggregated and applied.

Method: The study uses a structured task (adding $k$ to inputs) and analyzes Llama-3-8B's performance, localizing few-shot ability to three attention heads and a six-dimensional subspace.

Result: Llama-3-8B achieves high accuracy, with signal extraction localized to specific heads and subspaces. A self-correction mechanism is identified.

Conclusion: Tracking low-dimensional subspaces reveals fine-grained computational structures in transformers, providing insights into in-context learning.

Abstract: To perform in-context learning, language models must extract signals from
individual few-shot examples, aggregate these into a learned prediction rule,
and then apply this rule to new examples. How is this implemented in the
forward pass of modern transformer models? To study this, we consider a
structured family of few-shot learning tasks for which the true prediction rule
is to add an integer $k$ to the input. We find that Llama-3-8B attains high
accuracy on this task for a range of $k$, and localize its few-shot ability to
just three attention heads via a novel optimization approach. We further show
the extracted signals lie in a six-dimensional subspace, where four of the
dimensions track the unit digit and the other two dimensions track overall
magnitude. We finally examine how these heads extract information from
individual few-shot examples, identifying a self-correction mechanism in which
mistakes from earlier examples are suppressed by later examples. Our results
demonstrate how tracking low-dimensional subspaces across a forward pass can
provide insight into fine-grained computational structures.

</details>


### [314] [Representation Convergence: Mutual Distillation is Secretly a Form of Regularization](https://arxiv.org/pdf/2501.02481)
*Zhengpeng Xie, Jiahang Cao, Qiang Zhang, Jianxiong Zhang, Changwei Wang, Renjing Xu*

Main category: cs.LG

TL;DR: Mutual distillation in reinforcement learning acts as implicit regularization, improving generalization by reducing overfitting to irrelevant features.


<details>
  <summary>Details</summary>
Motivation: To challenge the conventional view of distillation as just knowledge transfer and explore its role in enhancing policy robustness and generalization.

Method: Theoretical proof and empirical demonstration of mutual distillation between policies to enhance robustness to irrelevant features.

Result: Mutual distillation improves generalization by fostering invariant representations in pixel inputs.

Conclusion: Mutual distillation offers a novel perspective on generalization in deep reinforcement learning, beyond traditional knowledge transfer.

Abstract: In this paper, we argue that mutual distillation between reinforcement
learning policies serves as an implicit regularization, preventing them from
overfitting to irrelevant features. We highlight two key contributions: (a)
Theoretically, for the first time, we prove that enhancing the policy
robustness to irrelevant features leads to improved generalization performance.
(b) Empirically, we demonstrate that mutual distillation between policies
contributes to such robustness, enabling the spontaneous emergence of invariant
representations over pixel inputs. Overall, our findings challenge the
conventional view of distillation as merely a means of knowledge transfer,
offering a novel perspective on the generalization in deep reinforcement
learning.

</details>


### [315] [Lightspeed Geometric Dataset Distance via Sliced Optimal Transport](https://arxiv.org/pdf/2501.18901)
*Khai Nguyen, Hai Nguyen, Tuan Pham, Nhat Ho*

Main category: cs.LG

TL;DR: The paper introduces s-OTDD, a method for dataset comparison that is model-agnostic, efficient, and handles disjoint label sets. It uses Moment Transform Projection (MTP) to map labels to real numbers, enabling linear computational complexity and strong correlation with performance metrics.


<details>
  <summary>Details</summary>
Motivation: Existing dataset comparison methods often lack efficiency, robustness to class variations, or the ability to handle disjoint label sets. s-OTDD addresses these limitations.

Method: The method involves Moment Transform Projection (MTP) to map labels to real numbers, transforming datasets into one-dimensional distributions. The s-OTDD is computed as the expected Wasserstein distance between these distributions.

Result: s-OTDD achieves near-linear computational complexity, correlates well with optimal transport dataset distance, and aligns with transfer learning and data augmentation performance.

Conclusion: s-OTDD is a robust, efficient, and effective method for dataset comparison, offering advantages over existing approaches.

Abstract: We introduce sliced optimal transport dataset distance (s-OTDD), a
model-agnostic, embedding-agnostic approach for dataset comparison that
requires no training, is robust to variations in the number of classes, and can
handle disjoint label sets. The core innovation is Moment Transform Projection
(MTP), which maps a label, represented as a distribution over features, to a
real number. Using MTP, we derive a data point projection that transforms
datasets into one-dimensional distributions. The s-OTDD is defined as the
expected Wasserstein distance between the projected distributions, with respect
to random projection parameters. Leveraging the closed form solution of
one-dimensional optimal transport, s-OTDD achieves (near-)linear computational
complexity in the number of data points and feature dimensions and is
independent of the number of classes. With its geometrically meaningful
projection, s-OTDD strongly correlates with the optimal transport dataset
distance while being more efficient than existing dataset discrepancy measures.
Moreover, it correlates well with the performance gap in transfer learning and
classification accuracy in data augmentation.

</details>


### [316] [Flexible Graph Similarity Computation With A Proactive Optimization Strategy](https://arxiv.org/pdf/2504.06533)
*Zhouyang Liu, Ning Liu, Yixin Chen, Jiezhong He, Dongsheng Li*

Main category: cs.LG

TL;DR: GEN is a learning-based method for flexible Graph Edit Distance (GED) approximation, improving accuracy and efficiency by integrating operation costs upfront and using proactive guidance.


<details>
  <summary>Details</summary>
Motivation: Existing GED approximation methods struggle with adapting to varying operation costs and rely on inefficient reactive refinements.

Method: GEN integrates operation costs prior to match establishment and uses proactive guidance to capture graph-level dependencies, enabling dynamic adaptation and informed matching.

Result: GEN reduces GED approximation error by up to 37.8% and inference time by 72.7% compared to state-of-the-art methods.

Conclusion: GEN outperforms existing methods in accuracy, efficiency, and robustness under diverse cost settings and graph sizes.

Abstract: Graph Edit Distance (GED) offers a principled and flexible measure of graph
similarity, as it quantifies the minimum cost needed to transform one graph
into another with customizable edit operation costs. Despite recent
learning-based efforts to approximate GED via vector space representations,
existing methods struggle with adapting to varying operation costs.
Furthermore, they suffer from inefficient, reactive mapping refinements due to
reliance on isolated node-level distance as guidance. To address these issues,
we propose GEN, a novel learning-based approach for flexible GED approximation.
GEN addresses the varying costs adaptation by integrating operation costs prior
to match establishment, enabling mappings to dynamically adapt to cost
variations. Furthermore, GEN introduces a proactive guidance optimization
strategy that captures graph-level dependencies between matches, allowing
informed matching decisions in a single step without costly iterative
refinements. Extensive evaluations on real-world and synthetic datasets
demonstrate that GEN achieves up to 37.8% reduction in GED approximation error
and 72.7% reduction in inference time compared with state-of-the-art methods,
while consistently maintaining robustness under diverse cost settings and graph
sizes.

</details>


### [317] [MTDT: A Multi-Task Deep Learning Digital Twin](https://arxiv.org/pdf/2405.00922)
*Nooshin Yousefzadeh, Rahul Sengupta, Yashaswi Karnati, Anand Rangarajan, Sanjay Ranka*

Main category: cs.LG

TL;DR: A multi-task deep learning model (MTDT) is proposed to simulate urban intersection traffic flow, addressing data scarcity and capturing spatiotemporal features better than traditional methods.


<details>
  <summary>Details</summary>
Motivation: Traffic congestion impacts economy and environment, but traditional MOEs and loop detector data are insufficient for accurate evaluation.

Method: Uses multi-task learning with graph and time series convolutions to estimate lane-wise flows, queue lengths, and travel times.

Result: MTDT adapts to local features (signal timing, topology, etc.) and outperforms existing deep learning methods, with GPU-enabled scalability.

Conclusion: The MTDT approach enhances traffic simulation accuracy and computational efficiency, offering a scalable solution for urban intersections.

Abstract: Traffic congestion has significant impacts on both the economy and the
environment. Measures of Effectiveness (MOEs) have long been the standard for
evaluating traffic intersections' level of service and operational efficiency.
However, the scarcity of traditional high-resolution loop detector data (ATSPM)
presents challenges in accurately measuring MOEs or capturing the intricate
spatiotemporal characteristics inherent in urban intersection traffic. To
address this challenge, we present a comprehensive intersection traffic flow
simulation that utilizes a multi-task learning paradigm. This approach combines
graph convolutions for primary estimating lane-wise exit and inflow with time
series convolutions for secondary assessing multi-directional queue lengths and
travel time distribution through any arbitrary urban traffic intersection.
Compared to existing deep learning methodologies, the proposed Multi-Task Deep
Learning Digital Twin (MTDT) distinguishes itself through its adaptability to
local temporal and spatial features, such as signal timing plans, intersection
topology, driving behaviors, and turning movement counts. We also show the
benefit of multi-task learning in the effectiveness of individual traffic
simulation tasks. Furthermore, our approach facilitates sequential computation
and provides complete parallelization through GPU implementation. This not only
streamlines the computational process but also enhances scalability and
performance.

</details>


### [318] [Hierarchical World Models as Visual Whole-Body Humanoid Controllers](https://arxiv.org/pdf/2405.18418)
*Nicklas Hansen, Jyothir S V, Vlad Sobal, Yann LeCun, Xiaolong Wang, Hao Su*

Main category: cs.LG

TL;DR: A hierarchical world model for visual whole-body humanoid control using reinforcement learning, achieving high performance in 8 tasks with a 56-DoF humanoid.


<details>
  <summary>Details</summary>
Motivation: Addressing the challenges of high-dimensionality and instability in humanoid control, especially when learning from visual observations.

Method: Proposes a hierarchical world model with high-level and low-level agents trained with rewards, without simplifying assumptions or skill primitives.

Result: Produces performant control policies and human-preferred motions in simulated tasks.

Conclusion: Demonstrates the effectiveness of data-driven reinforcement learning for complex humanoid control.

Abstract: Whole-body control for humanoids is challenging due to the high-dimensional
nature of the problem, coupled with the inherent instability of a bipedal
morphology. Learning from visual observations further exacerbates this
difficulty. In this work, we explore highly data-driven approaches to visual
whole-body humanoid control based on reinforcement learning, without any
simplifying assumptions, reward design, or skill primitives. Specifically, we
propose a hierarchical world model in which a high-level agent generates
commands based on visual observations for a low-level agent to execute, both of
which are trained with rewards. Our approach produces highly performant control
policies in 8 tasks with a simulated 56-DoF humanoid, while synthesizing
motions that are broadly preferred by humans.

</details>


### [319] [Machine Learning with Physics Knowledge for Prediction: A Survey](https://arxiv.org/pdf/2408.09840)
*Joe Watson, Chen Song, Oliver Weeger, Theo Gruner, An T. Le, Kay Pompetzki, Ahmed Hendawy, Oleg Arenz, Will Trojak, Miles Cranmer, Carlo D'Eramo, Fabian Bülow, Tanmay Goyal, Jan Peters, Martin W. Hoffman*

Main category: cs.LG

TL;DR: A survey on combining machine learning with physics knowledge, especially for PDEs, covering architectural and data-driven methods, with industrial applications and open-source tools.


<details>
  <summary>Details</summary>
Motivation: To improve predictive models in scientific and industrial settings by integrating physics knowledge with machine learning, addressing small- or large-scale datasets and inductive biases.

Method: Two-part approach: 1) Architectural incorporation via objective functions, structured models, and data augmentation; 2) Data-driven methods like multi-task, meta, and contextual learning.

Result: Overview of methods for physics-informed machine learning, highlighting their potential impact and practical applications.

Conclusion: The survey provides a comprehensive guide to integrating physics with machine learning, emphasizing both theoretical and practical advancements, including industrial use and open-source resources.

Abstract: This survey examines the broad suite of methods and models for combining
machine learning with physics knowledge for prediction and forecast, with a
focus on partial differential equations. These methods have attracted
significant interest due to their potential impact on advancing scientific
research and industrial practices by improving predictive models with small- or
large-scale datasets and expressive predictive models with useful inductive
biases. The survey has two parts. The first considers incorporating physics
knowledge on an architectural level through objective functions, structured
predictive models, and data augmentation. The second considers data as physics
knowledge, which motivates looking at multi-task, meta, and contextual learning
as an alternative approach to incorporating physics knowledge in a data-driven
fashion. Finally, we also provide an industrial perspective on the application
of these methods and a survey of the open-source ecosystem for physics-informed
machine learning.

</details>


### [320] [Fast and Robust: Task Sampling with Posterior and Diversity Synergies for Adaptive Decision-Makers in Randomized Environments](https://arxiv.org/pdf/2504.19139)
*Yun Qu, Qi Cheems Wang, Yixiu Mao, Yiqin Lv, Xiangyang Ji*

Main category: cs.LG

TL;DR: The paper proposes PDTS, a method for robust active task sampling in sequential decision-making, improving adaptation robustness and learning efficiency.


<details>
  <summary>Details</summary>
Motivation: Addressing the efficiency and robustness challenges in task adaptation, particularly in risk-averse scenarios, by prioritizing difficult tasks without costly evaluations.

Method: Introduces PDTS, a method combining posterior and diversity synergized task sampling, framed as a Markov decision process for optimization.

Result: PDTS enhances zero-shot and few-shot adaptation robustness and accelerates learning in challenging tasks.

Conclusion: PDTS effectively addresses robustness and efficiency in sequential decision-making, with practical benefits demonstrated in experiments.

Abstract: Task robust adaptation is a long-standing pursuit in sequential
decision-making. Some risk-averse strategies, e.g., the conditional
value-at-risk principle, are incorporated in domain randomization or meta
reinforcement learning to prioritize difficult tasks in optimization, which
demand costly intensive evaluations. The efficiency issue prompts the
development of robust active task sampling to train adaptive policies, where
risk-predictive models are used to surrogate policy evaluation. This work
characterizes the optimization pipeline of robust active task sampling as a
Markov decision process, posits theoretical and practical insights, and
constitutes robustness concepts in risk-averse scenarios. Importantly, we
propose an easy-to-implement method, referred to as Posterior and Diversity
Synergized Task Sampling (PDTS), to accommodate fast and robust sequential
decision-making. Extensive experiments show that PDTS unlocks the potential of
robust active task sampling, significantly improves the zero-shot and few-shot
adaptation robustness in challenging tasks, and even accelerates the learning
process under certain scenarios. Our project website is at
https://thu-rllab.github.io/PDTS_project_page.

</details>


### [321] [Scaling Laws for Black box Adversarial Attacks](https://arxiv.org/pdf/2411.16782)
*Chuan Liu, Huanran Chen, Yichi Zhang, Yinpeng Dong, Jun Zhu*

Main category: cs.LG

TL;DR: Scaling the number of surrogate models in adversarial attacks improves transferability, achieving high success rates on proprietary models like GPT-4o.


<details>
  <summary>Details</summary>
Motivation: To explore whether increasing the number of surrogate models in adversarial attacks enhances transferability, inspired by scaling laws in large foundation models.

Method: Theoretical analysis and empirical evaluations using standard image classifiers, defended models, and multimodal large language models with various attack methods.

Result: Clear scaling laws confirm that more surrogate models boost adversarial transferability, achieving over 90% success on proprietary models.

Conclusion: Scaling the number of surrogate models significantly improves adversarial attack transferability, with implications for interpretability and semantics of perturbations.

Abstract: Adversarial examples usually exhibit good cross-model transferability,
enabling attacks on black-box models with limited information about their
architectures and parameters, which are highly threatening in commercial
black-box scenarios. Model ensembling is an effective strategy to improve the
transferability of adversarial examples by attacking multiple surrogate models.
However, since prior studies usually adopt few models in the ensemble, there
remains an open question of whether scaling the number of models can further
improve black-box attacks. Inspired by the scaling law of large foundation
models, we investigate the scaling laws of black-box adversarial attacks in
this work. Through theoretical analysis and empirical evaluations, we conclude
with clear scaling laws that using more surrogate models enhances adversarial
transferability. Comprehensive experiments verify the claims on standard image
classifiers, diverse defended models and multimodal large language models using
various adversarial attack methods. Specifically, by scaling law, we achieve
90%+ transfer attack success rate on even proprietary models like GPT-4o.
Further visualization indicates that there is also a scaling law on the
interpretability and semantics of adversarial perturbations.

</details>


### [322] [Hierarchical Learning and Computing over Space-Ground Integrated Networks](https://arxiv.org/pdf/2408.14116)
*Jingyang Zhu, Yuanming Shi, Yong Zhou, Chunxiao Jiang, Linling Kuang*

Main category: cs.LG

TL;DR: A hierarchical learning framework for space-ground networks reduces communication overhead and privacy risks by leveraging LEO and GEO satellites for model aggregation, solved via a topology-aware energy-efficient routing algorithm.


<details>
  <summary>Details</summary>
Motivation: Addressing the challenges of massive data transfer and privacy concerns in IoT devices in remote areas lacking terrestrial infrastructure.

Method: Proposes a hierarchical framework using LEO and GEO satellites, formulates model aggregation as a Directed Steiner Tree problem, and introduces the TAEER algorithm for energy-efficient routing.

Result: TAEER significantly reduces energy consumption in real-world simulations compared to benchmarks.

Conclusion: The framework and algorithm effectively optimize energy use in space-ground networks, enhancing global IoT connectivity.

Abstract: Space-ground integrated networks hold great promise for providing global
connectivity, particularly in remote areas where large amounts of valuable data
are generated by Internet of Things (IoT) devices, but lacking terrestrial
communication infrastructure. The massive data is conventionally transferred to
the cloud server for centralized artificial intelligence (AI) models training,
raising huge communication overhead and privacy concerns. To address this, we
propose a hierarchical learning and computing framework, which leverages the
lowlatency characteristic of low-earth-orbit (LEO) satellites and the global
coverage of geostationary-earth-orbit (GEO) satellites, to provide global
aggregation services for locally trained models on ground IoT devices. Due to
the time-varying nature of satellite network topology and the energy
constraints of LEO satellites, efficiently aggregating the received local
models from ground devices on LEO satellites is highly challenging. By
leveraging the predictability of inter-satellite connectivity, modeling the
space network as a directed graph, we formulate a network energy minimization
problem for model aggregation, which turns out to be a Directed Steiner Tree
(DST) problem. We propose a topologyaware energy-efficient routing (TAEER)
algorithm to solve the DST problem by finding a minimum spanning arborescence
on a substitute directed graph. Extensive simulations under realworld
space-ground integrated network settings demonstrate that the proposed TAEER
algorithm significantly reduces energy consumption and outperforms benchmarks.

</details>


### [323] [DELTA: Dual Consistency Delving with Topological Uncertainty for Active Graph Domain Adaptation](https://arxiv.org/pdf/2409.08946)
*Pengyun Wang, Yadi Cao, Chris Russell, Yanxin Shen, Junyu Luo, Ming Zhang, Siyu Heng, Xiao Luo*

Main category: cs.LG

TL;DR: DELTA improves graph domain adaptation by selecting informative nodes for annotation using dual subnetworks and topological uncertainty.


<details>
  <summary>Details</summary>
Motivation: Address the challenge of poor performance in graph domain adaptation due to lack of semantic information on target graphs.

Method: Proposes DELTA, combining edge-oriented and path-oriented subnetworks to explore topological semantics, and uses consistency and uncertainty for node selection.

Result: DELTA outperforms state-of-the-art methods on benchmark datasets.

Conclusion: DELTA effectively enhances graph domain adaptation by leveraging topological semantics and active learning.

Abstract: Graph domain adaptation has recently enabled knowledge transfer across
different graphs. However, without the semantic information on target graphs,
the performance on target graphs is still far from satisfactory. To address the
issue, we study the problem of active graph domain adaptation, which selects a
small quantitative of informative nodes on the target graph for extra
annotation. This problem is highly challenging due to the complicated
topological relationships and the distribution discrepancy across graphs. In
this paper, we propose a novel approach named Dual Consistency Delving with
Topological Uncertainty (DELTA) for active graph domain adaptation. Our DELTA
consists of an edge-oriented graph subnetwork and a path-oriented graph
subnetwork, which can explore topological semantics from complementary
perspectives. In particular, our edge-oriented graph subnetwork utilizes the
message passing mechanism to learn neighborhood information, while our
path-oriented graph subnetwork explores high-order relationships from
sub-structures. To jointly learn from two subnetworks, we roughly select
informative candidate nodes with the consideration of consistency across two
subnetworks. Then, we aggregate local semantics from its K-hop subgraph based
on node degrees for topological uncertainty estimation. To overcome potential
distribution shifts, we compare target nodes and their corresponding source
nodes for discrepancy scores as an additional component for fine selection.
Extensive experiments on benchmark datasets demonstrate that DELTA outperforms
various state-of-the-art approaches. The code implementation of DELTA is
available at https://github.com/goose315/DELTA.

</details>


### [324] [MDL-Pool: Adaptive Multilevel Graph Pooling Based on Minimum Description Length](https://arxiv.org/pdf/2409.10263)
*Jan von Pichowski, Christopher Blöcker, Ingo Scholtes*

Main category: cs.LG

TL;DR: MDL-Pool is a graph pooling method using the MDL principle to model interdependencies between hierarchical levels and adapt to varying graph sizes, outperforming baselines in classification tasks.


<details>
  <summary>Details</summary>
Motivation: Current graph pooling methods ignore interdependencies between hierarchical levels and lack adaptability to graphs of varying sizes, limiting their effectiveness.

Method: Proposes MDL-Pool, leveraging the MDL principle and the map equation to balance model complexity and fit, enabling direct comparison of pooling depths.

Result: MDL-Pool shows competitive performance in graph classification benchmarks compared to existing methods.

Conclusion: MDL-Pool addresses limitations of current pooling methods by incorporating interdependencies and adaptability, proving effective in empirical evaluations.

Abstract: Graph pooling compresses graphs and summarises their topological properties
and features in a vectorial representation. It is an essential part of deep
graph representation learning and is indispensable in graph-level tasks like
classification or regression. Current approaches pool hierarchical structures
in graphs by iteratively applying shallow pooling operators up to a fixed
depth. However, they disregard the interdependencies between structures at
different hierarchical levels and do not adapt to datasets that contain graphs
with different sizes that may require pooling with various depths. To address
these issues, we propose MDL-Pool, a pooling operator based on the minimum
description length (MDL) principle, whose loss formulation explicitly models
the interdependencies between different hierarchical levels and facilitates a
direct comparison between multiple pooling alternatives with different depths.
MDP-Pool builds on the map equation, an information-theoretic objective
function for community detection, which naturally implements Occam's razor and
balances between model complexity and goodness-of-fit via the MDL. We
demonstrate MDL-Pool's competitive performance in an empirical evaluation
against various baselines across standard graph classification datasets.

</details>


### [325] [Intelligently Augmented Contrastive Tensor Factorization: Empowering Multi-dimensional Time Series Classification in Low-Data Environments](https://arxiv.org/pdf/2505.03825)
*Anushiya Arunan, Yan Qin, Xiaoli Li, Yuen Chau*

Main category: cs.LG

TL;DR: ITA-CTF is a data-efficient framework for classifying multi-dimensional time series by combining contrastive tensor factorization with intelligent augmentations to improve feature learning and classification performance.


<details>
  <summary>Details</summary>
Motivation: Standard DL struggles with low training data, leading to overfitting. The paper aims to address this by learning generalizable features from multi-dimensional time series despite limited data.

Method: ITA-CTF uses a CTF module for tensor factorization with contrastive loss and an ITA module for generating targeted augmentations. The ITA module dynamically samples a 'soft' class prototype to guide augmentations.

Result: The method outperforms standard TF and DL benchmarks, achieving up to 18.7% improvement in classification tasks.

Conclusion: ITA-CTF effectively learns complex features and dependencies in low-data environments, enhancing classification accuracy.

Abstract: Classification of multi-dimensional time series from real-world systems
require fine-grained learning of complex features such as cross-dimensional
dependencies and intra-class variations-all under the practical challenge of
low training data availability. However, standard deep learning (DL) struggles
to learn generalizable features in low-data environments due to model
overfitting. We propose a versatile yet data-efficient framework, Intelligently
Augmented Contrastive Tensor Factorization (ITA-CTF), to learn effective
representations from multi-dimensional time series. The CTF module learns core
explanatory components of the time series (e.g., sensor factors, temporal
factors), and importantly, their joint dependencies. Notably, unlike standard
tensor factorization (TF), the CTF module incorporates a new contrastive loss
optimization to induce similarity learning and class-awareness into the learnt
representations for better classification performance. To strengthen this
contrastive learning, the preceding ITA module generates targeted but
informative augmentations that highlight realistic intra-class patterns in the
original data, while preserving class-wise properties. This is achieved by
dynamically sampling a "soft" class prototype to guide the warping of each
query data sample, which results in an augmentation that is intelligently
pattern-mixed between the "soft" class prototype and the query sample. These
augmentations enable the CTF module to recognize complex intra-class variations
despite the limited original training data, and seek out invariant class-wise
properties for accurate classification performance. The proposed method is
comprehensively evaluated on five different classification tasks. Compared to
standard TF and several DL benchmarks, notable performance improvements up to
18.7% were achieved.

</details>


### [326] [TimeBridge: Non-Stationarity Matters for Long-term Time Series Forecasting](https://arxiv.org/pdf/2410.04442)
*Peiyuan Liu, Beiliang Wu, Yifan Hu, Naiqi Li, Tao Dai, Jigang Bao, Shu-tao Xia*

Main category: cs.LG

TL;DR: TimeBridge is a novel framework addressing non-stationarity in multivariate time series forecasting by balancing short-term and long-term modeling through Integrated and Cointegrated Attention.


<details>
  <summary>Details</summary>
Motivation: Non-stationarity complicates forecasting by affecting short-term and long-term relationships differently, yet existing methods fail to address this adequately.

Method: TimeBridge segments input series into patches, using Integrated Attention for short-term stability and Cointegrated Attention for long-term cointegration.

Result: TimeBridge achieves state-of-the-art performance in both short-term and long-term forecasting, including financial datasets like CSI 500 and S&P 500.

Conclusion: TimeBridge effectively bridges the gap between non-stationarity and dependency modeling, proving robust and versatile in real-world applications.

Abstract: Non-stationarity poses significant challenges for multivariate time series
forecasting due to the inherent short-term fluctuations and long-term trends
that can lead to spurious regressions or obscure essential long-term
relationships. Most existing methods either eliminate or retain
non-stationarity without adequately addressing its distinct impacts on
short-term and long-term modeling. Eliminating non-stationarity is essential
for avoiding spurious regressions and capturing local dependencies in
short-term modeling, while preserving it is crucial for revealing long-term
cointegration across variates. In this paper, we propose TimeBridge, a novel
framework designed to bridge the gap between non-stationarity and dependency
modeling in long-term time series forecasting. By segmenting input series into
smaller patches, TimeBridge applies Integrated Attention to mitigate short-term
non-stationarity and capture stable dependencies within each variate, while
Cointegrated Attention preserves non-stationarity to model long-term
cointegration across variates. Extensive experiments show that TimeBridge
consistently achieves state-of-the-art performance in both short-term and
long-term forecasting. Additionally, TimeBridge demonstrates exceptional
performance in financial forecasting on the CSI 500 and S&P 500 indices,
further validating its robustness and effectiveness. Code is available at
https://github.com/Hank0626/TimeBridge.

</details>


### [327] [Convolutional Neural Networks and Mixture of Experts for Intrusion Detection in 5G Networks and beyond](https://arxiv.org/pdf/2412.03483)
*Loukas Ilias, George Doukas, Vangelis Lamprou, Christos Ntanos, Dimitris Askounis*

Main category: cs.LG

TL;DR: The paper proposes a novel intrusion detection method for 6G/NextG networks using Mixture of Experts (MoE) with CNN layers, achieving high accuracy (99.95% F1-score).


<details>
  <summary>Details</summary>
Motivation: 6G/NextG networks face new security threats, and existing intrusion detection methods (shallow ML or static deep neural networks) are suboptimal.

Method: Convert 1D network traffic data to 2D matrix, process with CNN layers, and use a sparsely gated MoE layer for dynamic expert selection.

Result: Achieves 99.95% weighted F1-score on the 5G-NIDD dataset, outperforming existing methods.

Conclusion: The proposed MoE-based model is effective for intrusion detection in 6G/NextG networks, offering superior performance and flexibility.

Abstract: The advent of 6G/NextG networks comes along with a series of benefits,
including extreme capacity, reliability, and efficiency. However, these
networks may become vulnerable to new security threats. Therefore, 6G/NextG
networks must be equipped with advanced Artificial Intelligence algorithms, in
order to evade these attacks. Existing studies on the intrusion detection task
rely on the train of shallow machine learning classifiers, including Logistic
Regression, Decision Trees, and so on, yielding suboptimal performance. Others
are based on deep neural networks consisting of static components, which are
not conditional on the input. This limits their representation power and
efficiency. To resolve these issues, we present the first study integrating
Mixture of Experts (MoE) for identifying malicious traffic. Specifically, we
use network traffic data and convert the 1D array of features into a 2D matrix.
Next, we pass this matrix through convolutional neural network (CNN) layers
followed by batch normalization and max pooling layers. After obtaining the
representation vector via the CNN layers, a sparsely gated MoE layer is used.
This layer consists of a set of experts (dense layers) and a router, where the
router assigns weights to the output of each expert. Sparsity is achieved by
choosing the most relevant experts of the total ones. Finally, we perform a
series of ablation experiments to prove the effectiveness of our proposed
model. Experiments are conducted on the 5G-NIDD dataset, a network intrusion
detection dataset generated from a real 5G test network. Results show that our
introduced approach reaches weighted F1-score up to 99.95% achieving comparable
performance to existing approaches. Findings also show that our proposed model
achieves multiple advantages over state-of-the-art approaches.

</details>


### [328] [Rehearsal-Free Continual Federated Learning with Synergistic Synaptic Intelligence](https://arxiv.org/pdf/2412.13779)
*Yichen Li, Yuying Wang, Haozhao Wang, Yining Qi, Tianzhe Xiao, Ruixuan Li*

Main category: cs.LG

TL;DR: FedSSI is a regularization algorithm for Continual Federated Learning (CFL) that addresses data heterogeneity and avoids rehearsal, outperforming state-of-the-art methods.


<details>
  <summary>Details</summary>
Motivation: Current CFL approaches rely on rehearsal, which is memory-intensive and may violate privacy. The goal is to use cost-efficient regularization techniques instead.

Method: The paper applies traditional regularization to CFL, identifies limitations, and proposes FedSSI, a tailored version of synaptic intelligence for heterogeneous data.

Result: FedSSI reduces computational overhead and handles data heterogeneity effectively, achieving superior performance in experiments.

Conclusion: FedSSI is a simple yet effective solution for CFL, offering privacy and efficiency advantages over rehearsal-based methods.

Abstract: Continual Federated Learning (CFL) allows distributed devices to
collaboratively learn novel concepts from continuously shifting training data
while avoiding knowledge forgetting of previously seen tasks. To tackle this
challenge, most current CFL approaches rely on extensive rehearsal of previous
data. Despite effectiveness, rehearsal comes at a cost to memory, and it may
also violate data privacy. Considering these, we seek to apply regularization
techniques to CFL by considering their cost-efficient properties that do not
require sample caching or rehearsal. Specifically, we first apply traditional
regularization techniques to CFL and observe that existing regularization
techniques, especially synaptic intelligence, can achieve promising results
under homogeneous data distribution but fail when the data is heterogeneous.
Based on this observation, we propose a simple yet effective regularization
algorithm for CFL named FedSSI, which tailors the synaptic intelligence for the
CFL with heterogeneous data settings. FedSSI can not only reduce computational
overhead without rehearsal but also address the data heterogeneity issue.
Extensive experiments show that FedSSI achieves superior performance compared
to state-of-the-art methods.

</details>


### [329] [Multi-Objective Optimization-Based Anonymization of Structured Data for Machine Learning Application](https://arxiv.org/pdf/2501.01002)
*Yusi Wei, Hande Y. Benson, Joseph K. Agor, Muge Capan*

Main category: cs.LG

TL;DR: A novel multi-objective optimization model balances privacy and utility in data sharing, outperforming existing methods in reducing information loss and attack risks while maintaining ML performance.


<details>
  <summary>Details</summary>
Motivation: Organizations share data with external experts but face privacy risks. Existing methods degrade data utility, impacting ML models.

Method: Proposes a multi-objective optimization model minimizing information loss and maximizing protection, validated empirically.

Result: Lower information loss, reduced attack risks, and comparable ML performance compared to alternatives.

Conclusion: The model improves privacy protection and utility, offering a scalable framework for data sharing.

Abstract: Organizations are collecting vast amounts of data, but they often lack the
capabilities needed to fully extract insights. As a result, they increasingly
share data with external experts, such as analysts or researchers, to gain
value from it. However, this practice introduces significant privacy risks.
Various techniques have been proposed to address privacy concerns in data
sharing. However, these methods often degrade data utility, impacting the
performance of machine learning (ML) models. Our research identifies key
limitations in existing optimization models for privacy preservation,
particularly in handling categorical variables, and evaluating effectiveness
across diverse datasets. We propose a novel multi-objective optimization model
that simultaneously minimizes information loss and maximizes protection against
attacks. This model is empirically validated using diverse datasets and
compared with two existing algorithms. We assess information loss, the number
of individuals subject to linkage or homogeneity attacks, and ML performance
after anonymization. The results indicate that our model achieves lower
information loss and more effectively mitigates the risk of attacks, reducing
the number of individuals susceptible to these attacks compared to alternative
algorithms in some cases. Additionally, our model maintains comparable ML
performance relative to the original data or data anonymized by other methods.
Our findings highlight significant improvements in privacy protection and ML
model performance, offering a comprehensive and extensible framework for
balancing privacy and utility in data sharing.

</details>


### [330] [Multi-Objective Hyperparameter Selection via Hypothesis Testing on Reliability Graphs](https://arxiv.org/pdf/2501.13018)
*Amirmohammad Farzaneh, Osvaldo Simeone*

Main category: cs.LG

TL;DR: RG-PT is a new hyperparameter selection method for LLMs that balances reliability and cost while incorporating structured knowledge via a graph, outperforming existing methods like LTT and PT.


<details>
  <summary>Details</summary>
Motivation: Existing hyperparameter selection methods lack formal reliability guarantees or fail to use structured knowledge in the hyperparameter space.

Method: Introduces RG-PT, a framework using a directed acyclic graph to model hyperparameter relationships and the Bradley-Terry model for ranking.

Result: RG-PT outperforms LTT and PT by efficiently exploring the hyperparameter space with formal FDR guarantees.

Conclusion: RG-PT effectively combines reliability guarantees and structured knowledge for better hyperparameter selection in LLMs.

Abstract: The selection of hyperparameters, such as prompt templates in large language
models (LLMs), must often strike a balance between reliability and cost. In
many cases, structural relationships between the expected reliability levels of
the hyperparameters can be inferred from prior information and held-out data --
e.g., longer prompt templates may be more detailed and thus more reliable.
However, existing hyperparameter selection methods either do not provide formal
reliability guarantees or are unable to incorporate structured knowledge in the
hyperparameter space. This paper introduces reliability graph-based Pareto
testing (RG-PT), a novel multi-objective hyperparameter selection framework
that maintains formal reliability guarantees in terms of false discovery rate
(FDR), while accounting for known relationships among hyperparameters via a
directed acyclic graph. Edges in the graph reflect expected reliability and
cost trade-offs among hyperparameters, which are inferred via the Bradley-Terry
(BT) ranking model from prior information and held-out data. Experimental
evaluations demonstrate that RG-PT significantly outperforms existing methods
such as learn-then-test (LTT) and Pareto testing (PT) through a more efficient
exploration of the hyperparameter space.

</details>


### [331] [From Uncertain to Safe: Conformal Fine-Tuning of Diffusion Models for Safe PDE Control](https://arxiv.org/pdf/2502.02205)
*Peiyan Hu, Xiaowei Qian, Wenhao Deng, Rui Wang, Haodong Feng, Ruiqi Feng, Tao Zhang, Long Wei, Yue Wang, Zhi-Ming Ma, Tailin Wu*

Main category: cs.LG

TL;DR: SafeDiffCon introduces uncertainty quantile for safe PDE control, outperforming baselines in safety and performance.


<details>
  <summary>Details</summary>
Motivation: Addressing the lack of safety considerations in existing deep learning methods for PDE-constrained control.

Method: Post-trains a diffusion model with reweighted loss and dynamic adjustment during inference using uncertainty quantile.

Result: SafeDiffCon satisfies all safety constraints and achieves best control performance in tested tasks.

Conclusion: SafeDiffCon effectively integrates safety into PDE control, setting a new benchmark for safe deep learning applications.

Abstract: The application of deep learning for partial differential equation
(PDE)-constrained control is gaining increasing attention. However, existing
methods rarely consider safety requirements crucial in real-world applications.
To address this limitation, we propose Safe Diffusion Models for PDE Control
(SafeDiffCon), which introduce the uncertainty quantile as model uncertainty
quantification to achieve optimal control under safety constraints through both
post-training and inference phases. Firstly, our approach post-trains a
pre-trained diffusion model to generate control sequences that better satisfy
safety constraints while achieving improved control objectives via a reweighted
diffusion loss, which incorporates the uncertainty quantile estimated using
conformal prediction. Secondly, during inference, the diffusion model
dynamically adjusts both its generation process and parameters through
iterative guidance and fine-tuning, conditioned on control targets while
simultaneously integrating the estimated uncertainty quantile. We evaluate
SafeDiffCon on three control tasks: 1D Burgers' equation, 2D incompressible
fluid, and controlled nuclear fusion problem. Results demonstrate that
SafeDiffCon is the only method that satisfies all safety constraints, whereas
other classical and deep learning baselines fail. Furthermore, while adhering
to safety constraints, SafeDiffCon achieves the best control performance.

</details>


### [332] [Self-cross Feature based Spiking Neural Networks for Efficient Few-shot Learning](https://arxiv.org/pdf/2505.07921)
*Qi Xu, Junyang Zhu, Dongdong Zhou, Hao Chen, Yang Liu, Jiangrong Shen, Qiang Zhang*

Main category: cs.LG

TL;DR: A few-shot learning framework using Spiking Neural Networks (SNNs) improves performance and efficiency by combining self-feature extraction and cross-feature contrastive modules, achieving competitive results with low power consumption.


<details>
  <summary>Details</summary>
Motivation: DNNs are computationally expensive for few-shot learning (FSL), while SNNs offer energy efficiency but struggle with complex spatiotemporal features and cross-class comparisons.

Method: Proposes an SNN-based FSL framework with self-feature extractor and cross-feature contrastive modules, optimized using temporal efficient training loss and InfoNCE loss.

Result: Improved classification on N-Omniglot and competitive performance on CUB and miniImageNet with low power consumption.

Conclusion: The FSL-SNN framework enhances SNN performance in FSL, balancing accuracy and energy efficiency.

Abstract: Deep neural networks (DNNs) excel in computer vision tasks, especially,
few-shot learning (FSL), which is increasingly important for generalizing from
limited examples. However, DNNs are computationally expensive with scalability
issues in real world. Spiking Neural Networks (SNNs), with their event-driven
nature and low energy consumption, are particularly efficient in processing
sparse and dynamic data, though they still encounter difficulties in capturing
complex spatiotemporal features and performing accurate cross-class
comparisons. To further enhance the performance and efficiency of SNNs in
few-shot learning, we propose a few-shot learning framework based on SNNs,
which combines a self-feature extractor module and a cross-feature contrastive
module to refine feature representation and reduce power consumption. We apply
the combination of temporal efficient training loss and InfoNCE loss to
optimize the temporal dynamics of spike trains and enhance the discriminative
power. Experimental results show that the proposed FSL-SNN significantly
improves the classification performance on the neuromorphic dataset N-Omniglot,
and also achieves competitive performance to ANNs on static datasets such as
CUB and miniImageNet with low power consumption.

</details>


### [333] [Mechanisms of Projective Composition of Diffusion Models](https://arxiv.org/pdf/2502.04549)
*Arwen Bradley, Preetum Nakkiran, David Berthelot, James Thornton, Joshua M. Susskind*

Main category: cs.LG

TL;DR: The paper explores the theoretical foundations of composition in diffusion models, focusing on out-of-distribution extrapolation and length-generalization. It defines projective composition, analyzes when linear score combinations achieve it, and identifies conditions for success or failure.


<details>
  <summary>Details</summary>
Motivation: To address gaps in understanding how and why composition works in diffusion models, particularly for out-of-distribution tasks and length-generalization.

Method: Defines projective composition, investigates linear score combinations, reverse-diffusion sampling, and conditions for failure. Connects theory to prior empirical observations.

Result: Provides theoretical insights into when composition succeeds or fails and proposes a heuristic for predicting outcomes.

Conclusion: The study advances understanding of composition in diffusion models and offers practical guidance for future applications.

Abstract: We study the theoretical foundations of composition in diffusion models, with
a particular focus on out-of-distribution extrapolation and
length-generalization. Prior work has shown that composing distributions via
linear score combination can achieve promising results, including
length-generalization in some cases (Du et al., 2023; Liu et al., 2022).
However, our theoretical understanding of how and why such compositions work
remains incomplete. In fact, it is not even entirely clear what it means for
composition to "work". This paper starts to address these fundamental gaps. We
begin by precisely defining one possible desired result of composition, which
we call projective composition. Then, we investigate: (1) when linear score
combinations provably achieve projective composition, (2) whether
reverse-diffusion sampling can generate the desired composition, and (3) the
conditions under which composition fails. We connect our theoretical analysis
to prior empirical observations where composition has either worked or failed,
for reasons that were unclear at the time. Finally, we propose a simple
heuristic to help predict the success or failure of new compositions.

</details>


### [334] [Aggregating Concepts of Accuracy and Fairness in Prediction Algorithms](https://arxiv.org/pdf/2505.08829)
*David Kinney*

Main category: cs.LG

TL;DR: The paper proposes using a linear combination of accuracy and fairness metrics to evaluate predictive algorithms, addressing tensions between these goals and leveraging Harsanyi's preference aggregation theory.


<details>
  <summary>Details</summary>
Motivation: The rise of AI-driven predictive algorithms highlights the need to balance accuracy and fairness, but lacks clear guidelines for managing trade-offs between them.

Method: The author argues for a linear combination of accuracy and fairness metrics, supported by Harsanyi's preference aggregation theory, and applies this to the COMPAS dataset.

Result: The approach provides a formal framework for evaluating predictive algorithms when both accuracy and fairness are valued.

Conclusion: A linear combination of metrics offers a principled way to assess predictive algorithms, addressing the challenges of balancing accuracy and fairness.

Abstract: An algorithm that outputs predictions about the state of the world will
almost always be designed with the implicit or explicit goal of outputting
accurate predictions (i.e., predictions that are likely to be true). In
addition, the rise of increasingly powerful predictive algorithms brought about
by the recent revolution in artificial intelligence has led to an emphasis on
building predictive algorithms that are fair, in the sense that their
predictions do not systematically evince bias or bring about harm to certain
individuals or groups. This state of affairs presents two conceptual
challenges. First, the goals of accuracy and fairness can sometimes be in
tension, and there are no obvious normative guidelines for managing the
trade-offs between these two desiderata when they arise. Second, there are many
distinct ways of measuring both the accuracy and fairness of a predictive
algorithm; here too, there are no obvious guidelines on how to aggregate our
preferences for predictive algorithms that satisfy disparate measures of
fairness and accuracy to various extents. The goal of this paper is to address
these challenges by arguing that there are good reasons for using a linear
combination of accuracy and fairness metrics to measure the
all-things-considered value of a predictive algorithm for agents who care about
both accuracy and fairness. My argument depends crucially on a classic result
in the preference aggregation literature due to Harsanyi. After making this
formal argument, I apply my result to an analysis of accuracy-fairness
trade-offs using the COMPAS dataset compiled by Angwin et al.

</details>


### [335] [Graph Neural Network-based Spectral Filtering Mechanism for Imbalance Classification in Network Digital Twin](https://arxiv.org/pdf/2502.11505)
*Abubakar Isah, Ibrahim Aliyu, Sulaiman Muhammad Rashid, Jaehyung Park, Minsoo Hahn, Jinsul Kim*

Main category: cs.LG

TL;DR: The paper proposes CF-GNN, a graph neural network with class-oriented spectral filtering, to address imbalanced classification in 5G network digital twins.


<details>
  <summary>Details</summary>
Motivation: The challenge of imbalanced classification in multiclass settings for 5G core network digital twins, where rare failure types hinder practical graph data mining.

Method: Introduces CF-GNN, which uses eigenvalue and eigenvector spectral filtering to create class-specific filters for precise classification.

Result: CF-GNN effectively captures minority class variations and improves graph representation learning, as demonstrated in experiments.

Conclusion: CF-GNN offers a novel approach to handling multiclass imbalanced data in network digital twins, enhancing classifier performance.

Abstract: Graph neural networks are gaining attention in fifth-generation (5G) core
network digital twins, which are data-driven complex systems with numerous
components. Analyzing these data can be challenging due to rare failure types,
leading to imbalanced classification in multiclass settings. Digital twins of
5G networks increasingly employ graph classification as the main method for
identifying failure types. However, the skewed distribution of failure
occurrences is a significant class-imbalance problem that prevents practical
graph data mining. Previous studies have not sufficiently addressed this
complex problem. This paper, proposes class-Fourier GNN (CF-GNN) that
introduces a class-oriented spectral filtering mechanism to ensure precise
classification by estimating a unique spectral filter for each class. This work
employs eigenvalue and eigenvector spectral filtering to capture and adapt to
variations in minority classes, ensuring accurate class-specific feature
discrimination, and adept at graph representation learning for complex local
structures among neighbors in an end-to-end setting. The extensive experiments
demonstrate that the proposed CF-GNN could help create new techniques for
enhancing classifiers and investigate the characteristics of the multiclass
imbalanced data in a network digital twin system.

</details>


### [336] [R2VF: A Two-Step Regularization Algorithm to Cluster Categories in GLMs](https://arxiv.org/pdf/2503.01521)
*Yuval Ben Dror*

Main category: cs.LG

TL;DR: R2VF is a two-step method for efficiently clustering nominal and ordinal categories in GLMs, balancing complexity and interpretability.


<details>
  <summary>Details</summary>
Motivation: Overcoming the challenge of clustering nominal categories in GLMs without high computational costs.

Method: Ranking to Variable Fusion (R2VF): transforms nominal features into ordinal via regularized regression, then applies variable fusion.

Result: R2VF outperforms other methods in addressing overfitting and selecting covariates.

Conclusion: R2VF effectively balances model complexity and interpretability in GLMs.

Abstract: Over recent decades, extensive research has aimed to overcome the restrictive
underlying assumptions required for a Generalized Linear Model to generate
accurate and meaningful predictions. These efforts include regularizing
coefficients, selecting features, and clustering ordinal categories, among
other approaches. Despite these advances, efficiently clustering nominal
categories in GLMs without incurring high computational costs remains a
challenge. This paper introduces Ranking to Variable Fusion (R2VF), a two-step
method designed to efficiently fuse nominal and ordinal categories in GLMs. By
first transforming nominal features into an ordinal framework via regularized
regression and then applying variable fusion, R2VF strikes a balance between
model complexity and interpretability. We demonstrate the effectiveness of R2VF
through comparisons with other methods, highlighting its performance in
addressing overfitting and identifying an appropriate set of covariates.

</details>


### [337] [Heterogeneous graph neural networks for species distribution modeling](https://arxiv.org/pdf/2503.11900)
*Lauren Harrell, Christine Kaeser-Chen, Burcu Karagol Ayan, Keith Anderson, Michelangelo Conserva, Elise Kleeman, Maxim Neumann, Matt Overlan, Melissa Chapman, Drew Purves*

Main category: cs.LG

TL;DR: A novel presence-only SDM using GNNs treats species and locations as node sets, predicting detection records as edges. It outperforms traditional SDMs and neural networks.


<details>
  <summary>Details</summary>
Motivation: To improve species distribution modeling by capturing fine-grained interactions between species and environmental factors using GNNs.

Method: Uses a heterogeneous GNN model with species and locations as distinct node sets, predicting detection records as edges. Evaluated on the NCEAS six-region dataset.

Result: The GNN model matches or outperforms traditional single-species SDMs and a neural network baseline across all regions.

Conclusion: GNNs offer a promising approach for SDMs by effectively modeling species-environment interactions.

Abstract: Species distribution models (SDMs) are necessary for measuring and predicting
occurrences and habitat suitability of species and their relationship with
environmental factors. We introduce a novel presence-only SDM with graph neural
networks (GNN). In our model, species and locations are treated as two distinct
node sets, and the learning task is predicting detection records as the edges
that connect locations to species. Using GNN for SDM allows us to model
fine-grained interactions between species and the environment. We evaluate the
potential of this methodology on the six-region dataset compiled by National
Center for Ecological Analysis and Synthesis (NCEAS) for benchmarking SDMs. For
each of the regions, the heterogeneous GNN model is comparable to or
outperforms previously-benchmarked single-species SDMs as well as a
feed-forward neural network baseline model.

</details>


### [338] [Malliavin Calculus for Score-based Diffusion Models](https://arxiv.org/pdf/2503.16917)
*Ehsan Mirafzali, Utkarsh Gupta, Patrick Wyrod, Frank Proske, Daniele Venturi, Razvan Marinescu*

Main category: cs.LG

TL;DR: A new framework using Malliavin calculus to derive exact analytical expressions for the score function in SDEs, applicable to both linear and nonlinear cases, with performance comparable to state-of-the-art methods.


<details>
  <summary>Details</summary>
Motivation: To establish a rigorous connection between Malliavin calculus and diffusion generative models, enabling systematic computation of the score function for SDEs.

Method: Combines integration-by-parts techniques with stochastic analysis tools (Bismut's formula, Malliavin calculus) for linear and nonlinear SDEs.

Result: Derived closed-form expressions for the score function, validated in generative tasks with performance matching state-of-the-art methods.

Conclusion: The framework generalizes to broader SDE classes, enabling new score-based diffusion generative models.

Abstract: We introduce a new framework based on Malliavin calculus to derive exact
analytical expressions for the score function $\nabla \log p_t(x)$, i.e., the
gradient of the log-density associated with the solution to stochastic
differential equations (SDEs). Our approach combines classical
integration-by-parts techniques with modern stochastic analysis tools, such as
Bismut's formula and Malliavin calculus, and it works for both linear and
nonlinear SDEs. In doing so, we establish a rigorous connection between the
Malliavin derivative, its adjoint, the Malliavin divergence (Skorokhod
integral), and diffusion generative models, thereby providing a systematic
method for computing $\nabla \log p_t(x)$. In the linear case, we present a
detailed analysis showing that our formula coincides with the analytical score
function derived from the solution of the Fokker--Planck equation. For
nonlinear SDEs with state-independent diffusion coefficients, we derive a
closed-form expression for $\nabla \log p_t(x)$. We evaluate the proposed
framework across multiple generative tasks and find that its performance is
comparable to state-of-the-art methods. These results can be generalised to
broader classes of SDEs, paving the way for new score-based diffusion
generative models.

</details>


### [339] [Unitless Unrestricted Markov-Consistent SCM Generation: Better Benchmark Datasets for Causal Discovery](https://arxiv.org/pdf/2503.17037)
*Rebecca J. Herman, Jonas Wahl, Urmi Ninad, Jakob Runge*

Main category: cs.LG

TL;DR: The paper discusses artifacts in causal discovery data generation, proposes a method to avoid them, and extends it to time series.


<details>
  <summary>Details</summary>
Motivation: Addressing unrealistic artifacts in simulated data used for evaluating causal discovery algorithms.

Method: Proposes a method for drawing coefficients to better sample SCMs and extends it to time series.

Result: Identifies and mitigates var- and R2-sortability artifacts, improving realism in evaluations.

Conclusion: The proposed method enhances the reliability of causal discovery evaluations, especially for real-world applications.

Abstract: Causal discovery aims to extract qualitative causal knowledge in the form of
causal graphs from data. Because causal ground truth is rarely known in the
real world, simulated data plays a vital role in evaluating the performance of
the various causal discovery algorithms proposed in the literature. But recent
work highlighted certain artifacts of commonly used data generation techniques
for a standard class of structural causal models (SCM) that may be nonphysical,
including var- and R2-sortability, where the variables' variance and
coefficients of determination (R2) after regressing on all other variables,
respectively, increase along the causal order. Some causal methods exploit such
artifacts, leading to unrealistic expectations for their performance on
real-world data. Some modifications have been proposed to remove these
artifacts; notably, the internally-standardized structural causal model (iSCM)
avoids varsortability and largely alleviates R2-sortability on sparse causal
graphs, but exhibits a reversed R2-sortability pattern for denser graphs not
featured in their work. We analyze which sortability patterns we expect to see
in real data, and propose a method for drawing coefficients that we argue more
effectively samples the space of SCMs. Finally, we propose a novel extension of
our SCM generation method to the time series setting.

</details>


### [340] [System Log Parsing with Large Language Models: A Review](https://arxiv.org/pdf/2504.04877)
*Viktor Beck, Max Landauer, Markus Wurzenberger, Florian Skopik, Andreas Rauber*

Main category: cs.LG

TL;DR: This paper reviews 29 LLM-based log parsing methods, benchmarks 7 on public datasets, and provides insights on reporting, datasets, metrics, and terminology for this emerging field.


<details>
  <summary>Details</summary>
Motivation: The need for automated log parsing due to vast log volumes and the emergence of LLM-based approaches, which lack a structured overview.

Method: Systematic review of 29 LLM-based log parsing methods and benchmarking of 7 on public datasets.

Result: Findings highlight advances, inconsistencies, and best practices for reporting results, datasets, and metrics.

Conclusion: The study offers guidance for future research in LLM-based log parsing, promoting transparency and reproducibility.

Abstract: Log data provides crucial insights for tasks like monitoring, root cause
analysis, and anomaly detection. Due to the vast volume of logs, automated log
parsing is essential to transform semi-structured log messages into structured
representations. Recent advances in large language models (LLMs) have
introduced the new research field of LLM-based log parsing. Despite promising
results, there is no structured overview of the approaches in this relatively
new research field with the earliest advances published in late 2023. This work
systematically reviews 29 LLM-based log parsing methods. We benchmark seven of
them on public datasets and critically assess their comparability and the
reproducibility of their reported results. Our findings summarize the advances
of this new research field, with insights on how to report results, which data
sets, metrics and which terminology to use, and which inconsistencies to avoid,
with code and results made publicly available for transparency.

</details>


### [341] [Towards More Efficient, Robust, Instance-adaptive, and Generalizable Sequential Decision making](https://arxiv.org/pdf/2504.09192)
*Zhiyong Wang*

Main category: cs.LG

TL;DR: The paper aims to develop efficient, robust, and adaptive algorithms for sequential decision-making in RL and bandits, addressing real-world challenges like model misspecifications and adversarial perturbations.


<details>
  <summary>Details</summary>
Motivation: Existing algorithms often rely on idealized models, failing in real-world scenarios with uncertainty or adversarial conditions. The study seeks to improve robustness, adaptability, and generalization.

Method: Focuses on developing provably efficient and practical algorithms for RL and bandits, emphasizing instance-adaptive and generalizable solutions.

Result: Expected outcomes include more reliable and efficient algorithms for applications like recommendation systems, LLMs, and dynamic environments.

Conclusion: The research aims to bridge the gap between theoretical guarantees and practical performance, enhancing the applicability of sequential decision-making methods in real-world settings.

Abstract: The primary goal of my Ph.D. study is to develop provably efficient and
practical algorithms for data-driven sequential decision-making under
uncertainty. My work focuses on reinforcement learning (RL), multi-armed
bandits, and their applications, including recommendation systems, computer
networks, video analytics, and large language models (LLMs). Sequential
decision-making methods, such as bandits and RL, have demonstrated remarkable
success - ranging from outperforming human players in complex games like Atari
and Go to advancing robotics, recommendation systems, and fine-tuning LLMs.
Despite these successes, many established algorithms rely on idealized models
that can fail under model misspecifications or adversarial perturbations,
particularly in settings where accurate prior knowledge of the underlying model
class is unavailable or where malicious users operate within dynamic systems.
These challenges are pervasive in real-world applications, where robust and
adaptive solutions are critical. Furthermore, while worst-case guarantees
provide theoretical reliability, they often fail to capture instance-dependent
performance, which can lead to more efficient and practical solutions. Another
key challenge lies in generalizing to new, unseen environments, a crucial
requirement for deploying these methods in dynamic and unpredictable settings.
To address these limitations, my research aims to develop more efficient,
robust, instance-adaptive, and generalizable sequential decision-making
algorithms for both reinforcement learning and bandits. Towards this end, I
focus on developing more efficient, robust, instance-adaptive, and
generalizable for both general reinforcement learning (RL) and bandits.

</details>


### [342] [Why Ask One When You Can Ask $k$? Two-Stage Learning-to-Defer to the Top-$k$ Experts](https://arxiv.org/pdf/2504.12988)
*Yannis Montreuil, Axel Carlier, Lai Xing Ng, Wei Tsang Ooi*

Main category: cs.LG

TL;DR: The paper introduces Top-$k$ Learning-to-Defer (L2D), a framework allowing deferral to multiple experts, improving decision-making by leveraging collective expertise. It also proposes an adaptive variant, Top-$k(x)$, optimizing the number of experts per query.


<details>
  <summary>Details</summary>
Motivation: Existing L2D frameworks limit deferral to a single expert, missing opportunities to utilize collective expertise in complex scenarios.

Method: The paper introduces Top-$k$ L2D for deferring to $k$ experts and Top-$k(x)$ for adaptive deferral. A novel surrogate loss ensures consistency and efficiency.

Result: Experiments show improved accuracy-cost trade-offs, validating the framework's effectiveness.

Conclusion: The work generalizes classical L2D and cascaded inference, offering a new direction for multi-expert deferral.

Abstract: Although existing Learning-to-Defer (L2D) frameworks support multiple
experts, they allocate each query to a single expert, limiting their ability to
leverage collective expertise in complex decision-making scenarios. To address
this, we introduce the first framework for Top-$k$ Learning-to-Defer, enabling
systems to defer each query to the $k$ most cost-effective experts. Our
formulation strictly generalizes classical two-stage L2D by supporting
multi-expert deferral-a capability absent in prior work. We further propose
Top-$k(x)$ Learning-to-Defer, an adaptive extension that learns the optimal
number of experts per query based on input complexity, expert quality, and
consultation cost. We introduce a novel surrogate loss that is
Bayes-consistent, $(\mathcal{R}, \mathcal{G})$-consistent, and independent of
the cardinality parameter $k$, enabling efficient reuse across different values
of $k$. We show that classical model cascades arise as a special case of our
method, situating our framework as a strict generalization of both selective
deferral and cascaded inference. Experiments on classification and regression
demonstrate that Top-$k$ and Top-$k(x)$ yield improved accuracy--cost
trade-offs, establishing a new direction for multi-expert deferral in
Learning-to-Defer.

</details>


### [343] [TGDT: A Temporal Graph-based Digital Twin for Urban Traffic Corridors](https://arxiv.org/pdf/2504.18008)
*Nooshin Yousefzadeh, Rahul Sengupta, Jeremy Dilmore, Sanjay Ranka*

Main category: cs.LG

TL;DR: TGDT is a scalable framework combining Temporal Convolutional Networks and Attentional Graph Neural Networks for real-time, direction-aware traffic modeling, outperforming existing methods in accuracy and robustness.


<details>
  <summary>Details</summary>
Motivation: Urban congestion causes delays, economic losses, and emissions, while current deep learning models lack spatial generalizability and real-time deployment capabilities.

Method: TGDT integrates Temporal Convolutional Networks and Attentional Graph Neural Networks for dynamic traffic modeling, estimating key traffic measures at intersection and corridor levels.

Result: TGDT outperforms state-of-the-art baselines, providing high-dimensional, concurrent multi-output estimates with high robustness and accuracy across diverse conditions.

Conclusion: TGDT offers a cost-effective, interpretable, and real-time solution for urban traffic management, capable of simulating thousands of scenarios quickly.

Abstract: Urban congestion at signalized intersections leads to significant delays,
economic losses, and increased emissions. Existing deep learning models often
lack spatial generalizability, rely on complex architectures, and struggle with
real-time deployment. To address these limitations, we propose the Temporal
Graph-based Digital Twin (TGDT), a scalable framework that integrates Temporal
Convolutional Networks and Attentional Graph Neural Networks for dynamic,
direction-aware traffic modeling and assessment at urban corridors. TGDT
estimates key Measures of Effectiveness (MOEs) for traffic flow optimization at
both the intersection level (e.g., queue length, waiting time) and the corridor
level (e.g., traffic volume, travel time). Its modular architecture and
sequential optimization scheme enable easy extension to any number of
intersections and MOEs. The model outperforms state-of-the-art baselines by
accurately producing high-dimensional, concurrent multi-output estimates. It
also demonstrates high robustness and accuracy across diverse traffic
conditions, including extreme scenarios, while relying on only a minimal set of
traffic features. Fully parallelized, TGDT can simulate over a thousand
scenarios within a matter of seconds, offering a cost-effective, interpretable,
and real-time solution for urban traffic management and optimization.

</details>


### [344] [ABKD: Pursuing a Proper Allocation of the Probability Mass in Knowledge Distillation via $α$-$β$-Divergence](https://arxiv.org/pdf/2505.04560)
*Guanghui Wang, Zhiyong Yang, Zitai Wang, Shi Wang, Qianqian Xu, Qingming Huang*

Main category: cs.LG

TL;DR: ABKD introduces α-β-divergence to balance Hardness-Concentration and Confidence-Concentration effects in Knowledge Distillation, outperforming FKLD and RKLD.


<details>
  <summary>Details</summary>
Motivation: The core challenge in KD is balancing Hardness-Concentration and Confidence-Concentration effects, which are entangled in FKLD and RKLD.

Method: Proposes ABKD, a framework using α-β-divergence to interpolate between FKLD and RKLD, balancing the two effects.

Result: ABKD achieves a trade-off between the effects, validated on 17 datasets with 12 teacher-student settings.

Conclusion: ABKD effectively addresses the imbalance in KD, offering a superior alternative to FKLD and RKLD.

Abstract: Knowledge Distillation (KD) transfers knowledge from a large teacher model to
a smaller student model by minimizing the divergence between their output
distributions, typically using forward Kullback-Leibler divergence (FKLD) or
reverse KLD (RKLD). It has become an effective training paradigm due to the
broader supervision information provided by the teacher distribution compared
to one-hot labels. We identify that the core challenge in KD lies in balancing
two mode-concentration effects: the \textbf{\textit{Hardness-Concentration}}
effect, which refers to focusing on modes with large errors, and the
\textbf{\textit{Confidence-Concentration}} effect, which refers to focusing on
modes with high student confidence. Through an analysis of how probabilities
are reassigned during gradient updates, we observe that these two effects are
entangled in FKLD and RKLD, but in extreme forms. Specifically, both are too
weak in FKLD, causing the student to fail to concentrate on the target class.
In contrast, both are too strong in RKLD, causing the student to overly
emphasize the target class while ignoring the broader distributional
information from the teacher. To address this imbalance, we propose ABKD, a
generic framework with $\alpha$-$\beta$-divergence. Our theoretical results
show that ABKD offers a smooth interpolation between FKLD and RKLD, achieving
an effective trade-off between these effects. Extensive experiments on 17
language/vision datasets with 12 teacher-student settings confirm its efficacy.
The code is available at https://github.com/ghwang-s/abkd.

</details>


### [345] [Autoencoder-Based Hybrid Replay for Class-Incremental Learning](https://arxiv.org/pdf/2505.05926)
*Milad Khademi Nori, Il-Min Kim, Guanghui Wang*

Main category: cs.LG

TL;DR: Proposes an autoencoder-based hybrid replay (AHR) strategy for class-incremental learning, reducing memory complexity to O(0.1t) while maintaining state-of-the-art performance.


<details>
  <summary>Details</summary>
Motivation: Addresses the challenges of task confusion and catastrophic forgetting in class-incremental learning, aiming to reduce memory and compute complexities.

Method: Uses a hybrid autoencoder (HAE) as a compressor for exemplar replay, employing energy minimization and repulsive force algorithms for incremental embedding.

Result: AHR outperforms recent baselines with reduced memory usage and maintains high performance.

Conclusion: The proposed AHR strategy effectively balances memory efficiency and performance in class-incremental learning.

Abstract: In class-incremental learning (CIL), effective incremental learning
strategies are essential to mitigate task confusion and catastrophic
forgetting, especially as the number of tasks $t$ increases. Current exemplar
replay strategies impose $\mathcal{O}(t)$ memory/compute complexities. We
propose an autoencoder-based hybrid replay (AHR) strategy that leverages our
new hybrid autoencoder (HAE) to function as a compressor to alleviate the
requirement for large memory, achieving $\mathcal{O}(0.1 t)$ at the worst case
with the computing complexity of $\mathcal{O}(t)$ while accomplishing
state-of-the-art performance. The decoder later recovers the exemplar data
stored in the latent space, rather than in raw format. Additionally, HAE is
designed for both discriminative and generative modeling, enabling
classification and replay capabilities, respectively. HAE adopts the charged
particle system energy minimization equations and repulsive force algorithm for
the incremental embedding and distribution of new class centroids in its latent
space. Our results demonstrate that AHR consistently outperforms recent
baselines across multiple benchmarks while operating with the same
memory/compute budgets. The source code is included in the supplementary
material and will be open-sourced upon publication.

</details>


### [346] [A systematic review of challenges and proposed solutions in modeling multimodal data](https://arxiv.org/pdf/2505.06945)
*Maryam Farhadizadeh, Maria Weymann, Michael Blaß, Johann Kraus, Christopher Gundler, Sebastian Walter, Noah Hempen, Harald Binder, Nadine Binder*

Main category: cs.LG

TL;DR: A systematic review of 69 studies on multimodal data modeling in clinical research, identifying challenges and highlighting recent methodological advances to improve diagnostic accuracy and personalized care.


<details>
  <summary>Details</summary>
Motivation: To address the technical challenges of integrating diverse data types (e.g., imaging, genomics, wearable sensors) in clinical research and improve diagnostic and personalized care outcomes.

Method: Systematic review of 69 studies to identify common obstacles (e.g., missing modalities, dimensionality imbalance) and evaluate recent methodological advances like transfer learning and generative models.

Result: Identified key challenges and promising solutions, such as attention mechanisms and neural architecture search, for multimodal data modeling in medical applications.

Conclusion: The review provides a comprehensive overview and practical insights to guide future research in multimodal modeling for healthcare.

Abstract: Multimodal data modeling has emerged as a powerful approach in clinical
research, enabling the integration of diverse data types such as imaging,
genomics, wearable sensors, and electronic health records. Despite its
potential to improve diagnostic accuracy and support personalized care,
modeling such heterogeneous data presents significant technical challenges.
This systematic review synthesizes findings from 69 studies to identify common
obstacles, including missing modalities, limited sample sizes, dimensionality
imbalance, interpretability issues, and finding the optimal fusion techniques.
We highlight recent methodological advances, such as transfer learning,
generative models, attention mechanisms, and neural architecture search that
offer promising solutions. By mapping current trends and innovations, this
review provides a comprehensive overview of the field and offers practical
insights to guide future research and development in multimodal modeling for
medical applications.

</details>


### [347] [Personalized Federated Learning under Model Dissimilarity Constraints](https://arxiv.org/pdf/2505.07575)
*Samuel Erickson, Mikael Johansson*

Main category: cs.LG

TL;DR: KARULA is a regularized strategy for personalized federated learning that addresses statistical heterogeneity by constraining model dissimilarities between clients based on distribution differences.


<details>
  <summary>Details</summary>
Motivation: Statistical heterogeneity among clients is a major challenge in federated learning, which KARULA aims to solve by adapting to complex interrelations between clients.

Method: KARULA uses a surrogate for the 1-Wasserstein distance to constrain pairwise model dissimilarities and employs an inexact projected stochastic gradient algorithm for optimization.

Result: The algorithm converges to a neighborhood of a stationary point with rate O(1/K) for smooth, possibly non-convex losses.

Conclusion: KARULA demonstrates effectiveness on synthetic and real federated datasets, outperforming clustered approaches.

Abstract: One of the defining challenges in federated learning is that of statistical
heterogeneity among clients. We address this problem with KARULA, a regularized
strategy for personalized federated learning, which constrains the pairwise
model dissimilarities between clients based on the difference in their
distributions, as measured by a surrogate for the 1-Wasserstein distance
adapted for the federated setting. This allows the strategy to adapt to highly
complex interrelations between clients, that e.g., clustered approaches fail to
capture. We propose an inexact projected stochastic gradient algorithm to solve
the constrained problem that the strategy defines, and show theoretically that
it converges with smooth, possibly non-convex losses to a neighborhood of a
stationary point with rate O(1/K). We demonstrate the effectiveness of KARULA
on synthetic and real federated data sets.

</details>


### [348] [Generating time-consistent dynamics with discriminator-guided image diffusion models](https://arxiv.org/pdf/2505.09089)
*Philipp Hess, Maximilian Gelbrecht, Christof Schötz, Michael Aich, Yu Huang, Shangshang Yang, Niklas Boers*

Main category: cs.LG

TL;DR: A time-consistency discriminator is proposed to adapt pretrained image diffusion models for realistic spatiotemporal dynamics, matching VDMs in performance with added benefits like improved uncertainty calibration.


<details>
  <summary>Details</summary>
Motivation: Realistic temporal dynamics are essential for applications like weather prediction and climate simulations, but training VDMs from scratch is resource-intensive.

Method: A time-consistency discriminator is introduced to guide sampling in pretrained image diffusion models without modifying or finetuning them.

Result: The method matches VDMs in temporal consistency, improves uncertainty calibration, reduces biases, and enables stable long-term climate simulations.

Conclusion: The proposed discriminator efficiently leverages pretrained models for spatiotemporal tasks, offering a practical alternative to VDMs.

Abstract: Realistic temporal dynamics are crucial for many video generation, processing
and modelling applications, e.g. in computational fluid dynamics, weather
prediction, or long-term climate simulations. Video diffusion models (VDMs) are
the current state-of-the-art method for generating highly realistic dynamics.
However, training VDMs from scratch can be challenging and requires large
computational resources, limiting their wider application. Here, we propose a
time-consistency discriminator that enables pretrained image diffusion models
to generate realistic spatiotemporal dynamics. The discriminator guides the
sampling inference process and does not require extensions or finetuning of the
image diffusion model. We compare our approach against a VDM trained from
scratch on an idealized turbulence simulation and a real-world global
precipitation dataset. Our approach performs equally well in terms of temporal
consistency, shows improved uncertainty calibration and lower biases compared
to the VDM, and achieves stable centennial-scale climate simulations at daily
time steps.

</details>


### [349] [SafePath: Conformal Prediction for Safe LLM-Based Autonomous Navigation](https://arxiv.org/pdf/2505.09427)
*Achref Doula, Max Mühlhäuser, Alejandro Sanchez Guinea*

Main category: cs.LG

TL;DR: SafePath enhances LLM-based path planning in autonomous driving with formal safety guarantees using conformal prediction, reducing uncertainty and collision rates.


<details>
  <summary>Details</summary>
Motivation: Addressing safety concerns like overconfidence and hallucinations in LLMs for autonomous driving path planning.

Method: A three-stage modular framework: LLM generates diverse paths, filters high-risk ones with conformal prediction, and selects the safest path or delegates to humans.

Result: SafePath reduces planning uncertainty by 77% and collision rates by up to 70%.

Conclusion: SafePath effectively balances autonomy and safety in LLM-driven path planning.

Abstract: Large Language Models (LLMs) show growing promise in autonomous driving by
reasoning over complex traffic scenarios to generate path plans. However, their
tendencies toward overconfidence, and hallucinations raise critical safety
concerns. We introduce SafePath, a modular framework that augments LLM-based
path planning with formal safety guarantees using conformal prediction.
SafePath operates in three stages. In the first stage, we use an LLM that
generates a set of diverse candidate paths, exploring possible trajectories
based on agent behaviors and environmental cues. In the second stage, SafePath
filters out high-risk trajectories while guaranteeing that at least one safe
option is included with a user-defined probability, through a multiple-choice
question-answering formulation that integrates conformal prediction. In the
final stage, our approach selects the path with the lowest expected collision
risk when uncertainty is low or delegates control to a human when uncertainty
is high. We theoretically prove that SafePath guarantees a safe trajectory with
a user-defined probability, and we show how its human delegation rate can be
tuned to balance autonomy and safety. Extensive experiments on nuScenes and
Highway-env show that SafePath reduces planning uncertainty by 77\% and
collision rates by up to 70\%, demonstrating effectiveness in making LLM-driven
path planning more safer.

</details>


### [350] [Establishing Linear Surrogate Regret Bounds for Convex Smooth Losses via Convolutional Fenchel-Young Losses](https://arxiv.org/pdf/2505.09432)
*Yuzhou Cao, Han Bao, Lei Feng, Bo An*

Main category: cs.LG

TL;DR: The paper addresses the trade-off between smoothness and linear regret bounds in convex smooth surrogate losses, proposing a solution for discrete target losses using Fenchel-Young losses and infimal convolution.


<details>
  <summary>Details</summary>
Motivation: The study aims to resolve the perceived trade-off between smoothness and linear regret bounds in surrogate losses, which affects optimization and estimation efficiency.

Method: The authors construct a convex smooth surrogate loss using Fenchel-Young losses generated by convolutional negentropy, leveraging infimal convolution to maintain linear regret bounds.

Result: The proposed method achieves a smooth surrogate loss with linear regret bounds and provides a consistent estimator for class probability.

Conclusion: The work demonstrates how convex analysis can enhance optimization and statistical efficiency in risk minimization, overcoming prior limitations.

Abstract: Surrogate regret bounds, also known as excess risk bounds, bridge the gap
between the convergence rates of surrogate and target losses, with linear
bounds favorable for their lossless regret transfer. While convex smooth
surrogate losses are appealing in particular due to the efficient estimation
and optimization, the existence of a trade-off between the smoothness and
linear regret bound has been believed in the community. That being said, the
better optimization and estimation properties of convex smooth surrogate losses
may inevitably deteriorate after undergoing the regret transfer onto a target
loss. We overcome this dilemma for arbitrary discrete target losses by
constructing a convex smooth surrogate loss, which entails a linear surrogate
regret bound composed with a tailored prediction link. The construction is
based on Fenchel-Young losses generated by the convolutional negentropy, which
are equivalent to the infimal convolution of a generalized negentropy and the
target Bayes risk. Consequently, the infimal convolution enables us to derive a
smooth loss while maintaining the surrogate regret bound linear. We
additionally benefit from the infimal convolution to have a consistent
estimator of the underlying class probability. Our results are overall a novel
demonstration of how convex analysis penetrates into optimization and
statistical efficiency in risk minimization.

</details>


### [351] [Towards Fair In-Context Learning with Tabular Foundation Models](https://arxiv.org/pdf/2505.09503)
*Patrik Kenfack, Samira Ebrahimi Kahou, Ulrich Aïvodji*

Main category: cs.LG

TL;DR: The paper explores fairness in tabular in-context learning (ICL), comparing it to traditional methods and testing bias-mitigation strategies. Uncertainty-based demonstration selection improves fairness.


<details>
  <summary>Details</summary>
Motivation: To understand how biases manifest in tabular ICL and evaluate strategies to mitigate them, as biases in traditional ML are well-documented but unclear in ICL.

Method: Investigates fairness in tabular ICL using three preprocessing strategies: correlation removal, group-balanced demonstration selection, and uncertainty-based demonstration selection.

Result: Uncertainty-based demonstration selection consistently improves group fairness in ICL predictions.

Conclusion: Tabular ICL can achieve fairness with uncertainty-based demonstration selection, offering a viable alternative to traditional methods.

Abstract: Tabular foundational models have exhibited strong in-context learning (ICL)
capabilities on structured data, allowing them to make accurate predictions on
test sets without parameter updates, using training examples as context. This
emerging approach positions itself as a competitive alternative to traditional
gradient-boosted tree methods. However, while biases in conventional machine
learning models are well documented, it remains unclear how these biases
manifest in tabular ICL. The paper investigates the fairness implications of
tabular ICL and explores three preprocessing strategies--correlation removal,
group-balanced demonstration selection, and uncertainty-based demonstration
selection--to address bias. Comprehensive experiments indicate that
uncertainty-based demonstration selection consistently enhances group fairness
of in-context predictions. The source code for reproducing the results of this
work can be found at https://github.com/patrikken/Fair-TabICL.

</details>


<div id='cs.MA'></div>

# cs.MA [[Back]](#toc)

### [352] [Hamilton's Rule for Enabling Altruism in Multi-Agent Systems](https://arxiv.org/pdf/2505.09841)
*Brooks A. Butler, Magnus Egerstedt*

Main category: cs.MA

TL;DR: The paper applies Hamilton's rule to altruistic decision-making in multi-agent systems, using a graph-based model and collaborative control Lyapunov functions to improve collective efficiency.


<details>
  <summary>Details</summary>
Motivation: Inspired by biological altruism, the study aims to determine when agents should act altruistically to benefit neighbors, adapting Hamilton's rule for task productivity.

Method: A graph-based model formalizes altruistic decision-making, using collaborative control Lyapunov functions to ensure collective goal efficiency.

Result: Simulations on a multi-agent navigation task show improved coordination when agent importance levels guide altruistic decisions.

Conclusion: The framework successfully integrates Hamilton's rule into multi-agent systems, enhancing altruistic decision-making for collective efficiency.

Abstract: This paper explores the application of Hamilton's rule to altruistic
decision-making in multi-agent systems. Inspired by biological altruism, we
introduce a framework that evaluates when individual agents should incur costs
to benefit their neighbors. By adapting Hamilton's rule, we define agent
``fitness" in terms of task productivity rather than genetic survival. We
formalize altruistic decision-making through a graph-based model of multi-agent
interactions and propose a solution using collaborative control Lyapunov
functions. The approach ensures that altruistic behaviors contribute to the
collective goal-reaching efficiency of the system. We illustrate this framework
on a multi-agent way-point navigation problem, where we show through simulation
how agent importance levels influence altruistic decision-making, leading to
improved coordination in navigation tasks.

</details>


### [353] [Multi-Agent Path Finding For Large Agents Is Intractable](https://arxiv.org/pdf/2505.10387)
*Artem Agafonov, Konstantin Yakovlev*

Main category: cs.MA

TL;DR: The paper proves that MAPF with large agents is NP-hard, contrasting with Classical MAPF, which is solvable in polynomial time.


<details>
  <summary>Details</summary>
Motivation: Practical applications like robotics require accounting for agent sizes to ensure safe execution, but the computational complexity of such MAPF variants was unclear.

Method: The authors reduce the NP-complete 3SAT problem to MAPF with large agents, constructing a graph where 3SAT satisfiability corresponds to path-finding solvability.

Result: MAPF with large agents is shown to be NP-hard, implying no polynomial-time solution exists unless P=NP.

Conclusion: The study confirms the increased complexity of MAPF when agent sizes are considered, closing a gap in understanding its computational limits.

Abstract: The multi-agent path finding (MAPF) problem asks to find a set of paths on a
graph such that when synchronously following these paths the agents never
encounter a conflict. In the most widespread MAPF formulation, the so-called
Classical MAPF, the agents sizes are neglected and two types of conflicts are
considered: occupying the same vertex or using the same edge at the same time
step. Meanwhile in numerous practical applications, e.g. in robotics, taking
into account the agents' sizes is vital to ensure that the MAPF solutions can
be safely executed. Introducing large agents yields an additional type of
conflict arising when one agent follows an edge and its body overlaps with the
body of another agent that is actually not using this same edge (e.g. staying
still at some distinct vertex of the graph). Until now it was not clear how
harder the problem gets when such conflicts are to be considered while
planning. Specifically, it was known that Classical MAPF problem on an
undirected graph can be solved in polynomial time, however no complete
polynomial-time algorithm was presented to solve MAPF with large agents. In
this paper we, for the first time, establish that the latter problem is NP-hard
and, thus, if P!=NP no polynomial algorithm for it can, unfortunately, be
presented. Our proof is based on the prevalent in the field technique of
reducing the seminal 3SAT problem (which is known to be an NP-complete problem)
to the problem at hand. In particular, for an arbitrary 3SAT formula we
procedurally construct a dedicated graph with specific start and goal vertices
and show that the given 3SAT formula is satisfiable iff the corresponding path
finding instance has a solution.

</details>


### [354] [Centrally Coordinated Multi-Agent Reinforcement Learning for Power Grid Topology Control](https://arxiv.org/pdf/2502.08681)
*Barbera de Mol, Davide Barbieri, Jan Viebahn, Davide Grossi*

Main category: cs.MA

TL;DR: A centrally coordinated multi-agent (CCMA) architecture is proposed to address the high-dimensional action space challenge in power grid operation, outperforming baseline methods in efficiency and performance.


<details>
  <summary>Details</summary>
Motivation: The complexity of power grid operation due to renewable energy growth and the combinatorial action space challenge in L2RPN competitions necessitate innovative solutions like action space factorization.

Method: The CCMA architecture involves regional agents proposing actions and a coordinating agent selecting the final action, tested against L2RPN baselines.

Result: CCMA shows higher sample efficiency and better performance than baselines, indicating promise for real-world power grid applications.

Conclusion: The CCMA approach is effective for high-dimensional power grid challenges and has potential for broader real-world use.

Abstract: Power grid operation is becoming more complex due to the increase in
generation of renewable energy. The recent series of Learning To Run a Power
Network (L2RPN) competitions have encouraged the use of artificial agents to
assist human dispatchers in operating power grids. However, the combinatorial
nature of the action space poses a challenge to both conventional optimizers
and learned controllers. Action space factorization, which breaks down
decision-making into smaller sub-tasks, is one approach to tackle the curse of
dimensionality. In this study, we propose a centrally coordinated multi-agent
(CCMA) architecture for action space factorization. In this approach, regional
agents propose actions and subsequently a coordinating agent selects the final
action. We investigate several implementations of the CCMA architecture, and
benchmark in different experimental settings against various L2RPN baseline
approaches. The CCMA architecture exhibits higher sample efficiency and
superior final performance than the baseline approaches. The results suggest
high potential of the CCMA approach for further application in
higher-dimensional L2RPN as well as real-world power grid settings.

</details>


<div id='cs.MM'></div>

# cs.MM [[Back]](#toc)

### [355] [Mitigating Modality Bias in Multi-modal Entity Alignment from a Causal Perspective](https://arxiv.org/pdf/2504.19458)
*Taoyu Su, Jiawei Sheng, Duohe Ma, Xiaodong Li, Juwei Yue, Mengxiao Song, Yingkai Tang, Tingwen Liu*

Main category: cs.MM

TL;DR: CDMEA is a counterfactual debiasing framework for Multi-Modal Entity Alignment (MMEA) that reduces visual modality bias by leveraging causal effects, outperforming 14 state-of-the-art methods.


<details>
  <summary>Details</summary>
Motivation: Existing MMEA methods overly rely on visual features, which can bias the model and degrade performance for entities with low-similarity images.

Method: CDMEA estimates the Total Effect (TE) of visual and graph modalities, excludes the Natural Direct Effect (NDE) of visual modality, and focuses on the Total Indirect Effect (TIE) to reduce bias.

Result: CDMEA outperforms 14 state-of-the-art methods across 9 benchmark datasets, particularly in low-similarity, high-noise, and low-resource scenarios.

Conclusion: The proposed CDMEA framework effectively mitigates visual modality bias in MMEA, improving alignment performance by leveraging causal analysis.

Abstract: Multi-Modal Entity Alignment (MMEA) aims to retrieve equivalent entities from
different Multi-Modal Knowledge Graphs (MMKGs), a critical information
retrieval task. Existing studies have explored various fusion paradigms and
consistency constraints to improve the alignment of equivalent entities, while
overlooking that the visual modality may not always contribute positively.
Empirically, entities with low-similarity images usually generate
unsatisfactory performance, highlighting the limitation of overly relying on
visual features. We believe the model can be biased toward the visual modality,
leading to a shortcut image-matching task. To address this, we propose a
counterfactual debiasing framework for MMEA, termed CDMEA, which investigates
visual modality bias from a causal perspective. Our approach aims to leverage
both visual and graph modalities to enhance MMEA while suppressing the direct
causal effect of the visual modality on model predictions. By estimating the
Total Effect (TE) of both modalities and excluding the Natural Direct Effect
(NDE) of the visual modality, we ensure that the model predicts based on the
Total Indirect Effect (TIE), effectively utilizing both modalities and reducing
visual modality bias. Extensive experiments on 9 benchmark datasets show that
CDMEA outperforms 14 state-of-the-art methods, especially in low-similarity,
high-noise, and low-resource data scenarios.

</details>


<div id='eess.AS'></div>

# eess.AS [[Back]](#toc)

### [356] [Who Said What WSW 2.0? Enhanced Automated Analysis of Preschool Classroom Speech](https://arxiv.org/pdf/2505.09972)
*Anchen Sun, Tiantian Feng, Gabriela Gutierrez, Juan J Londono, Anfeng Xu, Batya Elbaum, Shrikanth Narayanan, Lynn K Perry, Daniel S Messinger*

Main category: eess.AS

TL;DR: WSW2.0 is an automated framework for analyzing preschool classroom vocal interactions, combining wav2vec2 and Whisper for speaker classification and transcription, achieving high accuracy and scalability.


<details>
  <summary>Details</summary>
Motivation: To enhance accuracy and scalability in analyzing vocal interactions in preschool classrooms using deep learning and NLP techniques.

Method: Integrates wav2vec2 for speaker classification and Whisper for speech transcription, validated against expert human annotations.

Result: Achieves high F1 score (.845) and accuracy (.846) for speaker classification, with moderate to high transcription quality (WER .119-.238). Demonstrates scalability on a 1,592-hour dataset.

Conclusion: WSW2.0 shows promise for revolutionizing educational research by providing accurate measures of classroom speech, aiding intervention strategies and language development.

Abstract: This paper introduces an automated framework WSW2.0 for analyzing vocal
interactions in preschool classrooms, enhancing both accuracy and scalability
through the integration of wav2vec2-based speaker classification and Whisper
(large-v2 and large-v3) speech transcription. A total of 235 minutes of audio
recordings (160 minutes from 12 children and 75 minutes from 5 teachers), were
used to compare system outputs to expert human annotations. WSW2.0 achieves a
weighted F1 score of .845, accuracy of .846, and an error-corrected kappa of
.672 for speaker classification (child vs. teacher). Transcription quality is
moderate to high with word error rates of .119 for teachers and .238 for
children. WSW2.0 exhibits relatively high absolute agreement intraclass
correlations (ICC) with expert transcriptions for a range of classroom language
features. These include teacher and child mean utterance length, lexical
diversity, question asking, and responses to questions and other utterances,
which show absolute agreement intraclass correlations between .64 and .98. To
establish scalability, we apply the framework to an extensive dataset spanning
two years and over 1,592 hours of classroom audio recordings, demonstrating the
framework's robustness for broad real-world applications. These findings
highlight the potential of deep learning and natural language processing
techniques to revolutionize educational research by providing accurate measures
of key features of preschool classroom speech, ultimately guiding more
effective intervention strategies and supporting early childhood language
development.

</details>


### [357] [Spatially Selective Active Noise Control for Open-fitting Hearables with Acausal Optimization](https://arxiv.org/pdf/2505.10372)
*Tong Xiao, Simon Doclo*

Main category: eess.AS

TL;DR: An improved spatially selective active noise control method using acausal relative impulse responses outperforms causal designs in hearables.


<details>
  <summary>Details</summary>
Motivation: Enhance spatial selectivity in hearables by improving noise suppression while preserving desired sounds from specific directions.

Method: Incorporates acausal relative impulse responses into optimization, evaluated via simulations with open-fitting hearables in an anechoic environment.

Result: Acausal optimization consistently outperforms causal methods in speech distortion, noise reduction, and SNR improvement.

Conclusion: Acausal filters better characterize desired source responses, offering superior performance in spatially selective noise control.

Abstract: Recent advances in active noise control have enabled the development of
hearables with spatial selectivity, which actively suppress undesired noise
while preserving desired sound from specific directions. In this work, we
propose an improved approach to spatially selective active noise control that
incorporates acausal relative impulse responses into the optimization process,
resulting in significantly improved performance over the causal design. We
evaluate the system through simulations using a pair of open-fitting hearables
with spatially localized speech and noise sources in an anechoic environment.
Performance is evaluated in terms of speech distortion, noise reduction, and
signal-to-noise ratio improvement across different delays and degrees of
acausality. Results show that the proposed acausal optimization consistently
outperforms the causal approach across all metrics and scenarios, as acausal
filters more effectively characterize the response of the desired source.

</details>


### [358] [Quantized Approximate Signal Processing (QASP): Towards Homomorphic Encryption for audio](https://arxiv.org/pdf/2505.10500)
*Tu Duyen Nguyen, Adrien Lesage, Clotilde Cantini, Rachid Riad*

Main category: eess.AS

TL;DR: The paper introduces a secure pipeline using FHE for private audio processing, addressing challenges in time-frequency representations and demonstrating improved performance with approximations.


<details>
  <summary>Details</summary>
Motivation: Privacy concerns in audio data collection for machine learning applications necessitate secure methods like FHE to enable computations on encrypted data.

Method: A fully secure pipeline using FHE and quantized neural networks to compute time-frequency representations (STFT, Mel filterbanks, MFCCs, gammatone filters) and support private CNN classifiers.

Result: Experiments on VocalSet and OxVoc datasets showed improved performance with STFT approximations, reducing error rates and enabling fully private audio classification.

Conclusion: The proposed methods offer practical, secure solutions for audio processing, with heuristics for parameter selection to aid researchers and practitioners.

Abstract: Audio and speech data are increasingly used in machine learning applications
such as speech recognition, speaker identification, and mental health
monitoring. However, the passive collection of this data by audio listening
devices raises significant privacy concerns. Fully homomorphic encryption (FHE)
offers a promising solution by enabling computations on encrypted data and
preserving user privacy. Despite its potential, prior attempts to apply FHE to
audio processing have faced challenges, particularly in securely computing time
frequency representations, a critical step in many audio tasks.
  Here, we addressed this gap by introducing a fully secure pipeline that
computes, with FHE and quantized neural network operations, four fundamental
time-frequency representations: Short-Time Fourier Transform (STFT), Mel
filterbanks, Mel-frequency cepstral coefficients (MFCCs), and gammatone
filters. Our methods also support the private computation of audio descriptors
and convolutional neural network (CNN) classifiers. Besides, we proposed
approximate STFT algorithms that lighten computation and bit use for
statistical and machine learning analyses.
  We ran experiments on the VocalSet and OxVoc datasets demonstrating the fully
private computation of our approach. We showed significant performance
improvements with STFT approximation in private statistical analysis of audio
markers, and for vocal exercise classification with CNNs. Our results reveal
that our approximations substantially reduce error rates compared to
conventional STFT implementations in FHE. We also demonstrated a fully private
classification based on the raw audio for gender and vocal exercise
classification. Finally, we provided a practical heuristic for parameter
selection, making quantized approximate signal processing accessible to
researchers and practitioners aiming to protect sensitive audio data.

</details>


### [359] [Acoustic Disturbance Sensing Level Detection for ASD Diagnosis and Intelligibility Enhancement](https://arxiv.org/pdf/2401.11832)
*Marcelo Pillonetto, Anderson Queiroz, Rosângela Coelho*

Main category: eess.AS

TL;DR: The paper examines the acoustic sensitivity of ASD individuals, proposes a diagnostic aid based on disturbance sensing levels, and introduces an intelligibility enhancement scheme using harmonic features and auditory filterbanks.


<details>
  <summary>Details</summary>
Motivation: To address the impact of high internal noise (HIN) on ASD individuals' intelligibility in noisy environments and improve their acoustic perception.

Method: Perceptual listening tests to assess disturbance sensing levels and a novel intelligibility enhancement scheme using harmonic features and auditory filterbanks with gain factors.

Result: The proposed method improved acoustic intelligibility for both ASD and neurotypical individuals across various noise conditions and signal-to-noise ratios.

Conclusion: The study offers a potential diagnostic tool for ASD and an effective intelligibility enhancement solution for noisy environments.

Abstract: The acoustic sensitivity of Autism Spectrum Disorder (ASD) individuals highly
impacts their intelligibility in noisy urban environments. In this Letter, the
disturbance sensing level is examined with perceptual listening tests that
demonstrate the impact of their append High Internal Noise (HIN) profile on
intelligibility. This particular sensing level is then proposed as additional
aid to ASD diagnosis. In this Letter, a novel intelligibility enhancement
scheme is also introduced for ASD particular circumstances. For this proposal,
harmonic features estimated from speech signal frames are considered as center
frequencies of auditory filterbanks. A gain factor is further applied to the
output of the filtered samples. The experimental results demonstrate that the
proposal improved the acoustic intelligibility of ASD and Neurotypicals (NT)
people considering four acoustic noises at different signal-to-noise ratios.

</details>


### [360] [In-Materia Speech Recognition](https://arxiv.org/pdf/2410.10434)
*Mohamadreza Zolfagharinejad, Julian Büchel, Lorenzo Cassola, Sachin Kinge, Ghazi Sarwat Syed, Abu Sebastian, Wilfred G. van der Wiel*

Main category: eess.AS

TL;DR: A novel edge temporal-signal processor using in-materia computing achieves 96.2% accuracy for speech recognition, combining a dopant-network-processing-unit for feature extraction and an analogue in-memory computing chip for classification.


<details>
  <summary>Details</summary>
Motivation: The need for efficient, secure, and low-power processing of time-dependent signals at the edge due to limitations of centralized computing and modern processors.

Method: Two in-materia computing systems: a DNPU for analogue feature extraction and an AIMC chip for classification.

Result: Achieved 96.2% accuracy for TI-46-Word speech recognition with ultra-low power consumption (100s nW for DNPU, <10 fJ per operation for AIMC).

Conclusion: The proposed system offers a compact, efficient, and high-performance solution for smart edge processors.

Abstract: With the rise of decentralized computing, as in the Internet of Things,
autonomous driving, and personalized healthcare, it is increasingly important
to process time-dependent signals at the edge efficiently: right at the place
where the temporal data are collected, avoiding time-consuming, insecure, and
costly communication with a centralized computing facility (or cloud). However,
modern-day processors often cannot meet the restrained power and time budgets
of edge systems because of intrinsic limitations imposed by their architecture
(von Neumann bottleneck) or domain conversions (analogue-to-digital and
time-to-frequency). Here, we propose an edge temporal-signal processor based on
two in-materia computing systems for both feature extraction and
classification, reaching a software-level accuracy of 96.2% for the TI-46-Word
speech-recognition task. First, a nonlinear, room-temperature
dopant-network-processing-unit (DNPU) layer realizes analogue, time-domain
feature extraction from the raw audio signals, similar to the human cochlea.
Second, an analogue in-memory computing (AIMC) chip, consisting of memristive
crossbar arrays, implements a compact neural network trained on the extracted
features for classification. With the DNPU feature extraction consuming 100s nW
and AIMC-based classification having the potential for less than 10 fJ per
multiply-accumulate operation, our findings offer a promising avenue for
advancing the compactness, efficiency, and performance of heterogeneous smart
edge processors through in-materia computing hardware.

</details>


### [361] [FlexSpeech: Towards Stable, Controllable and Expressive Text-to-Speech](https://arxiv.org/pdf/2505.05159)
*Linhan Ma, Dake Guo, He Wang, Jin Xu, Lei Xie*

Main category: eess.AS

TL;DR: FlexSpeech combines autoregressive (AR) and non-autoregressive (NAR) methods for stable, controllable, and expressive speech generation, achieving SOTA results in zero-shot TTS and lightweight style transfer.


<details>
  <summary>Details</summary>
Motivation: To address the trade-off between stability (NAR) and naturalness (AR) in speech generation by integrating Markov dependencies and preference optimization into duration prediction.

Method: Decomposes speech generation into an AR duration predictor and a NAR acoustic model, optimizing the former for style transfer while keeping the latter stable.

Result: Achieves state-of-the-art stability and naturalness in zero-shot TTS, with rapid style transfer using only ~100 samples.

Conclusion: FlexSpeech successfully balances stability and naturalness, enabling efficient style adaptation without retraining the acoustic model.

Abstract: Current speech generation research can be categorized into two primary
classes: non-autoregressive and autoregressive. The fundamental distinction
between these approaches lies in the duration prediction strategy employed for
predictable-length sequences. The NAR methods ensure stability in speech
generation by explicitly and independently modeling the duration of each
phonetic unit. Conversely, AR methods employ an autoregressive paradigm to
predict the compressed speech token by implicitly modeling duration with Markov
properties. Although this approach improves prosody, it does not provide the
structural guarantees necessary for stability. To simultaneously address the
issues of stability and naturalness in speech generation, we propose
FlexSpeech, a stable, controllable, and expressive TTS model. The motivation
behind FlexSpeech is to incorporate Markov dependencies and preference
optimization directly on the duration predictor to boost its naturalness while
maintaining explicit modeling of the phonetic units to ensure stability.
Specifically, we decompose the speech generation task into two components: an
AR duration predictor and a NAR acoustic model. The acoustic model is trained
on a substantial amount of data to learn to render audio more stably, given
reference audio prosody and phone durations. The duration predictor is
optimized in a lightweight manner for different stylistic variations, thereby
enabling rapid style transfer while maintaining a decoupled relationship with
the specified speaker timbre. Experimental results demonstrate that our
approach achieves SOTA stability and naturalness in zero-shot TTS. More
importantly, when transferring to a specific stylistic domain, we can
accomplish lightweight optimization of the duration module solely with about
100 data samples, without the need to adjust the acoustic model, thereby
enabling rapid and stable style transfer.

</details>


<div id='eess.IV'></div>

# eess.IV [[Back]](#toc)

### [362] [ImplicitStainer: Data-Efficient Medical Image Translation for Virtual Antibody-based Tissue Staining Using Local Implicit Functions](https://arxiv.org/pdf/2505.09831)
*Tushar Kataria, Beatrice Knudsen, Shireen Y. Elhabian*

Main category: eess.IV

TL;DR: ImplicitStainer improves virtual staining of H&E images to IHC using local implicit functions, reducing data needs and outperforming GAN/diffusion models.


<details>
  <summary>Details</summary>
Motivation: H&E lacks molecular detail; IHC is slow and limited. Virtual staining via deep learning offers a faster alternative.

Method: Uses local implicit functions for pixel-level predictions, enhancing robustness with limited data.

Result: Outperforms 15+ state-of-the-art models, validated on two datasets with comprehensive metrics.

Conclusion: ImplicitStainer is efficient for virtual staining, even with small datasets, and will be publicly released.

Abstract: Hematoxylin and eosin (H&E) staining is a gold standard for microscopic
diagnosis in pathology. However, H&E staining does not capture all the
diagnostic information that may be needed. To obtain additional molecular
information, immunohistochemical (IHC) stains highlight proteins that mark
specific cell types, such as CD3 for T-cells or CK8/18 for epithelial cells.
While IHC stains are vital for prognosis and treatment guidance, they are
typically only available at specialized centers and time consuming to acquire,
leading to treatment delays for patients. Virtual staining, enabled by deep
learning-based image translation models, provides a promising alternative by
computationally generating IHC stains from H&E stained images. Although many
GAN and diffusion based image to image (I2I) translation methods have been used
for virtual staining, these models treat image patches as independent data
points, which results in increased and more diverse data requirements for
effective generation. We present ImplicitStainer, a novel approach that
leverages local implicit functions to improve image translation, specifically
virtual staining performance, by focusing on pixel-level predictions. This
method enhances robustness to variations in dataset sizes, delivering
high-quality results even with limited data. We validate our approach on two
datasets using a comprehensive set of metrics and benchmark it against over
fifteen state-of-the-art GAN- and diffusion based models. Full Code and models
trained will be released publicly via Github upon acceptance.

</details>


### [363] [Ordered-subsets Multi-diffusion Model for Sparse-view CT Reconstruction](https://arxiv.org/pdf/2505.09985)
*Pengfei Yu, Bin Huang, Minghui Zhang, Weiwen Wu, Shaoyu Wang, Qiegen Liu*

Main category: eess.IV

TL;DR: The paper introduces the ordered-subsets multi-diffusion model (OSMM) for sparse-view CT reconstruction, improving detail retention and noise resilience by dividing data into subsets and using a global constraint.


<details>
  <summary>Details</summary>
Motivation: Existing score-based diffusion models struggle with large, redundant projection data, leading to poor detail reconstruction and high learning difficulty.

Method: OSMM divides CT projection data into subsets, applies multi-subsets diffusion model (MSDM) for targeted learning, and integrates a one-whole diffusion model (OWDM) for global constraints.

Result: OSMM outperforms traditional models in image quality and noise resilience, adapting well to varying sparsity levels.

Conclusion: OSMM offers a robust, versatile solution for sparse-view CT reconstruction, enhancing detail and consistency.

Abstract: Score-based diffusion models have shown significant promise in the field of
sparse-view CT reconstruction. However, the projection dataset is large and
riddled with redundancy. Consequently, applying the diffusion model to
unprocessed data results in lower learning effectiveness and higher learning
difficulty, frequently leading to reconstructed images that lack fine details.
To address these issues, we propose the ordered-subsets multi-diffusion model
(OSMM) for sparse-view CT reconstruction. The OSMM innovatively divides the CT
projection data into equal subsets and employs multi-subsets diffusion model
(MSDM) to learn from each subset independently. This targeted learning approach
reduces complexity and enhances the reconstruction of fine details.
Furthermore, the integration of one-whole diffusion model (OWDM) with complete
sinogram data acts as a global information constraint, which can reduce the
possibility of generating erroneous or inconsistent sinogram information.
Moreover, the OSMM's unsupervised learning framework provides strong robustness
and generalizability, adapting seamlessly to varying sparsity levels of CT
sinograms. This ensures consistent and reliable performance across different
clinical scenarios. Experimental results demonstrate that OSMM outperforms
traditional diffusion models in terms of image quality and noise resilience,
offering a powerful and versatile solution for advanced CT imaging in
sparse-view scenarios.

</details>


### [364] [Whitened Score Diffusion: A Structured Prior for Imaging Inverse Problems](https://arxiv.org/pdf/2505.10311)
*Jeffrey Alido, Tongyu Li, Yu Sun, Lei Tian*

Main category: eess.IV

TL;DR: Whitened Score (WS) diffusion models avoid covariance inversion in anisotropic Gaussian diffusion, improving stability and performance over conventional methods.


<details>
  <summary>Details</summary>
Motivation: Address limitations of conventional score-based diffusion models with anisotropic Gaussian diffusion processes.

Method: Propose WS diffusion models, learning Whitened Score function to bypass covariance inversion.

Result: WS models outperform conventional methods on tasks like CIFAR and CelebA, especially with anisotropic noise.

Conclusion: WS diffusion models offer a robust alternative for Gaussian noising processes and imaging inverse problems.

Abstract: Conventional score-based diffusion models (DMs) may struggle with anisotropic
Gaussian diffusion processes due to the required inversion of covariance
matrices in the denoising score matching training objective
\cite{vincent_connection_2011}. We propose Whitened Score (WS) diffusion
models, a novel SDE-based framework that learns the Whitened Score function
instead of the standard score. This approach circumvents covariance inversion,
extending score-based DMs by enabling stable training of DMs on arbitrary
Gaussian forward noising processes. WS DMs establish equivalence with FM for
arbitrary Gaussian noise, allow for tailored spectral inductive biases, and
provide strong Bayesian priors for imaging inverse problems with structured
noise. We experiment with a variety of computational imaging tasks using the
CIFAR and CelebA ($64\times64$) datasets and demonstrate that WS diffusion
priors trained on anisotropic Gaussian noising processes consistently
outperform conventional diffusion priors based on isotropic Gaussian noise.

</details>


### [365] [Visual Fidelity Index for Generative Semantic Communications with Critical Information Embedding](https://arxiv.org/pdf/2505.10405)
*Jianhao Huang, Qunsong Zeng, Kaibin Huang*

Main category: eess.IV

TL;DR: The paper introduces a hybrid Gen-SemCom system with a CIE framework to enhance image reconstruction by combining text prompts and critical features, along with a new GVIF metric for evaluation.


<details>
  <summary>Details</summary>
Motivation: To address the loss of fine-grained visual details in purely prompt-driven Gen-SemCom systems and the lack of systematic evaluation metrics.

Method: Proposes semantic filtering for critical feature extraction, integrates text prompts and features for reconstruction using a diffusion model, and introduces the GVIF metric for quality assessment.

Result: The GVIF metric correlates with visual fidelity, and the optimized system outperforms benchmarks in PSNR and FID scores.

Conclusion: The hybrid system and GVIF metric effectively improve visual quality and adaptability in Gen-SemCom for 6G networks.

Abstract: Generative semantic communication (Gen-SemCom) with large artificial
intelligence (AI) model promises a transformative paradigm for 6G networks,
which reduces communication costs by transmitting low-dimensional prompts
rather than raw data. However, purely prompt-driven generation loses
fine-grained visual details. Additionally, there is a lack of systematic
metrics to evaluate the performance of Gen-SemCom systems. To address these
issues, we develop a hybrid Gen-SemCom system with a critical information
embedding (CIE) framework, where both text prompts and semantically critical
features are extracted for transmissions. First, a novel approach of semantic
filtering is proposed to select and transmit the semantically critical features
of images relevant to semantic label. By integrating the text prompt and
critical features, the receiver reconstructs high-fidelity images using a
diffusion-based generative model. Next, we propose the generative visual
information fidelity (GVIF) metric to evaluate the visual quality of the
generated image. By characterizing the statistical models of image features,
the GVIF metric quantifies the mutual information between the distorted
features and their original counterparts. By maximizing the GVIF metric, we
design a channel-adaptive Gen-SemCom system that adaptively control the volume
of features and compression rate according to the channel state. Experimental
results validate the GVIF metric's sensitivity to visual fidelity, correlating
with both the PSNR and critical information volume. In addition, the optimized
system achieves superior performance over benchmarking schemes in terms of
higher PSNR and lower FID scores.

</details>


### [366] [HWA-UNETR: Hierarchical Window Aggregate UNETR for 3D Multimodal Gastric Lesion Segmentation](https://arxiv.org/pdf/2505.10464)
*Jiaming Liang, Lihuan Dai, Xiaoqi Sheng, Xiangguang Chen, Chun Yao, Guihua Tao, Qibin Leng, Honming Cai, Xi Zhong*

Main category: eess.IV

TL;DR: The paper introduces a new dataset (GCM 2025) and a 3D segmentation framework (HWA-UNETR) for gastric cancer lesion analysis, addressing challenges of multimodal data scarcity and misalignment.


<details>
  <summary>Details</summary>
Motivation: Challenges in gastric cancer lesion analysis due to scarce multimodal datasets and misaligned modalities, leading to resource-heavy training and reduced accuracy.

Method: Proposes HWA-UNETR, a 3D segmentation framework with HWA blocks for dynamic feature alignment and a tri-orientated fusion mamba mechanism for context modeling.

Result: Outperforms existing methods by up to 1.68% in Dice score, validated on GCM 2025 and BraTS 2021 datasets.

Conclusion: The released dataset and framework improve accuracy and robustness in multimodal medical image segmentation for gastric cancer.

Abstract: Multimodal medical image segmentation faces significant challenges in the
context of gastric cancer lesion analysis. This clinical context is defined by
the scarcity of independent multimodal datasets and the imperative to
amalgamate inherently misaligned modalities. As a result, algorithms are
constrained to train on approximate data and depend on application migration,
leading to substantial resource expenditure and a potential decline in analysis
accuracy. To address those challenges, we have made two major contributions:
First, we publicly disseminate the GCM 2025 dataset, which serves as the first
large-scale, open-source collection of gastric cancer multimodal MRI scans,
featuring professionally annotated FS-T2W, CE-T1W, and ADC images from 500
patients. Second, we introduce HWA-UNETR, a novel 3D segmentation framework
that employs an original HWA block with learnable window aggregation layers to
establish dynamic feature correspondences between different modalities'
anatomical structures, and leverages the innovative tri-orientated fusion mamba
mechanism for context modeling and capturing long-range spatial dependencies.
Extensive experiments on our GCM 2025 dataset and the publicly BraTS 2021
dataset validate the performance of our framework, demonstrating that the new
approach surpasses existing methods by up to 1.68\% in the Dice score while
maintaining solid robustness. The dataset and code are public via
https://github.com/JeMing-creater/HWA-UNETR.

</details>


### [367] [Multi-contrast laser endoscopy for in vivo gastrointestinal imaging](https://arxiv.org/pdf/2505.10492)
*Taylor L. Bobrow, Mayank Golhar, Suchapa Arayakarnkul, Anthony A. Song, Saowanee Ngamruengphong, Nicholas J. Durr*

Main category: eess.IV

TL;DR: Multi-contrast Laser Endoscopy (MLE) improves gastrointestinal imaging by enhancing tissue contrast, quantifying blood flow, and characterizing mucosal topography, outperforming traditional methods.


<details>
  <summary>Details</summary>
Motivation: Current white light endoscopy often misses subtle tissue abnormalities due to low contrast. MLE aims to address this limitation.

Method: MLE uses tunable spectral, coherent, and directional illumination for multispectral diffuse reflectance, laser speckle contrast imaging, and photometric stereo.

Result: MLE shows a three-fold contrast improvement and five-fold color difference enhancement over white light and narrow band imaging in 31 polyps.

Conclusion: MLE is a promising tool for improving gastrointestinal imaging by revealing multiple complementary tissue contrasts.

Abstract: White light endoscopy is the clinical gold standard for detecting diseases in
the gastrointestinal tract. Most applications involve identifying visual
abnormalities in tissue color, texture, and shape. Unfortunately, the contrast
of these features is often subtle, causing many clinically relevant cases to go
undetected. To overcome this challenge, we introduce Multi-contrast Laser
Endoscopy (MLE): a platform for widefield clinical imaging with rapidly tunable
spectral, coherent, and directional illumination. We demonstrate three
capabilities of MLE: enhancing tissue chromophore contrast with multispectral
diffuse reflectance, quantifying blood flow using laser speckle contrast
imaging, and characterizing mucosal topography using photometric stereo. We
validate MLE with benchtop models, then demonstrate MLE in vivo during clinical
colonoscopies. MLE images from 31 polyps demonstrate an approximate three-fold
improvement in contrast and a five-fold improvement in color difference
compared to white light and narrow band imaging. With the ability to reveal
multiple complementary types of tissue contrast while seamlessly integrating
into the clinical environment, MLE shows promise as an investigative tool to
improve gastrointestinal imaging.

</details>


### [368] [WeGA: Weakly-Supervised Global-Local Affinity Learning Framework for Lymph Node Metastasis Prediction in Rectal Cancer](https://arxiv.org/pdf/2505.10502)
*Yifan Gao, Yaoxian Dong, Wenbin Wu, Chaoyang Ge, Feng Yuan, Jiaxi Sheng, Haoyue Li, Xin Gao*

Main category: eess.IV

TL;DR: WeGA is a weakly-supervised framework for lymph node metastasis prediction in rectal cancer, leveraging global-local affinity learning to improve accuracy over existing methods.


<details>
  <summary>Details</summary>
Motivation: Current MRI-based LNM assessment lacks accuracy, and automated systems struggle due to missing node-level annotations and isolated node analysis.

Method: WeGA uses a dual-branch architecture (DINOv2 for global context, residual encoder for local details), cross-attention fusion for scale alignment, and regional affinity loss for structural coherence.

Result: WeGA outperforms existing methods with AUCs of 0.750, 0.822, and 0.802 across three test centers.

Conclusion: WeGA enhances LNM prediction by modeling node relationships and context, improving diagnostic precision for rectal cancer.

Abstract: Accurate lymph node metastasis (LNM) assessment in rectal cancer is essential
for treatment planning, yet current MRI-based evaluation shows unsatisfactory
accuracy, leading to suboptimal clinical decisions. Developing automated
systems also faces significant obstacles, primarily the lack of node-level
annotations. Previous methods treat lymph nodes as isolated entities rather
than as an interconnected system, overlooking valuable spatial and contextual
information. To solve this problem, we present WeGA, a novel weakly-supervised
global-local affinity learning framework that addresses these challenges
through three key innovations: 1) a dual-branch architecture with DINOv2
backbone for global context and residual encoder for local node details; 2) a
global-local affinity extractor that aligns features across scales through
cross-attention fusion; and 3) a regional affinity loss that enforces
structural coherence between classification maps and anatomical regions.
Experiments across one internal and two external test centers demonstrate that
WeGA outperforms existing methods, achieving AUCs of 0.750, 0.822, and 0.802
respectively. By effectively modeling the relationships between individual
lymph nodes and their collective context, WeGA provides a more accurate and
generalizable approach for lymph node metastasis prediction, potentially
enhancing diagnostic precision and treatment selection for rectal cancer
patients.

</details>


### [369] [Cyclic 2.5D Perceptual Loss for Cross-Modal 3D Medical Image Synthesis: T1w MRI to Tau PET](https://arxiv.org/pdf/2406.12632)
*Junho Moon, Symac Kim, Haejun Chung, Ikbeom Jang*

Main category: eess.IV

TL;DR: The paper proposes a cyclic 2.5D perceptual loss for synthesizing 3D tau PET images from 3D T1-weighted MR images, addressing challenges in 3D medical image synthesis.


<details>
  <summary>Details</summary>
Motivation: The need arises from limited access to imaging devices, regulatory issues, and patient follow-up challenges, with a focus on preserving high-level semantic features over pixel-level accuracy.

Method: A cyclic 2.5D perceptual loss is introduced, computing 2D average perceptual loss for axial, coronal, and sagittal planes sequentially. Tau PET images are standardized to preserve high-SUVR regions.

Result: The method improves quantitative and qualitative performance across models like U-Net, UNETR, SwinUNETR, CycleGAN, and Pix2Pix.

Conclusion: The proposed cyclic 2.5D perceptual loss effectively enhances 3D medical image synthesis, balancing loss reduction and preserving critical features.

Abstract: There is a demand for medical image synthesis or translation to generate
synthetic images of missing modalities from available data. This need stems
from challenges such as restricted access to high-cost imaging devices,
government regulations, or failure to follow up with patients or study
participants. In medical imaging, preserving high-level semantic features is
often more critical than achieving pixel-level accuracy. Perceptual loss
functions are widely employed to train medical image synthesis or translation
models, as they quantify differences in high-level image features using a
pre-trained feature extraction network. While 3D and 2.5D perceptual losses are
used in 3D medical image synthesis, they face challenges, such as the lack of
pre-trained 3D models or difficulties in balancing loss reduction across
different planes. In this work, we focus on synthesizing 3D tau PET images from
3D T1-weighted MR images. We propose a cyclic 2.5D perceptual loss that
sequentially computes the 2D average perceptual loss for each of the axial,
coronal, and sagittal planes over epochs, with the cycle duration gradually
decreasing. Additionally, we process tau PET images using by-manufacturer
standardization to enhance the preservation of high-SUVR regions indicative of
tau pathology and mitigate SUVR variability caused by inter-manufacturer
differences. We combine the proposed loss with SSIM and MSE losses and
demonstrate its effectiveness in improving both quantitative and qualitative
performance across various generative models, including U-Net, UNETR,
SwinUNETR, CycleGAN, and Pix2Pix.

</details>


### [370] [Translating Electrocardiograms to Cardiac Magnetic Resonance Imaging Useful for Cardiac Assessment and Disease Screening: A Multi-Center Study AI for ECG to CMR Translation Study](https://arxiv.org/pdf/2411.13602)
*Zhengyao Ding, Ziyu Li, Yujian Hu, Youyao Xu, Chengchen Zhao, Yiheng Mao, Haitao Li, Zhikang Li, Qian Li, Jing Wang, Yue Chen, Mengjia Chen, Longbo Wang, Xuesen Chu, Weichao Pan, Ziyi Liu, Fei Wu, Hongkun Zhang, Ting Chen, Zhengxing Huang*

Main category: eess.IV

TL;DR: CardioNets, a deep learning framework, translates ECG signals into CMR-level insights, offering a cost-effective alternative for CVD screening.


<details>
  <summary>Details</summary>
Motivation: CVDs are a global health priority, but CMR is costly and ECG lacks detail. CardioNets bridges this gap.

Method: Uses cross-modal contrastive learning and generative pretraining to align ECG with CMR and synthesize images.

Result: Improved cardiac phenotype regression by 24.8%, disease detection AUC by up to 39.3%, and outperformed human physicians.

Conclusion: CardioNets is a scalable, low-cost solution for CVD screening, especially in resource-limited settings.

Abstract: Cardiovascular diseases (CVDs) are the leading cause of global mortality,
necessitating accessible and accurate diagnostic tools. While cardiac magnetic
resonance imaging (CMR) provides gold-standard insights into cardiac structure
and function, its clinical utility is limited by high cost and complexity. In
contrast, electrocardiography (ECG) is inexpensive and widely available but
lacks the granularity of CMR. We propose CardioNets, a deep learning framework
that translates 12-lead ECG signals into CMR-level functional parameters and
synthetic images, enabling scalable cardiac assessment. CardioNets integrates
cross-modal contrastive learning and generative pretraining, aligning ECG with
CMR-derived cardiac phenotypes and synthesizing high-resolution CMR images via
a masked autoregressive model. Trained on 159,819 samples from five cohorts,
including the UK Biobank (n=42,483) and MIMIC-IV-ECG (n=164,550), and
externally validated on independent clinical datasets (n=3,767), CardioNets
achieved strong performance across disease screening and phenotype estimation
tasks. In the UK Biobank, it improved cardiac phenotype regression R2 by 24.8%
and cardiomyopathy AUC by up to 39.3% over baseline models. In MIMIC, it
increased AUC for pulmonary hypertension detection by 5.6%. Generated CMR
images showed 36.6% higher SSIM and 8.7% higher PSNR than prior approaches. In
a reader study, ECG-only CardioNets achieved 13.9% higher accuracy than human
physicians using both ECG and real CMR. These results suggest that CardioNets
offers a promising, low-cost alternative to CMR for large-scale CVD screening,
particularly in resource-limited settings. Future efforts will focus on
clinical deployment and regulatory validation of ECG-based synthetic imaging.

</details>


### [371] [An unsupervised method for MRI recovery: Deep image prior with structured sparsity](https://arxiv.org/pdf/2501.01482)
*Muhammad Ahmad Sultan, Chong Chen, Yingmin Liu, Katarzyna Gil, Karolina Zareba, Rizwan Ahmad*

Main category: eess.IV

TL;DR: DISCUS, an unsupervised MRI reconstruction method, outperforms existing techniques without needing fully sampled k-space data.


<details>
  <summary>Details</summary>
Motivation: To address challenges in acquiring fully sampled k-space data by proposing an unsupervised reconstruction method.

Method: Extends deep image prior (DIP) with group sparsity for low-dimensional manifold discovery, validated via simulations and patient studies.

Result: DISCUS shows superior reconstruction quality in NMSE, SSIM, and expert scoring compared to compressed sensing and DIP-based methods.

Conclusion: DISCUS is a promising unsupervised method for MRI reconstruction, especially where fully sampled data is hard to obtain.

Abstract: Objective: To propose and validate an unsupervised MRI reconstruction method
that does not require fully sampled k-space data. Materials and Methods: The
proposed method, deep image prior with structured sparsity (DISCUS), extends
the deep image prior (DIP) by introducing group sparsity to frame-specific code
vectors, enabling the discovery of a low-dimensional manifold for capturing
temporal variations. \discus was validated using four studies: (I) simulation
of a dynamic Shepp-Logan phantom to demonstrate its manifold discovery
capabilities, (II) comparison with compressed sensing and DIP-based methods
using simulated single-shot late gadolinium enhancement (LGE) image series from
six distinct digital cardiac phantoms in terms of normalized mean square error
(NMSE) and structural similarity index measure (SSIM), (III) evaluation on
retrospectively undersampled single-shot LGE data from eight patients, and (IV)
evaluation on prospectively undersampled single-shot LGE data from eight
patients, assessed via blind scoring from two expert readers. Results: DISCUS
outperformed competing methods, demonstrating superior reconstruction quality
in terms of NMSE and SSIM (Studies I--III) and expert reader scoring (Study
IV). Discussion: An unsupervised image reconstruction method is presented and
validated on simulated and measured data. These developments can benefit
applications where acquiring fully sampled data is challenging.

</details>


### [372] [A Trust-Guided Approach to MR Image Reconstruction with Side Information](https://arxiv.org/pdf/2501.03021)
*Arda Atalık, Sumit Chopra, Daniel K. Sodickson*

Main category: eess.IV

TL;DR: TGVN is a deep learning framework for MRI reconstruction that integrates side information to improve image quality and speed up scans.


<details>
  <summary>Details</summary>
Motivation: Reducing MRI scan times benefits patient care and lowers costs, but sparse k-space data reconstruction is challenging. Incorporating prior knowledge, like side information, can address ambiguities.

Method: TGVN is an end-to-end deep learning framework that integrates auxiliary data (side information) into linear inverse problems for MRI reconstruction.

Result: TGVN outperforms baselines in image quality, preserves pathological features, and works robustly across contrasts, anatomies, and field strengths.

Conclusion: TGVN effectively speeds up MRI acquisition while minimizing artifacts, making it a reliable tool for diagnostic-quality image reconstruction.

Abstract: Reducing MRI scan times can improve patient care and lower healthcare costs.
Many acceleration methods are designed to reconstruct diagnostic-quality images
from sparse k-space data, via an ill-posed or ill-conditioned linear inverse
problem (LIP). To address the resulting ambiguities, it is crucial to
incorporate prior knowledge into the optimization problem, e.g., in the form of
regularization. Another form of prior knowledge less commonly used in medical
imaging is the readily available auxiliary data (a.k.a. side information)
obtained from sources other than the current acquisition. In this paper, we
present the Trust- Guided Variational Network (TGVN), an end-to-end deep
learning framework that effectively and reliably integrates side information
into LIPs. We demonstrate its effectiveness in multi-coil, multi-contrast MRI
reconstruction, where incomplete or low-SNR measurements from one contrast are
used as side information to reconstruct high-quality images of another contrast
from heavily under-sampled data. TGVN is robust across different contrasts,
anatomies, and field strengths. Compared to baselines utilizing side
information, TGVN achieves superior image quality while preserving subtle
pathological features even at challenging acceleration levels, drastically
speeding up acquisition while minimizing hallucinations. Source code and
dataset splits are available on github.com/sodicksonlab/TGVN.

</details>


### [373] [Benchmarking Self-Supervised Learning Methods for Accelerated MRI Reconstruction](https://arxiv.org/pdf/2502.14009)
*Andrew Wang, Steven McDonagh, Mike Davies*

Main category: eess.IV

TL;DR: SSIBench is a framework for benchmarking self-supervised MRI reconstruction methods without ground truth, evaluating 18 methods across 4 scenarios and proposing a novel loss for future improvements.


<details>
  <summary>Details</summary>
Motivation: The need for self-supervised MRI reconstruction methods due to the impracticality of obtaining fully-sampled ground truth images, and the lack of standardized comparisons hindering research and adoption.

Method: SSIBench, a modular framework, evaluates 18 self-supervised methods on real MRI data across 4 scenarios, introducing a novel loss called Multi-Operator Equivariant Imaging.

Result: Performance varies widely across methods, scenarios, and metrics, highlighting the need for further research and the potential for combining complementary methods.

Conclusion: SSIBench provides a standardized, open-source platform to accelerate reproducible research and adoption of self-supervised MRI reconstruction methods.

Abstract: Reconstructing MRI from highly undersampled measurements is crucial for
accelerating medical imaging, but is challenging due to the ill-posedness of
the inverse problem. While supervised deep learning (DL) approaches have shown
remarkable success, they traditionally rely on fully-sampled ground truth (GT)
images, which are expensive or impossible to obtain in real scenarios. This
problem has created a recent surge in interest in self-supervised learning
methods that do not require GT. Although recent methods are now fast
approaching "oracle" supervised performance, the lack of systematic comparison
and standard experimental setups are hindering targeted methodological research
and precluding widespread trustworthy industry adoption. We present SSIBench, a
modular and flexible comparison framework to unify and thoroughly benchmark
Self-Supervised Imaging methods (SSI) without GT. We evaluate 18 methods across
4 realistic MRI scenarios on real data, showing a wide performance landscape
whose method ranking differs across scenarios and metrics, exposing the need
for further SSI research. Our insights also show how complementary methods
could be compounded for future improvements, exemplified by a novel loss we
propose, Multi-Operator Equivariant Imaging. To accelerate reproducible
research and lower the barrier to entry, we provide the extensible benchmark
and open-source reimplementations of all methods at
https://andrewwango.github.io/ssibench, allowing researchers to rapidly and
fairly contribute and evaluate new methods on the standardised setup for
potential leaderboard ranking, or benchmark existing methods on custom
datasets, forward operators, or models, unlocking the application of SSI to
other valuable GT free domains such as 4D MRI and other nascent scientific
imaging modalities.

</details>


### [374] [A portable diagnosis model for Keratoconus using a smartphone](https://arxiv.org/pdf/2505.08616)
*Yifan Li, Peter Ho, Jo Woon Chong*

Main category: eess.IV

TL;DR: A smartphone-based method for diagnosing keratoconus (KC) using Placido disc images and a two-stage diagnostic approach achieves high accuracy in identifying and localizing KC.


<details>
  <summary>Details</summary>
Motivation: Traditional KC diagnostic tools are bulky, costly, and require professional operation, necessitating a portable and accessible alternative.

Method: The method involves capturing corneal reflections of a smartphone-generated Placido disc, followed by a two-stage diagnosis: k-means clustering for KC identification and distance matrix analysis for localization.

Result: The logistic regression model achieved 96.94% accuracy in classifying KC-affected areas, enabling precise localization of corneal protrusions.

Conclusion: The smartphone-based approach offers an effective, portable solution for KC diagnosis and treatment planning.

Abstract: Keratoconus (KC) is a corneal disorder that results in blurry and distorted
vision. Traditional diagnostic tools, while effective, are often bulky, costly,
and require professional operation. In this paper, we present a portable and
innovative methodology for diagnosing. Our proposed approach first captures the
image reflected on the eye's cornea when a smartphone screen-generated Placido
disc sheds its light on an eye, then utilizes a two-stage diagnosis for
identifying the KC cornea and pinpointing the location of the KC on the cornea.
The first stage estimates the height and width of the Placido disc extracted
from the captured image to identify whether it has KC. In this KC
identification, k-means clustering is implemented to discern statistical
characteristics, such as height and width values of extracted Placido discs,
from non-KC (control) and KC-affected groups. The second stage involves the
creation of a distance matrix, providing a precise localization of KC on the
cornea, which is critical for efficient treatment planning. The analysis of
these distance matrices, paired with a logistic regression model and robust
statistical analysis, reveals a clear distinction between control and KC
groups. The logistic regression model, which classifies small areas on the
cornea as either control or KC-affected based on the corresponding inter-disc
distances in the distance matrix, reported a classification accuracy of 96.94%,
which indicates that we can effectively pinpoint the protrusion caused by KC.
This comprehensive, smartphone-based method is expected to detect KC and
streamline timely treatment.

</details>


### [375] [BiECVC: Gated Diversification of Bidirectional Contexts for Learned Video Compression](https://arxiv.org/pdf/2505.09193)
*Wei Jiang, Junru Li, Kai Zhang, Li Zhang*

Main category: eess.IV

TL;DR: BiECVC, a learned bidirectional video compression framework, outperforms VTM 13.2 by enhancing local and non-local context modeling and introducing adaptive context gating.


<details>
  <summary>Details</summary>
Motivation: Existing bidirectional video compression (BVC) methods lag behind forward-only ones due to limited context extraction and adaptability issues.

Method: BiECVC improves local context by reusing high-quality features and aligning them with decoded motion vectors. It uses linear attention for non-local dependencies and introduces Bidirectional Context Gating to dynamically filter harmful contexts.

Result: BiECVC reduces bit-rate by 13.4% and 15.7% compared to VTM 13.2 under RA configuration, achieving state-of-the-art performance.

Conclusion: BiECVC is the first learned video codec to surpass VTM 13.2 RA across all test datasets, demonstrating the effectiveness of diversified context modeling and adaptive gating.

Abstract: Recent forward prediction-based learned video compression (LVC) methods have
achieved impressive results, even surpassing VVC reference software VTM under
the Low Delay B (LDB) configuration. In contrast, learned bidirectional video
compression (BVC) remains underexplored and still lags behind its forward-only
counterparts. This performance gap is mainly due to the limited ability to
extract diverse and accurate contexts: most existing BVCs primarily exploit
temporal motion while neglecting non-local correlations across frames.
Moreover, they lack the adaptability to dynamically suppress harmful contexts
arising from fast motion or occlusion. To tackle these challenges, we propose
BiECVC, a BVC framework that incorporates diversified local and non-local
context modeling along with adaptive context gating. For local context
enhancement, BiECVC reuses high-quality features from lower layers and aligns
them using decoded motion vectors without introducing extra motion overhead. To
model non-local dependencies efficiently, we adopt a linear attention mechanism
that balances performance and complexity. To further mitigate the impact of
inaccurate context prediction, we introduce Bidirectional Context Gating,
inspired by data-dependent decay in recent autoregressive language models, to
dynamically filter contextual information based on conditional coding results.
Extensive experiments demonstrate that BiECVC achieves state-of-the-art
performance, reducing the bit-rate by 13.4% and 15.7% compared to VTM 13.2
under the Random Access (RA) configuration with intra periods of 32 and 64,
respectively. To our knowledge, BiECVC is the first learned video codec to
surpass VTM 13.2 RA across all standard test datasets. Code will be available
at https://github.com/JiangWeibeta/ECVC.

</details>
